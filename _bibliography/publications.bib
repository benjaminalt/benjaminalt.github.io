@inproceedings{altOpenReproducibleTrustworthy2025,
  title = {Open, {{Reproducible}} and {{Trustworthy Robot-Based Experiments}} with {{Virtual Labs}} and {{Digital-Twin-Based Execution Tracing}}},
  booktitle = {1st {{IEEE IROS}} 2025 {{Workshop}} on {{Embodied AI}} and {{Robotics}} for {{Future Scientific Discovery}} ({{AIR4S}})},
  author = {Alt, Benjamin and Picklum, Mareike and Arion, Sorin and Kenfack, Franklin Kenghagho and Beetz, Michael},
  year = {2025},
  month = oct,
  eprint = {2508.11406},
  primaryclass = {cs},
  publisher = {arXiv},
  address = {Hangzhou, China},
  doi = {10.48550/arXiv.2508.11406},
  urldate = {2025-10-24},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics,my},
  pdf = {vrb_scientific_discovery_IROS25_AIR4S.pdf},
  arxiv = {2508.11406},
  abstract = {We envision a future in which autonomous robots conduct scientific experiments in ways that are not only precise and repeatable, but also open, trustworthy, and transparent. To realize this vision, we present two key contributions: a semantic execution tracing framework that logs sensor data together with semantically annotated robot belief states, ensuring that automated experimentation is transparent and replicable; and the AICOR Virtual Research Building (VRB), a cloud-based platform for sharing, replicating, and validating robot task executions at scale. Together, these tools enable reproducible, robot-driven science by integrating deterministic execution, semantic memory, and open knowledge representation, laying the foundation for autonomous systems to participate in scientific discovery.}
}

@article{schusslerSemiAutonomousRoboticAssistance2025,
  title = {Semi-{{Autonomous Robotic Assistance}} for {{Gallbladder Retraction}} in {{Surgery}}},
  author = {Sch{\"u}{\ss}ler, Alexander and Kunz, Christian and Younis, Rayan and Alt, Benjamin and Paik, Jamie and Wagner, Martin and {Mathis-Ullrich}, Franziska},
  year = {2025},
  journal = {IEEE Robotics and Automation Letters},
  pages = {1--8},
  issn = {2377-3766},
  doi = {10.1109/LRA.2025.3577430},
  urldate = {2025-06-08},
  keywords = {Feature extraction,Gallbladder,Imitation learning,informed machine learning,learning from demonstration,Liver,minimally invasive surgery,my,Phantoms,Point cloud compression,Robots,Surgery,surgical robotics,Three-dimensional displays,Trajectory,Vectors},
  pdf = {RAL_DKMP.pdf},
  abstract = { (Semi-)autonomous robotic assistance in minimally invasive surgery has the potential to alleviate surgical staff shortage and decrease the workload of medical professionals. These robots must execute complex tasks within unpredictable and unstructured environments encountered during surgery. Although imitation learning approaches have the potential to learn complex surgical skills, the interpretation of robot behavior during safety-critical scenarios, such as surgery, remains a challenge. Through combining interpretable 3D point cloud feature vectors based on domain knowledge with feedforward neural networks and probabilistic movement primitives, domain knowledge-informed movement primitives effectively learn surgical skills while improving the interpretation of robot behavior. The evaluation on test data proves that the proposed method can effectively learn surgical skills based on a small number of demonstrations. Using the proposed imitation learning method, a semi-autonomous robotic assistance for directed gallbladder retraction is introduced and evaluated during gallbladder removal on a silicone liver phantom and ex vivo porcine livers. Achieving over 91 % and 92 % successful gallbladder retractions, the robotic assistance enables effective support for surgeons during these surgical interventions.}
}

@misc{melnikDigitalTwinGeneration2025,
  title = {Digital {{Twin Generation}} from {{Visual Data}}: {{A Survey}}},
  author = {Melnik, Andrew and Alt, Benjamin and Nguyen, Giang and Wilkowski, Artur and Stefa{\'n}czyk, Maciej and Wu, Qirui and Harms, Sinan and Rhodin, Helge and Savva, Manolis and Beetz, Michael},
  year = {2025},
  month = apr,
  number = {arXiv:2504.13159},
  eprint = {2504.13159},
  abstract = {This survey explores recent developments in generating digital twins from videos. Such digital twins can be used for robotics application, media content creation, or design and construction works. We analyze various approaches, including 3D Gaussian Splatting, generative in-painting, semantic segmentation, and foundation models highlighting their advantages and limitations. Additionally, we discuss challenges such as occlusions, lighting variations, and scalability, as well as potential future research directions. This survey aims to provide a comprehensive overview of state-of-the-art methodologies and their implications for real-world applications. Awesome list: https://github.com/ndrwmlnk/awesome-digital-twins},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.13159},
  urldate = {2025-05-11},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,my},
  arxiv = {2504.13159}
}

@misc{kienleAIbasedFrameworkRobust2025,
  title = {{{AI-based Framework}} for {{Robust Model-Based Connector Mating}} in {{Robotic Wire Harness Installation}}},
  author = {Kienle, Claudius and Alt, Benjamin and Schneider, Finn and Pertlwieser, Tobias and J{\"a}kel, Rainer and Rayyes, Rania},
  year = {2025},
  month = mar,
  number = {arXiv:2503.09409},
  eprint = {2503.09409},
  abstract = {Despite the widespread adoption of industrial robots in automotive assembly, wire harness installation remains a largely manual process, as it requires precise and flexible manipulation. To address this challenge, we design a novel AI-based framework that automates cable connector mating by integrating force control with deep visuotactile learning. Our system optimizes search-and-insertion strategies using first-order optimization over a multimodal transformer architecture trained on visual, tactile, and proprioceptive data. Additionally, we design a novel automated data collection and optimization pipeline that minimizes the need for machine learning expertise. The framework optimizes robot programs that run natively on standard industrial controllers, permitting human experts to audit and certify them. Experimental validations on a center console assembly task demonstrate significant improvements in cycle times and robustness compared to conventional robot programming approaches. Videos are available under https://claudius-kienle.github.io/AppMuTT.},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.09409},
  urldate = {2025-03-17},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Engineering Finance and Science,Computer Science - Machine Learning,Computer Science - Robotics,my},
  arxiv = {2503.09409},
  pdf = {wire_harness_CASE25.pdf}
}

@misc{kienleLODGEJointHierarchical2025,
  title = {{{LODGE}}: {{Joint Hierarchical Task Planning}} and {{Learning}} of {{Domain Models}} with {{Grounded Execution}}},
  author = {Kienle, Claudius and Alt, Benjamin and Arenz, Oleg and Peters, Jan},
  year = {2025},
  month = may,
  number = {arXiv:2505.13497},
  eprint = {2505.13497},
  abstract = {Large Language Models (LLMs) enable planning from natural language instructions using implicit world knowledge, but often produce flawed plans that require refinement. Instead of directly predicting plans, recent methods aim to learn a problem domain that can be solved for different goal states using classical planners. However, these approaches require significant human feedback to obtain useful models. We address this shortcoming by learning hierarchical domains, where low-level predicates and actions are composed into higher-level counterparts, and by leveraging simulation to validate their preconditions and effects. This hierarchical approach is particularly powerful for long-horizon planning, where LLM-based planning approaches typically struggle. Furthermore, we introduce a central error reasoner to ensure consistency among the different planning levels. Evaluation on two challenging International Planning Competition (IPC) domains and a long-horizon robot manipulation task demonstrates higher planning success rates than state-of-the-art domain synthesis and LLM-modulo planning methods, while constructing high-quality models of the domain.},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.13497},
  urldate = {2025-05-26},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics,my},
  arxiv = {2505.13497}
}

@inproceedings{kienle_querycad_2025,
  title = {{{QueryCAD}}: {{Grounded Question Answering}} for {{CAD Models}}},
  shorttitle = {{{QueryCAD}}},
  booktitle = {2025 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Kienle, Claudius and Alt, Benjamin and Katic, Darko and J{\"a}kel, Rainer},
  year = {2025},
  month = may,
  eprint = {2409.08704},
  primaryclass = {cs},
  publisher = {IEEE},
  address = {Atlanta, USA},
  doi = {10.48550/arXiv.2409.08704},
  urldate = {2024-09-27},
  abstract = {CAD models are widely used in industry and are essential for robotic automation processes. However, these models are rarely considered in novel AI-based approaches, such as the automatic synthesis of robot programs, as there are no readily available methods that would allow CAD models to be incorporated for the analysis, interpretation, or extraction of information. To address these limitations, we propose QueryCAD, the first system designed for CAD question answering, enabling the extraction of precise information from CAD models using natural language queries. QueryCAD incorporates SegCAD, an open-vocabulary instance segmentation model we developed to identify and select specific parts of the CAD model based on part descriptions. We further propose a CAD question answering benchmark to evaluate QueryCAD and establish a foundation for future research. Lastly, we integrate QueryCAD within an automatic robot program synthesis framework, validating its ability to enhance deep-learning solutions for robotics by enabling them to process CAD models (https://claudius-kienle.github.com/querycad).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics,my},
  pdf = {querycad.pdf},
  arxiv = {2409.08704}
}

@inproceedings{alt_shadow_2025,
  title = {Shadow {{Program Inversion}} with {{Differentiable Planning}}: {{A Framework}} for {{Unified Robot Program Parameter}} and {{Trajectory Optimization}}},
  shorttitle = {Shadow {{Program Inversion}} with {{Differentiable Planning}}},
  booktitle = {2025 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Alt, Benjamin and Kienle, Claudius and Katic, Darko and J{\"a}kel, Rainer and Beetz, Michael},
  year = {2025},
  month = may,
  eprint = {2409.08678},
  primaryclass = {cs},
  publisher = {IEEE},
  address = {Atlanta, USA},
  doi = {10.48550/arXiv.2409.08678},
  urldate = {2024-09-23},
  abstract = {This paper presents SPI-DP, a novel first-order optimizer capable of optimizing robot programs with respect to both high-level task objectives and motion-level constraints. To that end, we introduce DGPMP2-ND, a differentiable collision-free motion planner for serial N-DoF kinematics, and integrate it into an iterative, gradient-based optimization approach for generic, parameterized robot program representations. SPI-DP allows first-order optimization of planned trajectories and program parameters with respect to objectives such as cycle time or smoothness subject to e.g. collision constraints, while enabling humans to understand, modify or even certify the optimized programs. We provide a comprehensive evaluation on two practical household and industrial applications.},
  archiveprefix = {arXiv},
  keywords = {68T40,Computer Science - Artificial Intelligence,Computer Science - Robotics,D.1,I.2},
  pdf = {spi_dp.pdf},
  arxiv = {2409.08678},
  selected = true
}

@inproceedings{alt_bansai_2024,
  title = {{{BANSAI}}: {{Towards Bridging}} the {{AI Adoption Gap}} in {{Industrial Robotics}} with {{Neurosymbolic Programming}}},
  shorttitle = {{{BANSAI}}},
  booktitle = {Procedia {{CIRP}}},
  author = {Alt, Benjamin and Dvorak, Julia and Katic, Darko and J{\"a}kel, Rainer and Beetz, Michael and Lanza, Gisela},
  year = {2024},
  month = jan,
  volume = {130},
  eprint = {2404.13652},
  primaryclass = {cs},
  pages = {532--537},
  publisher = {Elsevier B.V.},
  address = {P{\'o}voa de Varzim, Portugal},
  doi = {10.1016/j.procir.2024.10.125},
  urldate = {2024-12-02},
  abstract = {Over the past decade, deep learning helped solve manipulation problems across all domains of robotics. At the same time, industrial robots continue to be programmed overwhelmingly using traditional program representations and interfaces. This paper undertakes an analysis of this "AI adoption gap" from an industry practitioner's perspective. In response, we propose the BANSAI approach (Bridging the AI Adoption Gap via Neurosymbolic AI). It systematically leverages principles of neurosymbolic AI to establish data-driven, subsymbolic program synthesis and optimization in modern industrial robot programming workflow. BANSAI conceptually unites several lines of prior research and proposes a path toward practical, real-world validation.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {68T40,Computer Science - Artificial Intelligence,Computer Science - Computational Engineering Finance and Science,Computer Science - Machine Learning,Computer Science - Robotics,I.2.1,I.2.2,I.2.9,J.6,J.7,my},
  pdf = {bansai_CMS24.pdf},
  arxiv = {2404.13652}
}

@inproceedings{alt_domain-specific_2024,
  title = {Domain-{{Specific Fine-Tuning}} of~{{Large Language Models}} for~{{Interactive Robot Programming}}},
  booktitle = {European {{Robotics Forum}} 2024},
  author = {Alt, Benjamin and Ke{\ss}ner, Urs and Taranovic, Aleksandar and Katic, Darko and Hermann, Andreas and J{\"a}kel, Rainer and Neumann, Gerhard},
  editor = {Secchi, Cristian and Marconi, Lorenzo},
  year = {2024},
  month = mar,
  series = {Springer {{Proceedings}} in {{Advanced Robotics}}},
  volume = {32},
  eprint = {2312.13905},
  primaryclass = {cs},
  pages = {274--279},
  publisher = {Springer Nature Switzerland},
  address = {Rimini, Italy},
  doi = {10.1007/978-3-031-76424-0_49},
  abstract = {Industrial robots are applied in a widening range of industries, but robot programming mostly remains a task limited to programming experts. We propose a natural language-based assistant for programming of advanced, industrial robotic applications and investigate strategies for domain-specific fine-tuning of foundation models with limited data and compute.},
  archiveprefix = {arXiv},
  isbn = {978-3-031-76424-0},
  langid = {english},
  keywords = {my,relevant},
  pdf = {llm_ERF24.pdf},
  arxiv = {2312.13905}
}

@inproceedings{alt_efficientpps_2023,
  title = {{{EfficientPPS}}: {{Part-aware Panoptic Segmentation}} of {{Transparent Objects}} for {{Robotic Manipulation}}},
  booktitle = {{{ISR Europe}} 2023},
  author = {Alt, Benjamin and Nguyen, Minh Dang and Hermann, Andreas and Katic, Darko and J{\"a}kel, Rainer and Dillmann, R{\"u}diger and Sax, Eric},
  year = {2023},
  month = sep,
  publisher = {VDE Verlag},
  address = {Stuttgart, Germany},
  abstract = {The use of autonomous robots for assistance tasks in hospitals has the potential to free up qualified staff and improve patient care. However, the ubiquity of deformable and transparent objects in hospital settings poses significant challenges to vision-based perception systems. We present EfficientPPS, a neural architecture for part-aware panoptic segmentation that provides robots with semantically rich visual information for grasping and manipulation tasks. We also present an unsupervised data collection and labelling method to reduce the need for human involvement in the training process. EfficientPPS is evaluated on a dataset containing real-world hospital objects and demonstrated to be robust and efficient in grasping transparent transfusion bags with a collaborative robot arm.},
  copyright = {All rights reserved},
  isbn = {978-3-8007-6140-1},
  keywords = {my},
  arxiv = {2312.13906},
  pdf = {efficientpps_ISR23.pdf}
}

@inproceedings{alt_heuristic-free_2022,
  title = {Heuristic-{{Free Optimization}} of {{Force-Controlled Robot Search Strategies}} in {{Stochastic Environments}}},
  booktitle = {2022 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Alt, Benjamin and Katic, Darko and J{\"a}kel, Rainer and Beetz, Michael},
  year = {2022},
  month = oct,
  pages = {8887--8893},
  issn = {2153-0866},
  doi = {10.1109/IROS47612.2022.9982093},
  abstract = {In both industrial and service domains, a central benefit of the use of robots is their ability to quickly and reliably execute repetitive tasks. However, even relatively simple peg-in-hole tasks are typically subject to stochastic variations, requiring search motions to find relevant features such as holes. While search improves robustness, it comes at the cost of increased runtime: More exhaustive search will maximize the probability of successfully executing a given task, but will significantly delay any downstream tasks. This trade-off is typically resolved by human experts according to simple heuristics, which are rarely optimal. This paper introduces an automatic, data-driven and heuristic-free approach to optimize robot search strategies. By training a neural model of the search strategy on a large set of simulated stochastic environments, conditioning it on few real-world examples and inverting the model, we can infer search strategies which adapt to the time-variant characteristics of the underlying probability distributions, while requiring very few real-world measurements. We evaluate our approach on two different industrial robots in the context of spiral and probe search for THT electronics assembly.**See github.com/benjaminalt/dpse for code and data.},
  copyright = {All rights reserved},
  keywords = {Adaptation models,my,relevant,Runtime,Search problems,Service robots,Spirals,Stochastic processes,Training},
  pdf = {dpse_IROS22.pdf},
  arxiv = {2207.07524}
}

@inproceedings{alt_human-ai_2024,
  title = {Human-{{AI Interaction}} in {{Industrial Robotics}}: {{Design}} and {{Empirical Evaluation}} of a {{User Interface}} for {{Explainable AI-Based Robot Program Optimization}}},
  shorttitle = {Human-{{AI Interaction}} in {{Industrial Robotics}}},
  booktitle = {57th {{CIRP Conference}} on {{Manufacturing Systems}} 2024},
  author = {Alt, Benjamin and Zahn, Johannes and Kienle, Claudius and Dvorak, Julia and May, Marvin and Katic, Darko and J{\"a}kel, Rainer and Kopp, Tobias and Beetz, Michael and Lanza, Gisela},
  year = {2024},
  month = apr,
  eprint = {2404.19349},
  primaryclass = {cs},
  publisher = {arXiv},
  address = {P{\'o}voa de Varzim, Portugal},
  doi = {10.48550/arXiv.2404.19349},
  urldate = {2024-05-10},
  abstract = {While recent advances in deep learning have demonstrated its transformative potential, its adoption for real-world manufacturing applications remains limited. We present an Explanation User Interface (XUI) for a state-of-the-art deep learning-based robot program optimizer which provides both naive and expert users with different user experiences depending on their skill level, as well as Explainable AI (XAI) features to facilitate the application of deep learning methods in real-world applications. To evaluate the impact of the XUI on task performance, user satisfaction and cognitive load, we present the results of a preliminary user survey and propose a study design for a large-scale follow-up study.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {68T40,Computer Science - Artificial Intelligence,Computer Science - Computational Engineering Finance and Science,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics,I.2.1,I.2.2,I.2.9,J.6,J.7,my},
  pdf = {xui_CMS24.pdf},
  arxiv = {2404.19349}
}

@inproceedings{alt_knowledge-driven_2023,
  title = {Knowledge-{{Driven Robot Program Synthesis}} from {{Human VR Demonstrations}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Principles}} of {{Knowledge Representation}} and {{Reasoning}}},
  author = {Alt, Benjamin and Kenfack, Franklin Kenghagho and Haidu, Andrei and Katic, Darko and J{\"a}kel, Rainer and Beetz, Michael},
  year = {2023},
  month = sep,
  pages = {34--43},
  publisher = {IJCAI},
  address = {Rhodes, Greece},
  abstract = {Aging societies, labor shortages and increasing wage costs call for assistance robots capable of autonomously performing a wide array of real-world tasks. Such open-ended robotic manipulation requires not only powerful knowledge representations and reasoning (KR\&R) algorithms, but also methods for humans to instruct robots what tasks to perform and how to perform them. In this paper, we present a system for automatically generating executable robot control programs from human task demonstrations in virtual reality (VR). We leverage common-sense knowledge and game engine-based physics to semantically interpret human VR demonstrations, as well as an expressive and general task representation and automatic path planning and code generation, embedded into a state-of-the-art cognitive architecture. We demonstrate our approach in the context of force-sensitive fetch-and-place for a robotic shopping assistant. The source code is available at https://github.com/ease-crc/vr-program-synthesis.},
  copyright = {All rights reserved},
  isbn = {978-1-956792-02-7},
  keywords = {68T30,Computer Science - Artificial Intelligence,Computer Science - Robotics,D.1,F.3,I.2,my,relevant},
  pdf = {knowledge_KR23.pdf},
  arxiv = {2306.02739},
  doi = {10.24963/kr.2023/4}
}

@inproceedings{alt_lapseg3d_2022,
  title = {{{LapSeg3D}}: {{Weakly Supervised Semantic Segmentation}} of {{Point Clouds Representing Laparoscopic Scenes}}},
  shorttitle = {{{LapSeg3D}}},
  booktitle = {2022 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Alt, Benjamin and Kunz, Christian and Katic, Darko and Younis, Rayan and J{\"a}kel, Rainer and {M{\"u}ller-Stich}, Beat Peter and Wagner, Martin and {Mathis-Ullrich}, Franziska},
  year = {2022},
  month = oct,
  pages = {5265--5270},
  issn = {2153-0866},
  doi = {10.1109/IROS47612.2022.9981178},
  abstract = {The semantic segmentation of surgical scenes is a prerequisite for task automation in robot assisted interventions. We propose LapSeg3D, a novel DNN-based approach for the voxel-wise annotation of point clouds representing surgical scenes. As the manual annotation of training data is highly time consuming, we introduce a semi-autonomous clustering-based pipeline for the annotation of the gallbladder, which is used to generate segmented labels for the DNN. When evaluated against manually annotated data, LapSeg3D achieves an F1 score of 0.94 for gallbladder segmentation on various datasets of ex-vivo porcine livers. We show LapSeg3D to generalize accurately across different gallbladders and datasets recorded with different RGB-D camera systems.},
  copyright = {All rights reserved},
  keywords = {Annotations,Laparoscopes,my,Pipelines,Point cloud compression,Robot vision systems,Semantic segmentation,Training data},
  pdf = {lapseg3d_IROS22.pdf},
  arxiv = {2207.07418}
}

@patent{alt_method_2022,
  title = {Method and {{System}} for {{Determining Optimized Program Parameters}} for a {{Robot Program}}},
  author = {Alt, Benjamin and J{\"a}kel, Rainer and Katic, Darko},
  year = {2022},
  month = feb,
  number = {WO2022022784A1},
  address = {Karlsruhe},
  urldate = {2023-01-28},
  assignee = {ArtiMinds Robotics GmbH},
  copyright = {All rights reserved},
  nationality = {Germany},
  keywords = {my,relevant},
  annotation = {IPC:}
}

@inproceedings{alt_modulare_2020,
  title = {Modulare, Datengetriebene {{Roboterprogrammierung}} F{\"u}r Die {{L{\"o}sung}} Komplexer {{Handhabungsaufgaben}} in {{Alltagsumgebungen}}},
  booktitle = {{{AAL-Kongress}} 2020},
  author = {Alt, Benjamin and Aumann, Florian and Gienger, Lennart and Jordan, Florian and Katic, Darko and J{\"a}kel, Rainer and Graf, Birgit},
  year = {2020},
  pages = {17--22},
  publisher = {VDE Verlag},
  address = {Berlin},
  abstract = {Increasing demand for high-standard elderly care and stagnant availability of qualified labor makes the need for partially autonomous robotic ambient assisted living (AAL) solutions increasingly urgent. The preparation and serving of food and beverages is an important subdomain of care robotics, which is rendered challenging by unstructured environments, a large variety of manipulated objects and the need to interact with human users. This work presents a modular, hardware agnostic approach for programming robots to perform complex assistance functions in unstructured environments. We propose a robot-agnostic library of parameterized robot skills which encapsulate primitive tasks central to food preparation and serving, from which hierarchical manipulation sequences can be constructed. We further propose an approach for the automatic adaption of generated motions based on 3D point clouds of the environment to manipulate objects of different dimensions and shapes. We evaluate our solution on two 6-DOF robotic manipulators as well as a bimanual humanoid assistance robot.},
  copyright = {All rights reserved},
  isbn = {978-3-8007-5342-0},
  keywords = {my},
  pdf = {ropha.pdf}
}

@inproceedings{alt_robogrind_2024,
  title = {{{RoboGrind}}: {{Intuitive}} and {{Interactive Surface Treatment}} with {{Industrial Robots}}},
  shorttitle = {{{RoboGrind}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Alt, Benjamin and St{\"o}ckl, Florian and M{\"u}ller, Silvan and Braun, Christopher and Raible, Julian and Alhasan, Saad and Rettig, Oliver and Ringle, Lukas and Katic, Darko and J{\"a}kel, Rainer and Beetz, Michael and Strand, Marcus and Huber, Marco F.},
  year = {2024},
  month = may,
  eprint = {2402.16542},
  primaryclass = {cs},
  publisher = {IEEE},
  address = {Yokohama, Japan},
  doi = {10.1109/ICRA57147.2024.10611143},
  urldate = {2024-03-05},
  abstract = {Surface treatment tasks such as grinding, sanding or polishing are a vital step of the value chain in many industries, but are notoriously challenging to automate. We present RoboGrind, an integrated system for the intuitive, interactive automation of surface treatment tasks with industrial robots. It combines a sophisticated 3D perception pipeline for surface scanning and automatic defect identification, an interactive voice-controlled wizard system for the AI-assisted bootstrapping and parameterization of robot programs, and an automatic planning and execution pipeline for force-controlled robotic surface treatment. RoboGrind is evaluated both under laboratory and real-world conditions in the context of refabricating fiberglass wind turbine blades.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  isbn = {979-8-3503-8457-4},
  keywords = {68T40,Computer Science - Artificial Intelligence,Computer Science - Robotics,I.2.2,I.2.6,I.2.9,my,relevant},
  pdf = {robogrind_ICRA24.pdf},
  arxiv = {2402.16542},
  selected = true
}

@inproceedings{alt_robot_2021,
  title = {Robot {{Program Parameter Inference}} via {{Differentiable Shadow Program Inversion}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Alt, Benjamin and Katic, Darko and J{\"a}kel, Rainer and Bozcuoglu, Asil Kaan and Beetz, Michael},
  year = {2021},
  month = may,
  pages = {4672--4678},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9561206},
  abstract = {Challenging manipulation tasks can be solved effectively by combining individual robot skills, which must be parameterized for the concrete physical environment and task at hand. This is time-consuming and difficult for human programmers, particularly for force-controlled skills. To this end, we present Shadow Program Inversion (SPI), a novel approach to infer optimal skill parameters directly from data. SPI leverages unsupervised learning to train an auxiliary differentiable program representation ("shadow program") and realizes parameter inference via gradient-based model inversion. Our method enables the use of efficient first-order optimizers to infer optimal parameters for originally non-differentiable skills, including many skill variants currently used in production. SPI zero-shot generalizes across task objectives, meaning that shadow programs do not need to be retrained to infer parameters for different task variants. We evaluate our methods on three different robots and skill frameworks in industrial and household scenarios. Code and examples are available at https://innolab.artiminds.com/icra2021.},
  copyright = {All rights reserved},
  keywords = {Automation,Codes,Conferences,my,Production,relevant,Service robots,Task analysis,Unsupervised learning},
  pdf = {spi_ICRA21.pdf},
  arxiv = {2103.14452},
  selected = true
}

@inproceedings{dittus_localization_2021,
  title = {Localization and {{Tracking}} of {{User-Defined Points}} on {{Deformable Objects}} for {{Robotic Manipulation}}},
  booktitle = {{{IEEE ICRA Workshop}} on {{Representing}} and {{Manipulating Deformable Objects}}},
  author = {Dittus, Sven and Alt, Benjamin and Hermann, Andreas and Katic, Darko and J{\"a}kel, Rainer and Fleischer, J{\"u}rgen},
  year = {2021},
  month = may,
  eprint = {2105.09067},
  publisher = {IEEE},
  address = {Xi'an, China},
  urldate = {2021-06-02},
  abstract = {This paper introduces an efficient procedure to localize user-defined points on the surface of deformable objects and track their positions in 3D space over time. To cope with a deformable object's infinite number of DOF, we propose a discretized deformation field, which is estimated during runtime using a multi-step non-linear solver pipeline. The resulting high-dimensional energy minimization problem describes the deviation between an offline-defined reference model and a pre-processed camera image. An additional regularization term allows for assumptions about the object's hidden areas and increases the solver's numerical stability. Our approach is capable of solving the localization problem online in a data-parallel manner, making it ideally suitable for the perception of non-rigid objects in industrial manufacturing processes.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics,my,workshop},
  pdf = {deformable_ICRA21.pdf},
  arxiv = {2105.09067}
}

@incollection{graf_ropha_2020,
  title = {{{RoPHa}} - {{Robuste Wahrnehmungsf{\"a}higkeiten}} F{\"u}r {{Roboter}} Zur {{Unterst{\"u}tzung}} {\"A}lterer {{Nutzer}} Im H{\"a}uslichen {{Umfeld}}},
  booktitle = {Autonome {{Roboter}} F{\"u}r {{Assistenz}}funktionen},
  author = {Graf, Birgit and Jordan, Florian and Blume, Gabriele and Martin, Ronny and {Abdel-Keream}, Mona and Mania, Patrick and Gronbach, Fabian and Emmerich, Christian and Schaller, Raphael and Katic, Darko and Alt, Benjamin},
  year = {2020},
  pages = {118--133},
  publisher = {Bundesanstalt f{\"u}r Arbeitsschutz und Arbeitsmedizin},
  address = {Dortmund},
  abstract = {Ziel des RoPHa-Projekts war es, Grundfunktionen f{\"u}r die Unterst{\"u}tzung {\"a}lterer und pfleged{\"u}rftiger Menschen bei der Handhabung typischer Alltagsobjekte zu entwickeln. Als Use Case f{\"u}r die beispielhafte Umsetzung und Integration der entwickelten Grundfunktionen wurde die mundgerechte Bereitstellung von Nahrung betrachtet. Daf{\"u}r wurden zun{\"a}chst eine systematische Anforderungsanalyse inklusive der Betrachtung von ELS-Aspekten und eine Risikobetrachtung durchgef{\"u}hrt. Danach wurde ein Interaktionskonzept basierend auf einer tabletgest{\"u}tzten GUI und Sprachausgaben entwickelt. Im Bereich der Perzeption wurden Funktionen f{\"u}r die Objekterkennung, die Lokalisierung und Klassifizierung von Speisen, die Wahrnehmung des Menschen und f{\"u}r die aufgabenspezifische Auswahl und Parametrierung von Perzeptionsfunktionen entwickelt und in ein Gesamtsystem integriert. F{\"u}r die sichere Manipulation wurden unterschiedliche elementare Handhabungsfertigkeiten, bspw. f{\"u}r das Schneiden, Bestreuen, Aufnehmen und Anreichen von Speisen entwickelt. Im Rahmen von zwei Testreihen mit Mitarbeitern aus der Pflege wurden die entwickelten Szenarien evaluiert.},
  copyright = {All rights reserved},
  keywords = {my},
  html = {https://publica.fraunhofer.de/entities/publication/0cc4a818-1f2c-4198-b1e3-b614b2ae9960}
}

@inproceedings{kienle_mutt_2024,
  title = {{{MuTT}}: {{A Multimodal Trajectory Transformer}} for {{Robot Skills}}},
  shorttitle = {{{MuTT}}},
  booktitle = {{{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Kienle, Claudius and Alt, Benjamin and Celik, Onur and Becker, Philipp and Katic, Darko and J{\"a}kel, Rainer and Neumann, Gerhard},
  year = {2024},
  month = aug,
  eprint = {2407.15660},
  primaryclass = {cs},
  publisher = {IEEE},
  address = {Abu Dhabi},
  doi = {10.48550/arXiv.2407.15660},
  urldate = {2024-09-19},
  abstract = {High-level robot skills represent an increasingly popular paradigm in robot programming. However, configuring the skills' parameters for a specific task remains a manual and time-consuming endeavor. Existing approaches for learning or optimizing these parameters often require numerous real-world executions or do not work in dynamic environments. To address these challenges, we propose MuTT, a novel encoder-decoder transformer architecture designed to predict environment-aware executions of robot skills by integrating vision, trajectory, and robot skill parameters. Notably, we pioneer the fusion of vision and trajectory, introducing a novel trajectory projection. Furthermore, we illustrate MuTT's efficacy as a predictor when combined with a model-based robot skill optimizer. This approach facilitates the optimization of robot skill parameters for the current environment, without the need for real-world executions during optimization. Designed for compatibility with any representation of robot skills, MuTT demonstrates its versatility across three comprehensive experiments, showcasing superior performance across two different skill representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  pdf = {mutt_IROS24.pdf},
  arxiv = {2407.15660}
}

@incollection{kluy_mensch-roboter-kollaboration_2022,
  title = {{Mensch-Roboter-Kollaboration in KMU -- Potenziale identifizieren, analysieren und realisieren}},
  booktitle = {{Digitalisierung der Arbeitswelt im Mittelstand 1: Ergebnisse und Best Practice des BMBF-Forschungsschwerpunkts "Zukunft der Arbeit: Mittelstand -- innovativ und sozial"}},
  author = {Kluy, Lina and K{\"o}lmel, Lena and Alt, Benjamin and Baumgartner, Marco and Deml, Barbara and Hornung, Luisa and Katic, Darko and Kinkel, Steffen and Kopp, Tobias and Lorenz, Maureen and Nicolai, Philip and Riedel, Norman and Sch{\"a}fer, Arndt and Wurll, Christian},
  editor = {Nitsch, Verena and Brandl, Christopher and H{\"a}u{\ss}ling, Roger and Lemm, Jacqueline and Gries, Thomas and Schmenk, Bernhard},
  year = {2022},
  pages = {55--97},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-64803-2_3},
  urldate = {2022-08-01},
  abstract = {Die Einf{\"u}hrung von kollaborativen Robotern (Cobots) verspricht eine aus arbeitswissenschaftlicher Sichtweise sinnvolle Zusammenarbeit zwischen Mensch und Roboter. Gerade kleinen und mittleren Unternehmen (KMU) gelingt es allerdings h{\"a}ufig nicht oder nur teilweise, die damit verbundenen Potenziale zu erschlie{\ss}en. Das vorliegende Kapitel beschreibt zum einen die Identifizierung, Analyse und Realisierung von Cobot-Potenzialen im Projekt ProBot (,,Proaktive Diagnose und Gestaltung des CoBot-Einsatzes in kleinen und mittleren Unternehmen``). Zum anderen wird die Entwicklung und der Aufbau einer interaktiven Einf{\"u}hrungsunterst{\"u}tzung zur niederschwelligen Planung und Umsetzung von Mensch-Cobot-ArbeitssystemenMensch-Cobot-Arbeitssystem (MCA) (MCA) skizziert, wobei einige der enthaltenen Methoden vertieft vorgestellt werden. Dar{\"u}ber hinaus werden anhand realer Fallbeispiele von KMU die M{\"o}glichkeiten und Grenzen von Mensch-Roboter-Kollaborationen (MRK) anschaulich dargestellt. Die interaktive Einf{\"u}hrungsunterst{\"u}tzung (KMU Cobot Coach) wird interessierten KMU unter folgender Adresse zur freien Verf{\"u}gung online bereitgestellt: https://www.kmu-cobot-coach.de/.},
  copyright = {All rights reserved},
  isbn = {978-3-662-64803-2},
  langid = {ngerman},
  keywords = {my}
}

@inproceedings{raible_artificial_2023,
  title = {Artificial {{Neural Network Guided Compensation}} of {{Nonlinear Payload}} and {{Wear Effects}} for {{Industrial Robots}}},
  booktitle = {2023 {{IEEE}} 19th {{International Conference}} on {{Automation Science}} and {{Engineering}} ({{CASE}})},
  author = {Raible, Julian and Rettig, Oliver and Alt, Benjamin and Yaman, Alper and Gauger, Isabelle and Biasi, Lorenzo and M{\"u}ller, Silvan and Katic, Darko and Strand, Marcus and Huber, Marco F.},
  year = {2023},
  month = aug,
  pages = {1--8},
  issn = {2161-8089},
  doi = {10.1109/CASE56687.2023.10260559},
  urldate = {2023-12-16},
  abstract = {The absolute accuracy of industrial robots is influ-enced by numerous geometric and non-geometric errors. Most state-of-the-art calibration and compensation methods consider only the geometric errors and neglect the non-geometric ones. In this paper, a hybrid compensation approach is proposed that combines well-known kinematic models with a model-free data-driven component using a neural network. This allows the use of established calibration methods for the geometric influences captured in the kinematic model to improve the non-geometric error compensation by the neural network. The proposed approach is applied in two use cases: payload and wear compensation. Simulations and real experiments show the improved absolute accuracy of the hybrid compensation approach compared to classical calibration methods.},
  copyright = {All rights reserved},
  keywords = {my}
  }

@article{schultheis_easy_2024,
  title = {{{EASY}}: {{Energy-Efficient Analysis}} and {{Control Processes}} in the {{Dynamic Edge-Cloud Continuum}} for {{Industrial Manufacturing}}},
  shorttitle = {{{EASY}}},
  author = {Schultheis, Alexander and Alt, Benjamin and Bast, Sebastian and Guldner, Achim and Jilg, David and Katic, Darko and Mundorf, Johannes and Schlagenhauf, Tobias and Weber, Sebastian and Bergmann, Ralph and Bergweiler, Simon and Creutz, Lars and Dartmann, Guido and Malburg, Lukas and Naumann, Stefan and Rezapour, Mahdi and Ruskowski, Martin},
  year = {2024},
  month = sep,
  journal = {KI - K{\"u}nstliche Intelligenz},
  issn = {1610-1987},
  doi = {10.1007/s13218-024-00868-3},
  urldate = {2024-09-05},
  abstract = {According to the guiding principles of Industry~4.0, edge computing enables the data-sovereign and near-real-time processing of data directly at the point of origin. Using these edge devices in manufacturing organization will drive the use of industrial analysis, control, and Artificial Intelligence (AI) applications close to production. The goal of the EASY project is to make the added value of edge computing available by providing an easily usable Edge-Cloud Continuum with a runtime environment and services for the execution of AI-based Analysis and Control processes. Within this continuum, a dynamic, distributed, and optimized execution of services is automated across the entire spectrum from centralized cloud to decentralized edge instances to increase productivity and resource efficiency.},
  langid = {english},
  keywords = {Analysis and Control Processes,Artificial Intelligence,Edge-Cloud Continuum,Energy- and Resource-Efficiency,my},
  pdf = {easy_journal.pdf}
}

@inproceedings{stockl_autonomous_2023-1,
  title = {Autonomous {{Surface Grinding}} of {{Wind Turbine Blades}}},
  booktitle = {Intelligent {{Autonomoous Systems}} 18},
  author = {St{\"o}ckl, Florian and Strand, Marcus and M{\"u}ller, Silvan and Huber, Marco and Raible, Julian and Braun, Christopher and Katic, Darko and Alt, Benjamin and Merkt, Holger},
  editor = {Lee, Soon-Geul and An, Jinung and Chong, Nak Young and Strand, Marcus and Kim, Joo H.},
  year = {2023},
  month = jul,
  pages = {451--457},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-44981-9_38},
  abstract = {Discarded wind turbine blades generate a considerable amount of waste that could be reduced by remanufacturing. Manual remanufacturing is too costly, which is why research is being conducted into automation techniques. The main problem is the individuality of work pieces due to damages. This work presents a workflow that includes damage analysis based on scans of the blade, subsequent path planning, control engineering with an AI controller for grinding and automatic review of the grinding process. Current problems are the inaccuracy of the robot used for scanning and the colour sensitivity of the used laser scanner. Our next steps besides solving the mentioned problems are to train a supervised machine learning algorithm with damage examples and to implement a specific and multi-step path planning algorithm.},
  copyright = {All rights reserved},
  isbn = {978-3-031-44981-9},
  langid = {english},
  keywords = {Autonomous grinding,Hybrid AI,my,Remanufacturing}
}
