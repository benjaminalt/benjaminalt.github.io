<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://benjaminalt.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://benjaminalt.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-13T13:45:38+00:00</updated><id>https://benjaminalt.github.io/feed.xml</id><title type="html">blank</title><subtitle>Roboticist &amp; AI Researcher </subtitle><entry><title type="html">Reproducible Robot Experiments in the Cloud</title><link href="https://benjaminalt.github.io/blog/2025/reproducible-robot-experiments-in-the-cloud/" rel="alternate" type="text/html" title="Reproducible Robot Experiments in the Cloud"/><published>2025-10-13T04:00:00+00:00</published><updated>2025-10-13T04:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2025/reproducible-robot-experiments-in-the-cloud</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2025/reproducible-robot-experiments-in-the-cloud/"><![CDATA[<h2 id="making-robots-active-participants-in-scientific-discovery">Making Robots Active Participants in Scientific Discovery</h2> <p>While it is unclear to what extent robotics is undergoing a replication crisis, most researchers agree that roboticists find it highly challenging to replicate each other’s work. The heterogeneity of the hardware used by each lab is a driving factor of this challenge, as is the use case specific nature of robotic experimentation that makes the experiment setup (hardware, sensors, specific software packages) highly contingent on the task and environment at hand.</p> <p>What is true for robotics as a scientific discipline is doubly true when robots are used as tools, or agents, for scientific discovery in other disciplines. As roboticists, we are not just responsible for transparency of research in our own field, but should serve, as best we can, the needs of those fields - materials science, experimental physics, biomedical research, among others - that increasingly use robots as parts of their experimental setups.</p> <p>In our new paper, “Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing”, accepted to the AIR4S Workshop at IROS 2025, we explore how autonomous robots can help address this problem.</p> <p>You can find the paper on <a href="https://arxiv.org/abs/2508.11406">ArXiv</a> and as a PDF <a href="/assets/vrb_scientific_discovery_IROS25_AIR4S.pdf">here <i class="fas fa-file-pdf"></i></a>.</p> <p>Authors:<br/> Benjamin Alt<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, Mareike Picklum<sup id="fnref:1:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, Sorin Arion<sup id="fnref:1:2"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, Franklin Kenghagho Kenfack<sup id="fnref:1:3"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, Michael Beetz<sup id="fnref:1:4"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <h2 id="robots-as-engines-of-reproducible-science">Robots as Engines of Reproducible Science</h2> <p>Robots promise to execute experiments with high precision and reduced bias by following well-defined protocols, ensuring that procedures are consistent across repetitions. But more importantly, robots can make their reasoning and perception processes transparent. Decisions, percepts and beliefs about the world can be logged, shared, and examined post-hoc. This transparency opens the door to a new kind of “robot-driven science” in which experimental results are both repeatable and inspectable.</p> <h2 id="semantic-execution-tracing">Semantic Execution Tracing</h2> <p>Our first contribution is a <strong>semantic execution tracing framework</strong>. It records not only low-level sensor data and actions, but also the robot’s internal reasoning, its hypotheses, and the causal explanations behind its decisions. The framework integrates perception, simulation, and knowledge representation into a single traceable data model. This allows researchers to replay, analyze, and verify robot executions in a scientifically rigorous way.</p> <h2 id="the-aicor-virtual-research-building">The AICOR Virtual Research Building</h2> <p>The second contribution is the <a href="https://vrb.ease-crc.org/"><strong>AICOR Virtual Research Building (VRB)</strong></a>, a cloud-based platform that hosts entire robot experiments as virtual laboratories. Each experiment is packaged in a containerized environment that includes the code, data, and simulation setup required to reproduce it bit-for-bit. Researchers around the world can explore, replicate, and extend these virtual experiments—much like open-source software, but for embodied scientific procedures.</p> <h2 id="toward-open-trustworthy-and-robot-driven-science">Toward Open, Trustworthy, and Robot-Driven Science</h2> <p>By combining semantic execution tracing with virtual laboratories, we take a step toward a future in which scientific experiments are transparent, verifiable, and open to everyone. Autonomous robots, equipped with structured reasoning and digital twins, can not only perform experiments but also explain them. This is a foundation for a new generation of embodied AI systems that do not merely assist in science, but actively participate in the scientific process itself. In the <a href="https://www.eurobin-project.eu/">euROBIN network</a> and the <a href="https://robotics-institute-germany.de/">Robotics Institute Germany (RIG)</a>, we are building communities of researchers committed to reproducible experimentation in robotics and with robots. Open, reproducible science is a bottom-up effort that can only become reality if everyone contributes. I’m looking forward to discussing how we can continue evolving the VRB and the <a href="https://www.sciencedirect.com/science/article/pii/S1389041725000555">CRAM robot control framework</a> to serve the needs of the scientific community!</p> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>AICOR Institute for Artificial Intelligence, University of Bremen, Germany <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:1:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:1:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a> <a href="#fnref:1:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog"/><category term="robot-learning"/><category term="scientific-discovery"/><category term="open-science"/><category term="infrastructure"/><category term="paper"/><summary type="html"><![CDATA[At the 1st Workshop on Embodied AI and Robotics for Future Scientific Discovery (AIR4S) at IROS 2025, my colleagues and I are presenting a robot execution tracing system and virtual lab environment to facilitate reproducible research with robots.]]></summary></entry><entry><title type="html">AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation</title><link href="https://benjaminalt.github.io/blog/2025/ai-based-framework-for-wire-harness-assembly/" rel="alternate" type="text/html" title="AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation"/><published>2025-06-09T05:00:00+00:00</published><updated>2025-06-09T05:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2025/ai-based-framework-for-wire-harness-assembly</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2025/ai-based-framework-for-wire-harness-assembly/"><![CDATA[<p>In our recent work “AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation”, my coauthors and I present an AI-driven approach to automate connector mating in automotive wire harness installation. The paper has been accepted for presentation at the IEEE International Conference on Automation Science and Engineering (CASE) 2025.</p> <p>You can find the paper on <a href="https://arxiv.org/abs/2503.09409">ArXiv</a> and as a PDF <a href="/assets/pdf/wire_harness_CASE25.pdf">here <i class="fas fa-file-pdf"></i></a>.</p> <p>Demonstration videos as well as additional information can be found on the <a href="https://claudius-kienle.github.io/AppMuTT">paper website</a>.</p> <p>Authors:<br/> Claudius Kienle<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> <sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, Benjamin Alt<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> <sup id="fnref:4:1"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, Finn Schneider<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> <sup id="fnref:4:2"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, Tobias Pertlwieser<sup id="fnref:3:1"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>, Rainer Jäkel<sup id="fnref:4:3"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, Rania Rayyes<sup id="fnref:3:2"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">4</a></sup></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wire-harness-case25/setup.png" sizes="95vw"/> <img src="/assets/img/wire-harness-case25/setup.png" class="img-fluid rounded z-depth-1 w-50 mx-auto d-block" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="motivation">Motivation</h1> <p>Robotic wire harness installation remains one of the least automated processes in automotive assembly, primarily due to the high flexibility of cables, process variance, and the precision required for connector mating. Manual installation dominates the industry, introducing a bottleneck in an otherwise highly automated production line.</p> <p>Traditional robotic connector mating strategies rely on vision and force-controlled search but are limited by trade-offs between robustness and cycle time. These systems are often sensitive to proc ess noise, require extensive manual tuning, and struggle with the sub-millimeter tolerances typical in automotive connectors.</p> <h1 id="core-idea-ai-driven-connector-mating-with-model-based-optimization">Core Idea: AI-Driven Connector Mating with Model-Based Optimization</h1> <p>Our framework combines visuotactile learning with first-order trajectory optimization to improve the robustness and efficiency of connector mating. Specifically, we introduce a <a href="https://ieeexplore.ieee.org/abstract/document/10802198">Multimodal Trajectory Transformer (MuTT)</a> that predicts robot-environment interactions based on visual, tactile, and proprioceptive inputs. MuTT is embedded in a differentiable shadow program that serves as a predictive model for optimizing robot program parameters via gradient-based search.</p> <p>Unlike model-free reinforcement learning, our approach leverages the structure of existing industrial robot programs and enables parameter optimization directly on the controller. This ensures that the optimized programs remain interpretable, auditable, and compatible with standard safety functions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wire-harness-case25/framework.png" sizes="95vw"/> <img src="/assets/img/wire-harness-case25/framework.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> AI-based Framework for Visuotactile Connector Mating: A programmer creates a initial robot program using industry-standard tools (left). During the ramp-up phase, the robot executes the program repeatedly with varying search parameterizations. The resulting dataset is used to train <a href="https://ieeexplore.ieee.org/abstract/document/10802198">MuTT</a>, a predictive visuotactile model of the robot and environment dynamics (center). MuTT serves as a sim2real predictor for the first-order optimizer <a href="https://ieeexplore.ieee.org/document/9561206">SPI</a>, which optimizes program parameters for robust and fast connector mating given the observed process variance and current environment image (right). </div> </div> <h1 id="experimental-setup">Experimental Setup</h1> <p>We validated the proposed framework on an automotive center console assembly task using a UR5e robot equipped with a force-torque sensor, a 2D camera, and a Schunk pneumatic gripper. The system was tested on five distinct connector geometries representative of modern automotive wire harnesses.</p> <p>The experimental process involved:</p> <ul> <li>Data-driven training with more than 4,000 connector mating trials per connector type.</li> <li>Simulation of process variance via randomized socket positions and initial robot parameters.</li> <li>Automated data collection and continuous model refinement.</li> </ul> <h1 id="results">Results</h1> <p>The optimized connector mating programs achieved:</p> <ul> <li>Significant reduction in cycle times across all connector types (up to 35% faster).</li> <li>Increased success rates, reaching up to 98% post-optimization.</li> <li>Robust performance under stochastic variances and across diverse connector geometries.</li> </ul> <p>The system outperformed traditional force-controlled search strategies by efficiently adapting the search pattern based on visuotactile feedback and environmental images.</p> <h1 id="broader-implications">Broader Implications</h1> <p>Our work provides a practical pathway for deploying AI-driven optimization in industrial assembly tasks with tight tolerances and process noise. By leveraging model-based program optimization rather than end-to-end control, the framework offers a safe, certifiable, and data-efficient alternative to conventional machine learning pipelines.</p> <p>The core ideas of multimodal learning, visuotactile modeling, and gradient-based robot program optimization extend beyond wire harness installation and are applicable to other challenging industrial insertion and assembly tasks.</p> <h3 id="citation">Citation</h3> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">kienleAIbasedFrameworkRobust2025</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AI-based Framework}</span> <span class="nv">for</span> <span class="s">{Robust Model-Based Connector Mating}</span> <span class="nv">in</span> <span class="s">{Robotic Wire Harness Installation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kienle, Claudius and Alt, Benjamin and Schneider, Finn and Pertlwieser, Tobias and J{\"a}kel, Rainer and Rayyes, Rania}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{arXiv:2503.09409}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2503.09409}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2503.09409}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2025-03-17}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Computer Science - Artificial Intelligence,Computer Science - Computational Engineering Finance and Science,Computer Science - Machine Learning,Computer Science - Robotics,my}</span>
<span class="p">}</span>
</code></pre></div></div> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>IAS Lab, Computer Science Department, TU Darmstadt, Germany <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4"> <p>ArtiMinds Robotics, Karlsruhe, Germany <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:4:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:4:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a> <a href="#fnref:4:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a></p> </li> <li id="fn:2"> <p>AICOR Institute for Artificial Intelligence, University of Bremen, Germany <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>Institute for Material Handling and Logistics (IFL), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:3:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog"/><category term="robot-assembly"/><category term="robot-learning"/><category term="visuotactile-learning"/><category term="optimization"/><category term="paper"/><summary type="html"><![CDATA[At CASE 2025, my colleagues and I are presenting a novel AI-based framework for robust, data-driven connector mating in robotic wire harness installation, combining visuotactile learning with model-based optimization.]]></summary></entry><entry><title type="html">Semi-Autonomous Robotic Assistance for Gallbladder Retraction in Surgery</title><link href="https://benjaminalt.github.io/blog/2025/semi-autonomous-robotic-assistance-for-gallbladder-retraction/" rel="alternate" type="text/html" title="Semi-Autonomous Robotic Assistance for Gallbladder Retraction in Surgery"/><published>2025-06-08T05:00:00+00:00</published><updated>2025-06-08T05:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2025/semi-autonomous-robotic-assistance-for-gallbladder-retraction</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2025/semi-autonomous-robotic-assistance-for-gallbladder-retraction/"><![CDATA[<p>In our recent work “Semi-Autonomous Robotic Assistance for Gallbladder Retraction in Surgery”, my coauthors and I introduce a novel framework for robot learning of gallbladder retraction motions in the context of laparoscopic gallbladder surgery. The paper is to appear in the IEEE Robotics and Automation Letters (RA-L).</p> <p>You can find the paper on <a href="https://ieeexplore.ieee.org/document/11027660">IEEE Xplore</a>, as well as a PDF <a href="/assets/pdf/RAL_DKMP.pdf">here</a>.</p> <p>Authors: Alexander Schüßler<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, Christian Kunz<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, Rayan Younis<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, Benjamin Alt<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>, Jamie Paik<sup id="fnref:1:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, Martin Wagner<sup id="fnref:3:1"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, Franziska Mathis-Ullrich<sup id="fnref:2:1"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gallbladder-ral/experiment_setup.jpg" sizes="95vw"/> <img src="/assets/img/gallbladder-ral/experiment_setup.jpg" class="img-fluid rounded z-depth-1 w-75 mx-auto d-block" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="motivation">Motivation</h1> <p>Robotic assistance in surgery is a rapidly advancing field, but full autonomy remains impractical for most procedures due to the complexity and unpredictability of intraoperative environments. A more viable near-term approach is semi-autonomous robotic assistance, where robots support human surgeons by executing subtasks under supervision.</p> <p>One such subtask is gallbladder retraction during a cholecystectomy (gallbladder removal). Retraction is critical for providing access and visibility but is labor-intensive and error-prone when performed manually. It is also ideally suited for robotic support due to its structured nature and spatial constraints.</p> <p>However, standard end-to-end learning methods (e.g., deep imitation learning) suffer from poor interpretability, making them difficult to trust and certify in clinical settings. This work proposes a framework that incorporates domain knowledge directly into the learning process to enhance interpretability without compromising performance.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gallbladder-ral/dkmp_overview.jpg" sizes="95vw"/> <img src="/assets/img/gallbladder-ral/dkmp_overview.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="core-idea-domain-knowledge-informed-movement-primitives-dkmp">Core Idea: Domain Knowledge-Informed Movement Primitives (DKMP)</h1> <p>Our approach, termed Domain Knowledge-Informed Movement Primitives (DKMP), combines semantic feature extraction from 3D point clouds using anatomical landmarks manually selected by the surgeon, neural network-based prediction of retraction endpoints using these features as input, and trajectory generation via Probabilistic Movement Primitives (ProMPs) conditioned on the predicted endpoint. In contrast to end-to-end systems that map raw images directly to motions, DKMP leverages explicit spatial features grounded in surgical intuition, such as the location of the next dissection point and the intended direction of retraction force.</p> <h1 id="experimental-setup">Experimental Setup</h1> <p>We trained and evaluated DKMP using a dual-arm robotic setup:</p> <ul> <li> <p>A Franka Emika Panda robot executes retraction motions via a laparoscopic grasper.</p> </li> <li> <p>A UR5 arm holds a ZED2 stereo camera that captures the 3D structure of the surgical scene.</p> </li> <li> <p>Surgical landmarks are manually labeled by the surgeon through a GUI.</p> </li> </ul> <p>Experiments were conducted on both a silicone liver phantom and ex vivo porcine livers. To train DKMP, we collected over 500 retraction demonstrations, clustering them based on anatomical context (left, middle, or right retraction depending on the dissection line).</p> <h1 id="results">Results</h1> <p>DKMP was compared to a DeepMP (Deep Movement Primitives) baseline. On both phantom and ex vivo datasets:</p> <ul> <li> <p>DKMP achieved comparable or superior accuracy in endpoint and trajectory prediction.</p> </li> <li> <p>DKMP showed higher interpretability due to its structured input representation.</p> </li> </ul> <p>When trained on phantom data but tested on ex vivo, performance dropped significantly, underscoring the domain gap between synthetic and biological tissue.</p> <h3 id="surgical-trials">Surgical Trials</h3> <p>In pre-clinical trials (5 phantom, 5 ex vivo surgeries), DKMP achieved 91% and 92% success rates, respectively. Success was defined by correct retraction direction and sufficient tension as judged by a clinical expert. The system executed retractions semi-autonomously with surgeon supervision (LoA 2).</p> <h1 id="broader-implications">Broader Implications</h1> <p>This work contributes a data-efficient, interpretable imitation learning framework for high-stakes human-robot collaboration tasks. The idea of injecting domain knowledge into both feature design and trajectory generation is applicable far beyond surgery, potentially benefiting assistive robotics, industrial teleoperation, or inspection systems where interpretability is as important as performance.</p> <p>From a machine learning standpoint, the paper bridges symbolic priors with neural prediction and probabilistic control, exemplifying a hybrid approach to interpretable robot behavior synthesis.</p> <h3 id="citation">Citation</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A. Schüßler et al., “Semi-Autonomous Robotic Assistance for Gallbladder Retraction in Surgery,” IEEE Robotics and Automation Letters, pp. 1–8, 2025, doi: 10.1109/LRA.2025.3577430.
</code></pre></div></div> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Reconfigurable Robotics Laboratory, Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:2"> <p>Surgical Planning and Robotic Cognition Laboratory, Department Artificial Intelligence in Biomedical Engineering (AIBE), Friedrich-Alexander-University Erlangen-Nuremberg, Germany <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:3"> <p>Department of Visceral, Thoracic and Vascular Surgery, Faculty of Medicine, University Hospital Carl Gustav Carus &amp; Center for the Tactile Internet with Human-in-the-Loop (CeTI), Technische Universität Dresden, Germany <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:4"> <p>AICOR Institute for Artificial Intelligence, University of Bremen, Germany <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog"/><category term="robot-surgery"/><category term="robot-learning"/><category term="learning-from-demonstration"/><category term="paper"/><summary type="html"><![CDATA[A summary of our recent paper on a semi-autonomous robotic assistance system for minimally invasive surgery that allows surgeons to demonstrate retraction tasks while ensuring the interpretability of resulting behavior.]]></summary></entry><entry><title type="html">The AICOR Virtual Research Building Collaborative, Open Science and Education in the Cloud</title><link href="https://benjaminalt.github.io/blog/2025/aicor-virtual-research-building/" rel="alternate" type="text/html" title="The AICOR Virtual Research Building Collaborative, Open Science and Education in the Cloud"/><published>2025-06-01T05:00:00+00:00</published><updated>2025-06-01T05:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2025/aicor-virtual-research-building</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2025/aicor-virtual-research-building/"><![CDATA[<p>At this year’s IEEE International Conference on Robotics and Automation (ICRA 2025), I had the privilege of presenting our work on the <a href="https://vrb.ease-crc.org/">AICOR Virtual Research Building (VRB)</a> - a digital infrastructure initiative aimed at redefining how we conduct and disseminate robotics research and education in the cloud.</p> <p>You can find my slides <a href="https://nc.uni-bremen.de/index.php/s/ZEpMQDeNSHmcpRe">here</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vrb/intel4coro.png" sizes="95vw"/> <img src="/assets/img/vrb/intel4coro.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="democratizing-access-to-robotics-research">Democratizing Access to Robotics Research</h1> <p>The VRB is designed as an open, collaborative platform supporting a global research ecosystem. Our mission is threefold:</p> <ul> <li> <p>Foster a globally connected robotics research community</p> </li> <li> <p>Make high-quality robotics education accessible and inclusive</p> </li> <li> <p>Accelerate open science and reproducibility in robotics</p> </li> </ul> <p>At its core, the VRB hosts shared research infrastructure such as <a href="https://www.open-ease.org/">OpenEASE</a>, a data repository that supports reproducible workflows and automated knowledge extraction.</p> <h1 id="robotics-education-in-the-cloud">Robotics Education in the Cloud</h1> <p>The platform also includes a suite of educational tools:</p> <ul> <li> <p>Interactive modules and AI-driven tutoring systems</p> </li> <li> <p>Virtual laboratories enabling hands-on learning for students with limited physical access</p> </li> <li> <p>A collaborative, open-source textbook framework designed to grow with the community</p> </li> </ul> <p>Through initiatives like the EASE Fall School and student competitions, we’re engaging learners with real-world projects and skill-building opportunities.</p> <h1 id="software-infrastructure-for-scalable-research">Software Infrastructure for Scalable Research</h1> <p>The backbone of the VRB is a modular open-source software stack for cognitive robotics developed at the <a href="https://ai.uni-bremen.de/">IAI</a> at University of Bremen:</p> <ul> <li> <p><a href="https://github.com/Multiverse-Framework/Multiverse">Multiverse</a>: a unified simulation backend based on USD, enabling interoperability across simulators</p> </li> <li> <p><a href="https://github.com/cram2/pycram">PyCRAM</a>: a general-purpose plan executive supporting multiple robots and task domains</p> </li> <li> <p><a href="https://github.com/SemRoCo/giskardpy">Giskard</a>, <a href="https://gitlab.informatik.uni-bremen.de/robokudo/robokudo">RoboKudo</a>, and <a href="https://github.com/knowrob/knowrob">KnowRob</a>: motion planning, perception, and reasoning engines</p> </li> <li> <p><a href="https://arxiv.org/abs/2304.14119">CRAM Cognitive Architecture</a>: enabling complex robot behavior grounded in cognitive principles</p> </li> </ul> <p>This modular architecture supports rapid prototyping, reproducibility, and collaborative development at scale.</p> <h1 id="join-us">Join Us!</h1> <p>We invite universities, research institutions, and educators worldwide to become active contributors to the AICOR Virtual Research Building. By joining our community, you support a shared vision of open, reproducible science and equitable access to robotics education. Whether through sharing datasets, integrating educational content, or collaborating on cognitive architecture development, your participation helps build a sustainable, interoperable infrastructure for global research.</p> <p>We will be presenting the VRB at ROSCon UK in Edinburgh (September 15-17), and are always looking for new use cases and partners.</p> <p>Contact us at <a href="https://vrb.ease-crc.org/contact">https://vrb.ease-crc.org/contact</a> to become part of the community!</p>]]></content><author><name></name></author><category term="blog"/><category term="open-source"/><category term="open-science"/><category term="robotics"/><category term="education"/><category term="talk"/><summary type="html"><![CDATA[The AICOR Virtual Research Building (VRB) is our contribution to open science, replicable experiments and open education in robotics.]]></summary></entry><entry><title type="html">A Human-Friendly Introduction to AI Safety</title><link href="https://benjaminalt.github.io/blog/2023/introduction-to-ai-safety/" rel="alternate" type="text/html" title="A Human-Friendly Introduction to AI Safety"/><published>2023-03-17T01:00:00+00:00</published><updated>2023-03-17T01:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2023/introduction-to-ai-safety</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2023/introduction-to-ai-safety/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" style="display: flex; justify-content: center;"> <figure> <iframe src="https://www.youtube.com/embed/9MUrQ1yJGCo" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> <p>An overview of some aspects of technical AI safety, which motivates and introduces the field from first principles.</p>]]></content><author><name></name></author><category term="talk"/><category term="ai"/><category term="ai-safety"/><category term="talk"/><summary type="html"><![CDATA[A talk I gave at an online LessWrong Meetup in Karlsruhe, Germany, covering the basics of technical AI safety.]]></summary></entry><entry><title type="html">A(G)I 2022: A Year in Review</title><link href="https://benjaminalt.github.io/blog/2023/agi-2022-a-year-in-review/" rel="alternate" type="text/html" title="A(G)I 2022: A Year in Review"/><published>2023-01-23T01:00:00+00:00</published><updated>2023-01-23T01:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2023/agi-2022-a-year-in-review</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2023/agi-2022-a-year-in-review/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" style="display: flex; justify-content: center;"> <figure> <iframe src="https://www.youtube.com/embed/GLgjKpQbaSg" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> <p>A review of AI progress in 2022, with a focus on emerging and changing narratives in AI capabilities and AI safety.</p> <p>The talk was given on Jan. 22, 2023 at the Effective Altruism Retreat in Sigmaringen, Germany.</p>]]></content><author><name></name></author><category term="talk"/><category term="ai"/><category term="ai-safety"/><category term="talk"/><summary type="html"><![CDATA[A talk I gave at the 2023 Effective Altruism Retreat in Sigmaringen, Germany, outlining how pivotal AI technologies shifted narratives around AI and its impacts on society.]]></summary></entry><entry><title type="html">Every Researcher Needs a Pet Fish</title><link href="https://benjaminalt.github.io/blog/2022/every-researcher-needs-a-pet-fish/" rel="alternate" type="text/html" title="Every Researcher Needs a Pet Fish"/><published>2022-09-25T01:00:00+00:00</published><updated>2022-09-25T01:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2022/every-researcher-needs-a-pet-fish</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2022/every-researcher-needs-a-pet-fish/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pet_fish/pet_fish.png" sizes="95vw"/> <img src="/assets/img/pet_fish/pet_fish.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A pet fish, according to <a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a> </div> <p>I’ve been learning a lot about science communication lately. This journey started at this year’s <a href="https://interdisciplinary-college.org">Interdisciplinary College (IK)</a>, where I participated in a science communication workshop organized by <a href="http://jenssteffenscherer.com/">Jens-Steffen Scherer</a>, Valerie Vaquet and Kayson Fahar. The workshop reinforced my prior intuition that science communication is immensely important, particularly for PhD students, but also that much of my daily work already consisted of science communication, even if I didn’t realize it.</p> <h2 id="science-communication-what-it-is-and-why-it-is-important">Science Communication: What It Is and Why It Is Important</h2> <p>Every PhD student or researcher, at university or in private companies, constantly communicates about what it is they are working on and why it is important. At university, this includes casual conversations with other PhD students, professors, collaborators from other departments or lab visitors. In a corporate setting, industry PhDs are additionally faced with communicating their research to stakeholders outside of academia, such as work colleagues, supervisors, budget officers or clients. Even more than traditional PhDs, industry PhD students constantly navigate the gap between research and application, and constantly communicate what they do and why it is important to non-researchers inside and outside their organization. <strong>This is science communication.</strong> Pitching one’s research agenda to one’s boss is science communication. Presenting early-stage results to project partners is science communication. Giving talks at academic conferences is science communication. Chatting about one’s research over lunch - science communication. I’ve been doing it all along.</p> <p>Getting good at science communication - talking to others about what one does and why it is important - has a high expected payoff. The greatest part of this payoff stems from external stakeholders gaining a better intuitive understanding of the (potentially very complex) research problems that are being addressed; and, crucially, <em>why they should care about them</em>. It is hard enough for researchers to descend into the murky depths of maths and data for weeks at a time in the pursuit of some fundamental research question, and to remember this fundamental research question once they resurface. They only do so because they have been trained to, and because their experienced mentors remind them of it every once in a while. Take external stakeholders, colleagues from different subfields or the audience at an academic conference to the same murky depths along the same paths, and they will assuredly get lost.</p> <p>As a consequence, I started to tweak the way I communicate about my research. I found that getting the level of abstraction right - expressing ideas in high-level concepts such as “learning”, “optimization”, “robot motion” by default, and using more technical concepts such as “gradient descent”, “differentiable programming”, “inverse kinematics” only when necessary, and always with appropriate visual aids - makes a big difference in how effectively I can convey arguments and results, particularly to audiences outside of my field. But communicating at the right level of abstraction only serves to make arguments more <em>understandable</em>; it does not help make them more <em>memorable</em>. For that, <strong>symbols and slogans</strong><sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> are much more effective.</p> <h2 id="the-pet-fish-problem">The Pet Fish Problem</h2> <p>I first learned about the pet fish problem in <a href="https://www.antoniolieto.net/">Antonio Lieto</a>’s talk on TCL at the <a href="https://ease-crc.org/ease-fall-school-2022/">2022 EASE Fall School</a>. <a href="https://www.antoniolieto.net/tcl_logic.html">TCL (typicality-based compositional logic)</a> is a formal logic for combining (proto-)typical knowledge<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> of existing concepts into new concepts. To illustrate what typicality-based compositional logic is and why it is important, Prof. Lieto gave the following example: Close your eyes, and imagine a pet. What attributes does this imagined pet have? (Someone said “furry”). Now imagine a fish. What color does it have? (Someone answered “gray”). Finally, imagine a pet fish - does it combine the properties of the prototypical pet and fish? It does not - indeed, most people imagine a pet fish as neither furry nor gray, but red.</p> <p>The pet fish problem is a very simple and intuitive way to communicate a fundamental problem in compositional logic: Prototypical concepts are not compositional (under the logic of typicality $\mathcal{ALC}+\mathbf{T_R}$), and additional modeling is required to combine prototypical concepts. Abstract statements like “prototypical concepts are not compositional” mean nothing to an audience unfamiliar with formal logic or knowledge representation. Even researchers from adjacent fields require a bit of explanation to understand what this problem actually <em>means</em>, and how such an abstract problem can manifest in the real world. The pet fish problem is elegant because it grounds an abstract research question - how to combine prototypical concepts - in the real world, and uses real-world anchors (“pet” and “fish”) which everyone in the audience is familiar with.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pet_fish/zebra_shark.png" sizes="95vw"/> <img src="/assets/img/pet_fish/zebra_shark.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A zebra shark, according to <a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a> </div> <p>I only understood the ultimate power of the pet fish problem as a science communication device when several hours after the talk, I randomly thought of the pet fish problem again. This usually doesn’t happen with scientific concepts (at least not to me): Most scientific content I consume ends up in a type of long-term associative memory buffer, from where it can be retrieved when something else reminds me of it (a keyword, a formula, an image, …). Randomly thinking of the pet fish reinforced the concept again, and I am now more likely to remember the non-compositionality of prototypical concepts long-term. I randomly thought of the pet fish problem because I found it funny, because it had a ring to it, because it triggered an uncanny sense of dissonance: The pet fish problem as an illustrative device is a great <strong>symbol</strong>. It is memorable. This was reinforced by Dr. Lieto’s use of great AI-generated images of “zebra sharks” and other hybrid animals, where features (stripes, legs, shark-like heads) are combined in ways completely uncanny to the human observer.</p> <p>The pet fish problem is more than a visual and mental symbol: It is also a great <strong>slogan</strong>. Dr. Lieto referred to the pet fish problem repeatedly during his talk, and used it (in this exact phrasing: “the pet fish problem”) to ground complex arguments about TCL in a way relatable to the audience. <a href="https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)">Reinforcement by repetition</a> is a powerful rhetorical device, and repeating core ideas or examples over the course of a talk helps to “bring back” the audience, even when attention may have been drifting. But the repetition of catchy terms, particularly terms attached to a symbol, creates a slogan which will stick in the listener’s <a href="https://pubmed.ncbi.nlm.nih.gov/15779526/">long-term memory</a>, even if they don’t consciously repeat it themselves.</p> <h2 id="every-researcher-needs-a-pet-fish">Every Researcher Needs a Pet Fish</h2> <p>Dr. Lieto’s use of the pet fish problem in his talk was a tour de force of science communication: It raised the level of abstraction to a level at which every member of the audience could follow; it grounded abstract concepts in familiar real-world objects; it created a memorable visual symbol, providing an anchor for later retrieval; and it was, by way of repetition, transformed into a verbal slogan, reinforcing the concept yet again through audio-visual synaesthesia. Every researcher should find his own pet fish problem.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Two of the five S in <a href="https://youtu.be/Unzc731iCUY?t=3012">Winston’s star</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p><em>Typical</em> or <em>prototypical</em> knowledge is the association between a concept (e.g. “dog”) and the properties or class relationships which <em>commonly hold</em> for this concept in a given (e.g. cultural) context. The prototypical dog, for example, is furry and has a tail. Prototypical knowledge does not apply to all individuals of a category (there are black, blonde and white dogs, and certain breeds of dogs don’t have tails), and heavily depends on socio-cultural context. Nevertheless, prototypical knowledge is highly useful as a baseline for commonsense reasoning in the absence of additional information. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog"/><category term="science-communication"/><summary type="html"><![CDATA[Why researchers should care about science communication, and how to use symbols and slogans to make a talk memorable.]]></summary></entry><entry><title type="html">Knowledge Representation &amp;amp; Reasoning in Industrial Robotics</title><link href="https://benjaminalt.github.io/blog/2022/krr-in-industrial-robotics/" rel="alternate" type="text/html" title="Knowledge Representation &amp;amp; Reasoning in Industrial Robotics"/><published>2022-09-23T01:00:00+00:00</published><updated>2022-09-23T01:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2022/krr-in-industrial-robotics</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2022/krr-in-industrial-robotics/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" style="display: flex; justify-content: center;"> <figure> <iframe src="https://youtube.com/embed/7oP4aU44jic" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> <p>A lecture I gave at the 2022 EASE Fall School in Bremen, Germany. The lecture covers important open problems in industrial robotics and motivates the use of knowledge-augmented learning methods to build self-learning robotic systems for industrial applications.</p> <p>The lecture was given on Sep. 23, 2022 at the Institute for Artificial Intelligence at the University of Bremen, Germany.</p>]]></content><author><name></name></author><category term="lecture"/><category term="ai"/><category term="robot-learning"/><category term="robotics"/><category term="talk"/><summary type="html"><![CDATA[A lecture I gave at the 2022 EASE Fall School in Bremen, Germany, on the combination of explicit knowledge representation, reasoning and deep learning for industrial robotics.]]></summary></entry><entry><title type="html">ICRA 2021 Reading List</title><link href="https://benjaminalt.github.io/blog/2021/icra-reading-list/" rel="alternate" type="text/html" title="ICRA 2021 Reading List"/><published>2021-06-03T01:00:00+00:00</published><updated>2021-06-03T01:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2021/icra-reading-list</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2021/icra-reading-list/"><![CDATA[<p>This is my personal post-ICRA 2021 reading list with papers I came across while attending the conference and which I particularly want to read. It is also intended as a resource for my colleagues who did not attend ICRA this year.</p> <p>Most papers will be about task- or motion-level robot learning, but many intersect with other domains as well. If you presented a paper at ICRA you think I would like and it is not on this list, please <a href="mailto:benjamin.alt@uni-bremen.de">send me an email</a> and I promise to read it!</p> <p><strong>Highlighted</strong> are papers I read (or attended the presentation) and found especially insightful. This list is subject to change as I read my way through it or add more from the proceedings.</p> <div class="publications"> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li><div class="row"> <div id="kulak_active_2021" class="col-sm-10"> <div class="title" style="font-weight: bold;">Active Learning of Bayesian Probabilistic Movement Primitives</div> <div class="author"> Thibaut Kulak,&nbsp;Hakan Girgin,&nbsp;Jean-Marc Odobez, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sylvain Calinon' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, Apr 2021 </div> <div class="links"> <a href="https://doi.org/10.1109/LRA.2021.3060414" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> </div> </div> </li> <li><div class="row"> <div id="du_auto-tuned_2021" class="col-sm-10"> <div class="title">Auto-Tuned Sim-to-Real Transfer</div> <div class="author"> Yuqing Du,&nbsp;Olivia Watkins,&nbsp;Trevor Darrell, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Pieter Abbeel, Deepak Pathak' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2104.07662 [cs]</em>, May 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="driess_learning_2021" class="col-sm-10"> <div class="title">Learning Geometric Reasoning and Control for Long-Horizon Tasks from Visual Input</div> <div class="author"> Danny Driess,&nbsp;Jung-Su Ha,&nbsp;Russ Tedrake, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Marc Toussaint' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2021 IEEE International Conference on Robotics and Automation (ICRA)</em>, May 2021 </div> <div class="links"> <a href="https://doi.org/10.1109/ICRA48506.2021.9560934" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> </div> </div> </li> <li><div class="row"> <div id="li_reactive_2021" class="col-sm-10"> <div class="title" style="font-weight: bold;">Reactive Task and Motion Planning under Temporal Logic Specifications</div> <div class="author"> Shen Li,&nbsp;Daehyung Park,&nbsp;Yoonchang Sung, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Julie A. Shah, Nicholas Roy' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2103.14464 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="malla_social-stage_2021" class="col-sm-10"> <div class="title">Social-STAGE: Spatio-Temporal Multi-Modal Future Trajectory Forecast</div> <div class="author"> Srikanth Malla,&nbsp;Chiho Choi,&nbsp;and&nbsp;Behzad Dariush </div> <div class="periodical"> <em>arXiv:2011.04853 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="tosatto_contextual_2021" class="col-sm-10"> <div class="title" style="font-weight: bold;">Contextual Latent-Movements Off-Policy Optimization for Robotic Manipulation Skills</div> <div class="author"> Samuele Tosatto,&nbsp;Georgia Chalvatzaki,&nbsp;and&nbsp;Jan Peters </div> <div class="periodical"> <em>arXiv:2010.13766 [cs]</em>, May 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="sundermeyer_contact-graspnet_2021" class="col-sm-10"> <div class="title">Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes</div> <div class="author"> Martin Sundermeyer,&nbsp;Arsalan Mousavian,&nbsp;Rudolph Triebel, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Dieter Fox' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv:2103.14127 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="dinev_sparsity-inducing_2021" class="col-sm-10"> <div class="title">Sparsity-Inducing Optimal Control via Differential Dynamic Programming</div> <div class="author"> Traiko Dinev,&nbsp;Wolfgang Merkt,&nbsp;Vladimir Ivan, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Ioannis Havoutis, Sethu Vijayakumar' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2011.07325 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="luo_self-imitation_2021" class="col-sm-10"> <div class="title">Self-Imitation Learning by Planning</div> <div class="author"> Sha Luo,&nbsp;Hamidreza Kasaei,&nbsp;and&nbsp;Lambert Schomaker </div> <div class="periodical"> <em>arXiv:2103.13834 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="pairet_path_2021" class="col-sm-10"> <div class="title" style="font-weight: bold;">Path Planning for Manipulation using Experience-driven Random Trees</div> <div class="author"> Èric Pairet,&nbsp;Constantinos Chamzas,&nbsp;Yvan Petillot, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Lydia E. Kavraki' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Robot. Autom. Lett.</em>, Apr 2021 </div> <div class="links"> <a href="https://doi.org/10.1109/LRA.2021.3063063" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> </div> </div> </li> <li><div class="row"> <div id="wen_end--end_2021" class="col-sm-10"> <div class="title">End-To-End Semi-supervised Learning for Differentiable Particle Filters</div> <div class="author"> Hao Wen,&nbsp;Xiongjie Chen,&nbsp;Georgios Papagiannis, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Conghui Hu, Yunpeng Li' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2011.05748 [cs, stat]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="wulfmeier_representation_2021" class="col-sm-10"> <div class="title">Representation Matters: Improving Perception and Exploration for Robotics</div> <div class="author"> Markus Wulfmeier,&nbsp;Arunkumar Byravan,&nbsp;Tim Hertweck, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Irina Higgins, Ankush Gupta, Tejas Kulkarni, Malcolm Reynolds, Denis Teplyashin, Roland Hafner, Thomas Lampe, Martin Riedmiller' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>arXiv:2011.01758 [cs, stat]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="liu_deep_2021" class="col-sm-10"> <div class="title">Deep Structured Reactive Planning</div> <div class="author"> Jerry Liu,&nbsp;Wenyuan Zeng,&nbsp;Raquel Urtasun, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Ersin Yumer' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv:2101.06832 [cs]</em>, Apr 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="chen_batch_2021" class="col-sm-10"> <div class="title">Batch Exploration with Examples for Scalable Robotic Reinforcement Learning</div> <div class="author"> Annie S. Chen,&nbsp;HyunJi Nam,&nbsp;Suraj Nair, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Chelsea Finn' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Robot. Autom. Lett.</em>, Jul 2021 </div> <div class="links"> <a href="https://doi.org/10.1109/LRA.2021.3068655" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> </div> </div> </li> <li><div class="row"> <div id="morgan_model_2021" class="col-sm-10"> <div class="title">Model Predictive Actor-Critic: Accelerating Robot Skill Acquisition with Deep Reinforcement Learning</div> <div class="author"> Andrew S. Morgan,&nbsp;Daljeet Nandha,&nbsp;Georgia Chalvatzaki, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Carlo D’Eramo, Aaron M. Dollar, Jan Peters' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>arXiv:2103.13842 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="lee_causal_2021" class="col-sm-10"> <div class="title">Causal Reasoning in Simulation for Structure and Transfer Learning of Robot Manipulation Policies</div> <div class="author"> Timothy E. Lee,&nbsp;Jialiang Zhao,&nbsp;Amrita S. Sawhney, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Siddharth Girdhar, Oliver Kroemer' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2103.16772 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="allshire_laser_2021" class="col-sm-10"> <div class="title">LASER: Learning a Latent Action Space for Efficient Reinforcement Learning</div> <div class="author"> Arthur Allshire,&nbsp;Roberto Martín-Martín,&nbsp;Charles Lin, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Shawn Manuel, Silvio Savarese, Animesh Garg' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>arXiv:2103.15793 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="power_keep_2021" class="col-sm-10"> <div class="title" style="font-weight: bold;">Keep it Simple: Data-efficient Learning for Controlling Complex Systems with Simple Models</div> <div class="author"> Thomas Power,&nbsp;and&nbsp;Dmitry Berenson </div> <div class="periodical"> <em>arXiv:2102.02493 [cs]</em>, Feb 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="boggess_towards_2021" class="col-sm-10"> <div class="title">Towards Personalized Explanation of Robot Path Planning via User Feedback</div> <div class="author"> Kayla Boggess,&nbsp;Shenghui Chen,&nbsp;and&nbsp;Lu Feng </div> <div class="periodical"> <em>arXiv:2011.00524 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="johns_coarse--fine_2021" class="col-sm-10"> <div class="title">Coarse-to-Fine Imitation Learning: Robot Manipulation from a Single Demonstration</div> <div class="author"> Edward Johns </div> <div class="periodical"> <em>arXiv:2105.06411 [cs]</em>, May 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="sathya_weighted_2021" class="col-sm-10"> <div class="title">A Weighted Method for Fast Resolution of Strictly Hierarchical Robot Task Specifications Using Exact Penalty Functions</div> <div class="author"> Ajay Suresha Sathya,&nbsp;Goele Pipeleers,&nbsp;Wilm Decré, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jan Swevers' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, Apr 2021 </div> <div class="links"> <a href="https://doi.org/10.1109/LRA.2021.3063026" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> </div> </div> </li> <li><div class="row"> <div id="sidiropoulos_human-robot_2021" class="col-sm-10"> <div class="title">Human-robot collaborative object transfer using human motion prediction based on Cartesian pose Dynamic Movement Primitives</div> <div class="author"> Antonis Sidiropoulos,&nbsp;Yiannis Karayiannidis,&nbsp;and&nbsp;Zoe Doulgeri </div> <div class="periodical"> <em>arXiv:2104.03155 [cs]</em>, Apr 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="lechner_adversarial_2021" class="col-sm-10"> <div class="title">Adversarial Training is Not Ready for Robot Learning</div> <div class="author"> Mathias Lechner,&nbsp;Ramin Hasani,&nbsp;Radu Grosu, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Daniela Rus, Thomas A. Henzinger' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2103.08187 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="ha_distilling_2021" class="col-sm-10"> <div class="title">Distilling a Hierarchical Policy for Planning and Control via Representation and Reinforcement Learning</div> <div class="author"> Jung-Su Ha,&nbsp;Young-Jin Park,&nbsp;Hyeok-Joo Chae, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Soon-Seo Park, Han-Lim Choi' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2011.08345 [cs]</em>, Apr 2021 </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li><div class="row"> <div id="haidu_automated_2020" class="col-sm-10"> <div class="title" style="font-weight: bold;">Automated acquisition of structured, semantic models of manipulation activities from human VR demonstration</div> <div class="author"> Andrei Haidu,&nbsp;and&nbsp;Michael Beetz </div> <div class="periodical"> <em>arXiv:2011.13689 [cs]</em>, Nov 2020 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="ho_retinagan_2020" class="col-sm-10"> <div class="title">RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer</div> <div class="author"> Daniel Ho,&nbsp;Kanishka Rao,&nbsp;Zhuo Xu, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Eric Jang, Mohi Khansari, Yunfei Bai' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>arXiv:2011.03148 [cs]</em>, Nov 2020 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="meyer_laserflow_2020" class="col-sm-10"> <div class="title">LaserFlow: Efficient and Probabilistic Object Detection and Motion Forecasting</div> <div class="author"> Gregory P. Meyer,&nbsp;Jake Charland,&nbsp;Shreyash Pandey, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Ankit Laddha, Shivam Gautam, Carlos Vallespi-Gonzalez, Carl K. Wellington' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>arXiv:2003.05982 [cs]</em>, Oct 2020 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="akbulut_reward_2020" class="col-sm-10"> <div class="title">Reward Conditioned Neural Movement Primitives for Population Based Variational Policy Optimization</div> <div class="author"> M. Tuluhan Akbulut,&nbsp;Utku Bozdogan,&nbsp;Ahmet Tekden, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Emre Ugur' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv:2011.04282 [cs]</em>, Nov 2020 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="cioflan_ms-ranas_2020" class="col-sm-10"> <div class="title">MS-RANAS: Multi-Scale Resource-Aware Neural Architecture Search</div> <div class="author"> Cristian Cioflan,&nbsp;and&nbsp;Radu Timofte </div> <div class="periodical"> <em>arXiv:2009.13940 [cs]</em>, Sep 2020 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="bechtle_leveraging_2020" class="col-sm-10"> <div class="title" style="font-weight: bold;">Leveraging Forward Model Prediction Error for Learning Control</div> <div class="author"> Sarah Bechtle,&nbsp;Bilal Hammoud,&nbsp;Akshara Rai, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Franziska Meier, Ludovic Righetti' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2011.03859 [cs]</em>, Nov 2020 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="wu_shaping_2020" class="col-sm-10"> <div class="title">Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models</div> <div class="author"> Yuchen Wu,&nbsp;Melissa Mozifian,&nbsp;and&nbsp;Florian Shkurti </div> <div class="periodical"> <em>arXiv:2011.01298 [cs]</em>, Nov 2020 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="lutter_differentiable_2020" class="col-sm-10"> <div class="title">Differentiable Physics Models for Real-world Offline Model-based Reinforcement Learning</div> <div class="author"> Michael Lutter,&nbsp;Johannes Silberbauer,&nbsp;Joe Watson, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jan Peters' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv:2011.01734 [cs]</em>, Nov 2020 </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li><div class="row"> <div id="lee_ikea_2019" class="col-sm-10"> <div class="title">IKEA Furniture Assembly Environment for Long-Horizon Complex Manipulation Tasks</div> <div class="author"> Youngwoon Lee,&nbsp;Edward S. Hu,&nbsp;Zhengyu Yang, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Alex Yin, Joseph J. Lim' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:1911.07246 [cs]</em>, Nov 2019 </div> <div class="links"> </div> </div> </div> </li></ol> </div>]]></content><author><name></name></author><category term="reading-list"/><category term="reading-list"/><category term="robotics"/><summary type="html"><![CDATA[My personal post-ICRA 2021 reading list with papers I came across while attending the conference and which I particularly want to read. It is also intended as a resource for my colleagues who did not attend ICRA this year.]]></summary></entry><entry><title type="html">Robot Program Parameter Inference via Differentiable Shadow Program Inversion</title><link href="https://benjaminalt.github.io/blog/2021/shadow-program-inversion/" rel="alternate" type="text/html" title="Robot Program Parameter Inference via Differentiable Shadow Program Inversion"/><published>2021-03-06T01:00:00+00:00</published><updated>2021-03-06T01:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2021/shadow-program-inversion</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2021/shadow-program-inversion/"><![CDATA[<p>This is the companion blog post to our 2021 ICRA paper. You can find more on the paper (including a full-text version) <a href="/spi">here</a>.</p> <p>Programming industrial robots to perform complex force-controlled tasks is challenging and time-consuming when done by hand. <em>Skill-based robot programming</em> is an established paradigm across industrial and service robotics, where commonly used robot actions are encapsulated as robot skills which must only be learned or programmed once and can then be reused or combined to higher-level skills. This makes the creation of an initial robot program much easier, but leaves the problem of <em>skill parameterization</em>: For each skill in a program, the robot programmer must find a suitable set of parameters to adapt that skill to the concrete task and environment at hand.</p> <p>In our paper, we present Shadow Program Inversion (SPI), a novel approach to infer optimal skill parameters directly from data. SPI leverages unsupervised learning to train an auxiliary differentiable program representation (“shadow program”) and realizes parameter inference via gradient-based model inversion. Our method enables the use of efficient first-order optimizers to infer optimal parameters for originally non-differentiable skills, including many skill variants currently used in production. SPI zero-shot generalizes across task objectives, meaning that shadow programs do not need to be retrained to infer parameters for different task variants.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spi/overview.png" sizes="95vw"/> <img src="/assets/img/spi/overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> SPI decomposes robot program parameter inference into unsupervised learning of a composable, differentiable model of robot skills (top) followed by gradient-based inversion of the learned model (bottom). </div> <p>This brings the benefits of differentiable programming (\(\partial P\)) - the possibility to optimize program parameters with gradient-based optimizers such as Stochastic Gradient Descent - to skill-based robot programming.</p> <h2 id="core-idea-splitting-the-optimization-problem">Core Idea: Splitting the Optimization Problem</h2> <p>Optimizing program parameters is hard: Typically, the space of possible values is high-dimensional (programs can have arbitrarily many parameters) and continuous (most program parameters, such as target forces or grasp points, are real-valued). Iterative gradient-free optimizers such as Evolutionary Algorithms require the repeated execution of the program with candidate parameters (to determine e.g. the fitness value), which is very time-consuming and potentially dangerous in production environments. Supervised learning (i.e. training a neural network to directly compute the optimal parameters <strong>\(x^*\)</strong>) requires labelled training data, which implies that the optimal parameters would have to be known in advance - eliminating the need for optimization in the first place.</p> <p><strong>To make parameter optimization tractable, we use a model-based approach and split the hard problem of optimizing program parameters into the much easier problem of first learning a <em>model</em> of the program (a mapping from program inputs to outputs, i.e. robot trajectories), which we design to be easily invertible, and then inverting that model to obtain the optimal inputs.</strong> In other words, we turn a hard inverse problem into an easy forward problem and an easy inverse problem.</p> <h3 id="the-forward-problem-model-learning">The Forward Problem: Model Learning</h3> <p>Unlike optimizing program parameters, learning a forward model of a robot program only requires input-output training tuples <strong>\((x, Y)\)</strong>, which can be collected by simply observing what the robot does when the program is given inputs <strong>\(x\)</strong>. Crucially, the <em>optimal</em> parameters don’t need to be known. Data can be collected in an entirely unsupervised fashion, by randomly sampling <strong>\(x\)</strong> and observing the resulting trajectories <strong>\(Y\)</strong>. In industrial scenarios, this enables the efficient use of existing robot data, collected and persisted e.g. by IIoT platforms.</p> <h3 id="the-inverse-problem-parameter-optimization">The Inverse Problem: Parameter Optimization</h3> <p>By design, the learned models are fully differentiable (see <a href="#shadow-skills">Shadow Skills</a>). This allows for the use of gradient-based optimizers to effectively <em>invert</em> the model: To compute the set of parameters <strong>\(x^*\)</strong> which maximize some user objective <strong>\(\mathcal{G}({Y})\)</strong> over the robot trajectory. We do this by iterative gradient descent in the input space of the model - we iteratively use our model to predict <strong>\(Y\)</strong>, evaluate <strong>\(\mathcal{G}({Y})\)</strong> and use PyTorch’s <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">Autograd</a> engine to compute gradients \(\frac{\partial{\mathcal{L}(Y)}}{\partial{x}}\), which we use to nudge <strong>\(x\)</strong> toward maximizing <strong>\(\mathcal{G}({Y})\)</strong>.</p> <p>By eschewing direct optimization of the parameters in favor of learning a differentiable model of the program and performing parameter optimization via gradient descent over its input space, the joint optimization of very large numbers of program parameters becomes tractable without requiring labelled training data, supervised learning or reinforcement learning.</p> <h2 id="shadow-skills">Shadow Skills</h2> <p>Our learnable forward model of robot programs is highly modular and composable, reflecting both the innate structure of most tasks and the typical hierarchical structure of most robot programming frameworks. A general-purpose differentiable and “shadow skill” architecture must meet three general requirements:</p> <ul> <li>It must be <strong>implementation-agnostic</strong> with respect to the skill it represents, i.e. it must be capable of accurately representing possibly non-differentiable robot skills, and even hand-written skills in a low-level robot programming language such as KRL or URScript.</li> <li>It must have a <strong>learnable</strong> component which can learn the non-deterministic aspects of skills, such as the forces and torques resulting from interactions with the environment.</li> <li>It must be <strong>chainable</strong> to represent complex robot programs composed of multiple skills.</li> </ul> <p>To meet these requirements, we designed a hybrid robot skill architecture which <strong>combines a differentiable motion planner</strong> (an implementation of a motion planner in a \(\partial P\) framework such as PyTorch) <strong>with a recurrent neural network, and wrap both components in a differentiable computational graph.</strong> The combination of a differentiable implementation of a traditional planner with a trainable neural network permits the architecture to represent a wide variety of skills, including complex force-controlled skills, in a data-efficient way. Echoing recent <a href="http://proceedings.mlr.press/v80/garnelo18a.html">related</a> <a href="https://arxiv.org/pdf/1805.11122.pdf">work</a>, we find that the explicit inclusion of domain knowledge in the form of an algorithmic prior greatly reduces the amount of data and compute required to learn complex skills.</p> <p>To the outside, every shadow skill provides the same interface: All skills accept a parameter vector <strong>\(x\)</strong> containing the skill parameters (which we want to optimize) and a state vector <strong>\(s_{in}\)</strong> describing the current state of the robot and the environment, and produce an expected trajectory <strong>\(\hat{Y}\)</strong> and a final state <strong>\(s_{out}\)</strong>. Because both the differentiable motion planner and the neural network are differentiable computational graphs, a shadow skill is also a differentiable computational graph whose outputs \(\hat{Y}\) and \(s_{out}\) are differentiable w.r.t. its inputs \(x\), conditional on the initial state \(s_{in}\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spi/shadow_skill.png" sizes="95vw"/> <img src="/assets/img/spi/shadow_skill.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A Shadow Skill as part of a larger Shadow Program. </div> <p><strong>This skill architecture can be used to learn new skills from scratch (e.g. from human demonstrations), but its most useful application is modeling existing skills from different frameworks.</strong> There are many existing, highly expressive robot program representations which are used in production. Some of them are highly domain-specific (e.g. optimized for welding applications), others are hardware-specific (e.g. the KRL or URScript textual programming languages), and yet others are based on differential equations (DMPs, ProMPs and variants). Our architecture is designed with the aim of being capable of learning to model skills from any such framework or language. Instead of forcing robot programmers to program their robots a priori in a differentiable language, we designed our skill architecture to be sufficiently flexible to learn the <em>execution semantics</em> (input-output relationships, “what happens when the skill is executed”) of any existing skill from any skill framework or robot programming language.</p> <p>Training a Shadow Skill for an existing robot skill is easy: Simply sample skill inputs <strong>\(x\)</strong>, observe the resulting robot trajectories <strong>\(Y\)</strong> and train the learnable Shadow Skill components (namely, the RNN) to predict <strong>\(Y\)</strong> given <strong>\(x\)</strong>.</p> <h2 id="shadow-programs">Shadow Programs</h2> <p>While our framework can be used to infer optimal parameters for individual robot skills, many real-world applications require the combination of several skills to achieve more complex goals. To take a cookie out of a jar, for instance, a robot must first open the jar, grasp the cookie and remove the cookie from the jar. To open the jar, the robot must in turn open its gripper, approach the lid, close its gripper around the lid, twist the lid and depart, etc. Most real-world tasks are intrinsically hierarchical and can be broken down into sequences of primitive actions. Consequently, most robot programming frameworks are themselves hierarchical, and allow the composition of complex tasks from sequences of primitive skills. By extension, optimizing the parameters of a robot program requires the <strong>joint</strong> optimization of the parameters of its constituent skills: The values of skill parameters at the beginning of the program cause changes to the state of the world and the robot, which may influence the optimal parameter values later in the program.</p> <p>End-to-end learning addresses this by learning one large model for the entire program. In practice this is not tractable, as learning to model highly complex programs requires substantial computing power and amounts of training data far in excess of what can be collected on real robots in industrial settings. <strong>We propose to train individual Shadow Skills in isolation, and to combine them afterward to complex programs.</strong> Training Shadow Skills in isolation allows for the building of a trained skill library, which only needs to be done once, and skills don’t need to be re-trained to be combined into new programs. If you are a robot programmer and want to optimize your program parameters, this is great news: You only need to train <em>one</em> Shadow Skill for each skill in your skill library, and are instantly able to optimize the parameters of any program you can compose.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spi/shadow_program.png" sizes="95vw"/> <img src="/assets/img/spi/shadow_program.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A Shadow Program composed of two Shadow Skills. Just as Shadow Skills are models for individual robot skills, Shadow Programs model complex robot programs (left to right) and allow for the joint optimization of program parameters via gradient descent (right to left). </div> <p>Because the individual Shadow Skills are forward models of robot skills, their combination to a Shadow Program constitutes a forward model of a robot program. Just as Shadow Skills are differentiable computation graphs, a Shadow Model is a differentiable computation graph. Via gradient descent in the input space, the parameters of all skills in the program can be jointly optimized w.r.t. some function of the expected behavior of the robot, such as cycle time, parts per minute (PPM) or the forces experienced on contact with an object.</p> <h2 id="applications">Applications</h2> <p>We applied SPI to several very different parameter inference problems in industrial and service robotics. In one experiment, we inferred the parameters of an robot program in the <a href="https://www.artiminds.com/">ArtiMinds ARTM programming language</a> to pick up a glass and deposit it in a sink from a single human demonstration in Virtual Reality (VR). In other experiments, we use SPI to optimize the parameters of ARTM programs, URScript programs and DMPs for force-sensitive contact motions with materials of different spring and damping characteristics. In a final experiment, we optimize the parameters of force-controlled spiral search motions to find the position of a hole on a PCB quickly, robustly or efficiently, respectively. Our experiments indicate that SPI is a highly versatile alternative to end-to-end supervised or reinforcement learning, particularly in industrial settings, as it is capable to infer optimal parameters for any existing programming framework ranging from textual programming languages like URScript to symbolic representations like ARTM or subsymbolic skill representations like DMPs. By learning an auxiliary representation (the Shadow Skills) for optimization, <strong>SPI can retrofit these established robot programming frameworks which are widely used (but neither differentiable nor learnable) with differentiable programming, and by extension learning and gradient-based optimization.</strong> This property is crucial for real-world application in the industry, as robot programmers can continue using the languages they know and like, without foregoing the benefits of differentiability and learning.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" style="display: flex; justify-content: center;"> <figure> <iframe src="https://www.youtube.com/embed/GwQdFN5lmLk" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>One caveat is that robot skills typically cannot be learned independent from the state of the world (such as the positions of objects in the environment) at the time of their execution. This implies that some fine-tuning on the current state of the world will be required, at least when skills (or programs) are transferred from one environment to another. We are currently investigating methods to do this efficiently. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog"/><category term="differentiable-programming"/><category term="robot-learning"/><category term="ml"/><category term="robotics"/><summary type="html"><![CDATA[A companion blog post to our 2021 ICRA paper on data-driven robot program parameter optimization.]]></summary></entry></feed>