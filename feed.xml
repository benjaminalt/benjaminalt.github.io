<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://benjaminalt.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://benjaminalt.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-02T09:23:58+00:00</updated><id>https://benjaminalt.github.io/feed.xml</id><title type="html">blank</title><subtitle>Roboticist &amp; AI Researcher </subtitle><entry><title type="html">A Human-Friendly Introduction to AI Safety</title><link href="https://benjaminalt.github.io/blog/2023/introduction-to-ai-safety/" rel="alternate" type="text/html" title="A Human-Friendly Introduction to AI Safety"/><published>2023-03-17T01:00:00+00:00</published><updated>2023-03-17T01:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2023/introduction-to-ai-safety</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2023/introduction-to-ai-safety/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" style="display: flex; justify-content: center;"> <figure> <iframe src="https://www.youtube.com/embed/9MUrQ1yJGCo" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> <p>An overview of some aspects of technical AI safety, which motivates and introduces the field from first principles.</p>]]></content><author><name></name></author><category term="talk"/><category term="ai"/><category term="ai-safety"/><category term="talk"/><summary type="html"><![CDATA[A talk I gave at an online LessWrong Meetup in Karlsruhe, Germany, covering the basics of technical AI safety.]]></summary></entry><entry><title type="html">A(G)I 2022: A Year in Review</title><link href="https://benjaminalt.github.io/blog/2023/agi-2022-a-year-in-review/" rel="alternate" type="text/html" title="A(G)I 2022: A Year in Review"/><published>2023-01-23T01:00:00+00:00</published><updated>2023-01-23T01:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2023/agi-2022-a-year-in-review</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2023/agi-2022-a-year-in-review/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" style="display: flex; justify-content: center;"> <figure> <iframe src="https://www.youtube.com/embed/GLgjKpQbaSg" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> <p>A review of AI progress in 2022, with a focus on emerging and changing narratives in AI capabilities and AI safety.</p> <p>The talk was given on Jan. 22, 2023 at the Effective Altruism Retreat in Sigmaringen, Germany.</p>]]></content><author><name></name></author><category term="talk"/><category term="ai"/><category term="ai-safety"/><category term="talk"/><summary type="html"><![CDATA[A talk I gave at the 2023 Effective Altruism Retreat in Sigmaringen, Germany, outlining how pivotal AI technologies shifted narratives around AI and its impacts on society.]]></summary></entry><entry><title type="html">Every Researcher Needs a Pet Fish</title><link href="https://benjaminalt.github.io/blog/2022/every-researcher-needs-a-pet-fish/" rel="alternate" type="text/html" title="Every Researcher Needs a Pet Fish"/><published>2022-09-25T01:00:00+00:00</published><updated>2022-09-25T01:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2022/every-researcher-needs-a-pet-fish</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2022/every-researcher-needs-a-pet-fish/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pet_fish/pet_fish.png" sizes="95vw"/> <img src="/assets/img/pet_fish/pet_fish.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A pet fish, according to <a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a> </div> <p>I’ve been learning a lot about science communication lately. This journey started at this year’s <a href="https://interdisciplinary-college.org">Interdisciplinary College (IK)</a>, where I participated in a science communication workshop organized by <a href="http://jenssteffenscherer.com/">Jens-Steffen Scherer</a>, Valerie Vaquet and Kayson Fahar. The workshop reinforced my prior intuition that science communication is immensely important, particularly for PhD students, but also that much of my daily work already consisted of science communication, even if I didn’t realize it.</p> <h2 id="science-communication-what-it-is-and-why-it-is-important">Science Communication: What It Is and Why It Is Important</h2> <p>Every PhD student or researcher, at university or in private companies, constantly communicates about what it is they are working on and why it is important. At university, this includes casual conversations with other PhD students, professors, collaborators from other departments or lab visitors. In a corporate setting, industry PhDs are additionally faced with communicating their research to stakeholders outside of academia, such as work colleagues, supervisors, budget officers or clients. Even more than traditional PhDs, industry PhD students constantly navigate the gap between research and application, and constantly communicate what they do and why it is important to non-researchers inside and outside their organization. <strong>This is science communication.</strong> Pitching one’s research agenda to one’s boss is science communication. Presenting early-stage results to project partners is science communication. Giving talks at academic conferences is science communication. Chatting about one’s research over lunch - science communication. I’ve been doing it all along.</p> <p>Getting good at science communication - talking to others about what one does and why it is important - has a high expected payoff. The greatest part of this payoff stems from external stakeholders gaining a better intuitive understanding of the (potentially very complex) research problems that are being addressed; and, crucially, <em>why they should care about them</em>. It is hard enough for researchers to descend into the murky depths of maths and data for weeks at a time in the pursuit of some fundamental research question, and to remember this fundamental research question once they resurface. They only do so because they have been trained to, and because their experienced mentors remind them of it every once in a while. Take external stakeholders, colleagues from different subfields or the audience at an academic conference to the same murky depths along the same paths, and they will assuredly get lost.</p> <p>As a consequence, I started to tweak the way I communicate about my research. I found that getting the level of abstraction right - expressing ideas in high-level concepts such as “learning”, “optimization”, “robot motion” by default, and using more technical concepts such as “gradient descent”, “differentiable programming”, “inverse kinematics” only when necessary, and always with appropriate visual aids - makes a big difference in how effectively I can convey arguments and results, particularly to audiences outside of my field. But communicating at the right level of abstraction only serves to make arguments more <em>understandable</em>; it does not help make them more <em>memorable</em>. For that, <strong>symbols and slogans</strong><sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> are much more effective.</p> <h2 id="the-pet-fish-problem">The Pet Fish Problem</h2> <p>I first learned about the pet fish problem in <a href="https://www.antoniolieto.net/">Antonio Lieto</a>’s talk on TCL at the <a href="https://ease-crc.org/ease-fall-school-2022/">2022 EASE Fall School</a>. <a href="https://www.antoniolieto.net/tcl_logic.html">TCL (typicality-based compositional logic)</a> is a formal logic for combining (proto-)typical knowledge<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> of existing concepts into new concepts. To illustrate what typicality-based compositional logic is and why it is important, Prof. Lieto gave the following example: Close your eyes, and imagine a pet. What attributes does this imagined pet have? (Someone said “furry”). Now imagine a fish. What color does it have? (Someone answered “gray”). Finally, imagine a pet fish - does it combine the properties of the prototypical pet and fish? It does not - indeed, most people imagine a pet fish as neither furry nor gray, but red.</p> <p>The pet fish problem is a very simple and intuitive way to communicate a fundamental problem in compositional logic: Prototypical concepts are not compositional (under the logic of typicality $\mathcal{ALC}+\mathbf{T_R}$), and additional modeling is required to combine prototypical concepts. Abstract statements like “prototypical concepts are not compositional” mean nothing to an audience unfamiliar with formal logic or knowledge representation. Even researchers from adjacent fields require a bit of explanation to understand what this problem actually <em>means</em>, and how such an abstract problem can manifest in the real world. The pet fish problem is elegant because it grounds an abstract research question - how to combine prototypical concepts - in the real world, and uses real-world anchors (“pet” and “fish”) which everyone in the audience is familiar with.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pet_fish/zebra_shark.png" sizes="95vw"/> <img src="/assets/img/pet_fish/zebra_shark.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A zebra shark, according to <a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a> </div> <p>I only understood the ultimate power of the pet fish problem as a science communication device when several hours after the talk, I randomly thought of the pet fish problem again. This usually doesn’t happen with scientific concepts (at least not to me): Most scientific content I consume ends up in a type of long-term associative memory buffer, from where it can be retrieved when something else reminds me of it (a keyword, a formula, an image, …). Randomly thinking of the pet fish reinforced the concept again, and I am now more likely to remember the non-compositionality of prototypical concepts long-term. I randomly thought of the pet fish problem because I found it funny, because it had a ring to it, because it triggered an uncanny sense of dissonance: The pet fish problem as an illustrative device is a great <strong>symbol</strong>. It is memorable. This was reinforced by Dr. Lieto’s use of great AI-generated images of “zebra sharks” and other hybrid animals, where features (stripes, legs, shark-like heads) are combined in ways completely uncanny to the human observer.</p> <p>The pet fish problem is more than a visual and mental symbol: It is also a great <strong>slogan</strong>. Dr. Lieto referred to the pet fish problem repeatedly during his talk, and used it (in this exact phrasing: “the pet fish problem”) to ground complex arguments about TCL in a way relatable to the audience. <a href="https://en.wikipedia.org/wiki/Repetition_(rhetorical_device)">Reinforcement by repetition</a> is a powerful rhetorical device, and repeating core ideas or examples over the course of a talk helps to “bring back” the audience, even when attention may have been drifting. But the repetition of catchy terms, particularly terms attached to a symbol, creates a slogan which will stick in the listener’s <a href="https://pubmed.ncbi.nlm.nih.gov/15779526/">long-term memory</a>, even if they don’t consciously repeat it themselves.</p> <h2 id="every-researcher-needs-a-pet-fish">Every Researcher Needs a Pet Fish</h2> <p>Dr. Lieto’s use of the pet fish problem in his talk was a tour de force of science communication: It raised the level of abstraction to a level at which every member of the audience could follow; it grounded abstract concepts in familiar real-world objects; it created a memorable visual symbol, providing an anchor for later retrieval; and it was, by way of repetition, transformed into a verbal slogan, reinforcing the concept yet again through audio-visual synaesthesia. Every researcher should find his own pet fish problem.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Two of the five S in <a href="https://youtu.be/Unzc731iCUY?t=3012">Winston’s star</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p><em>Typical</em> or <em>prototypical</em> knowledge is the association between a concept (e.g. “dog”) and the properties or class relationships which <em>commonly hold</em> for this concept in a given (e.g. cultural) context. The prototypical dog, for example, is furry and has a tail. Prototypical knowledge does not apply to all individuals of a category (there are black, blonde and white dogs, and certain breeds of dogs don’t have tails), and heavily depends on socio-cultural context. Nevertheless, prototypical knowledge is highly useful as a baseline for commonsense reasoning in the absence of additional information. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog"/><category term="science-communication"/><summary type="html"><![CDATA[Why researchers should care about science communication, and how to use symbols and slogans to make a talk memorable.]]></summary></entry><entry><title type="html">Knowledge Representation &amp;amp; Reasoning in Industrial Robotics</title><link href="https://benjaminalt.github.io/blog/2022/krr-in-industrial-robotics/" rel="alternate" type="text/html" title="Knowledge Representation &amp;amp; Reasoning in Industrial Robotics"/><published>2022-09-23T01:00:00+00:00</published><updated>2022-09-23T01:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2022/krr-in-industrial-robotics</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2022/krr-in-industrial-robotics/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" style="display: flex; justify-content: center;"> <figure> <iframe src="https://youtube.com/embed/7oP4aU44jic" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> <p>A lecture I gave at the 2022 EASE Fall School in Bremen, Germany. The lecture covers important open problems in industrial robotics and motivates the use of knowledge-augmented learning methods to build self-learning robotic systems for industrial applications.</p> <p>The lecture was given on Sep. 23, 2022 at the Institute for Artificial Intelligence at the University of Bremen, Germany.</p>]]></content><author><name></name></author><category term="lecture"/><category term="ai"/><category term="robot-learning"/><category term="robotics"/><category term="talk"/><summary type="html"><![CDATA[A lecture I gave at the 2022 EASE Fall School in Bremen, Germany, on the combination of explicit knowledge representation, reasoning and deep learning for industrial robotics.]]></summary></entry><entry><title type="html">ICRA 2021 Reading List</title><link href="https://benjaminalt.github.io/blog/2021/icra-reading-list/" rel="alternate" type="text/html" title="ICRA 2021 Reading List"/><published>2021-06-03T01:00:00+00:00</published><updated>2021-06-03T01:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2021/icra-reading-list</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2021/icra-reading-list/"><![CDATA[<p>This is my personal post-ICRA 2021 reading list with papers I came across while attending the conference and which I particularly want to read. It is also intended as a resource for my colleagues who did not attend ICRA this year.</p> <p>Most papers will be about task- or motion-level robot learning, but many intersect with other domains as well. If you presented a paper at ICRA you think I would like and it is not on this list, please <a href="mailto:benjamin.alt@uni-bremen.de">send me an email</a> and I promise to read it!</p> <p><strong>Highlighted</strong> are papers I read (or attended the presentation) and found especially insightful. This list is subject to change as I read my way through it or add more from the proceedings.</p> <div class="publications"> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li><div class="row"> <div id="kulak_active_2021" class="col-sm-10"> <div class="title" style="font-weight: bold;">Active Learning of Bayesian Probabilistic Movement Primitives</div> <div class="author"> Thibaut Kulak,&nbsp;Hakan Girgin,&nbsp;Jean-Marc Odobez, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sylvain Calinon' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, Apr 2021 </div> <div class="links"> <a href="https://doi.org/10.1109/LRA.2021.3060414" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> </div> </div> </li> <li><div class="row"> <div id="du_auto-tuned_2021" class="col-sm-10"> <div class="title">Auto-Tuned Sim-to-Real Transfer</div> <div class="author"> Yuqing Du,&nbsp;Olivia Watkins,&nbsp;Trevor Darrell, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Pieter Abbeel, Deepak Pathak' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2104.07662 [cs]</em>, May 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="driess_learning_2021" class="col-sm-10"> <div class="title">Learning Geometric Reasoning and Control for Long-Horizon Tasks from Visual Input</div> <div class="author"> Danny Driess,&nbsp;Jung-Su Ha,&nbsp;Russ Tedrake, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Marc Toussaint' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2021 IEEE International Conference on Robotics and Automation (ICRA)</em>, May 2021 </div> <div class="links"> <a href="https://doi.org/10.1109/ICRA48506.2021.9560934" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> </div> </div> </li> <li><div class="row"> <div id="li_reactive_2021" class="col-sm-10"> <div class="title" style="font-weight: bold;">Reactive Task and Motion Planning under Temporal Logic Specifications</div> <div class="author"> Shen Li,&nbsp;Daehyung Park,&nbsp;Yoonchang Sung, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Julie A. Shah, Nicholas Roy' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2103.14464 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="malla_social-stage_2021" class="col-sm-10"> <div class="title">Social-STAGE: Spatio-Temporal Multi-Modal Future Trajectory Forecast</div> <div class="author"> Srikanth Malla,&nbsp;Chiho Choi,&nbsp;and&nbsp;Behzad Dariush </div> <div class="periodical"> <em>arXiv:2011.04853 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="tosatto_contextual_2021" class="col-sm-10"> <div class="title" style="font-weight: bold;">Contextual Latent-Movements Off-Policy Optimization for Robotic Manipulation Skills</div> <div class="author"> Samuele Tosatto,&nbsp;Georgia Chalvatzaki,&nbsp;and&nbsp;Jan Peters </div> <div class="periodical"> <em>arXiv:2010.13766 [cs]</em>, May 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="sundermeyer_contact-graspnet_2021" class="col-sm-10"> <div class="title">Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes</div> <div class="author"> Martin Sundermeyer,&nbsp;Arsalan Mousavian,&nbsp;Rudolph Triebel, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Dieter Fox' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv:2103.14127 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="dinev_sparsity-inducing_2021" class="col-sm-10"> <div class="title">Sparsity-Inducing Optimal Control via Differential Dynamic Programming</div> <div class="author"> Traiko Dinev,&nbsp;Wolfgang Merkt,&nbsp;Vladimir Ivan, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Ioannis Havoutis, Sethu Vijayakumar' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2011.07325 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="luo_self-imitation_2021" class="col-sm-10"> <div class="title">Self-Imitation Learning by Planning</div> <div class="author"> Sha Luo,&nbsp;Hamidreza Kasaei,&nbsp;and&nbsp;Lambert Schomaker </div> <div class="periodical"> <em>arXiv:2103.13834 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="pairet_path_2021" class="col-sm-10"> <div class="title" style="font-weight: bold;">Path Planning for Manipulation using Experience-driven Random Trees</div> <div class="author"> Èric Pairet,&nbsp;Constantinos Chamzas,&nbsp;Yvan Petillot, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Lydia E. Kavraki' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Robot. Autom. Lett.</em>, Apr 2021 </div> <div class="links"> <a href="https://doi.org/10.1109/LRA.2021.3063063" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> </div> </div> </li> <li><div class="row"> <div id="wen_end--end_2021" class="col-sm-10"> <div class="title">End-To-End Semi-supervised Learning for Differentiable Particle Filters</div> <div class="author"> Hao Wen,&nbsp;Xiongjie Chen,&nbsp;Georgios Papagiannis, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Conghui Hu, Yunpeng Li' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2011.05748 [cs, stat]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="wulfmeier_representation_2021" class="col-sm-10"> <div class="title">Representation Matters: Improving Perception and Exploration for Robotics</div> <div class="author"> Markus Wulfmeier,&nbsp;Arunkumar Byravan,&nbsp;Tim Hertweck, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Irina Higgins, Ankush Gupta, Tejas Kulkarni, Malcolm Reynolds, Denis Teplyashin, Roland Hafner, Thomas Lampe, Martin Riedmiller' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>arXiv:2011.01758 [cs, stat]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="liu_deep_2021" class="col-sm-10"> <div class="title">Deep Structured Reactive Planning</div> <div class="author"> Jerry Liu,&nbsp;Wenyuan Zeng,&nbsp;Raquel Urtasun, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Ersin Yumer' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv:2101.06832 [cs]</em>, Apr 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="chen_batch_2021" class="col-sm-10"> <div class="title">Batch Exploration with Examples for Scalable Robotic Reinforcement Learning</div> <div class="author"> Annie S. Chen,&nbsp;HyunJi Nam,&nbsp;Suraj Nair, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Chelsea Finn' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Robot. Autom. Lett.</em>, Jul 2021 </div> <div class="links"> <a href="https://doi.org/10.1109/LRA.2021.3068655" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> </div> </div> </li> <li><div class="row"> <div id="morgan_model_2021" class="col-sm-10"> <div class="title">Model Predictive Actor-Critic: Accelerating Robot Skill Acquisition with Deep Reinforcement Learning</div> <div class="author"> Andrew S. Morgan,&nbsp;Daljeet Nandha,&nbsp;Georgia Chalvatzaki, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Carlo D’Eramo, Aaron M. Dollar, Jan Peters' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>arXiv:2103.13842 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="lee_causal_2021" class="col-sm-10"> <div class="title">Causal Reasoning in Simulation for Structure and Transfer Learning of Robot Manipulation Policies</div> <div class="author"> Timothy E. Lee,&nbsp;Jialiang Zhao,&nbsp;Amrita S. Sawhney, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Siddharth Girdhar, Oliver Kroemer' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2103.16772 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="allshire_laser_2021" class="col-sm-10"> <div class="title">LASER: Learning a Latent Action Space for Efficient Reinforcement Learning</div> <div class="author"> Arthur Allshire,&nbsp;Roberto Martín-Martín,&nbsp;Charles Lin, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Shawn Manuel, Silvio Savarese, Animesh Garg' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>arXiv:2103.15793 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="power_keep_2021" class="col-sm-10"> <div class="title" style="font-weight: bold;">Keep it Simple: Data-efficient Learning for Controlling Complex Systems with Simple Models</div> <div class="author"> Thomas Power,&nbsp;and&nbsp;Dmitry Berenson </div> <div class="periodical"> <em>arXiv:2102.02493 [cs]</em>, Feb 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="boggess_towards_2021" class="col-sm-10"> <div class="title">Towards Personalized Explanation of Robot Path Planning via User Feedback</div> <div class="author"> Kayla Boggess,&nbsp;Shenghui Chen,&nbsp;and&nbsp;Lu Feng </div> <div class="periodical"> <em>arXiv:2011.00524 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="johns_coarse--fine_2021" class="col-sm-10"> <div class="title">Coarse-to-Fine Imitation Learning: Robot Manipulation from a Single Demonstration</div> <div class="author"> Edward Johns </div> <div class="periodical"> <em>arXiv:2105.06411 [cs]</em>, May 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="sathya_weighted_2021" class="col-sm-10"> <div class="title">A Weighted Method for Fast Resolution of Strictly Hierarchical Robot Task Specifications Using Exact Penalty Functions</div> <div class="author"> Ajay Suresha Sathya,&nbsp;Goele Pipeleers,&nbsp;Wilm Decré, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jan Swevers' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em>, Apr 2021 </div> <div class="links"> <a href="https://doi.org/10.1109/LRA.2021.3063026" class="btn btn-sm z-depth-0" role="button">DOI</a> </div> </div> </div> </li> <li><div class="row"> <div id="sidiropoulos_human-robot_2021" class="col-sm-10"> <div class="title">Human-robot collaborative object transfer using human motion prediction based on Cartesian pose Dynamic Movement Primitives</div> <div class="author"> Antonis Sidiropoulos,&nbsp;Yiannis Karayiannidis,&nbsp;and&nbsp;Zoe Doulgeri </div> <div class="periodical"> <em>arXiv:2104.03155 [cs]</em>, Apr 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="lechner_adversarial_2021" class="col-sm-10"> <div class="title">Adversarial Training is Not Ready for Robot Learning</div> <div class="author"> Mathias Lechner,&nbsp;Ramin Hasani,&nbsp;Radu Grosu, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Daniela Rus, Thomas A. Henzinger' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2103.08187 [cs]</em>, Mar 2021 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="ha_distilling_2021" class="col-sm-10"> <div class="title">Distilling a Hierarchical Policy for Planning and Control via Representation and Reinforcement Learning</div> <div class="author"> Jung-Su Ha,&nbsp;Young-Jin Park,&nbsp;Hyeok-Joo Chae, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Soon-Seo Park, Han-Lim Choi' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2011.08345 [cs]</em>, Apr 2021 </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li><div class="row"> <div id="haidu_automated_2020" class="col-sm-10"> <div class="title" style="font-weight: bold;">Automated acquisition of structured, semantic models of manipulation activities from human VR demonstration</div> <div class="author"> Andrei Haidu,&nbsp;and&nbsp;Michael Beetz </div> <div class="periodical"> <em>arXiv:2011.13689 [cs]</em>, Nov 2020 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="ho_retinagan_2020" class="col-sm-10"> <div class="title">RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer</div> <div class="author"> Daniel Ho,&nbsp;Kanishka Rao,&nbsp;Zhuo Xu, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Eric Jang, Mohi Khansari, Yunfei Bai' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>arXiv:2011.03148 [cs]</em>, Nov 2020 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="meyer_laserflow_2020" class="col-sm-10"> <div class="title">LaserFlow: Efficient and Probabilistic Object Detection and Motion Forecasting</div> <div class="author"> Gregory P. Meyer,&nbsp;Jake Charland,&nbsp;Shreyash Pandey, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Ankit Laddha, Shivam Gautam, Carlos Vallespi-Gonzalez, Carl K. Wellington' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>arXiv:2003.05982 [cs]</em>, Oct 2020 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="akbulut_reward_2020" class="col-sm-10"> <div class="title">Reward Conditioned Neural Movement Primitives for Population Based Variational Policy Optimization</div> <div class="author"> M. Tuluhan Akbulut,&nbsp;Utku Bozdogan,&nbsp;Ahmet Tekden, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Emre Ugur' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv:2011.04282 [cs]</em>, Nov 2020 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="cioflan_ms-ranas_2020" class="col-sm-10"> <div class="title">MS-RANAS: Multi-Scale Resource-Aware Neural Architecture Search</div> <div class="author"> Cristian Cioflan,&nbsp;and&nbsp;Radu Timofte </div> <div class="periodical"> <em>arXiv:2009.13940 [cs]</em>, Sep 2020 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="bechtle_leveraging_2020" class="col-sm-10"> <div class="title" style="font-weight: bold;">Leveraging Forward Model Prediction Error for Learning Control</div> <div class="author"> Sarah Bechtle,&nbsp;Bilal Hammoud,&nbsp;Akshara Rai, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Franziska Meier, Ludovic Righetti' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2011.03859 [cs]</em>, Nov 2020 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="wu_shaping_2020" class="col-sm-10"> <div class="title">Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models</div> <div class="author"> Yuchen Wu,&nbsp;Melissa Mozifian,&nbsp;and&nbsp;Florian Shkurti </div> <div class="periodical"> <em>arXiv:2011.01298 [cs]</em>, Nov 2020 </div> <div class="links"> </div> </div> </div> </li> <li><div class="row"> <div id="lutter_differentiable_2020" class="col-sm-10"> <div class="title">Differentiable Physics Models for Real-world Offline Model-based Reinforcement Learning</div> <div class="author"> Michael Lutter,&nbsp;Johannes Silberbauer,&nbsp;Joe Watson, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jan Peters' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv:2011.01734 [cs]</em>, Nov 2020 </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li><div class="row"> <div id="lee_ikea_2019" class="col-sm-10"> <div class="title">IKEA Furniture Assembly Environment for Long-Horizon Complex Manipulation Tasks</div> <div class="author"> Youngwoon Lee,&nbsp;Edward S. Hu,&nbsp;Zhengyu Yang, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Alex Yin, Joseph J. Lim' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:1911.07246 [cs]</em>, Nov 2019 </div> <div class="links"> </div> </div> </div> </li></ol> </div>]]></content><author><name></name></author><category term="reading-list"/><category term="reading-list"/><category term="robotics"/><summary type="html"><![CDATA[My personal post-ICRA 2021 reading list with papers I came across while attending the conference and which I particularly want to read. It is also intended as a resource for my colleagues who did not attend ICRA this year.]]></summary></entry><entry><title type="html">Robot Program Parameter Inference via Differentiable Shadow Program Inversion</title><link href="https://benjaminalt.github.io/blog/2021/shadow-program-inversion/" rel="alternate" type="text/html" title="Robot Program Parameter Inference via Differentiable Shadow Program Inversion"/><published>2021-03-06T01:00:00+00:00</published><updated>2021-03-06T01:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2021/shadow-program-inversion</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2021/shadow-program-inversion/"><![CDATA[<p>This is the companion blog post to our 2021 ICRA paper. You can find more on the paper (including a full-text version) <a href="/spi">here</a>.</p> <p>Programming industrial robots to perform complex force-controlled tasks is challenging and time-consuming when done by hand. <em>Skill-based robot programming</em> is an established paradigm across industrial and service robotics, where commonly used robot actions are encapsulated as robot skills which must only be learned or programmed once and can then be reused or combined to higher-level skills. This makes the creation of an initial robot program much easier, but leaves the problem of <em>skill parameterization</em>: For each skill in a program, the robot programmer must find a suitable set of parameters to adapt that skill to the concrete task and environment at hand.</p> <p>In our paper, we present Shadow Program Inversion (SPI), a novel approach to infer optimal skill parameters directly from data. SPI leverages unsupervised learning to train an auxiliary differentiable program representation (“shadow program”) and realizes parameter inference via gradient-based model inversion. Our method enables the use of efficient first-order optimizers to infer optimal parameters for originally non-differentiable skills, including many skill variants currently used in production. SPI zero-shot generalizes across task objectives, meaning that shadow programs do not need to be retrained to infer parameters for different task variants.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spi/overview.png" sizes="95vw"/> <img src="/assets/img/spi/overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> SPI decomposes robot program parameter inference into unsupervised learning of a composable, differentiable model of robot skills (top) followed by gradient-based inversion of the learned model (bottom). </div> <p>This brings the benefits of differentiable programming (\(\partial P\)) - the possibility to optimize program parameters with gradient-based optimizers such as Stochastic Gradient Descent - to skill-based robot programming.</p> <h2 id="core-idea-splitting-the-optimization-problem">Core Idea: Splitting the Optimization Problem</h2> <p>Optimizing program parameters is hard: Typically, the space of possible values is high-dimensional (programs can have arbitrarily many parameters) and continuous (most program parameters, such as target forces or grasp points, are real-valued). Iterative gradient-free optimizers such as Evolutionary Algorithms require the repeated execution of the program with candidate parameters (to determine e.g. the fitness value), which is very time-consuming and potentially dangerous in production environments. Supervised learning (i.e. training a neural network to directly compute the optimal parameters <strong>\(x^*\)</strong>) requires labelled training data, which implies that the optimal parameters would have to be known in advance - eliminating the need for optimization in the first place.</p> <p><strong>To make parameter optimization tractable, we use a model-based approach and split the hard problem of optimizing program parameters into the much easier problem of first learning a <em>model</em> of the program (a mapping from program inputs to outputs, i.e. robot trajectories), which we design to be easily invertible, and then inverting that model to obtain the optimal inputs.</strong> In other words, we turn a hard inverse problem into an easy forward problem and an easy inverse problem.</p> <h3 id="the-forward-problem-model-learning">The Forward Problem: Model Learning</h3> <p>Unlike optimizing program parameters, learning a forward model of a robot program only requires input-output training tuples <strong>\((x, Y)\)</strong>, which can be collected by simply observing what the robot does when the program is given inputs <strong>\(x\)</strong>. Crucially, the <em>optimal</em> parameters don’t need to be known. Data can be collected in an entirely unsupervised fashion, by randomly sampling <strong>\(x\)</strong> and observing the resulting trajectories <strong>\(Y\)</strong>. In industrial scenarios, this enables the efficient use of existing robot data, collected and persisted e.g. by IIoT platforms.</p> <h3 id="the-inverse-problem-parameter-optimization">The Inverse Problem: Parameter Optimization</h3> <p>By design, the learned models are fully differentiable (see <a href="#shadow-skills">Shadow Skills</a>). This allows for the use of gradient-based optimizers to effectively <em>invert</em> the model: To compute the set of parameters <strong>\(x^*\)</strong> which maximize some user objective <strong>\(\mathcal{G}({Y})\)</strong> over the robot trajectory. We do this by iterative gradient descent in the input space of the model - we iteratively use our model to predict <strong>\(Y\)</strong>, evaluate <strong>\(\mathcal{G}({Y})\)</strong> and use PyTorch’s <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">Autograd</a> engine to compute gradients \(\frac{\partial{\mathcal{L}(Y)}}{\partial{x}}\), which we use to nudge <strong>\(x\)</strong> toward maximizing <strong>\(\mathcal{G}({Y})\)</strong>.</p> <p>By eschewing direct optimization of the parameters in favor of learning a differentiable model of the program and performing parameter optimization via gradient descent over its input space, the joint optimization of very large numbers of program parameters becomes tractable without requiring labelled training data, supervised learning or reinforcement learning.</p> <h2 id="shadow-skills">Shadow Skills</h2> <p>Our learnable forward model of robot programs is highly modular and composable, reflecting both the innate structure of most tasks and the typical hierarchical structure of most robot programming frameworks. A general-purpose differentiable and “shadow skill” architecture must meet three general requirements:</p> <ul> <li>It must be <strong>implementation-agnostic</strong> with respect to the skill it represents, i.e. it must be capable of accurately representing possibly non-differentiable robot skills, and even hand-written skills in a low-level robot programming language such as KRL or URScript.</li> <li>It must have a <strong>learnable</strong> component which can learn the non-deterministic aspects of skills, such as the forces and torques resulting from interactions with the environment.</li> <li>It must be <strong>chainable</strong> to represent complex robot programs composed of multiple skills.</li> </ul> <p>To meet these requirements, we designed a hybrid robot skill architecture which <strong>combines a differentiable motion planner</strong> (an implementation of a motion planner in a \(\partial P\) framework such as PyTorch) <strong>with a recurrent neural network, and wrap both components in a differentiable computational graph.</strong> The combination of a differentiable implementation of a traditional planner with a trainable neural network permits the architecture to represent a wide variety of skills, including complex force-controlled skills, in a data-efficient way. Echoing recent <a href="http://proceedings.mlr.press/v80/garnelo18a.html">related</a> <a href="https://arxiv.org/pdf/1805.11122.pdf">work</a>, we find that the explicit inclusion of domain knowledge in the form of an algorithmic prior greatly reduces the amount of data and compute required to learn complex skills.</p> <p>To the outside, every shadow skill provides the same interface: All skills accept a parameter vector <strong>\(x\)</strong> containing the skill parameters (which we want to optimize) and a state vector <strong>\(s_{in}\)</strong> describing the current state of the robot and the environment, and produce an expected trajectory <strong>\(\hat{Y}\)</strong> and a final state <strong>\(s_{out}\)</strong>. Because both the differentiable motion planner and the neural network are differentiable computational graphs, a shadow skill is also a differentiable computational graph whose outputs \(\hat{Y}\) and \(s_{out}\) are differentiable w.r.t. its inputs \(x\), conditional on the initial state \(s_{in}\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spi/shadow_skill.png" sizes="95vw"/> <img src="/assets/img/spi/shadow_skill.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A Shadow Skill as part of a larger Shadow Program. </div> <p><strong>This skill architecture can be used to learn new skills from scratch (e.g. from human demonstrations), but its most useful application is modeling existing skills from different frameworks.</strong> There are many existing, highly expressive robot program representations which are used in production. Some of them are highly domain-specific (e.g. optimized for welding applications), others are hardware-specific (e.g. the KRL or URScript textual programming languages), and yet others are based on differential equations (DMPs, ProMPs and variants). Our architecture is designed with the aim of being capable of learning to model skills from any such framework or language. Instead of forcing robot programmers to program their robots a priori in a differentiable language, we designed our skill architecture to be sufficiently flexible to learn the <em>execution semantics</em> (input-output relationships, “what happens when the skill is executed”) of any existing skill from any skill framework or robot programming language.</p> <p>Training a Shadow Skill for an existing robot skill is easy: Simply sample skill inputs <strong>\(x\)</strong>, observe the resulting robot trajectories <strong>\(Y\)</strong> and train the learnable Shadow Skill components (namely, the RNN) to predict <strong>\(Y\)</strong> given <strong>\(x\)</strong>.</p> <h2 id="shadow-programs">Shadow Programs</h2> <p>While our framework can be used to infer optimal parameters for individual robot skills, many real-world applications require the combination of several skills to achieve more complex goals. To take a cookie out of a jar, for instance, a robot must first open the jar, grasp the cookie and remove the cookie from the jar. To open the jar, the robot must in turn open its gripper, approach the lid, close its gripper around the lid, twist the lid and depart, etc. Most real-world tasks are intrinsically hierarchical and can be broken down into sequences of primitive actions. Consequently, most robot programming frameworks are themselves hierarchical, and allow the composition of complex tasks from sequences of primitive skills. By extension, optimizing the parameters of a robot program requires the <strong>joint</strong> optimization of the parameters of its constituent skills: The values of skill parameters at the beginning of the program cause changes to the state of the world and the robot, which may influence the optimal parameter values later in the program.</p> <p>End-to-end learning addresses this by learning one large model for the entire program. In practice this is not tractable, as learning to model highly complex programs requires substantial computing power and amounts of training data far in excess of what can be collected on real robots in industrial settings. <strong>We propose to train individual Shadow Skills in isolation, and to combine them afterward to complex programs.</strong> Training Shadow Skills in isolation allows for the building of a trained skill library, which only needs to be done once, and skills don’t need to be re-trained to be combined into new programs. If you are a robot programmer and want to optimize your program parameters, this is great news: You only need to train <em>one</em> Shadow Skill for each skill in your skill library, and are instantly able to optimize the parameters of any program you can compose.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spi/shadow_program.png" sizes="95vw"/> <img src="/assets/img/spi/shadow_program.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A Shadow Program composed of two Shadow Skills. Just as Shadow Skills are models for individual robot skills, Shadow Programs model complex robot programs (left to right) and allow for the joint optimization of program parameters via gradient descent (right to left). </div> <p>Because the individual Shadow Skills are forward models of robot skills, their combination to a Shadow Program constitutes a forward model of a robot program. Just as Shadow Skills are differentiable computation graphs, a Shadow Model is a differentiable computation graph. Via gradient descent in the input space, the parameters of all skills in the program can be jointly optimized w.r.t. some function of the expected behavior of the robot, such as cycle time, parts per minute (PPM) or the forces experienced on contact with an object.</p> <h2 id="applications">Applications</h2> <p>We applied SPI to several very different parameter inference problems in industrial and service robotics. In one experiment, we inferred the parameters of an robot program in the <a href="https://www.artiminds.com/">ArtiMinds ARTM programming language</a> to pick up a glass and deposit it in a sink from a single human demonstration in Virtual Reality (VR). In other experiments, we use SPI to optimize the parameters of ARTM programs, URScript programs and DMPs for force-sensitive contact motions with materials of different spring and damping characteristics. In a final experiment, we optimize the parameters of force-controlled spiral search motions to find the position of a hole on a PCB quickly, robustly or efficiently, respectively. Our experiments indicate that SPI is a highly versatile alternative to end-to-end supervised or reinforcement learning, particularly in industrial settings, as it is capable to infer optimal parameters for any existing programming framework ranging from textual programming languages like URScript to symbolic representations like ARTM or subsymbolic skill representations like DMPs. By learning an auxiliary representation (the Shadow Skills) for optimization, <strong>SPI can retrofit these established robot programming frameworks which are widely used (but neither differentiable nor learnable) with differentiable programming, and by extension learning and gradient-based optimization.</strong> This property is crucial for real-world application in the industry, as robot programmers can continue using the languages they know and like, without foregoing the benefits of differentiability and learning.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" style="display: flex; justify-content: center;"> <figure> <iframe src="https://www.youtube.com/embed/GwQdFN5lmLk" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>One caveat is that robot skills typically cannot be learned independent from the state of the world (such as the positions of objects in the environment) at the time of their execution. This implies that some fine-tuning on the current state of the world will be required, at least when skills (or programs) are transferred from one environment to another. We are currently investigating methods to do this efficiently. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog"/><category term="differentiable-programming"/><category term="robot-learning"/><category term="ml"/><category term="robotics"/><summary type="html"><![CDATA[A companion blog post to our 2021 ICRA paper on data-driven robot program parameter optimization.]]></summary></entry><entry><title type="html">Object Tracking With Spiking Neural Networks</title><link href="https://benjaminalt.github.io/blog/2018/object-tracking-with-spiking-neural-networks/" rel="alternate" type="text/html" title="Object Tracking With Spiking Neural Networks"/><published>2018-04-25T01:00:00+00:00</published><updated>2018-04-25T01:00:00+00:00</updated><id>https://benjaminalt.github.io/blog/2018/object-tracking-with-spiking-neural-networks</id><content type="html" xml:base="https://benjaminalt.github.io/blog/2018/object-tracking-with-spiking-neural-networks/"><![CDATA[<p>Last semester, I had the fortune of doing some work with the <a href="https://www.humanbrainproject.eu/en/">Human Brain Project</a>, an EU-funded research project with the aim of understanding, mapping and partly reconstructing the human brain through computer simulations and actual hardware. Using the <a href="https://neurorobotics.net/">Neurorobotics Platform (NRP)</a>, an environment for developing and running experiments with spiking neural networks and simulated robots, I implemented a closed loop control-based solution for tracking an object in space using spiking neural networks. In this post, I am outlining spiking neural networks, the Neurorobotics platform and how one can implement simple control loops for solving tracking tasks using spiking neural networks and the NRP toolset. The code for the corresponding NRP experiment can be found <a href="https://github.com/Scaatis/hbpprak_perception">on GitHub</a>.</p> <h2 id="spiking-neural-networks">Spiking Neural Networks</h2> <p>Spiking neural networks attempt to model the neurons of the human brain much more closely than the “classical” neural networks used for deep learning. While in deep neural networks, units instantly react to their inputs and produce outputs immediately, spiking neural networks are dynamical systems: The output at any given time is dependent on previous inputs as well as on the state of the neuron at that time. The physiology of biological neurons and the relevance of dynamical systems in the modelling of neurons is explained in some detail in <a href="http://jackterwilliger.com/biological-neural-networks-part-i-spiking-neurons/">Jack Terwilliger’s blog post</a>.</p> <h3 id="neuron-models">Neuron Models</h3> <p>Many different neuron models exist and spiking neural networks often combine multiple models. In all models, neurons fire impulses (or spikes) dependent on the current and past input voltage. The input voltage is computed based on the spike activity of other, connected neurons. The most simple <a href="http://neuronaldynamics.epfl.ch/online/Ch1.S3.html">“integrate-and-fire” model</a> integrates over the input voltage, emits a spike when the integrated input reaches a threshold and resets the integrator to zero. Most commonly, the integrator is <em>leaky</em>: Its value decreases over time. Mathematically, a leaky integrator can be modelled using a simple linear differential equation and is functionally equivalent to an RC circuit. Much more complex models have been devised in literature to add a refractory period, during which the neuron cannot fire, or the capacity to oscillate or resonate. While these models model the behavior of human neurons much more closely, simple models such as integrate-and-fire incur less computational overhead and often suffice to solve real-world problems with spiking neural networks.</p> <h3 id="computing-with-spiking-neural-networks">Computing with Spiking Neural Networks</h3> <p>It has been shown that <a href="https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf">every possible function can be realized with a traditional deep neural network</a>. Simples spiking neural networks work similarly to traditional deep neural networks: The network is specified by connecting <em>input populations</em> of neurons to <em>output populations</em> using <em>synapses</em>. Arbitrarily many layers can be formed. In its simplest form, a synapse connects <em>M</em> input neurons to <em>N</em> output neurons. With the simplest possible synapse, the strength of each connection is specified by a multiplier, the <em>synapse weight</em>, resulting in an <em>NxM</em> weight matrix for a fully connected synapse which encodes for each output neuron which input neuron contributes how much to its input voltage. In other words, each output neuron’s input voltage is the weighted sum of its inputs - just like in “regular” deep neural networks. More complex synapse models differ from the connections in traditional neural networks in that they model synapses in the human brain more closely, can be stateful and “learn” by changing their characteristics depending on the input spike frequency and timing, a property called <a href="http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity">spike-timing dependent plasticity</a>. <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0161335">Learning with spiking neural networks</a> still presents a number of open problems and constitutes a highly productive field of research.</p> <p>Spiking neural networks mostly differ from deep neural networks because they are dynamical systems - their outputs depend on the inputs <em>over time</em>. Unlike deep neural networks, where information is encoded in tensors, spiking neural networks encode information in the <em>spike train</em>, the function of spiking activity over time, through the frequency and timing of spikes. Depending on the neuron model, spike emission may be stochastic; in this case, the information encoding is called a <a href="https://link.springer.com/referenceworkentry/10.1007%2F978-3-540-92910-9_10">spike density code</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbp/screenshot_spike_train.png" sizes="95vw"/> <img src="/assets/img/hbp/screenshot_spike_train.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A spike train of a population of neurons, visualized in the Neurorobotics platform. </div> <h2 id="the-neurorobotics-platform">The Neurorobotics Platform</h2> <p>The Neurorobotics Platform (NRP) is an application developed by the Neurorobotics group of the Human Brain Project for developing and running experiments about controlling robots using spiking neural networks. Its backend is a combination of <a href="http://www.ros.org/">ROS</a> for infrastructure and interaction with (simulated) robots, the <a href="http://gazebosim.org/">Gazebo robot simulator</a> and the <a href="http://www.nest-simulator.org/">NEST neural network simulator</a>; it has a Javascript-based web interface and allows manipulation of the 3D environment, neural network and experiment setup in a browser. An NRP experiment is defined in an XML file and consists of a state machine, a brain file, an (SDF) environment model and an optional ROS launch file.</p> <h3 id="state-machine">State Machine</h3> <p>The state machine defines any action on the part of the environment. If something in the environment needs to be moved, such as the target object for object tracking, or if objects need to be spawned at runtime, it can be done in a Python <a href="http://library.isr.ist.utl.pt/docs/roswiki/smach.html">smach</a> state machine. It has access to the environment, additionally loaded 3D models as well as to all available ROS topics and services.</p> <h3 id="brain-definition-and-transfer-functions">Brain Definition and Transfer Functions</h3> <p>In the NRP, the brain consists of two components: A <a href="http://neuralensemble.org/PyNN/">PyNN</a> brain description file and one or more <em>transfer functions</em>. PyNN provides a simulator-independent interface to neural network simulators such as NEST. In the brain file, populations of spiking neurons are defined and connected through synapses. An example brain file for object tracking is provided below. Transfer functions map neuron activity to robot motion or vice versa - they are Python functions which are called in every cycle of the NRP’s <a href="https://developer.humanbrainproject.eu/docs/projects/HBP%20Neurorobotics%20Platform/0.9/nrp/developer_manual/CLE/cle_architecture.html"><em>Closed Loop Engine</em></a>, and have access to both the brain as defined in the brain file and the ROS topics for robot control. In their purest form, transfer functions implement <em>visuomotor coupling</em> - a closed loop between neural activity and the resulting motion. This closed loop is crucial for implementing object tracking in the NRP.</p> <h2 id="closed-loop-control-for-object-tracking">Closed-Loop Control for Object Tracking</h2> <p>There are <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.8588&amp;rep=rep1&amp;type=pdf">many approaches for tracking objects</a>, many of which rely on direct or indirect solutions to the correspondence problem or particle filters. When humans track objects, however, they do not analytically compute two-dimensional convolutions with a reference pattern, like many traditional filter-based approaches do. Instead, humans move their eyes to keep the object in the center of their field of vision. When trying to evict a fly from your living room, for example, you constantly keep your eyes centered on the fly and follow its motion. This can be modelled in a straightforward way with a closed-loop control circuit: The sensors on the retina cause excitations of the neurons of the visual cortex; these excitation patterns instruct the eye and neck muscles to move in order to keep the tracked object in the center. Small deviations in the position of the tracked object cause compensatory movements of the eyes, which in turn cause the image on the retina to change, etc. In control theory terms, the retina is the sensor, the eye muscles are the controller and the position and orientation of the eyes are the controlled system.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbp/control_loop.png" sizes="95vw"/> <img src="/assets/img/hbp/control_loop.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Object tracking realized with closed-loop control. </div> <p>Because the NRP comes with a Closed Loop Engine (CLE), implementing closed loop control with spiking neural networks is relatively straightforward in the NRP. The spiking activity of an input population (“virtual retina”) encodes where in the field of vision a tracked object is located at a given time. Via a set of synapses and transfer functions (in the human brain, this would happen in the visual cortex and <a href="https://www.researchgate.net/figure/Summary-of-sensorimotor-circuitry-for-the-generation-of-visuomotor-and-vestibulomotor-eye_fig4_7428849">sensorimotor circuits</a>), the outputs of this population are connected to the inputs of a motor population which, in turn, controls the joint actuators in the eye and neck of the robot.</p> <h3 id="thimblerigger-experiment">Thimblerigger Experiment</h3> <p>To try this out, my colleagues and I created an experiment (the code is on <a href="https://github.com/Scaatis/hbpprak_perception">GitHub</a>) in which a simulated iCub robot is to play the thimblerigger game: A ball is hidden under one of three cups; after the cups are shuffled, the robot has to guess which cup the ball is hidden under. Humans instinctively approach this task by trying to follow the cup hiding the ball with their eyes. While it would be easier and probably more robust to solve this problem using traditional image processing and frame-by-frame analysis, we chose to implement a closed-loop solution for tracking which is as close to the human approach as possible to demonstrate how closed-loop control for visuomotor coupling can be implemented with spiking neural networks in just a few lines of code.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbp/icub_perception.png" sizes="95vw"/> <img src="/assets/img/hbp/icub_perception.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The thimblerigger experiment in the Neurorobotics platform. </div> <p>The experiment itself is defined using an SMACH state machine (the code for it is <a href="https://github.com/Scaatis/hbpprak_perception/blob/master/state_machine.exd">on GitHub</a>). When the user starts the experiment, three red cups are spawned. One cup is selected at random to conceal a green ball. This cup is lifted to briefly reveal the ball, then lowered again. The cups are shuffled by moving them to a series of random permutations. In the end, the ball is revealed again - if the tracking system worked, the robot is now looking directly at the ball.</p> <h3 id="virtual-retina--brain-file">Virtual Retina &amp; Brain File</h3> <p>The central component of our implementation is the brain file. The brain itself consists of two populations: An input population, representing the virtual retina, and an output population whose outputs are used to control the robot’s joints.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">hbp_nrp_cle.brainsim</span> <span class="kn">import</span> <span class="n">simulator</span> <span class="k">as</span> <span class="n">sim</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">resolution</span> <span class="o">=</span> <span class="mi">17</span>
<span class="n">n_motors</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># down, up, left, right
</span>
<span class="n">sensors</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">Population</span><span class="p">(</span><span class="n">resolution</span> <span class="o">*</span> <span class="n">resolution</span><span class="p">,</span> <span class="n">cellclass</span><span class="o">=</span><span class="n">sim</span><span class="p">.</span><span class="nc">IF_curr_exp</span><span class="p">())</span>
<span class="n">down</span><span class="p">,</span> <span class="n">up</span><span class="p">,</span> <span class="n">left</span><span class="p">,</span> <span class="n">right</span> <span class="o">=</span> <span class="p">[</span><span class="n">sim</span><span class="p">.</span><span class="nc">Population</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">cellclass</span><span class="o">=</span><span class="n">sim</span><span class="p">.</span><span class="nc">IF_curr_exp</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_motors</span><span class="p">)]</span>

<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">resolution</span> <span class="o">*</span> <span class="n">resolution</span><span class="p">).</span><span class="nf">reshape</span><span class="p">((</span><span class="n">resolution</span><span class="p">,</span> <span class="n">resolution</span><span class="p">))</span>
<span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">((</span><span class="n">resolution</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">resolution</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="n">upper_half</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">PopulationView</span><span class="p">(</span><span class="n">sensors</span><span class="p">,</span> <span class="n">indices</span><span class="p">[:</span><span class="n">resolution</span> <span class="o">//</span> <span class="mi">2</span><span class="p">].</span><span class="nf">flatten</span><span class="p">())</span>
<span class="n">lower_half</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">PopulationView</span><span class="p">(</span><span class="n">sensors</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="n">resolution</span> <span class="o">-</span> <span class="n">resolution</span><span class="o">//</span><span class="mi">2</span><span class="p">:].</span><span class="nf">flatten</span><span class="p">())</span>
<span class="n">left_half</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">PopulationView</span><span class="p">(</span><span class="n">sensors</span><span class="p">,</span> <span class="n">indices</span><span class="p">[:,</span> <span class="p">:</span><span class="n">resolution</span> <span class="o">//</span> <span class="mi">2</span><span class="p">].</span><span class="nf">flatten</span><span class="p">())</span>
<span class="n">right_half</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">PopulationView</span><span class="p">(</span><span class="n">sensors</span><span class="p">,</span> <span class="n">indices</span><span class="p">[:,</span> <span class="n">resolution</span> <span class="o">-</span> <span class="n">resolution</span><span class="o">//</span><span class="mi">2</span><span class="p">:].</span><span class="nf">flatten</span><span class="p">())</span>

<span class="n">pro_down</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">Projection</span><span class="p">(</span><span class="n">lower_half</span><span class="p">,</span> <span class="n">down</span><span class="p">,</span> <span class="n">sim</span><span class="p">.</span><span class="nc">AllToAllConnector</span><span class="p">(),</span> <span class="n">sim</span><span class="p">.</span><span class="nc">StaticSynapse</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">ones</span><span class="p">))</span>
<span class="n">pro_up</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">Projection</span><span class="p">(</span><span class="n">upper_half</span><span class="p">,</span> <span class="n">up</span><span class="p">,</span> <span class="n">sim</span><span class="p">.</span><span class="nc">AllToAllConnector</span><span class="p">(),</span> <span class="n">sim</span><span class="p">.</span><span class="nc">StaticSynapse</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">ones</span><span class="p">))</span>
<span class="n">pro_left</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">Projection</span><span class="p">(</span><span class="n">left_half</span><span class="p">,</span> <span class="n">left</span><span class="p">,</span> <span class="n">sim</span><span class="p">.</span><span class="nc">AllToAllConnector</span><span class="p">(),</span> <span class="n">sim</span><span class="p">.</span><span class="nc">StaticSynapse</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">ones</span><span class="p">))</span>
<span class="n">pro_right</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">Projection</span><span class="p">(</span><span class="n">right_half</span><span class="p">,</span> <span class="n">right</span><span class="p">,</span> <span class="n">sim</span><span class="p">.</span><span class="nc">AllToAllConnector</span><span class="p">(),</span> <span class="n">sim</span><span class="p">.</span><span class="nc">StaticSynapse</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">ones</span><span class="p">))</span>

<span class="n">circuit</span> <span class="o">=</span> <span class="n">sensors</span> <span class="o">+</span> <span class="n">down</span> <span class="o">+</span> <span class="n">up</span> <span class="o">+</span> <span class="n">left</span> <span class="o">+</span> <span class="n">right</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">sensors</code> population contains the sensor neurons whose spike train correlates with the position of the tracked object. Each neuron is responsible for a small rectangular part of the field of view and spikes when a part of the tracked object is inside this rectangle. For our retina, we chose a resolution of 17x17 neurons, so <code class="language-plaintext highlighter-rouge">sensors</code> is a population of 289 neurons.</p> <p>The <code class="language-plaintext highlighter-rouge">down</code>, <code class="language-plaintext highlighter-rouge">up</code>, <code class="language-plaintext highlighter-rouge">left</code> and <code class="language-plaintext highlighter-rouge">right</code> populations consist of one neuron each. They are our <em>output</em> or <em>motor populations</em> and encode whether and how quickly the eye is to move in either direction. For simplicity and mechanical stability,<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> we chose to actuate only iCub’s left eye as opposed to both eyes and used only the image from the left eye’s camera as opposed the combined (stereo) image.</p> <p><code class="language-plaintext highlighter-rouge">upper_half</code>, <code class="language-plaintext highlighter-rouge">lower_half</code>, <code class="language-plaintext highlighter-rouge">left_half</code> and <code class="language-plaintext highlighter-rouge">right_half</code> are <em>PopulationViews</em> which divide the <code class="language-plaintext highlighter-rouge">sensors</code> population into four different groups corresponding to larger regions in the image. Our <code class="language-plaintext highlighter-rouge">down</code>, <code class="language-plaintext highlighter-rouge">up</code>, <code class="language-plaintext highlighter-rouge">left</code> and <code class="language-plaintext highlighter-rouge">right</code> populations will end up controlling the eye, but we still need to somehow connect “what the retina sees” to these output populations. To that end, we divide the retina into these upper, lower, left and right areas. If the tracked object is in the upper half, the neurons in the <code class="language-plaintext highlighter-rouge">upper_half</code> PopulationView fire more frequently than those in the other areas, so we know that we have to move our eye upwards to keep the tracked object in the center of the field of view.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbp/retina_regions.png" sizes="95vw"/> <img src="/assets/img/hbp/retina_regions.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The PopulationViews dividing the retina into four regions. </div> <p>The coupling between the output of the retinal sensors and the <code class="language-plaintext highlighter-rouge">down</code>, <code class="language-plaintext highlighter-rouge">up</code>, <code class="language-plaintext highlighter-rouge">left</code> and <code class="language-plaintext highlighter-rouge">right</code> motor populations is done via the <em>Projections</em> <code class="language-plaintext highlighter-rouge">pro_down</code>, <code class="language-plaintext highlighter-rouge">pro_up</code>, <code class="language-plaintext highlighter-rouge">pro_left</code> and <code class="language-plaintext highlighter-rouge">pro_right</code>. Projections are connections between populations (or, in our case, PopulationViews) which define which neurons from the input population are connected to which neurons from the output population (fully connected (all to all) in our case) as well as the synapse type used for the connections. We use <code class="language-plaintext highlighter-rouge">StaticSynapse</code>s whose properties do not change over time. The <code class="language-plaintext highlighter-rouge">weight</code> parameter is a matrix defining the connection weights. Because we used an <code class="language-plaintext highlighter-rouge">AllToAllConnector</code>, <code class="language-plaintext highlighter-rouge">weight</code> is a 136 * 1 matrix assigning a weight for each of the incoming connections from the (17//2)*17 input neurons per half of the retina. Our <code class="language-plaintext highlighter-rouge">weight</code> matrix contains only ones - each input neuron is weighted equally for the computation of the output. How the input spike trains are aggregated in each motor neuron and what their output looks like is defined by the <code class="language-plaintext highlighter-rouge">cellclass</code> used when defining the motor neuron populations. We use PyNN’s <a href="http://pynn.readthedocs.io/en/latest/reference/neuronmodels.html#pyNN.standardmodels.cells.IF_curr_exp"><code class="language-plaintext highlighter-rouge">IF_curr_exp</code></a>, a <a href="http://pynn.readthedocs.io/en/latest/reference/neuronmodels.html">basic integrate-and-fire model</a>.</p> <h3 id="transfer-functions">Transfer Functions</h3> <p>In the NRP, <em>transfer functions</em> connect the brain defined in the brain file to the robot and the environment. Activating sensor populations based on input from (simulated) sensors such as the cameras in iCub’s eyes or from joint encoders as well as the coupling between the outputs of the motor neurons and the joint actuators is done in transfer functions. To implement our object-tracking solution, we need two transfer functions: One to connect the sensory input (the camera image) to the sensor population, and another to connect the motor neurons to the eye’s actuators.</p> <h4 id="object-detection">Object detection</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># detect_object.py
</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">sensor_msgs.msg</span>
<span class="kn">import</span> <span class="n">std_msgs.msg</span>
<span class="kn">from</span> <span class="n">cv_bridge</span> <span class="kn">import</span> <span class="n">CvBridge</span>
<span class="nd">@nrp.MapRobotSubscriber</span><span class="p">(</span><span class="sh">"</span><span class="s">camera</span><span class="sh">"</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">"</span><span class="s">/icub_model/left_eye_camera/image_raw</span><span class="sh">"</span><span class="p">,</span> <span class="n">sensor_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">Image</span><span class="p">))</span>
<span class="nd">@nrp.MapRobotSubscriber</span><span class="p">(</span><span class="sh">"</span><span class="s">shuffle_status_sub</span><span class="sh">"</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">"</span><span class="s">/group_3/shuffling</span><span class="sh">"</span><span class="p">,</span> <span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">Bool</span><span class="p">))</span>
<span class="nd">@nrp.MapSpikeSource</span><span class="p">(</span><span class="sh">"</span><span class="s">sensors</span><span class="sh">"</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="nf">map_neurons</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">brain_root</span><span class="p">.</span><span class="n">resolution</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">nrp</span><span class="p">.</span><span class="n">brain</span><span class="p">.</span><span class="n">sensors</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">nrp</span><span class="p">.</span><span class="n">dc_source</span><span class="p">)</span>
<span class="nd">@nrp.Robot2Neuron</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">grab_image</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">camera</span><span class="p">,</span> <span class="n">shuffle_status_sub</span><span class="p">,</span> <span class="n">sensors</span><span class="p">):</span>
    <span class="n">resolution</span> <span class="o">=</span> <span class="n">nrp</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">brain_root</span><span class="p">.</span><span class="n">resolution</span>

    <span class="c1"># Take the image from the robot's left eye
</span>    <span class="n">image_msg</span> <span class="o">=</span> <span class="n">camera</span><span class="p">.</span><span class="n">value</span>
    <span class="k">if</span> <span class="n">image_msg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">cvBridge</span> <span class="o">=</span> <span class="nc">CvBridge</span><span class="p">()</span>

        <span class="n">img</span> <span class="o">=</span> <span class="n">cvBridge</span><span class="p">.</span><span class="nf">imgmsg_to_cv2</span><span class="p">(</span><span class="n">image_msg</span><span class="p">,</span> <span class="sh">"</span><span class="s">rgb8</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">img_height</span><span class="p">,</span> <span class="n">img_width</span><span class="p">,</span> <span class="n">color_dim</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span>

        <span class="n">detect_red</span> <span class="o">=</span> <span class="n">shuffle_status_sub</span><span class="p">.</span><span class="n">value</span><span class="p">.</span><span class="n">data</span> <span class="k">if</span> <span class="n">shuffle_status_sub</span><span class="p">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">False</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">detect_red</span><span class="p">:</span>
            <span class="c1"># Detect green in the whole image
</span>            <span class="n">col_width</span> <span class="o">=</span> <span class="n">img_width</span> <span class="o">//</span> <span class="n">resolution</span>
            <span class="n">row_height</span> <span class="o">=</span> <span class="n">img_height</span> <span class="o">//</span> <span class="n">resolution</span>
            <span class="n">green_threshold</span> <span class="o">=</span> <span class="mf">0.5</span>
            <span class="n">amp_scaling_factor</span> <span class="o">=</span> <span class="mf">32.</span>
            
            <span class="c1"># Split the image into regions of same size
</span>            <span class="c1"># Sensor neurons are addressed in row_major order, top left to bottom right
</span>            <span class="c1"># Loop over the neurons in the retina...
</span>            <span class="k">for</span> <span class="n">row_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">resolution</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">col_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">resolution</span><span class="p">):</span>
                    <span class="n">x_start</span> <span class="o">=</span> <span class="n">col_idx</span> <span class="o">*</span> <span class="n">col_width</span>
                    <span class="n">x_end</span> <span class="o">=</span> <span class="n">x_start</span> <span class="o">+</span> <span class="n">col_width</span>
                    <span class="n">y_start</span> <span class="o">=</span> <span class="n">row_idx</span> <span class="o">*</span> <span class="n">row_height</span>
                    <span class="n">y_end</span> <span class="o">=</span> <span class="n">y_start</span> <span class="o">+</span> <span class="n">row_height</span>
                    <span class="n">mean_red</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="n">y_start</span><span class="p">:</span><span class="n">y_end</span><span class="p">,</span><span class="n">x_start</span><span class="p">:</span><span class="n">x_end</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
                    <span class="n">mean_green</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="n">y_start</span><span class="p">:</span><span class="n">y_end</span><span class="p">,</span><span class="n">x_start</span><span class="p">:</span><span class="n">x_end</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
                    <span class="n">mean_blue</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="n">y_start</span><span class="p">:</span><span class="n">y_end</span><span class="p">,</span><span class="n">x_start</span><span class="p">:</span><span class="n">x_end</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>

                    <span class="n">green_proportion</span> <span class="o">=</span> <span class="n">mean_green</span> <span class="o">/</span> <span class="nf">float</span><span class="p">(</span><span class="n">mean_red</span> <span class="o">+</span> <span class="n">mean_green</span> <span class="o">+</span> <span class="n">mean_blue</span><span class="p">)</span>

                    <span class="n">idx</span> <span class="o">=</span> <span class="n">row_idx</span> <span class="o">*</span> <span class="n">resolution</span> <span class="o">+</span> <span class="n">col_idx</span>
                    <span class="n">amp</span> <span class="o">=</span> <span class="n">amp_scaling_factor</span> <span class="o">*</span> <span class="n">green_proportion</span> <span class="k">if</span> <span class="n">green_proportion</span> <span class="o">&gt;</span> <span class="n">green_threshold</span> <span class="k">else</span> <span class="mi">0</span>

                    <span class="n">sensors</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="n">amplitude</span> <span class="o">=</span> <span class="n">amp</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># Detect red in the center of the image
</span>          <span class="c1"># ...
</span></code></pre></div></div> <p><a href="https://github.com/Scaatis/hbpprak_perception/blob/master/detect_object.py">detect_object.py</a> is a <code class="language-plaintext highlighter-rouge">Robot2Neuron</code> transfer function, meaning that information is sent from the robot (or the environment) to the brain. The inputs of the transfer function are defined using the NRP’s <a href="https://developer.humanbrainproject.eu/docs/projects/HBP%20Neurorobotics%20Platform/1.2/nrp/user_manual/simulation_setup/transfer_functions.html">decorator syntax</a> and consist of a subscriber to the camera’s ROS topic, the sensor population and a subscriber to the topic published by the experiment state machine which announces the current state of the experiment (<code class="language-plaintext highlighter-rouge">true</code> if shuffling, <code class="language-plaintext highlighter-rouge">false</code> otherwise). This is required because during shuffling, the object to be tracked is the red cup concealing the ball, while during the reveal in the beginning, the green ball must be tracked.</p> <p>In the central loop, we iterate over all neurons in the retina, compute this neuron’s rectangular area of the field of view and compute the mean for each RGB channel over this region. The neuron’s amplitude (proportional to its firing rate) is set in proportion to the “amount” of green in the region relative to the other primary colors. This has the effect that neurons which “see” green fire more frequently than others. Because the sensor neurons are iterated using a nested loop, like for a 2D array, the sensor neuron activity spatially encodes the positions of green things in the image - and because of the wiring defined in the brain, the <code class="language-plaintext highlighter-rouge">down</code>, <code class="language-plaintext highlighter-rouge">up</code>, <code class="language-plaintext highlighter-rouge">left</code> and <code class="language-plaintext highlighter-rouge">right</code> motor populations are excited according to these positions: <code class="language-plaintext highlighter-rouge">left</code> fires frequently if there is a lot of green in the left side of the image, etc.</p> <p>The code for tracking red objects has been omitted for brevity but works similarly with the only exception that not the entire image is considered, but a rectangular area around the center of the image, causing the margins to be ignored. This is necessary because three cups may be in the image at any given time, but we only want to track the one we centered on when the green ball was revealed.</p> <h4 id="object-following">Object following</h4> <p>The last remaining component of our solution is the transfer function for following the moving object (depending on the stage of the experiment either the ball or the cup) with iCub’s left eye. To that end, the outputs of the <code class="language-plaintext highlighter-rouge">down</code>,<code class="language-plaintext highlighter-rouge">up</code>, <code class="language-plaintext highlighter-rouge">left</code> and <code class="language-plaintext highlighter-rouge">right</code> motor populations must be connected to the two actuators for moving the eye.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># follow_object.py
</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="nd">@nrp.MapSpikeSink</span><span class="p">(</span><span class="sh">"</span><span class="s">motors_down</span><span class="sh">"</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">brain</span><span class="p">.</span><span class="n">down</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">leaky_integrator_alpha</span><span class="p">)</span>
<span class="nd">@nrp.MapSpikeSink</span><span class="p">(</span><span class="sh">"</span><span class="s">motors_left</span><span class="sh">"</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">brain</span><span class="p">.</span><span class="n">left</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">leaky_integrator_alpha</span><span class="p">)</span>
<span class="nd">@nrp.MapSpikeSink</span><span class="p">(</span><span class="sh">"</span><span class="s">motors_up</span><span class="sh">"</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">brain</span><span class="p">.</span><span class="n">up</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">leaky_integrator_alpha</span><span class="p">)</span>
<span class="nd">@nrp.MapSpikeSink</span><span class="p">(</span><span class="sh">"</span><span class="s">motors_right</span><span class="sh">"</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">brain</span><span class="p">.</span><span class="n">right</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">leaky_integrator_alpha</span><span class="p">)</span>
<span class="nd">@nrp.MapRobotPublisher</span><span class="p">(</span><span class="sh">'</span><span class="s">eye_tilt_pos</span><span class="sh">'</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">'</span><span class="s">/robot/eye_tilt/pos</span><span class="sh">'</span><span class="p">,</span> <span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">Float64</span><span class="p">))</span>
<span class="nd">@nrp.MapRobotPublisher</span><span class="p">(</span><span class="sh">'</span><span class="s">eye_pan_pos</span><span class="sh">'</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">'</span><span class="s">/robot/left_eye_pan/pos</span><span class="sh">'</span><span class="p">,</span> <span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">Float64</span><span class="p">))</span>
<span class="nd">@nrp.MapRobotPublisher</span><span class="p">(</span><span class="sh">'</span><span class="s">eye_tilt_vel</span><span class="sh">'</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">'</span><span class="s">/robot/eye_tilt/vel</span><span class="sh">'</span><span class="p">,</span> <span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">Float64</span><span class="p">))</span>
<span class="nd">@nrp.MapRobotPublisher</span><span class="p">(</span><span class="sh">'</span><span class="s">eye_pan_vel</span><span class="sh">'</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">'</span><span class="s">/robot/left_eye_pan/vel</span><span class="sh">'</span><span class="p">,</span> <span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">Float64</span><span class="p">))</span>
<span class="nd">@nrp.MapRobotSubscriber</span><span class="p">(</span><span class="sh">"</span><span class="s">joint_state_sub</span><span class="sh">"</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">"</span><span class="s">/robot/joints</span><span class="sh">"</span><span class="p">,</span> <span class="n">sensor_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">JointState</span><span class="p">))</span>
<span class="nd">@nrp.MapRobotSubscriber</span><span class="p">(</span><span class="sh">"</span><span class="s">shuffle_status_sub</span><span class="sh">"</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">"</span><span class="s">/group_3/shuffling</span><span class="sh">"</span><span class="p">,</span> <span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">Bool</span><span class="p">))</span>
<span class="nd">@nrp.Neuron2Robot</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">center_on_green</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">motors_down</span><span class="p">,</span> <span class="n">motors_left</span><span class="p">,</span> <span class="n">motors_up</span><span class="p">,</span> <span class="n">motors_right</span><span class="p">,</span> <span class="n">eye_tilt_pos</span><span class="p">,</span> <span class="n">eye_pan_pos</span><span class="p">,</span> <span class="n">eye_tilt_vel</span><span class="p">,</span> <span class="n">eye_pan_vel</span><span class="p">,</span> <span class="n">joint_state_sub</span><span class="p">,</span> <span class="n">shuffle_status_sub</span><span class="p">):</span>

    <span class="n">stage_two</span> <span class="o">=</span> <span class="n">shuffle_status_sub</span><span class="p">.</span><span class="n">value</span><span class="p">.</span><span class="n">data</span> <span class="k">if</span> <span class="n">shuffle_status_sub</span><span class="p">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">False</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">stage_two</span><span class="p">:</span>
        <span class="c1"># Stage one: Velocity-controlled motion to green ball
</span>        <span class="n">scaling_factor</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="n">tilt</span> <span class="o">=</span> <span class="n">scaling_factor</span> <span class="o">*</span> <span class="p">(</span><span class="n">motors_up</span><span class="p">.</span><span class="n">voltage</span> <span class="o">-</span> <span class="n">motors_down</span><span class="p">.</span><span class="n">voltage</span><span class="p">)</span>
        <span class="n">pan</span> <span class="o">=</span> <span class="n">scaling_factor</span> <span class="o">*</span> <span class="p">(</span> <span class="n">motors_left</span><span class="p">.</span><span class="n">voltage</span> <span class="o">-</span> <span class="n">motors_right</span><span class="p">.</span><span class="n">voltage</span><span class="p">)</span>
        <span class="n">eye_tilt_vel</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="nc">Float64</span><span class="p">(</span><span class="n">tilt</span><span class="p">))</span>
        <span class="n">eye_pan_vel</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="nc">Float64</span><span class="p">(</span><span class="n">pan</span><span class="p">))</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Stage two: Position-controlled motion to red cup
</span>        <span class="n">scaling_factor</span> <span class="o">=</span> <span class="mf">0.03</span>
        <span class="n">joint_names</span> <span class="o">=</span> <span class="n">joint_state_sub</span><span class="p">.</span><span class="n">value</span><span class="p">.</span><span class="n">name</span>
        <span class="n">joint_positions</span> <span class="o">=</span> <span class="n">joint_state_sub</span><span class="p">.</span><span class="n">value</span><span class="p">.</span><span class="n">position</span>
        <span class="n">current_tilt</span> <span class="o">=</span> <span class="n">joint_positions</span><span class="p">[</span><span class="n">joint_names</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="sh">"</span><span class="s">eye_tilt</span><span class="sh">"</span><span class="p">)]</span>
        <span class="n">current_pan</span> <span class="o">=</span> <span class="n">joint_positions</span><span class="p">[</span><span class="n">joint_names</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="sh">"</span><span class="s">left_eye_pan</span><span class="sh">"</span><span class="p">)]</span>

        <span class="n">tilt</span> <span class="o">=</span> <span class="n">current_tilt</span> <span class="o">+</span> <span class="n">scaling_factor</span> <span class="o">*</span> <span class="p">(</span><span class="n">motors_up_stage_two</span><span class="p">.</span><span class="n">voltage</span> <span class="o">-</span> <span class="n">motors_down_stage_two</span><span class="p">.</span><span class="n">voltage</span><span class="p">)</span>
        <span class="n">pan</span> <span class="o">=</span> <span class="n">current_pan</span> <span class="o">+</span> <span class="n">scaling_factor</span> <span class="o">*</span> <span class="p">(</span> <span class="n">motors_left_stage_two</span><span class="p">.</span><span class="n">voltage</span> <span class="o">-</span> <span class="n">motors_right_stage_two</span><span class="p">.</span><span class="n">voltage</span><span class="p">)</span>

        <span class="n">eye_tilt_pos</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="nc">Float64</span><span class="p">(</span><span class="n">tilt</span><span class="p">))</span>
        <span class="n">eye_pan_pos</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="nc">Float64</span><span class="p">(</span><span class="n">pan</span><span class="p">))</span>
</code></pre></div></div> <p><a href="https://github.com/Scaatis/hbpprak_perception/blob/master/follow_object.py">follow_object.py</a> is a <code class="language-plaintext highlighter-rouge">Neuron2Robot</code> transfer function for mapping brain activity to robot motion. <code class="language-plaintext highlighter-rouge">eye_tilt_pos</code>, <code class="language-plaintext highlighter-rouge">eye_pan_pos</code>, <code class="language-plaintext highlighter-rouge">eye_tilt_vel</code> and <code class="language-plaintext highlighter-rouge">eye_pan_vel</code> are handles to the ROS topics for writing to the simulated robot’s velocity and position controllers for the left eye’s tilt and pan joints. In the first stage of the experiment, when centering on the green ball, we use the difference between <code class="language-plaintext highlighter-rouge">up</code> and <code class="language-plaintext highlighter-rouge">down</code>or <code class="language-plaintext highlighter-rouge">left</code> and <code class="language-plaintext highlighter-rouge">right</code> as inputs for the <em>velocity controllers</em> of the corresponding joints. We chose to use the velocity controllers for smoother control, as writing directly to the position controllers can cause very quick and jerky motion.</p> <p>In the second stage of the experiment, when tracking a fast-moving red cup, we write directly to the position controllers, mainly for speed and for avoiding the oscillations which can occur when the input to a velocity controllers changes quickly.</p> <h3 id="results">Results</h3> <p>The resulting control scheme turned out to be surprisingly stable with hardly any oscillations. Because the voltages used for writing to the robot controllers are computed by integrating over the spike train, noise is reliably smoothed out and erratic motions are avoided. Only when the cups are shuffled extremely quickly is the controller too slow to follow. Our approach shows that spiking neural networks are well suited for the implementation of closed control loops, particularly when coupling sensor input with motion, and illustrates how architectural features of the NRP such as the closed loop engine can be leveraged to concisely implement object tracking using closed-loop control.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" style="display: flex; justify-content: center;"> <figure> <iframe src="https://www.youtube.com/embed/7fiBUZ9i9GQ?rel=0" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>At least with the <a href="http://bulletphysics.org/wordpress/">Bullet</a> physics engine used in the NRP, the robot started oscillating and sometimes toppled over if the neck was moved too quickly. For quick-moving objects like the cups in our experiment, these oscillations in turn caused the input to our closed-loop controller (the image) to oscillate and made stable control very difficult. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="blog"/><category term="neural-networks"/><category term="human-brain-project"/><category term="ml"/><category term="robotics"/><summary type="html"><![CDATA[How Spiking Neural Networks (SNNs) and the Neurorobotics Platform can be leveraged for robot control and perception.]]></summary></entry></feed>