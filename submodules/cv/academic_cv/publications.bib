@article{.,
  title = {Discrete Laplace Operator for Mesh Laplacians}
}

@book{.01.10.201805.10.2018,
  title = {2018 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems ({{IROS}})},
  year = {2018},
  publisher = {IEEE},
  isbn = {978-1-5386-8094-0}
}

@book{.06.05.201310.05.2013,
  title = {2013 {{IEEE}} International Conference on Robotics and Automation},
  year = {2013},
  publisher = {IEEE},
  isbn = {978-1-4673-5643-5}
}

@book{.09.10.201614.10.2016,
  title = {2016 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems ({{IROS}})},
  year = {2016},
  publisher = {IEEE},
  isbn = {978-1-5090-3762-9}
}

@book{.10.09.201412.09.2014,
  title = {2014 {{IEEE}} International Symposium on Mixed and Augmented Reality ({{ISMAR}})},
  year = {2014},
  publisher = {IEEE},
  isbn = {978-1-4799-6184-9}
}

@book{.11.09.201114.09.2011,
  title = {2011 18th {{IEEE}} International Conference on Image Processing},
  year = {2011},
  publisher = {IEEE},
  isbn = {978-1-4577-1303-3}
}

@book{.12.05.200917.05.2009,
  title = {2009 {{IEEE}} International Conference on Robotics and Automation},
  year = {2009},
  publisher = {IEEE},
  isbn = {978-1-4244-2788-8}
}

@book{.14.05.201218.05.2012,
  title = {2012 {{IEEE}} International Conference on Robotics and Automation},
  year = {2012},
  publisher = {IEEE},
  isbn = {978-1-4673-1405-3}
}

@book{.1419Sept.2003,
  title = {2003 {{IEEE}} International Conference on Robotics and Automation (Cat. {{No}}.{{03CH37422}})},
  year = {2003},
  publisher = {IEEE},
  isbn = {0-7803-7736-2}
}

@book{.17.12.200820.12.2008,
  title = {2008 10th International Conference on Control, Automation, Robotics and Vision},
  year = {2008},
  publisher = {IEEE},
  isbn = {978-1-4244-2286-9}
}

@book{.1823Oct.1998,
  title = {Proceedings Visualization '98 (Cat. {{No}}.{{98CB36276}})},
  year = {1998},
  publisher = {IEEE},
  isbn = {0-8186-9176-X}
}

@book{.20.06.200925.06.2009,
  title = {2009 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  year = {2009},
  publisher = {IEEE},
  isbn = {978-1-4244-3992-8}
}

@book{.20.09.199927.09.1999,
  title = {Proceedings of the Seventh {{IEEE}} International Conference on Computer Vision},
  year = {1999},
  publisher = {IEEE},
  isbn = {0-7695-0164-8}
}

@book{.2007,
  title = {Proceedings of the Fifth Eurographics Symposium on Geometry Processing},
  year = {2007},
  series = {{{SGP}} '07},
  publisher = {Eurographics Association},
  address = {Aire-la-Ville, Switzerland, Switzerland},
  isbn = {978-3-905673-46-3}
}

@book{.2009,
  title = {International Conference on Computer Vision Theory and Application {{VISSAPP}}'09)},
  year = {2009},
  publisher = {INSTICC Press}
}

@book{.2014,
  title = {2014 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems},
  year = {2014},
  publisher = {IEEE},
  isbn = {978-1-4799-6934-0}
}

@book{.21.07.201726.07.2017,
  title = {2017 {{IEEE}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
  year = {2017},
  publisher = {IEEE},
  isbn = {978-1-5386-0457-1}
}

@book{.23.06.200828.06.2008,
  title = {2008 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  year = {2008},
  publisher = {IEEE},
  isbn = {978-1-4244-2242-5}
}

@book{.27.09.200904.10.2009,
  title = {2009 {{IEEE}} 12th International Conference on Computer Vision Workshops, {{ICCV}} Workshops},
  year = {2009},
  publisher = {IEEE},
  isbn = {978-1-4244-4442-7}
}

@book{.29.11.201201.12.2012,
  title = {2012 12th {{IEEE-RAS}} International Conference on Humanoid Robots (Humanoids 2012)},
  year = {2012},
  publisher = {IEEE},
  isbn = {978-1-4673-1369-8}
}

@book{.31.05.201002.06.2010,
  title = {2010 Canadian Conference on Computer and Robot Vision},
  year = {2010},
  publisher = {IEEE},
  isbn = {978-1-4244-6963-5}
}

@book{.35May2000,
  title = {Proceedings Computer Animation 2000},
  year = {2000},
  publisher = {IEEE Comput. Soc},
  isbn = {0-7695-0683-6}
}

@book{.37Sept.2000,
  title = {Proceedings 15th International Conference on Pattern Recognition. {{ICPR-2000}}},
  year = {2000},
  publisher = {IEEE Comput. Soc},
  isbn = {0-7695-0750-6}
}

@article{.b,
  title = {Singular Value Decomposition ({{SVD}}) Tutorial}
}

@book{.c,
  title = {Proceedings Visualization '98 Cat 1998}
}

@misc{14:00-17:00ISOTS15066,
  title = {{{ISO}}/{{TS}} 15066:2016},
  shorttitle = {{{ISO}}/{{TS}} 15066},
  author = {{14:00-17:00}},
  publisher = {International Organization for Standardization},
  urldate = {2022-05-06},
  abstract = {Robots and robotic devices --- Collaborative robots},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\4DACD5IS\62996.html}
}

@inproceedings{5531237,
  title = {State {{Fusion}} with {{Unknown Correlation}}: {{Ellipsoidal Intersection}}},
  booktitle = {Proceedings of the 2010 {{American Control Conference}}},
  author = {Sijs, Joris and Lazar, Mircea and {v. d. Bosch}, Paul P. J.},
  year = {2010},
  month = jun,
  pages = {3992--3997},
  issn = {0743-1619},
  doi = {10.1109/ACC.2010.5531237},
  keywords = {correlation methods,covariance analysis,multivaria}
}

@article{7515322,
  title = {Distributed {{Data Fusion}}: {{Neighbors}}, {{Rumors}}, and the {{Art}} of {{Collective Knowledge}}},
  author = {Campbell, Mark E and Ahmed, Nisar R},
  year = {2016},
  month = aug,
  journal = {IEEE Control Systems},
  volume = {36},
  number = {4},
  pages = {83--109},
  issn = {1066-033X},
  doi = {10.1109/MCS.2016.2558444},
  keywords = {autonomous aerial vehicles;sensor fusion;DDF;UAV;c}
}

@inproceedings{8009724,
  title = {Information {{Form Distributed Kalman Filtering}} ({{IDKF}}) with {{Explicit Inputs}}},
  booktitle = {2017 20th {{International Conference}} on {{Information Fusion}} ({{Fusion}})},
  author = {Pfaff, Florian and Noack, Benjamin and Hanebeck, Uwe D and Govaers, Felix and Koch, Wolfgang},
  year = {2017},
  month = jul,
  pages = {1--8},
  doi = {10.23919/ICIF.2017.8009724},
  keywords = {centralized Kalman filter,IDKF,info,Kalman filters}
}

@article{Abbott2000,
  title = {Synaptic Plasticity: Taming the Beast},
  author = {Abbott, Larry F and Nelson, Sacha B},
  year = {2000},
  journal = {Nature neuroscience},
  volume = {3},
  number = {11s},
  pages = {1178},
  publisher = {Nature Publishing Group}
}

@misc{abbRobotStudio3DPrinting,
  type = {Product Announcement},
  title = {{{RobotStudio 3D Printing PowerPac}}},
  author = {{ABB}},
  journal = {ABB Robotics},
  urldate = {2024-04-02},
  abstract = {ABB enables 3D Printing via RobotStudio{\textregistered} for faster digital manufacturing. 3D Printing PowerPac eliminates manual programming.},
  howpublished = {https://new.abb.com/products/robotics/software-and-digital/application-software/3d-printing-powerpac},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\J8QENTIW\3d-printing-powerpac.html}
}

@article{AbdelAziz.2015,
  title = {Direct Linear Transformation from Comparator Coordinates into Object Space Coordinates in Close-Range Photogrammetry},
  author = {{Abdel-Aziz}, Y. I. and Karara, H. M.},
  year = {2015},
  journal = {Photogrammetric Engineering \& Remote Sensing},
  volume = {81},
  number = {2},
  pages = {103--107},
  issn = {00991112},
  doi = {10.14358/PERS.81.2.103},
  pagination = {page}
}

@article{abelExpressivityMarkovReward2021,
  title = {On the {{Expressivity}} of {{Markov Reward}}},
  author = {Abel, David and Dabney, Will and Harutyunyan, Anna and Ho, Mark K. and Littman, Michael L. and Precup, Doina and Singh, Satinder},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.00876 [cs]},
  eprint = {2111.00876},
  primaryclass = {cs},
  urldate = {2021-12-05},
  abstract = {Reward is the driving force for reinforcement-learning agents. This paper is dedicated to understanding the expressivity of reward as a way to capture tasks that we would want an agent to perform. We frame this study around three new abstract notions of "task" that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) a partial ordering over trajectories. Our main results prove that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. We then provide a set of polynomial-time algorithms that construct a Markov reward function that allows an agent to optimize tasks of each of these three types, and correctly determine when no such reward function exists. We conclude with an empirical study that corroborates and illustrates our theoretical findings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\DWVVIMT9\2111.html}
}

@book{abelsonStructureInterpretationComputer1996,
  title = {{Structure and Interpretation of Computer Programs}},
  shorttitle = {{Structure and Interpretation of Computer Programs, second edition}},
  author = {Abelson, Harold and Sussman, Gerald Jay and Sussman, Julie},
  year = {1996},
  month = jul,
  edition = {2nd ed},
  publisher = {The MIT Press},
  address = {Cambridge, Mass.},
  abstract = {Structure and Interpretation of Computer Programs has had a dramatic impact on computer science curricula over the past decade. This long-awaited revision contains changes throughout the text. There are new implementations of most of the major programming systems in the book, including the interpreters and compilers, and the authors have incorporated many small changes that reflect their experience teaching the course at MIT since the first edition was published. A new theme has been introduced that emphasizes the central role played by different approaches to dealing with time in computational models: objects with state, concurrent programming, functional programming and lazy evaluation, and nondeterministic programming. There are new example sections on higher-order procedures in graphics and on applications of stream processing in numerical programming, and many new exercises. In addition, all the programs have been reworked to run in any Scheme implementation that adheres to the IEEE standard.},
  isbn = {978-0-262-51087-5},
  langid = {Englisch}
}

@inproceedings{abnarExploringLimitsLarge2021,
  title = {Exploring the {{Limits}} of {{Large Scale Pre-Training}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Abnar, Samira and Dehghani, Mostafa and Neyshabur, Behnam and Sedghi, Hanie},
  year = {2021},
  month = oct,
  urldate = {2024-09-29},
  abstract = {Recent developments in large-scale machine learning suggest that by scaling up data, model size and training time properly, one might observe that improvements in pre-training would transfer favorably to most downstream tasks. In this work we systematically study this phenomena and establish that, as we increase the upstream accuracy, performance of downstream tasks {\textbackslash}emph\{saturates\}. In particular, we investigate more than 4800 experiments on Vision Transformers, MLP-Mixers and ResNets with number of parameters ranging from ten million to ten billion, trained on the largest scale of available image data (JFT, ImageNet21K) and evaluated on more than 20 downstream image recognition tasks. We propose a model for downstream performance that reflects the saturation phenomena and captures the nonlinear relationship in performance of upstream and downstream tasks. Delving deeper to understand the reasons that give rise to these phenomena, we show that the observed saturation behavior is closely related to the way that representations evolve through the layers of the models. We showcase an even more extreme scenario where performance on upstream and downstream are at odds with each other. That is, in order to have a better downstream performance, we need to hurt upstream accuracy.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\E7A4HX9V\Abnar et al. - 2021 - Exploring the Limits of Large Scale Pre-training.pdf}
}

@article{abouallabanSystematicReviewRobotics2020,
  title = {A {{Systematic Review}} of {{Robotics Research}} in {{Support}} of {{In-Home Care}} for {{Older Adults}}},
  author = {Abou Allaban, Anas and Wang, Maozhen and Pad{\i}r, Ta{\c s}k{\i}n},
  year = {2020},
  month = feb,
  journal = {Information},
  volume = {11},
  number = {2},
  pages = {75},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/info11020075},
  urldate = {2020-08-21},
  abstract = {The aging population is growing at an unprecedented rate globally and robotics-enabled solutions are being developed to provide better independent living for older adults. In this study, we report the results from a systematic review of the state-of-the-art in home robotics research for caring for older adults. This review aims to address two questions: (1) What research is being done towards integrating robotics for caring for older adults? (2) What are the research and technology challenges that robots are facing in the home? Sixty-three papers have been identified and studied in this review by following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Common themes that are consistent across the reviewed papers are distinguished and consolidated as follows: (1) Ambient assisted living, where smart home environments and physical support tools are studied; (2) Robot ecosystem, where robotic devices are used to provide various services; (3) Social interaction, where the social isolation problem has been targeted. We also summarize the results of similar literature reviews we came across during our search. The results of this study present the current research trends and technologies used in each category. The challenges and limitations of robotics applications are also identified. Suggestions for accelerating the deployment of robots at home for providing older adults with independent care in the home are presented based on the results and insights from this study.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {assistive technology for older adults,robotics for elder care,service robotics,smart home},
  file = {C:\Users\benja\Zotero\storage\NGRJHTKY\75.html}
}

@article{abu-dakkaAdaptationManipulationSkills2015,
  title = {Adaptation of Manipulation Skills in Physical Contact with the Environment to Reference Force Profiles},
  author = {{Abu-Dakka}, Fares J. and Nemec, Bojan and J{\o}rgensen, Jimmy A. and Savarimuthu, Thiusius R. and Kr{\"u}ger, Norbert and Ude, Ale{\v s}},
  year = {2015},
  month = aug,
  journal = {Autonomous Robots},
  volume = {39},
  number = {2},
  pages = {199--217},
  issn = {1573-7527},
  doi = {10.1007/s10514-015-9435-2},
  urldate = {2024-04-22},
  abstract = {We propose a new methodology for learning and adaption of manipulation skills that involve physical contact with the environment. Pure position control is unsuitable for such tasks because even small errors in the desired trajectory can cause significant deviations from the desired forces and torques. The proposed algorithm takes a reference Cartesian trajectory and force/torque profile as input and adapts the movement so that the resulting forces and torques match the reference profiles. The learning algorithm is based on dynamic movement primitives and quaternion representation of orientation, which provide a mathematical machinery for efficient and stable adaptation. Experimentally we show that the robot's performance can be significantly improved within a few iteration steps, compensating for vision and other errors that might arise during the execution of the task. We also show that our methodology is suitable both for robots with admittance and for robots with impedance control.},
  langid = {english},
  keywords = {Manipulation and compliant assembly,Physical human-robot interaction,Programming by demonstration,Skill learning and adaptation}
}

@article{adadiPeekingBlackBoxSurvey2018,
  title = {Peeking {{Inside}} the {{Black-Box}}: {{A Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  shorttitle = {Peeking {{Inside}} the {{Black-Box}}},
  author = {Adadi, Amina and Berrada, Mohammed},
  year = {2018},
  journal = {IEEE Access},
  volume = {6},
  pages = {52138--52160},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2870052},
  abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  keywords = {Biological system modeling,black-box models,Conferences,Explainable artificial intelligence,interpretable machine learning,Machine learning,Machine learning algorithms,Market research,Prediction algorithms},
  file = {C:\Users\benja\Zotero\storage\EHWCQA6T\8466590.html}
}

@article{adamExecutingRobotTask,
  title = {Executing {{Robot Task Models}} in {{Dynamic Environments}}},
  author = {Adam, Kai and Butting, Arvid and Kautz, Oliver and Rumpe, Bernhard and Wortmann, Andreas},
  pages = {7},
  abstract = {Deploying successful robotics applications requires tremendous effort due to the need for contributions of experts from various domains. We present the iserveU family of executable DSLs that separate the concerns of domain experts and robotics experts and leverage model-transformation at system run-time to enable the robotic platform to flexibly fulfill tasks in a changing real-world environment. Current research in DSLs for robotics applications focuses on abstraction in the solution domain, whereas our DSLs support the domain expert in declaratively describing properties of the domain and loosely coupled tasks. To enable flexible task execution based on the domain expert's declarative models, these are translated into components of a reference architecture prior to deployment and into planning domain definition language (PDDL) problems at system runtime. Resulting problems are translated into executable plans using the Metric-FF solver and re-translated into iserveU models that ultimately are executed against a loosely coupled robotics middleware. Leveraging model transformation at run-time enables the flexibility necessary for robotics applications deployed to dynamic environments where design-time assumptions and run-time reality diverge easily.},
  langid = {english}
}

@article{adamsSurveyInverseReinforcement2022,
  title = {A Survey of Inverse Reinforcement Learning},
  author = {Adams, Stephen and Cody, Tyler and Beling, Peter A.},
  year = {2022},
  month = aug,
  journal = {Artificial Intelligence Review},
  volume = {55},
  number = {6},
  pages = {4307--4346},
  issn = {1573-7462},
  doi = {10.1007/s10462-021-10108-x},
  urldate = {2024-05-25},
  abstract = {Learning from demonstration, or imitation learning, is the process of learning to act in an environment from examples provided by a teacher. Inverse reinforcement learning (IRL) is a specific form of learning from demonstration that attempts to estimate the reward function of a Markov decision process from examples provided by the teacher. The reward function is often considered the most succinct description of a task. In simple applications, the reward function may be known or easily derived from properties of the system and hard coded into the learning process. However, in complex applications, this may not be possible, and it may be easier to learn the reward function by observing the actions of the teacher. This paper provides a comprehensive survey of the literature on IRL. This survey outlines the differences between IRL and two similar methods - apprenticeship learning and inverse optimal control. Further, this survey organizes the IRL literature based on the principal method, describes applications of IRL algorithms, and provides areas of future research.},
  langid = {english},
  keywords = {Apprenticeship learning,Inverse optimal control,Inverse reinforcement learning,Learning from demonstration,Reinforcement learning}
}

@article{adlerSolvingIllposedInverse2017,
  title = {Solving Ill-Posed Inverse Problems Using Iterative Deep Neural Networks},
  author = {Adler, Jonas and {\"O}ktem, Ozan},
  year = {2017},
  month = nov,
  journal = {Inverse Problems},
  volume = {33},
  number = {12},
  pages = {124007},
  issn = {0266-5611},
  doi = {10.1088/1361-6420/aa9581},
  urldate = {2019-05-17},
  abstract = {We propose a partially learned approach for the solution of ill-posed inverse problems with not necessarily linear forward operators. The method builds on ideas from classical regularisation theory and recent advances in deep learning to perform learning while making use of prior information about the inverse problem encoded in the forward operator, noise model and a regularising functional. The method results in a gradient-like iterative scheme, where the `gradient' component is learned using a convolutional network that includes the gradients of the data discrepancy and regulariser as input in each iteration. We present results of such a partially learned gradient scheme on a non-linear tomographic inversion problem with simulated data from both the Sheep-Logan phantom as well as a head CT. The outcome is compared against filtered backprojection and total variation reconstruction and the proposed method provides a 5.4 dB PSNR improvement over the total variation reconstruction while being significantly faster, giving reconstructions of pixel images in about 0.4 s using a single graphics processing unit (GPU).},
  langid = {english}
}

@misc{AdvancedAlgorithmsBosch,
  title = {{Advanced Algorithms for Bosch XDK -- Knowtion}},
  urldate = {2021-04-14},
  langid = {ngerman}
}

@article{agostinhoExplainabilityKeyIngredient2023,
  title = {Explainability as the Key Ingredient for {{AI}} Adoption in {{Industry}} 5.0 Settings},
  author = {Agostinho, Carlos and Dikopoulou, Zoumpolia and Lavasa, Eleni and Perakis, Konstantinos and Pitsios, Stamatis and Branco, Rui and Reji, Sangeetha and Hetterich, Jonas and Biliri, Evmorfia and Lampathaki, Fenareti and Rodr{\'i}guez Del Rey, Silvia and Gkolemis, Vasileios},
  year = {2023},
  month = dec,
  journal = {Frontiers in Artificial Intelligence},
  volume = {6},
  pages = {1264372},
  issn = {2624-8212},
  doi = {10.3389/frai.2023.1264372},
  urldate = {2024-01-26},
  abstract = {Explainable Artificial Intelligence (XAI) has gained significant attention as a means to address the transparency and interpretability challenges posed by black box AI models. In the context of the manufacturing industry, where complex problems and decision-making processes are widespread, the XMANAI platform emerges as a solution to enable transparent and trustworthy collaboration between humans and machines. By leveraging advancements in XAI and catering the prompt collaboration between data scientists and domain experts, the platform enables the construction of interpretable AI models that offer high transparency without compromising performance. This paper introduces the approach to building the XMANAI platform and highlights its potential to resolve the ``transparency paradox'' of AI. The platform not only addresses technical challenges related to transparency but also caters to the specific needs of the manufacturing industry, including lifecycle management, security, and trusted sharing of AI assets. The paper provides an overview of the XMANAI platform main functionalities, addressing the challenges faced during the development and presenting the evaluation framework to measure the performance of the delivered XAI solutions. It also demonstrates the benefits of the XMANAI approach in achieving transparency in manufacturing decision-making, fostering trust and collaboration between humans and machines, improving operational efficiency, and optimizing business value.},
  pmcid = {PMC10749339},
  pmid = {38146276}
}

@inproceedings{agrawalAnalyzingPerformanceMultilayer2014,
  title = {Analyzing the {{Performance}} of {{Multilayer Neural Networks}} for {{Object Recognition}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2014},
  author = {Agrawal, Pulkit and Girshick, Ross and Malik, Jitendra},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  pages = {329--344},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-10584-0_22},
  abstract = {In the last two years, convolutional neural networks (CNNs) have achieved an impressive suite of results on standard recognition datasets and tasks. CNN-based features seem poised to quickly replace engineered representations, such as SIFT and HOG. However, compared to SIFT and HOG, we understand much less about the nature of the features learned by large CNNs. In this paper, we experimentally probe several aspects of CNN feature learning in an attempt to help practitioners gain useful, evidence-backed intuitions about how to apply CNNs to computer vision problems.},
  isbn = {978-3-319-10584-0},
  langid = {english},
  keywords = {convolutional neural networks,empirical analysis,object recognition},
  file = {C:\Users\benja\Zotero\storage\JFFR6P2G\Agrawal et al. - 2014 - Analyzing the Performance of Multilayer Neural Networks for Object Recognition.pdf}
}

@inproceedings{agustinos20152d,
  title = {{{2D}}/{{3D}} Real-Time Tracking of Surgical Instruments Based on Endoscopic Image Processing},
  booktitle = {Computer-Assisted and Robotic Endoscopy},
  author = {Agustinos, Anthony and Voros, Sandrine},
  year = {2015},
  pages = {90--100},
  organization = {Springer}
}

@inproceedings{Ahmed.2008,
  title = {Dense Correspondence Finding for Parametrization-Free Animation Reconstruction from Video},
  booktitle = {2008 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Ahmed, Naveed and Theobalt, Christian and Rossl, Christian and Thrun, Sebastian and Seidel, Hans-Peter},
  year = {2008},
  pages = {1--8},
  publisher = {IEEE},
  doi = {10.1109/CVPR.2008.4587758},
  bookpagination = {page},
  isbn = {978-1-4244-2242-5}
}

@inproceedings{ahnCanNotSay2022,
  title = {Do {{As I Can}}, {{Not As I Say}}: {{Grounding Language}} in {{Robotic Affordances}}},
  shorttitle = {Do {{As I Can}}, {{Not As I Say}}},
  booktitle = {6th {{Annual Conference}} on {{Robot Learning}}},
  author = {Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Ho, Daniel and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jang, Eric and Ruano, Rosario Jauregui and Jeffrey, Kyle and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Luu, Linda and Parada, Carolina and Pastor, Peter and Quiambao, Jornell and Rao, Kanishka and Rettinghouse, Jarek and Reyes, Diego and Sermanet, Pierre and Sievers, Nicolas and Tan, Clayton and Toshev, Alexander and Vanhoucke, Vincent and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yan, Mengyuan and Zeng, Andy},
  year = {2022},
  month = aug,
  eprint = {2204.01691},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.01691},
  urldate = {2024-08-16},
  abstract = {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model's "hands and eyes," while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally-extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project's website and the video can be found at https://say-can.github.io/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics}
}

@inproceedings{ahnProjectorRobotAugmented2011,
  title = {Projector Robot for Augmented Children's Play},
  author = {Ahn, Jong-gil and Yang, Hyeonsuk and Kim, Gerard and Kim, Namgyu and Choi, Kyoung and Yeon, Hyemin and Hyun, Eunja and Jo, Miheon and Han, Jeong},
  year = {2011},
  month = mar,
  pages = {27--28},
  doi = {10.1145/1957656.1957666},
  abstract = {Participating in a play is one of integral curriculum for young children at nurseries and kindergartens. At the same time, it is not very easy to successfully run and manage a play for young children due to their low age and immaturity. Scripts are difficult to memorize and children's attention span is quite short. We are exploring the use of a robot and augmented reality (AR) technology to assist the nursery teachers in hopes to alleviate the difficult and complicated task of running the play, and also as a way to increase the learning effect by promoting the concentration and immersion (by the presence of the robot and novelty of the augmented display) [1, 2, 3]. For this purpose, we have devised a semi-autonomous remote-controlled projector robot with the capabilities of background projection and control, generating the synthesized augmented view, camera/movement control, producing story narration and various special effects. We have recently deployed the robot assistant for a play ('Three Little Pigs') at an actual nursery to observe and investigate various aspects of human robot interaction. For instance, the robot interacts with the actors on stage, leading and guiding them by showing (with small display on the robot) the synthesized augmented view, script guidance, and putting forth and changing the backdrop projection. It also assumes the role of the "camera man" and may instigate minute interplay with the actor as it zooms in and out on actors (by remote control). Our initial observation indicated that the use of the robot and AR indeed exhibited very high potential in drawing the attention of the children and enhancing the educational effect, but required the right amount of autonomy and external control and an intuitive interface.}
}

@article{ajaykumarSurveyEndUserRobot2021,
  title = {A {{Survey}} on {{End-User Robot Programming}}},
  author = {Ajaykumar, Gopika and Steele, Maureen and Huang, Chien-Ming},
  year = {2021},
  month = oct,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {8},
  pages = {164:1--164:36},
  issn = {0360-0300},
  doi = {10.1145/3466819},
  urldate = {2023-06-13},
  abstract = {As robots interact with a broader range of end-users, end-user robot programming has helped democratize robot programming by empowering end-users who may not have experience in robot programming to customize robots to meet their individual contextual needs. This article surveys work on end-user robot programming, with a focus on end-user program specification. It describes the primary domains, programming phases, and design choices represented by the end-user robot programming literature. The survey concludes by highlighting open directions for further investigation to enhance and widen the reach of end-user robot programming systems.},
  keywords = {end-user programming,End-user robot programming,human-robot interaction}
}

@article{ajaykumarSurveyEndUserRobot2021a,
  title = {A {{Survey}} on {{End-User Robot Programming}}},
  author = {Ajaykumar, Gopika and Steele, Maureen and Huang, Chien-Ming},
  year = {2021},
  month = oct,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {8},
  pages = {164:1--164:36},
  issn = {0360-0300},
  doi = {10.1145/3466819},
  urldate = {2024-03-06},
  abstract = {As robots interact with a broader range of end-users, end-user robot programming has helped democratize robot programming by empowering end-users who may not have experience in robot programming to customize robots to meet their individual contextual needs. This article surveys work on end-user robot programming, with a focus on end-user program specification. It describes the primary domains, programming phases, and design choices represented by the end-user robot programming literature. The survey concludes by highlighting open directions for further investigation to enhance and widen the reach of end-user robot programming systems.},
  keywords = {end-user programming,End-user robot programming,human-robot interaction}
}

@inproceedings{akbariVATTTransformersMultimodal2022,
  title = {{{VATT}}: {{Transformers}} for {{Multimodal Self-Supervised Learning}} from {{Raw Video}}, {{Audio}} and {{Text}}},
  shorttitle = {{{VATT}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
  year = {2022},
  month = jan,
  urldate = {2023-02-21},
  abstract = {We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1\% on Kinetics-400, 83.6\% on Kinetics-600, 72.7\% on Kinetics-700, and 41.1\% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7\% top-1 accuracy on ImageNet compared to 64.7\% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4\% on AudioSet without any supervised pre-training.},
  langid = {english}
}

@inproceedings{akbulutACNMPSkillTransfer2021,
  title = {{{ACNMP}}: {{Skill Transfer}} and {{Task Extrapolation}} through {{Learning}} from {{Demonstration}} and {{Reinforcement Learning}} via {{Representation Sharing}}},
  shorttitle = {{{ACNMP}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Robot Learning}}},
  author = {Akbulut, Mete and Oztop, Erhan and Seker, Muhammet Yunus and X, Hh and Tekden, Ahmet and Ugur, Emre},
  year = {2021},
  month = oct,
  pages = {1896--1907},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-09-29},
  abstract = {To equip robots with dexterous skills, an effective approach is to first transfer the desired skill via Learning from Demonstration (LfD), then let the robot improve it by self-exploration via Reinforcement Learning (RL). In this paper, we propose a novel LfD+RL framework, namely Adaptive Conditional Neural Movement Primitives (ACNMP), that allows efficient policy improvement in novel environments and effective skill transfer between different agents. This is achieved through exploiting the latent representation learned by the underlying Conditional Neural Process (CNP) model, and simultaneous training of the model with supervised learning (SL) for acquiring the demonstrated trajectories and via RL for new trajectory discovery. Through simulation experiments, we show that (i) ACNMP enables the system to extrapolate to situations where pure LfD fails; (ii) Simultaneous training of the system through SL and RL  preserves the shape of demonstrations while adapting to novel situations due to the shared representations used by both learners; (iii) ACNMP enables order-of-magnitude sample-efficient RL in extrapolation of reaching tasks compared to the existing approaches; (iv) ACNMPs can be used to implement skill transfer between robots having different morphology, with competitive learning speeds and importantly with less number of assumptions compared to the state-of-the-art approaches. Finally, we show the real-world suitability of ACNMPs through real robot experiments that involve obstacle avoidance, pick and place and pouring actions.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\7MTU8NQT\Akbulut et al. - 2021 - ACNMP Skill Transfer and Task Extrapolation through Learning from Demonstration and Reinforcement L.pdf}
}

@inproceedings{akbulutRewardConditionedNeural2021,
  title = {Reward {{Conditioned Neural Movement Primitives}} for {{Population-Based Variational Policy Optimization}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Akbulut, M. Tuluhan and Bozdogan, Utku and Tekden, Ahmet and Ugur, Emre},
  year = {2021},
  month = may,
  pages = {10808--10814},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9560897},
  urldate = {2024-09-29},
  abstract = {This paper aims to study the reward-based policy exploration problem in a supervised learning approach and enable robots to form complex movement trajectories in challenging reward settings and search spaces. For this, the experience of the robot, which can be bootstrapped from demonstrated trajectories, is used to train a novel Neural Processes-based deep network that samples from its latent space and generates the required trajectories given desired rewards. Our framework can generate progressively improved trajectories by sampling them from high reward landscapes, increasing the reward gradually. Variational inference is used to create a stochastic latent space to sample varying trajectories in generating a population of trajectories given target rewards. We benefit from Evolutionary Strategies and propose a novel crossover operation, which is applied in the self-organized latent space of the individual policies, allowing blending of the individuals that might address different factors in the reward function. Using a number of tasks that require sequential reaching to multiple points or passing through gaps between objects, we showed that our method provides stable learning progress and significantly higher sample efficiency compared to a number of state-of-the-art robotic reinforcement learning methods. Finally, we show the real-world suitability of our method through real robot execution involving obstacle avoidance.},
  keywords = {Reinforcement learning,Search problems,Sociology,Statistics,Stochastic processes,Supervised learning,Trajectory},
  file = {C\:\\Users\\benja\\Zotero\\storage\\7H7G7GRT\\Akbulut et al. - 2021 - Reward Conditioned Neural Movement Primitives for Population-Based Variational Policy Optimization.pdf;C\:\\Users\\benja\\Zotero\\storage\\YEMIGEZZ\\9560897.html}
}

@misc{akcayDaimlerIntegraStandards2016,
  title = {Daimler {{Integra Standards}} for {{Robotics}}},
  author = {Akcay, Ali},
  year = {2016},
  month = nov,
  address = {Birmingham, UK},
  urldate = {2024-04-16}
}

@article{akintundeFormalVerificationNeural2021,
  title = {Formal Verification of Neural Agents in Non-Deterministic Environments},
  author = {Akintunde, Michael E. and Botoeva, Elena and Kouvaros, Panagiotis and Lomuscio, Alessio},
  year = {2021},
  month = nov,
  journal = {Autonomous Agents and Multi-Agent Systems},
  volume = {36},
  number = {1},
  issn = {1573-7454},
  doi = {10.1007/s10458-021-09529-3},
  urldate = {2022-04-30},
  abstract = {We introduce a model for agent-environment systems where the agents are implemented via feed-forward ReLU neural networks and the environment is non-deterministic. We study the verification problem of such systems against CTL properties. We show that verifying these systems against reachability properties is undecidable. We introduce a bounded fragment of CTL, show its usefulness in identifying shallow bugs in the system, and prove that the verification problem against specifications in bounded CTL is in coNExpTime and PSpace-hard. We introduce sequential and parallel algorithms for MILP-based verification of agent-environment systems, present an implementation, and report the experimental results obtained against a variant of the VerticalCAS use-case and the frozen lake scenario.},
  langid = {english},
  keywords = {Model checking,Neural agents,Verification}
}

@inproceedings{akrourLocalBayesianOptimization2017,
  title = {Local {{Bayesian Optimization}} of {{Motor Skills}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Akrour, Riad and Sorokin, Dmitry and Peters, Jan and Neumann, Gerhard},
  year = {2017},
  month = jul,
  pages = {41--50},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2020-10-31},
  abstract = {Bayesian optimization is renowned for its sample efficiency but its application to higher dimensional tasks is impeded by its focus on global optimization. To scale to higher dimensional problems, ...},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\CU7KIP4K\akrour17a.html}
}

@inproceedings{alayracFlamingoVisualLanguage2022,
  title = {Flamingo: A {{Visual Language Model}} for {{Few-Shot Learning}}},
  shorttitle = {Flamingo},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  year = {2022},
  month = may,
  urldate = {2024-01-05},
  abstract = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer, captioning tasks, which evaluate the ability to describe a scene or an event, and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
  langid = {english}
}

@article{aleotti3DShapeSegmentation2012,
  title = {A {{3D}} Shape Segmentation Approach for Robot Grasping by Parts},
  author = {Aleotti, Jacopo and Caselli, Stefano},
  year = {2012},
  month = mar,
  journal = {Robotics and Autonomous Systems},
  series = {Autonomous {{Grasping}}},
  volume = {60},
  number = {3},
  pages = {358--366},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2011.07.022},
  urldate = {2023-02-27},
  abstract = {Neuro-psychological findings have shown that human perception of objects is based on part decomposition. Most objects are made of multiple parts which are likely to be the entities actually involved in grasp affordances. Therefore, automatic object recognition and robot grasping should take advantage from 3D shape segmentation. This paper presents an approach toward planning robot grasps across similar objects by part correspondence. The novelty of the method lies in the topological decomposition of objects that enables high-level semantic grasp planning. In particular, given a 3D model of an object, the representation is initially segmented by computing its Reeb graph. Then, automatic object recognition and part annotation are performed by applying a shape retrieval algorithm. After the recognition phase, queries are accepted for planning grasps on individual parts of the object. Finally, a robot grasp planner is invoked for finding stable grasps on the selected part of the object. Grasps are evaluated according to a widely used quality measure. Experiments performed in a simulated environment on a reasonably large dataset show the potential of topological segmentation to highlight candidate parts suitable for grasping.},
  langid = {english},
  keywords = {Grasping,Object recognition,Shape segmentation},
  file = {C:\Users\benja\Zotero\storage\W4PPU8VY\S0921889011001552.html}
}

@inproceedings{aleottiPartbasedRobotGrasp2011,
  title = {Part-Based Robot Grasp Planning from Human Demonstration},
  booktitle = {2011 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Aleotti, Jacopo and Caselli, Stefano},
  year = {2011},
  month = may,
  pages = {4554--4560},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2011.5979632},
  abstract = {In this work we introduce a novel approach for robot grasp planning. The proposed method combines the benefits of programming by human demonstration for teaching appropriate grasps with those of automatic 3D shape segmentation for object recognition and semantic modeling. The work is motivated by important studies on human manipulation suggesting that when an object is perceived for grasping it is first parsed in its constituent parts. Following these findings we present a manipulation planning system capable of grasping objects by their parts which learns new tasks from human demonstration. The central advantage over previous approaches is the use of a topological method for shape segmentation enabling both object retrieval and part-based grasp planning according to the affordances of an object. Manipulation tasks are demonstrated in a virtual reality environment using a data glove. After the learning phase, each task is planned and executed in a robot environment that is able to generalize to similar, but previously unknown, objects.},
  keywords = {Grasping,Humans,Planning,Robots,Shape,Solid modeling,Three dimensional displays},
  file = {C:\Users\benja\Zotero\storage\9YW6GKEY\5979632.html}
}

@inproceedings{alexandrovaRobotProgrammingDemonstration2014,
  title = {Robot {{Programming}} by {{Demonstration}} with {{Interactive Action Visualizations}}},
  booktitle = {Robotics: {{Science}} and {{Systems X}}},
  author = {Alexandrova, Sonya and Cakmak, Maya and Hsiao, Kaijen and Takayama, Leila},
  year = {2014},
  month = jul,
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10.15607/RSS.2014.X.048},
  urldate = {2024-08-15},
  abstract = {---Existing approaches to Robot Programming by Demonstration (PbD) require multiple demonstrations to capture task information that lets robots generalize to unseen situations. However, providing these demonstrations is cumbersome for end-users. In addition, users who are not familiar with the system often fail to demonstrate sufficiently varied demonstrations. We propose an alternative PbD framework that involves demonstrating the task once and then providing additional task information explicitly, through interactions with a visualization of the action. We present a simple action representation that supports this framework and describe a system that implements the framework on a two-armed mobile manipulator. We demonstrate the power of this system by evaluating it on a diverse task benchmark that involves manipulation of everyday objects. We then demonstrate that the system is easy to learn and use for novice users through a user study in which participants program a subset of the benchmark. We characterize the limitations of our system in task generalization and end-user interactions and present extensions that could address some of the limitations.},
  isbn = {978-0-9923747-0-9}
}

@article{allan2012toward,
  title = {Toward Detection and Localization of Instruments in Minimally Invasive Surgery},
  author = {Allan, Max and Ourselin, S{\'e}bastien and Thompson, Steve and Hawkes, David J and Kelly, John and Stoyanov, Danail},
  year = {2012},
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {60},
  number = {4},
  pages = {1050--1058},
  publisher = {IEEE}
}

@misc{allanStereoCorrespondenceReconstruction2021,
  title = {Stereo {{Correspondence}} and {{Reconstruction}} of {{Endoscopic Data Challenge}}},
  author = {Allan, Max and Mcleod, Jonathan and Wang, Congcong and Rosenthal, Jean Claude and Hu, Zhenglei and Gard, Niklas and Eisert, Peter and Fu, Ke Xue and Zeffiro, Trevor and Xia, Wenyao and Zhu, Zhanshi and Luo, Huoling and Jia, Fucang and Zhang, Xiran and Li, Xiaohong and Sharan, Lalith and Kurmann, Tom and Schmid, Sebastian and Sznitman, Raphael and Psychogyios, Dimitris and Azizian, Mahdi and Stoyanov, Danail and {Maier-Hein}, Lena and Speidel, Stefanie},
  year = {2021},
  month = jan,
  number = {arXiv:2101.01133},
  eprint = {2101.01133},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.01133},
  urldate = {2024-04-12},
  abstract = {The stereo correspondence and reconstruction of endoscopic data sub-challenge was organized during the Endovis challenge at MICCAI 2019 in Shenzhen, China. The task was to perform dense depth estimation using 7 training datasets and 2 test sets of structured light data captured using porcine cadavers. These were provided by a team at Intuitive Surgical. 10 teams participated in the challenge day. This paper contains 3 additional methods which were submitted after the challenge finished as well as a supplemental section from these teams on issues they found with the dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\benja\Zotero\storage\T93SBZ7X\2101.html}
}

@article{allshireLASERLearningLatent2021,
  title = {{{LASER}}: {{Learning}} a {{Latent Action Space}} for {{Efficient Reinforcement Learning}}},
  shorttitle = {{{LASER}}},
  author = {Allshire, Arthur and {Mart{\'i}n-Mart{\'i}n}, Roberto and Lin, Charles and Manuel, Shawn and Savarese, Silvio and Garg, Animesh},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.15793 [cs]},
  eprint = {2103.15793},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {The process of learning a manipulation task depends strongly on the action space used for exploration: posed in the incorrect action space, solving a task with reinforcement learning can be drastically inefficient. Additionally, similar tasks or instances of the same task family impose latent manifold constraints on the most effective action space: the task family can be best solved with actions in a manifold of the entire action space of the robot. Combining these insights we present LASER, a method to learn latent action spaces for efficient reinforcement learning. LASER factorizes the learning problem into two sub-problems, namely action space learning and policy learning in the new action space. It leverages data from similar manipulation task instances, either from an offline expert or online during policy learning, and learns from these trajectories a mapping from the original to a latent action space. LASER is trained as a variational encoder-decoder model to map raw actions into a disentangled latent action space while maintaining action reconstruction and latent space dynamic consistency. We evaluate LASER on two contact-rich robotic tasks in simulation, and analyze the benefit of policy learning in the generated latent action space. We show improved sample efficiency compared to the original action space from better alignment of the action space to the task space, as we observe with visualizations of the learned action space manifold. Additional details: https://www.pair.toronto.edu/laser},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\2WSPED4K\2103.html}
}

@mastersthesis{altAutomaticParameterizationRobot2019,
  title = {Automatic {{Parameterization}} of {{Robot Programs}} via {{Learning}} of {{Neural Program Representations}}},
  author = {Alt, Benjamin},
  year = {2019},
  month = sep,
  address = {Karlsruhe},
  school = {Karlsruhe Institute of Technology}
}

@inproceedings{altBANSAIBridgingAI2024,
  title = {{{BANSAI}}: {{Towards Bridging}} the {{AI Adoption Gap}} in {{Industrial Robotics}} with {{Neurosymbolic Programming}}},
  shorttitle = {{{BANSAI}}},
  booktitle = {57th {{CIRP Conference}} on {{Manufacturing Systems}} 2024},
  author = {Alt, Benjamin and Dvorak, Julia and Katic, Darko and J{\"a}kel, Rainer and Beetz, Michael and Lanza, Gisela},
  year = {2024},
  month = may,
  eprint = {2404.13652},
  primaryclass = {cs},
  publisher = {arXiv},
  address = {P{\'o}voa de Varzim, Portugal},
  doi = {10.48550/arXiv.2404.13652},
  urldate = {2024-04-30},
  abstract = {Over the past decade, deep learning helped solve manipulation problems across all domains of robotics. At the same time, industrial robots continue to be programmed overwhelmingly using traditional program representations and interfaces. This paper undertakes an analysis of this "AI adoption gap" from an industry practitioner's perspective. In response, we propose the BANSAI approach (Bridging the AI Adoption Gap via Neurosymbolic AI). It systematically leverages principles of neurosymbolic AI to establish data-driven, subsymbolic program synthesis and optimization in modern industrial robot programming workflow. BANSAI conceptually unites several lines of prior research and proposes a path toward practical, real-world validation.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {68T40,Computer Science - Artificial Intelligence,Computer Science - Computational Engineering Finance and Science,Computer Science - Machine Learning,Computer Science - Robotics,I.2.1,I.2.2,I.2.9,J.6,J.7,my},
  file = {C:\Users\benja\Zotero\storage\WJ3VL9EH\2404.html}
}

@misc{altCrossPlatformSensorEnabledIndustrial2021,
  title = {Cross-{{Platform Sensor-Enabled Industrial Robot Programming}} and {{Control}}},
  author = {Alt, Benjamin},
  year = {2021},
  month = dec
}

@inproceedings{altDomainSpecificFineTuningLarge2024,
  title = {Domain-{{Specific Fine-Tuning}} of {{Large Language Models}} for {{Interactive Robot Programming}}},
  booktitle = {European {{Robotics Forum}} 2024},
  author = {Alt, Benjamin and Ke{\ss}ner, Urs and Taranovic, Aleksandar and Katic, Darko and Hermann, Andreas and J{\"a}kel, Rainer and Neumann, Gerhard},
  year = {2024},
  month = mar,
  series = {Springer {{Proceedings}} in {{Advanced Robotics}}},
  eprint = {2312.13905},
  primaryclass = {cs},
  publisher = {Springer Nature},
  address = {Rimini, Italy},
  urldate = {2023-12-22},
  abstract = {Industrial robots are applied in a widening range of industries, but robot programming mostly remains a task limited to programming experts. We propose a natural language-based assistant for programming of advanced, industrial robotic applications and investigate strategies for domain-specific fine-tuning of foundation models with limited data and compute.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  isbn = {978-3-031-76428-8},
  keywords = {68T40,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics,I.2.5,I.2.6,I.2.7,I.2.9,my,relevant},
  file = {C:\Users\benja\Zotero\storage\L4YRMDV3\2312.html}
}

@inproceedings{altEfficientPPSPartawarePanoptic2023,
  title = {{{EfficientPPS}}: {{Part-aware Panoptic Segmentation}} of {{Transparent Objects}} for {{Robotic Manipulation}}},
  booktitle = {{{ISR Europe}} 2023},
  author = {Alt, Benjamin and Nguyen, Minh Dang and Hermann, Andreas and Katic, Darko and J{\"a}kel, Rainer and Dillmann, R{\"u}diger and Sax, Eric},
  year = {2023},
  month = sep,
  publisher = {VDE Verlag},
  address = {Stuttgart, Germany},
  abstract = {The use of autonomous robots for assistance tasks in hospitals has the potential to free up qualified staff and improve patient care. However, the ubiquity of deformable and transparent objects in hospital settings poses significant challenges to vision-based perception systems. We present EfficientPPS, a neural architecture for part-aware panoptic segmentation that provides robots with semantically rich visual information for grasping and manipulation tasks. We also present an unsupervised data collection and labelling method to reduce the need for human involvement in the training process. EfficientPPS is evaluated on a dataset containing real-world hospital objects and demonstrated to be robust and efficient in grasping transparent transfusion bags with a collaborative robot arm.},
  copyright = {All rights reserved},
  isbn = {978-3-8007-6140-1},
  keywords = {my}
}

@inproceedings{altHeuristicFreeOptimizationForceControlled2022,
  title = {Heuristic-{{Free Optimization}} of {{Force-Controlled Robot Search Strategies}} in {{Stochastic Environments}}},
  booktitle = {2022 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Alt, Benjamin and Katic, Darko and J{\"a}kel, Rainer and Beetz, Michael},
  year = {2022},
  month = oct,
  pages = {8887--8893},
  issn = {2153-0866},
  doi = {10.1109/IROS47612.2022.9982093},
  abstract = {In both industrial and service domains, a central benefit of the use of robots is their ability to quickly and reliably execute repetitive tasks. However, even relatively simple peg-in-hole tasks are typically subject to stochastic variations, requiring search motions to find relevant features such as holes. While search improves robustness, it comes at the cost of increased runtime: More exhaustive search will maximize the probability of successfully executing a given task, but will significantly delay any downstream tasks. This trade-off is typically resolved by human experts according to simple heuristics, which are rarely optimal. This paper introduces an automatic, data-driven and heuristic-free approach to optimize robot search strategies. By training a neural model of the search strategy on a large set of simulated stochastic environments, conditioning it on few real-world examples and inverting the model, we can infer search strategies which adapt to the time-variant characteristics of the underlying probability distributions, while requiring very few real-world measurements. We evaluate our approach on two different industrial robots in the context of spiral and probe search for THT electronics assembly.**See github.com/benjaminalt/dpse for code and data.},
  copyright = {All rights reserved},
  keywords = {Adaptation models,my,relevant,Runtime,Search problems,Service robots,Spirals,Stochastic processes,Training},
  file = {C\:\\Users\\benja\\Zotero\\storage\\SACZMZRB\\Alt et al. - 2022 - Heuristic-free Optimization of Force-Controlled Ro.pdf;C\:\\Users\\benja\\Zotero\\storage\\B4WTVLJP\\9982093.html}
}

@inproceedings{altHumanAIInteractionIndustrial2024,
  title = {Human-{{AI Interaction}} in {{Industrial Robotics}}: {{Design}} and {{Empirical Evaluation}} of a {{User Interface}} for {{Explainable AI-Based Robot Program Optimization}}},
  shorttitle = {Human-{{AI Interaction}} in {{Industrial Robotics}}},
  booktitle = {57th {{CIRP Conference}} on {{Manufacturing Systems}} 2024},
  author = {Alt, Benjamin and Zahn, Johannes and Kienle, Claudius and Dvorak, Julia and May, Marvin and Katic, Darko and J{\"a}kel, Rainer and Kopp, Tobias and Beetz, Michael and Lanza, Gisela},
  year = {2024},
  month = apr,
  eprint = {2404.19349},
  primaryclass = {cs},
  publisher = {arXiv},
  address = {P{\'o}voa de Varzim, Portugal},
  doi = {10.48550/arXiv.2404.19349},
  urldate = {2024-05-10},
  abstract = {While recent advances in deep learning have demonstrated its transformative potential, its adoption for real-world manufacturing applications remains limited. We present an Explanation User Interface (XUI) for a state-of-the-art deep learning-based robot program optimizer which provides both naive and expert users with different user experiences depending on their skill level, as well as Explainable AI (XAI) features to facilitate the application of deep learning methods in real-world applications. To evaluate the impact of the XUI on task performance, user satisfaction and cognitive load, we present the results of a preliminary user survey and propose a study design for a large-scale follow-up study.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {68T40,Computer Science - Artificial Intelligence,Computer Science - Computational Engineering Finance and Science,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics,I.2.1,I.2.2,I.2.9,J.6,J.7,my},
  file = {C:\Users\benja\Zotero\storage\ERH4U8FW\2404.html}
}

@inproceedings{altKnowledgeDrivenRobotProgram2023,
  title = {Knowledge-{{Driven Robot Program Synthesis}} from {{Human VR Demonstrations}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Principles}} of {{Knowledge Representation}} and {{Reasoning}}},
  author = {Alt, Benjamin and Kenfack, Franklin Kenghagho and Haidu, Andrei and Katic, Darko and J{\"a}kel, Rainer and Beetz, Michael},
  year = {2023},
  month = sep,
  pages = {34--43},
  publisher = {IJCAI},
  address = {Rhodes, Greece},
  doi = {10.24963/kr.2023/4},
  abstract = {Aging societies, labor shortages and increasing wage costs call for assistance robots capable of autonomously performing a wide array of real-world tasks. Such open-ended robotic manipulation requires not only powerful knowledge representations and reasoning (KR\&R) algorithms, but also methods for humans to instruct robots what tasks to perform and how to perform them. In this paper, we present a system for automatically generating executable robot control programs from human task demonstrations in virtual reality (VR). We leverage common-sense knowledge and game engine-based physics to semantically interpret human VR demonstrations, as well as an expressive and general task representation and automatic path planning and code generation, embedded into a state-of-the-art cognitive architecture. We demonstrate our approach in the context of force-sensitive fetch-and-place for a robotic shopping assistant. The source code is available at https://github.com/ease-crc/vr-program-synthesis.},
  copyright = {All rights reserved},
  isbn = {978-1-956792-02-7},
  keywords = {68T30,Computer Science - Artificial Intelligence,Computer Science - Robotics,D.1,F.3,I.2,my,relevant},
  file = {C:\Users\benja\Zotero\storage\9PHHGSAD\2306.html}
}

@inproceedings{altLapSeg3DWeaklySupervised2022,
  title = {{{LapSeg3D}}: {{Weakly Supervised Semantic Segmentation}} of {{Point Clouds Representing Laparoscopic Scenes}}},
  shorttitle = {{{LapSeg3D}}},
  booktitle = {2022 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Alt, Benjamin and Kunz, Christian and Katic, Darko and Younis, Rayan and J{\"a}kel, Rainer and {M{\"u}ller-Stich}, Beat Peter and Wagner, Martin and {Mathis-Ullrich}, Franziska},
  year = {2022},
  month = oct,
  pages = {5265--5270},
  issn = {2153-0866},
  doi = {10.1109/IROS47612.2022.9981178},
  abstract = {The semantic segmentation of surgical scenes is a prerequisite for task automation in robot assisted interventions. We propose LapSeg3D, a novel DNN-based approach for the voxel-wise annotation of point clouds representing surgical scenes. As the manual annotation of training data is highly time consuming, we introduce a semi-autonomous clustering-based pipeline for the annotation of the gallbladder, which is used to generate segmented labels for the DNN. When evaluated against manually annotated data, LapSeg3D achieves an F1 score of 0.94 for gallbladder segmentation on various datasets of ex-vivo porcine livers. We show LapSeg3D to generalize accurately across different gallbladders and datasets recorded with different RGB-D camera systems.},
  copyright = {All rights reserved},
  keywords = {Annotations,Laparoscopes,my,Pipelines,Point cloud compression,Robot vision systems,Semantic segmentation,Training data},
  file = {C\:\\Users\\benja\\Zotero\\storage\\RBQVYHM8\\Alt et al. - 2022 - LapSeg3D Weakly Supervised Semantic Segmentation .pdf;C\:\\Users\\benja\\Zotero\\storage\\5C3SRM9T\\9981178.html}
}

@phdthesis{altMachineLearningPose2017,
  type = {Bachelor's {{Thesis}}},
  title = {Machine {{Learning}} for {{Pose Optimization}}: {{An Integrated Framework}} for the {{Development}} and {{Monitoring}} of {{Adaptive Robot Programs}}},
  author = {Alt, Benjamin},
  year = {2017},
  month = aug,
  address = {Karlsruhe},
  school = {Karlsruhe Institute of Technology}
}

@patent{altMethodSystemDetermining2022,
  title = {Method and {{System}} for {{Determining Optimized Program Parameters}} for a {{Robot Program}}},
  author = {Alt, Benjamin and J{\"a}kel, Rainer and Katic, Darko},
  year = {2022},
  month = mar,
  number = {WO 2022/022784 A1},
  assignee = {ArtiMinds Robotics GmbH},
  nationality = {Germany}
}

@patent{altMethodSystemDetermining2022a,
  title = {Method and {{System}} for {{Determining Optimized Program Parameters}} for a {{Robot Program}}},
  author = {Alt, Benjamin and J{\"a}kel, Rainer and Katic, Darko},
  year = {2022},
  month = feb,
  number = {WO2022022784A1},
  address = {Karlsruhe},
  urldate = {2023-01-28},
  assignee = {ArtiMinds Robotics GmbH},
  copyright = {All rights reserved},
  nationality = {Germany},
  keywords = {my,relevant},
  annotation = {IPC:}
}

@inproceedings{altModulareDatengetriebeneRoboterprogrammierung2020,
  title = {Modulare, Datengetriebene {{Roboterprogrammierung}} F{\"u}r Die {{L{\"o}sung}} Komplexer {{Handhabungsaufgaben}} in {{Alltagsumgebungen}}},
  booktitle = {{{AAL-Kongress}} 2020},
  author = {Alt, Benjamin and Aumann, Florian and Gienger, Lennart and Jordan, Florian and Katic, Darko and J{\"a}kel, Rainer and Graf, Birgit},
  year = {2020},
  pages = {17--22},
  publisher = {VDE Verlag},
  address = {Berlin},
  abstract = {Increasing demand for high-standard elderly care and stagnant availability of qualified labor makes the need for partially autonomous robotic ambient assisted living (AAL) solutions increasingly urgent. The preparation and serving of food and beverages is an important subdomain of care robotics, which is rendered challenging by unstructured environments, a large variety of manipulated objects and the need to interact with human users. This work presents a modular, hardware agnostic approach for programming robots to perform complex assistance functions in unstructured environments. We propose a robot-agnostic library of parameterized robot skills which encapsulate primitive tasks central to food preparation and serving, from which hierarchical manipulation sequences can be constructed. We further propose an approach for the automatic adaption of generated motions based on 3D point clouds of the environment to manipulate objects of different dimensions and shapes. We evaluate our solution on two 6-DOF robotic manipulators as well as a bimanual humanoid assistance robot.},
  copyright = {All rights reserved},
  isbn = {978-3-8007-5342-0},
  keywords = {my},
  file = {C:\Users\benja\Zotero\storage\LT5KQM5A\N-608626.html}
}

@inproceedings{altRoboGrindIntuitiveInteractive2024,
  title = {{{RoboGrind}}: {{Intuitive}} and {{Interactive Surface Treatment}} with {{Industrial Robots}}},
  shorttitle = {{{RoboGrind}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Alt, Benjamin and St{\"o}ckl, Florian and M{\"u}ller, Silvan and Braun, Christopher and Raible, Julian and Alhasan, Saad and Rettig, Oliver and Ringle, Lukas and Katic, Darko and J{\"a}kel, Rainer and Beetz, Michael and Strand, Marcus and Huber, Marco F.},
  year = {2024},
  month = may,
  eprint = {2402.16542},
  primaryclass = {cs},
  publisher = {IEEE},
  address = {Yokohama, Japan},
  doi = {10.1109/ICRA57147.2024.10611143},
  urldate = {2024-03-05},
  abstract = {Surface treatment tasks such as grinding, sanding or polishing are a vital step of the value chain in many industries, but are notoriously challenging to automate. We present RoboGrind, an integrated system for the intuitive, interactive automation of surface treatment tasks with industrial robots. It combines a sophisticated 3D perception pipeline for surface scanning and automatic defect identification, an interactive voice-controlled wizard system for the AI-assisted bootstrapping and parameterization of robot programs, and an automatic planning and execution pipeline for force-controlled robotic surface treatment. RoboGrind is evaluated both under laboratory and real-world conditions in the context of refabricating fiberglass wind turbine blades.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  isbn = {979-8-3503-8457-4},
  keywords = {68T40,Computer Science - Artificial Intelligence,Computer Science - Robotics,I.2.2,I.2.6,I.2.9,my,relevant},
  file = {C:\Users\benja\Zotero\storage\MII54EVS\2402.html}
}

@inproceedings{altRobotProgramParameter2021,
  title = {Robot {{Program Parameter Inference}} via {{Differentiable Shadow Program Inversion}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Alt, Benjamin and Katic, Darko and J{\"a}kel, Rainer and Bozcuoglu, Asil Kaan and Beetz, Michael},
  year = {2021},
  month = may,
  pages = {4672--4678},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9561206},
  abstract = {Challenging manipulation tasks can be solved effectively by combining individual robot skills, which must be parameterized for the concrete physical environment and task at hand. This is time-consuming and difficult for human programmers, particularly for force-controlled skills. To this end, we present Shadow Program Inversion (SPI), a novel approach to infer optimal skill parameters directly from data. SPI leverages unsupervised learning to train an auxiliary differentiable program representation ("shadow program") and realizes parameter inference via gradient-based model inversion. Our method enables the use of efficient first-order optimizers to infer optimal parameters for originally non-differentiable skills, including many skill variants currently used in production. SPI zero-shot generalizes across task objectives, meaning that shadow programs do not need to be retrained to infer parameters for different task variants. We evaluate our methods on three different robots and skill frameworks in industrial and household scenarios. Code and examples are available at https://innolab.artiminds.com/icra2021.},
  copyright = {All rights reserved},
  keywords = {Automation,Codes,Conferences,my,Production,relevant,Service robots,Task analysis,Unsupervised learning},
  file = {C:\Users\benja\Zotero\storage\UD5WY2M9\9561206.html}
}

@misc{altSensorbasedRobotProjects2021,
  type = {Invited Talk},
  title = {Sensor-Based {{Robot Projects}}: {{Innovative Opportunities}} and {{Solutions}} for {{Demanding Robotic Applications}}},
  author = {Alt, Benjamin},
  year = {2021},
  month = sep,
  address = {Hochschule Karlsruhe}
}

@misc{altShadowProgramInversion2024,
  title = {Shadow {{Program Inversion}} with {{Differentiable Planning}}: {{A Framework}} for {{Unified Robot Program Parameter}} and {{Trajectory Optimization}}},
  shorttitle = {Shadow {{Program Inversion}} with {{Differentiable Planning}}},
  author = {Alt, Benjamin and Kienle, Claudius and Katic, Darko and J{\"a}kel, Rainer and Beetz, Michael},
  year = {2024},
  month = sep,
  number = {arXiv:2409.08678},
  eprint = {2409.08678},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.08678},
  urldate = {2024-09-23},
  abstract = {This paper presents SPI-DP, a novel first-order optimizer capable of optimizing robot programs with respect to both high-level task objectives and motion-level constraints. To that end, we introduce DGPMP2-ND, a differentiable collision-free motion planner for serial N-DoF kinematics, and integrate it into an iterative, gradient-based optimization approach for generic, parameterized robot program representations. SPI-DP allows first-order optimization of planned trajectories and program parameters with respect to objectives such as cycle time or smoothness subject to e.g. collision constraints, while enabling humans to understand, modify or even certify the optimized programs. We provide a comprehensive evaluation on two practical household and industrial applications.},
  archiveprefix = {arXiv},
  keywords = {68T40,Computer Science - Artificial Intelligence,Computer Science - Robotics,D.1,I.2},
  file = {C\:\\Users\\benja\\Zotero\\storage\\JPCMCRKQ\\Alt et al. - 2024 - Shadow Program Inversion with Differentiable Planning A Framework for Unified Robot Program Paramet.pdf;C\:\\Users\\benja\\Zotero\\storage\\6IP3PMQM\\2409.html}
}

@patent{altVerfahrenUndSystem2021,
  title = {Verfahren Und {{System}} Zur {{Bestimmung}} von Optimierten {{Programmparametern}} F{\"u}r Ein {{Roboterprogramm}}},
  author = {Alt, Benjamin and Katic, Darko and J{\"a}kel, Rainer},
  year = {2021},
  month = aug,
  number = {DE 10 2020 209 511 B3},
  assignee = {ArtiMinds Robotics GmbH},
  nationality = {Germany}
}

@inproceedings{alurScalingEnumerativeProgram2017,
  title = {Scaling {{Enumerative Program Synthesis}} via {{Divide}} and {{Conquer}}},
  booktitle = {Tools and {{Algorithms}} for the {{Construction}} and {{Analysis}} of {{Systems}}},
  author = {Alur, Rajeev and Radhakrishna, Arjun and Udupa, Abhishek},
  editor = {Legay, Axel and Margaria, Tiziana},
  year = {2017},
  pages = {319--336},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-54577-5_18},
  abstract = {Given a semantic constraint specified by a logical formula, and a syntactic constraint specified by a context-free grammar, the Syntax-Guided Synthesis (SyGuS) problem is to find an expression that satisfies both the syntactic and semantic constraints. An enumerative approach to solve this problem is to systematically generate all expressions from the syntactic space with some pruning, and has proved to be surprisingly competitive in the newly started competition of SyGuS solvers. It performs well on small to medium sized benchmarks, produces succinct expressions, and has the ability to generalize from input-output examples. However, its performance degrades drastically with the size of the smallest solution. To overcome this limitation, in this paper we propose an alternative approach to solve SyGuS instances.},
  isbn = {978-3-662-54577-5},
  langid = {english},
  keywords = {Conditional Expression,Decision Tree,Input Point,Semantic Constraint,Synthesis Problem}
}

@article{alurSearchbasedProgramSynthesis2018,
  title = {Search-Based Program Synthesis},
  author = {Alur, Rajeev and Singh, Rishabh and Fisman, Dana and {Solar-Lezama}, Armando},
  year = {2018},
  month = nov,
  journal = {Communications of the ACM},
  volume = {61},
  number = {12},
  pages = {84--93},
  issn = {0001-0782},
  doi = {10.1145/3208071},
  urldate = {2024-04-18},
  abstract = {A promising, useful tool for future programming development environments.}
}

@phdthesis{Alve16,
  title = {Online {{Starting Pose Optimization}} for {{Force Controlled Motions}} of {{Industrial Robots}}},
  author = {Alves, Felipe Velloso},
  year = {2016},
  school = {Karlsruhe Institute of Technology}
}

@article{alzahrani_biomedical_2021,
  title = {Biomedical {{Image Segmentation}}: {{A Survey}}},
  shorttitle = {Biomedical {{Image Segmentation}}},
  author = {Alzahrani, Yahya and Boufama, Boubakeur},
  year = {2021},
  month = may,
  journal = {SN COMPUT. SCI.},
  volume = {2},
  number = {4},
  pages = {310},
  abstract = {Medical Image Segmentation is the process of segmenting and detecting boundaries of anatomical structures in various types of 2D and 3D-medical images. The latter come from different modalities, such as Magnetic Resonance Imaging (MRI), X-Rays, Positron Emission Tomography (PET)/Single-Photon Emission Computed Tomography, Computed Tomography (CT), and Ultrasound (US). It is a key supporting technology for medical applications including diagnostics, planning, monitoring, and guidance. Hence, a large number of segmentation methods have been published in past decades. This paper presents a comprehensive review of the current medical segmentation techniques. In particular, we reviewed the most important medical segmentation methods that have been utilized for almost all types of medical images. We grouped these methods into categories and then compared, contrasted, and highlighted their main advantages and limitations.},
  langid = {english}
}

@article{alzahraniBiomedicalImageSegmentation2021,
  title = {Biomedical {{Image Segmentation}}: {{A Survey}}},
  shorttitle = {Biomedical {{Image Segmentation}}},
  author = {Alzahrani, Yahya and Boufama, Boubakeur},
  year = {2021},
  month = may,
  journal = {SN Computer Science},
  volume = {2},
  number = {4},
  pages = {310},
  issn = {2661-8907},
  doi = {10.1007/s42979-021-00704-7},
  urldate = {2022-02-28},
  abstract = {Medical Image Segmentation is the process of segmenting and detecting boundaries of anatomical structures in various types of 2D and 3D-medical images. The latter come from different modalities, such as Magnetic Resonance Imaging (MRI), X-Rays, Positron Emission Tomography (PET)/Single-Photon Emission Computed Tomography, Computed Tomography (CT), and Ultrasound (US). It is a key supporting technology for medical applications including diagnostics, planning, monitoring, and guidance. Hence, a large number of segmentation methods have been published in past decades. This paper presents a comprehensive review of the current medical segmentation techniques. In particular, we reviewed the most important medical segmentation methods that have been utilized for almost all types of medical images. We grouped these methods into categories and then compared, contrasted, and highlighted their main advantages and limitations.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\TJ6N3CD2\Alzahrani und Boufama - 2021 - Biomedical Image Segmentation A Survey.pdf}
}

@misc{amodeiConcreteProblemsAI2016,
  title = {Concrete {{Problems}} in {{AI Safety}}},
  author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  year = {2016},
  month = jul,
  number = {arXiv:1606.06565},
  eprint = {1606.06565},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1606.06565},
  urldate = {2024-01-28},
  abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\763NLF5Z\1606.html}
}

@inproceedings{amosDifferentiableMPCEndtoend2018,
  title = {Differentiable {{MPC}} for {{End-to-end Planning}} and {{Control}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Amos, Brandon and Jimenez, Ivan and Sacks, Jacob and Boots, Byron and Kolter, J. Zico},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-06-25},
  abstract = {We present foundations for using Model Predictive Control (MPC) as a differentiable policy class for reinforcement learning. This provides one way of leveraging and combining the advantages of model-free and model-based approaches. Specifically, we differentiate through MPC by using the KKT conditions of the convex approximation at a fixed point of the controller. Using this strategy, we are able to learn the cost and dynamics of a controller via end-to-end learning. Our experiments focus on imitation learning in the pendulum and cartpole domains, where we learn the cost and dynamics terms of an MPC policy class. We show that our MPC policies are significantly more data-efficient than a generic neural network and that our method is superior to traditional system identification in a setting where the expert is unrealizable.}
}

@inproceedings{andreasNeuralModuleNetworks2016,
  title = {Neural {{Module Networks}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  year = {2016},
  month = jun,
  pages = {39--48},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.12},
  abstract = {Visual question answering is fundamentally compositional in nature-a question like where is the dog? shares substructure with questions like what color is the dog? and where is the cat? This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning neural module networks, which compose collections of jointly-trained neural "modules" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.},
  keywords = {Computational modeling,Computer architecture,Dogs,Knowledge discovery,Neural networks,Semantics,Visualization},
  file = {C:\Users\benja\Zotero\storage\ADUVPRSD\7780381.html}
}

@article{andrychowiczLearningDexterousInhand2020,
  title = {Learning Dexterous In-Hand Manipulation},
  author = {Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and J{\'o}zefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
  year = {2020},
  month = jan,
  journal = {The International Journal of Robotics Research},
  volume = {39},
  number = {1},
  pages = {3--20},
  publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  doi = {10.1177/0278364919887447},
  urldate = {2020-07-01},
  abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies that can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system such as friction coefficients and an object's appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM.},
  langid = {english}
}

@inproceedings{andrychowiczLearningLearnGradient2016,
  title = {Learning to Learn by Gradient Descent by Gradient Descent},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Andrychowicz, Marcin and Denil, Misha and Colmenarejo, Sergio G{\'o}mez and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and {de Freitas}, Nando},
  year = {2016},
  month = dec,
  series = {{{NIPS}}'16},
  pages = {3988--3996},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2024-06-15},
  abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
  isbn = {978-1-5108-3881-9}
}

@misc{AnomalyDetectionAlgorithm,
  title = {Anomaly Detection Algorithm Keeps Robots in the Pink},
  urldate = {2021-04-14},
  howpublished = {https://new.abb.com/news/detail/70145/anomaly-detection-algorithm-keeps-robots-in-the-pink},
  langid = {english}
}

@misc{AnomalyDetectionService,
  title = {Anomaly {{Detection Service}}},
  journal = {Siemens MindSphere},
  urldate = {2021-04-14},
  howpublished = {https://developer.mindsphere.io/apis/analytics-anomalydetection/api-anomalydetection-overview.html},
  file = {C:\Users\benja\Zotero\storage\XP9YX798\api-anomalydetection-overview.html}
}

@techreport{anthropicModelCardClaude2pdf2023,
  title = {Model-{{Card-Claude-2}}.Pdf},
  author = {{Anthropic}},
  year = {2023},
  month = aug,
  institution = {Anthropic},
  urldate = {2023-10-30},
  file = {C:\Users\benja\Zotero\storage\GBR8SQHY\Model-Card-Claude-2.pdf}
}

@book{Arai.2019,
  title = {Intelligent Systems and Applications},
  editor = {Arai, Kohei and Kapoor, Supriya and Bhatia, Rahul},
  year = {2019},
  series = {Advances in Intelligent Systems and Computing},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-01057-7},
  isbn = {978-3-030-01056-0}
}

@inproceedings{arakiNewMethodInverting2003,
  title = {A New Method for Inverting Feedforward Neural Networks},
  booktitle = {{{SMC}}'03 {{Conference Proceedings}}. 2003 {{IEEE International Conference}} on {{Systems}}, {{Man}} and {{Cybernetics}}. {{Conference Theme}} - {{System Security}} and {{Assurance}} ({{Cat}}. {{No}}.{{03CH37483}})},
  author = {Araki, Y. and Ohki, T. and Citterio, D. and Hagiwara, M. and Suzuki, K.},
  year = {2003},
  month = oct,
  volume = {2},
  pages = {1612-1617 vol.2},
  doi = {10.1109/ICSMC.2003.1244643},
  abstract = {In this paper, we propose a new method for inverting feedforward neural networks. Inversion of neural networks means to find the inputs which produce given outputs. In general, this is an ill-posed problem whose solution isn't unique. Inversion using iterative optimization method (for example gradient descent, quasi-Newton method) is useful to this problem and it is called "iterative inversion". We propose a new iterative inversion using a Bottleneck Neural Network with Hidden layer's input units (BNNH), which we design on the basis of Bottleneck Neural Network (BNN). Compressing input space by BNNH, we reduce the dimension of search space, or input space to be searched with iterative inversion. With reduction of the search space's dimension, performance about computation time and accuracy is expected to become better. In experiments, the proposed method is applied to some examples. These results show the effectivity of the proposed method.},
  keywords = {bottleneck neural network with hidden layers input units,Chemistry,Computer science,Control systems,feedforward neural nets,Feedforward neural networks,feedforward neural networks invertion,gradient descent,ill-posed problem,Inverse problems,Iterative algorithms,iterative inversion,iterative methods,Iterative methods,iterative optimization,Neural networks,optimisation,Optimization methods,quasi-Newton method,search space dimension,Sensor systems},
  file = {C:\Users\benja\Zotero\storage\SNWFJCSD\1244643.html}
}

@inproceedings{ardizzoneAnalyzingInverseProblems2018,
  title = {Analyzing {{Inverse Problems}} with {{Invertible Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Ardizzone, Lynton and Kruse, Jakob and Rother, Carsten and K{\"o}the, Ullrich},
  year = {2018},
  month = sep,
  urldate = {2024-05-23},
  abstract = {For many applications, in particular in natural science, the task is to determine hidden system parameters from a set of measurements. Often, the forward process from parameter- to measurement-space is well-defined, whereas the inverse problem is ambiguous: multiple parameter sets can result in the same measurement. To fully characterize this ambiguity, the full posterior parameter distribution, conditioned on an observed measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task -- so-called Invertible Neural Networks (INNs). Unlike classical neural networks, which attempt to solve the ambiguous inverse problem directly, INNs focus on learning the forward process, using additional latent output variables to capture the information otherwise lost. Due to invertibility, a model of the corresponding inverse process is learned implicitly. Given a specific measurement and the distribution of the latent variables, the inverse pass of the INN provides the full posterior over parameter space. We prove theoretically and verify experimentally, on artificial data and real-world problems from medicine and astrophysics, that INNs are a powerful analysis tool to find multi-modalities in parameter space, uncover parameter correlations, and identify unrecoverable parameters.},
  langid = {english}
}

@inproceedings{arenasHowPromptYour2024,
  title = {How to {{Prompt Your Robot}}: {{A PromptBook}} for {{Manipulation Skills}} with {{Code}} as {{Policies}}},
  shorttitle = {How to {{Prompt Your Robot}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Arenas, Montserrat Gonzalez and Xiao, Ted and Singh, Sumeet and Jain, Vidhi and Ren, Allen and Vuong, Quan and Varley, Jake and Herzog, Alexander and Leal, Isabel and Kirmani, Sean and Prats, Mario and Sadigh, Dorsa and Sindhwani, Vikas and Rao, Kanishka and Liang, Jacky and Zeng, Andy},
  year = {2024},
  month = may,
  pages = {4340--4348},
  doi = {10.1109/ICRA57147.2024.10610784},
  urldate = {2024-08-28},
  abstract = {Large Language Models (LLMs) have demonstrated the ability to perform semantic reasoning, planning and write code for robotics tasks. However, most methods rely on pre-existing primitives (i.e. pick, open drawer) or similar examples of robot code alone, which heavily limits their scalability to new scenarios. We present PromptBook, a collection of different prompting paradigms to generate code for successfully executing new manipulation skills. We demonstrate example-based, instruction-based and chain-of-thought to write robot code; as well as a method to build the prompt leveraging LLMs and human feedback. We show PromptBook enables LLMs to write code for new low-level manipulation skills in a zero-shot manner: from picking diverse objects, opening/closing drawers, to whisking, and waving hello. We evaluate the new skills on a mobile manipulator with 83\% success rate at picking, 50-71\% at opening drawers and 100\% at closing them. Notably, the LLM is able to infer gripper orientation for grasping a drawer handle (z-axis aligned) vs. a top-down grasp (x-axis aligned).},
  keywords = {Codes,Cognition,Grasping,Large language models,Manipulators,Scalability,Semantics},
  file = {C:\Users\benja\Zotero\storage\AIDGY47B\10610784.html}
}

@article{arentsSmartIndustrialRobot2022,
  title = {Smart {{Industrial Robot Control Trends}}, {{Challenges}} and {{Opportunities}} within {{Manufacturing}}},
  author = {Arents, Janis and Greitans, Modris},
  year = {2022},
  month = jan,
  journal = {Applied Sciences},
  volume = {12},
  number = {2},
  pages = {937},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app12020937},
  urldate = {2022-05-06},
  abstract = {Industrial robots and associated control methods are continuously developing. With the recent progress in the field of artificial intelligence, new perspectives in industrial robot control strategies have emerged, and prospects towards cognitive robots have arisen. AI-based robotic systems are strongly becoming one of the main areas of focus, as flexibility and deep understanding of complex manufacturing processes are becoming the key advantage to raise competitiveness. This review first expresses the significance of smart industrial robot control in manufacturing towards future factories by listing the needs, requirements and introducing the envisioned concept of smart industrial robots. Secondly, the current trends that are based on different learning strategies and methods are explored. Current computer-vision, deep reinforcement learning and imitation learning based robot control approaches and possible applications in manufacturing are investigated. Gaps, challenges, limitations and open issues are identified along the way.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {artificial intelligence,cognitive robotics,computer vision,future factories,imitation learning,reinforcement learning,simulation,smart industrial robots,smart manufacturing,synthetic data}
}

@article{arinezArtificialIntelligenceAdvanced2020,
  title = {Artificial {{Intelligence}} in {{Advanced Manufacturing}}: {{Current Status}} and {{Future Outlook}}},
  shorttitle = {Artificial {{Intelligence}} in {{Advanced Manufacturing}}},
  author = {Arinez, Jorge F. and Chang, Qing and Gao, Robert X. and Xu, Chengying and Zhang, Jianjing},
  year = {2020},
  month = aug,
  journal = {Journal of Manufacturing Science and Engineering},
  volume = {142},
  number = {11},
  issn = {1087-1357},
  doi = {10.1115/1.4047855},
  urldate = {2022-04-05},
  abstract = {Today's manufacturing systems are becoming increasingly complex, dynamic, and connected. The factory operations face challenges of highly nonlinear and stochastic activity due to the countless uncertainties and interdependencies that exist. Recent developments in artificial intelligence (AI), especially Machine Learning (ML) have shown great potential to transform the manufacturing domain through advanced analytics tools for processing the vast amounts of manufacturing data generated, known as Big Data. The focus of this paper is threefold: (1) review the state-of-the-art applications of AI to representative manufacturing problems, (2) provide a systematic view for analyzing data and process dependencies at multiple levels that AI must comprehend, and (3) identify challenges and opportunities to not only further leverage AI for manufacturing, but also influence the future development of AI to better meet the needs of manufacturing. To satisfy these objectives, the paper adopts the hierarchical organization widely practiced in manufacturing plants in examining the interdependencies from the overall system level to the more detailed granular level of incoming material process streams. In doing so, the paper considers a wide range of topics from throughput and quality, supervisory control in human--robotic collaboration, process monitoring, diagnosis, and prognosis, finally to advances in materials engineering to achieve desired material property in process modeling and control.}
}

@article{arriola-riosModelingDeformableObjects2020,
  title = {Modeling of {{Deformable Objects}} for {{Robotic Manipulation}}: {{A Tutorial}} and {{Review}}},
  shorttitle = {Modeling of {{Deformable Objects}} for {{Robotic Manipulation}}},
  author = {{Arriola-Rios}, Veronica E. and Guler, Puren and Ficuciello, Fanny and Kragic, Danica and Siciliano, Bruno and Wyatt, Jeremy L.},
  year = {2020},
  journal = {Frontiers in Robotics and AI},
  volume = {7},
  issn = {2296-9144},
  urldate = {2022-08-17},
  abstract = {Manipulation of deformable objects has given rise to an important set of open problems in the field of robotics. Application areas include robotic surgery, household robotics, manufacturing, logistics, and agriculture, to name a few. Related research problems span modeling and estimation of an object's shape, estimation of an object's material properties, such as elasticity and plasticity, object tracking and state estimation during manipulation, and manipulation planning and control. In this survey article, we start by providing a tutorial on foundational aspects of models of shape and shape dynamics. We then use this as the basis for a review of existing work on learning and estimation of these models and on motion planning and control to achieve desired deformations. We also discuss potential future lines of work.}
}

@misc{Arti13,
  title = {{{ArtiMinds Robot Programming Suite}}},
  author = {{Schmidt-Rohr}, Sven R and J{\"a}kel, Rainer and Dirschl, Gerhard},
  urldate = {2017-08-27}
}

@misc{ArtiMindsCaseStory2020,
  title = {{{ArtiMinds Case Story}} - {{Strategic Collaboration}} {\textbar} {{B}}/{{S}}/{{H}}},
  year = {2020},
  month = jan,
  publisher = {ArtiMinds Robotics},
  urldate = {2020-10-22},
  abstract = {Recent changes in the industry require advanced planning and programming tools for automation projects. Learn how in collaboration with Siemens Digital Industries Software we enable BSH to globally roll out advanced automation solutions. For further information, visit us at https://www.artiminds.com}
}

@misc{ArtiMindsLAR2023,
  title = {{{ArtiMinds LAR}}},
  year = {2023},
  journal = {ArtiMinds Robotics GmbH},
  urldate = {2023-09-12},
  howpublished = {https://www.artiminds.com/robotics-software-and-services/learning-and-analytics-for-robots/},
  langid = {american},
  file = {C:\Users\benja\Zotero\storage\HN9SA5CE\learning-and-analytics-for-robots.html}
}

@misc{ArtiMindsLearningAnalytics2020,
  title = {{{ArtiMinds Learning}} \& {{Analytics}} for {{Robots}}},
  year = {2020},
  month = may,
  urldate = {2020-05-15}
}

@article{Arun.1987,
  title = {Least-Squares Fitting of Two 3-{{D}} Point Sets},
  author = {Arun, K. S. and Huang, T. S. and Blostein, S. D.},
  year = {1987},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-9},
  number = {5},
  pages = {698--700},
  issn = {01628828},
  doi = {10.1109/TPAMI.1987.4767965},
  pagination = {page}
}

@article{arvanitisAdaptiveRepresentationDynamic2019,
  title = {Adaptive Representation of Dynamic {{3D}} Meshes for Low-Latency Applications},
  author = {Arvanitis, Gerasimos and Lalos, Aris S. and Moustakas, Konstantinos},
  year = {2019},
  month = aug,
  journal = {Computer Aided Geometric Design},
  volume = {73},
  pages = {70--85},
  issn = {0167-8396},
  doi = {10.1016/j.cagd.2019.07.005},
  urldate = {2022-08-18},
  abstract = {Recently, 3D visual representations of highly deformable 3D models, such as dynamic 3D meshes, are becoming popular due to their capability to represent realistically the motion of real-world objects/humans, paving the road for new and more advanced immersive virtual, augmented and mixed reality experiences. However, the real-time streaming of such models introduces increasing challenges related to low cost, low-latency and scalable coding of the acquired information. In view of this, this article proposes an efficient scalable coding mechanism, that decomposes a mesh sequence into spatial and temporal layers that remove a single vertex at each layer. The removed vertices are predicted by performing Laplacian interpolation of the motion vectors. The artifacts that are introduced in low-resolution representations are mitigated using a subspace based normal-vector denoising procedure, that is optimized to support low-latency streaming scenarios using incremental SVD. A novel initialization strategy offers robustness to outliers generated due to local deformations. An extensive evaluation study using several synthetic and scanned dynamic 3D meshes highlights the benefits of the proposed approach in terms of both execution time and reconstruction quality even in very low throughput scenarios of bit-per-vertex-per-frame (bpvf).},
  langid = {english},
  keywords = {Incremental SVD,Laplacian interpolation,Robust PCA,Scalable coding},
  file = {C:\Users\benja\Zotero\storage\7FFEEK2X\S016783961930069X.html}
}

@article{Asfo13,
  title = {{{ARMAR-III}}: {{Advances}} in {{Humanoid Grasping}} and {{Manipulation}}},
  author = {Asfour, Tamim and Vahrenkamp, Nikolaus and Schiebener, David and Do, Martin and Przybylski, Markus and Welke, Kai and Schill, Julian and Dillmann, R\&uuml;diger},
  year = {2013},
  journal = {Journal of the Robotics Society of Japan},
  volume = {31},
  number = {4},
  pages = {341--346}
}

@article{asfourARMAR6HighPerformanceHumanoid2019,
  title = {{{ARMAR-6}}: {{A High-Performance Humanoid}} for {{Human-Robot Collaboration}} in {{Real-World Scenarios}}},
  shorttitle = {{{ARMAR-6}}},
  author = {Asfour, Tamim and Waechter, Mirko and Kaul, Lukas and Rader, Samuel and Weiner, Pascal and Ottenhaus, Simon and Grimm, Raphael and Zhou, You and Grotz, Markus and Paus, Fabian},
  year = {2019},
  month = dec,
  journal = {IEEE Robotics \& Automation Magazine},
  volume = {26},
  number = {4},
  pages = {108--121},
  issn = {1558-223X},
  doi = {10.1109/MRA.2019.2941246},
  urldate = {2024-08-15},
  abstract = {A major goal of humanoid robotics is to enable safe and reliable human-robot collaboration in realworld scenarios. In this article, we present ARMAR-6, a new high-performance humanoid robot for various tasks, including but not limited to grasping, mobile manipulation, integrated perception, bimanual collaboration, compliant-motion execution, and natural language understanding. We describe how the requirements arising from these tasks influenced our major design decisions, resulting in vertical integration during the joint hardware and software development phases. In particular, the entire hardware-including its structure, sensor-actuator units, and low-level controllers-as well as its perception, grasping and manipulation skills, task coordination, and the entire software architecture were all developed by one team of engineers. Component interaction is facilitated by our software framework ArmarX, which further facilitates the seamless integration and interchange of third-party contributions. To showcase the robot's capabilities, we present its performance in a challenging industrial maintenance scenario that requires human-robot collaboration, where the robot autonomously recognizes the human's need of help and offers said help in a proactive way.},
  keywords = {Humanoid robots,Maintenance engineering,Robot sensing systems,Software development management,Task analysis}
}

@inproceedings{asfourHumanlikeMotionHumanoid2003,
  title = {Human-like Motion of a Humanoid Robot Arm Based on a Closed-Form Solution of the Inverse Kinematics Problem},
  booktitle = {{{IROS}}},
  author = {Asfour, T. and Dillmann, R.},
  year = {2003},
  volume = {2},
  pages = {1407--1412},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/IROS.2003.1248841},
  urldate = {2020-08-25},
  abstract = {Humanoid robotics is a new challenging field. To cooperate with human beings, humanoid robots not only have to feature human-like form and structure but, more importantly, they must possess human-like characteristics regarding motion, communication and intelligence. In this paper, we propose an algorithm for solving the inverse kinematics problem associated with the redundant robot arm of the humanoid robot ARMAR. The formulation of the problem is based on the decomposition of the workspace of the arm and on the analytical description of the redundancy of the arm. The solution obtained is characterized by its accuracy and low cost of computation. The algorithm is enhanced in order to generate human-like manipulation motions from object trajectories.},
  isbn = {978-0-7803-7860-5},
  langid = {english}
}

@misc{ashwaniCauseEffectCan2024,
  title = {Cause and {{Effect}}: {{Can Large Language Models Truly Understand Causality}}?},
  shorttitle = {Cause and {{Effect}}},
  author = {Ashwani, Swagata and Hegde, Kshiteesh and Mannuru, Nishith Reddy and Jindal, Mayank and Sengar, Dushyant Singh and Kathala, Krishna Chaitanya Rao and Banga, Dishant and Jain, Vinija and Chadha, Aman},
  year = {2024},
  month = apr,
  number = {arXiv:2402.18139},
  eprint = {2402.18139},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.18139},
  urldate = {2024-08-16},
  abstract = {With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability. The proposed framework incorporates an explicit causal detection module with ConceptNet and counterfactual statements, as well as implicit causal detection through LLMs. Our framework goes one step further with a layer of counterfactual explanations to accentuate LLMs understanding of causality. The knowledge from ConceptNet enhances the performance of multiple causal reasoning tasks such as causal discovery, causal identification and counterfactual reasoning. The counterfactual sentences add explicit knowledge of the not caused by scenarios. By combining these powerful modules, our model aims to provide a deeper understanding of causal relationships, enabling enhanced interpretability. Evaluation of benchmark datasets shows improved performance across all metrics, such as accuracy, precision, recall, and F1 scores. We also introduce CausalNet, a new dataset accompanied by our code, to facilitate further research in this domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@article{astromSystemIdentificationSurvey1971,
  title = {System Identification---{{A}} Survey},
  author = {{\AA}str{\"o}m, K. J. and Eykhoff, P.},
  year = {1971},
  month = mar,
  journal = {Automatica},
  volume = {7},
  number = {2},
  pages = {123--162},
  issn = {0005-1098},
  doi = {10.1016/0005-1098(71)90059-8},
  urldate = {2024-01-08},
  abstract = {The field of identification and process-parameter estimation has developed rapidly during the past decade. In this survey paper the state-of-the-art/science is presented in a systematic way. Attention is paid to general properties and to classification of identification problems. Model structures are discussed; their choice hinges on the purpose of the identification and on the available a priori knowledge. For the identification of models that are linear in the parameters, the survey explains the least squares method and several of its variants which may solve the problem of correlated residuals, viz. repeated and generalized least squares, maximum likelihood method, instrumental variable method, tally principle. Recently the non-linear situation, the on-line and the real-time identification have witnessed extensive developments. These are also reported. There are 230 references listed, mostly to recent contributions. In appendices a resum{\'e} is given of parameter estimation principles and a more detailed exposition of an example of least squares estimation. R{\'e}sum{\'e} Le domaine de l'identification et de l'estimation des param{\`e}tres de processus s'est rapidement developp{\'e} durant la d{\'e}cade pass{\'e}e. Dans cette revue, l'{\'e}tat de l'art de cette science est pr{\'e}sent{\'e} d'une mani{\`e}re syst{\'e}matique. Une attention est accord{\'e}e aux propriet{\'e}s g{\'e}n{\'e}rales et {\`a} la classification des probl{\`e}mes d'identification. Les structures des mod{\`e}les sont discut{\'e}es; leur choix depend du but de l'identification et des informations disponibles {\`a} priori. Pour l'identification des mod{\`e}les qui sont lin{\'e}aires dans les param{\`e}tres, la revue explique la m{\'e}thode des moindres carr{\'e}s et plusieurs de ses variantes qui peuvent resoudre le probl{\`e}me des r{\'e}sidus correl{\'e}s, telles que les moindres carr{\'e}s iter{\'e}s et g{\'e}n{\'e}ralis{\'e}s, la m{\'e}thode de vraisemblance maximale, la m{\'e}thode de la variable influente, le principle de pointage. Rebemment, la situation non-lin{\'e}aire, l'identification dans la boucle et l'identification en temps r{\'e}el ont donn{\'e} lieu {\`a} des developpements considerables qui sont d{\'e}crits d'une mani{\`e}re coh{\'e}rente. Plus de 230 r{\'e}f{\'e}rences sont donn{\'e}es, la plupart en ce qui concerne des publications r{\'e}centes. Un appendix contient un r{\'e}sum{\'e} des principes d'estimation de param{\`e}tres et un autre donne un expos{\'e} plus d{\'e}taill{\'e} d'un exemple d'une estimation au moyen des moindres carr{\'e}s. Zusammenfassung Das Gebiet der Identifikation und Proze{$\beta$}parametersch{\"a}tzung entwickelte sich im letzten Jahrzehnt schr schnell. In dieser {\"U}bersicht ist der Stand dieses Wissenschaftsweiges systematisch dargestellt. Aufmerksamkeit wurde allgemeinen Eigenschaften und der Klassifikation der Identifikationsprobleme geschenkt. Modellstrukturen werden diskutiert; ihre Wahl dreht sich um den Zweck der Identifikation und um die verf{\"u}gbare a priori-Kenntnis. F{\"u}r die Identifikation von Modellen, die die Methode der kleinsten Quadrate und verschiedene ihrer Varianten, die das Problem der korrelierten Gleichungsfehler, n{\"a}mlich wiederholter und verallgemeinerter Methoden der kleinsten Quadrate, Maximum likelihood Methode, ``tally''---Prinzip charakterisieren. Neuerdings legte die nichtlineare Situation, die on-line und die real-time Identifikation ausgedehnte Entwicklungen nahe, {\"u}ber die zusammenh{\"a}ngend berichtet wird. Mehr als 230 Literaturstellen sind angegeben, die sich meist auf neue Arbeiten beziehen. In Anh{\"a}ngen wird eine Zusammenfassung der Prinzipien zur Parametersch{\"a}tzung und ein genauer behandeltes Beispiel einer Sch{\"a}tzung nach der Methode der kleinsten Quadrate angef{\"u}hrt. {\cyrchar\CYRR}{\cyrchar\cyre}{\cyrchar\cyrf}{\cyrchar\cyre}{\cyrchar\cyrr}{\cyrchar\cyra}{\cyrchar\cyrt} O{\cyrchar\cyrb}{\cyrchar\cyrl}ac{\cyrchar\cyrt}{\cyrchar\cyrsftsn} o{\cyrchar\cyrp}o{\cyrchar\cyrz}{\cyrchar\cyrn}a{\cyrchar\cyrv}a{\cyrchar\cyrn}{\cyrchar\cyri}{\cyrchar\cyrya} {\cyrchar\cyri} o{\cyrchar\cyrc}e{\cyrchar\cyrn}{\cyrchar\cyrk}{\cyrchar\cyri} {\cyrchar\cyrp}apa{\cyrchar\cyrm}e{\cyrchar\cyrt}po{\cyrchar\cyrv} {\cyrchar\cyrp}po{\cyrchar\cyrc}ecco{\cyrchar\cyrv} {\cyrchar\cyrb}{\cyrchar\cyrery}c{\cyrchar\cyrt}po pa{\cyrchar\cyrz}{\cyrchar\cyrv}{\cyrchar\cyri}{\cyrchar\cyrl}ac{\cyrchar\cyrsftsn} {\cyrchar\cyrv} {\cyrchar\cyrt}e{\cyrchar\cyrch}e{\cyrchar\cyrn}{\cyrchar\cyri}{\cyrchar\cyri} {\cyrchar\cyrp}po{\cyrchar\cyrsh}{\cyrchar\cyrl}o{\cyrchar\cyrg}o {\cyrchar\cyrd}ec{\cyrchar\cyrya}{\cyrchar\cyrt}{\cyrchar\cyri}{\cyrchar\cyrl}e{\cyrchar\cyrt}{\cyrchar\cyri}{\cyrchar\cyrya}. {\cyrchar\CYRB} {\cyrchar\cyrn}ac{\cyrchar\cyrt}o{\cyrchar\cyrya}{\cyrchar\cyrshch}e{\cyrchar\cyrm} o{\cyrchar\cyrb}{\cyrchar\cyrz}ope c{\cyrchar\cyri}c{\cyrchar\cyrt}e{\cyrchar\cyrm}a{\cyrchar\cyrt}{\cyrchar\cyri}{\cyrchar\cyrch}ec{\cyrchar\cyrk}{\cyrchar\cyri} {\cyrchar\cyrp}pe{\cyrchar\cyrd}c{\cyrchar\cyrt}a{\cyrchar\cyrv}{\cyrchar\cyrl}e{\cyrchar\cyrn}o {\cyrchar\cyrn}ac{\cyrchar\cyrt}o{\cyrchar\cyrya}{\cyrchar\cyrshch}ee coc{\cyrchar\cyrt}o{\cyrchar\cyrya}{\cyrchar\cyrn}{\cyrchar\cyri}e {\cyrchar\cyrerev}{\cyrchar\cyrt}o{\cyrchar\cyrishrt} {\cyrchar\cyrn}ay{\cyrchar\cyrk}{\cyrchar\cyri}. {\cyrchar\CYRU}{\cyrchar\cyrd}e{\cyrchar\cyrl}{\cyrchar\cyrya}e{\cyrchar\cyrt}c{\cyrchar\cyrya} {\cyrchar\cyrv}{\cyrchar\cyrn}{\cyrchar\cyri}{\cyrchar\cyrm}a{\cyrchar\cyrn}{\cyrchar\cyri}e o{\cyrchar\cyrb}{\cyrchar\cyrshch}{\cyrchar\cyri}{\cyrchar\cyrm} c{\cyrchar\cyrv}o{\cyrchar\cyrishrt}c{\cyrchar\cyrt}{\cyrchar\cyrv}a{\cyrchar\cyrm} {\cyrchar\cyri} {\cyrchar\cyrk}{\cyrchar\cyrl}acc{\cyrchar\cyri}{\cyrchar\cyrf}{\cyrchar\cyri}{\cyrchar\cyrk}a{\cyrchar\cyrc}{\cyrchar\cyri}{\cyrchar\cyri} {\cyrchar\cyrp}po{\cyrchar\cyrb}{\cyrchar\cyrl}e{\cyrchar\cyrm} o{\cyrchar\cyrp}o{\cyrchar\cyrz}{\cyrchar\cyrn}a{\cyrchar\cyrv}a{\cyrchar\cyrn}{\cyrchar\cyri}{\cyrchar\cyrya}. O{\cyrchar\cyrb}cy{\cyrchar\cyrzh}{\cyrchar\cyrd}a{\cyrchar\cyryu}{\cyrchar\cyrt}c{\cyrchar\cyrya} c{\cyrchar\cyrt}py{\cyrchar\cyrk}{\cyrchar\cyrt}yp{\cyrchar\cyrery} {\cyrchar\cyrm}o{\cyrchar\cyrd}e{\cyrchar\cyrl}e{\cyrchar\cyrishrt}; {\cyrchar\cyri}{\cyrchar\cyrch} {\cyrchar\cyrv}{\cyrchar\cyrery}{\cyrchar\cyrb}op {\cyrchar\cyrz}a{\cyrchar\cyrv}{\cyrchar\cyri}c{\cyrchar\cyri}{\cyrchar\cyrt} o{\cyrchar\cyrt} {\cyrchar\cyrc}e{\cyrchar\cyrl}{\cyrchar\cyri} o{\cyrchar\cyrp}o{\cyrchar\cyrz}{\cyrchar\cyrn}a{\cyrchar\cyrv}a{\cyrchar\cyrn}{\cyrchar\cyri}{\cyrchar\cyrya} {\cyrchar\cyri} o{\cyrchar\cyrt} {\cyrchar\cyrz}apa{\cyrchar\cyrn}ee {\cyrchar\cyrd}oc{\cyrchar\cyrt}y{\cyrchar\cyrp}{\cyrchar\cyrn}{\cyrchar\cyrery}{\cyrchar\cyrch} c{\cyrchar\cyrv}e{\cyrchar\cyrd}e{\cyrchar\cyrn}{\cyrchar\cyri}{\cyrchar\cyrishrt}. {\cyrchar\CYRD}{\cyrchar\cyrl}{\cyrchar\cyrya} o{\cyrchar\cyrp}o{\cyrchar\cyrz}{\cyrchar\cyrn}a{\cyrchar\cyrv}a{\cyrchar\cyrn}{\cyrchar\cyri}{\cyrchar\cyrya} {\cyrchar\cyrm}o{\cyrchar\cyrd}e{\cyrchar\cyrl}e{\cyrchar\cyrishrt} {\cyrchar\cyrl}{\cyrchar\cyri}{\cyrchar\cyrn}e{\cyrchar\cyrishrt}{\cyrchar\cyrn}{\cyrchar\cyrery}{\cyrchar\cyrch} {\cyrchar\cyrv} {\cyrchar\cyrp}apa{\cyrchar\cyrm}e{\cyrchar\cyrt}pa{\cyrchar\cyrch}, o{\cyrchar\cyrb}{\cyrchar\cyrz}op o{\cyrchar\cyrb}{\cyrchar\cyrhrdsn}{\cyrchar\cyrya}c{\cyrchar\cyrn}{\cyrchar\cyrya}e{\cyrchar\cyrt} {\cyrchar\cyrm}e{\cyrchar\cyrt}o{\cyrchar\cyrd} {\cyrchar\cyrn}a{\cyrchar\cyri}{\cyrchar\cyrm}e{\cyrchar\cyrn}{\cyrchar\cyrsftsn}{\cyrchar\cyrsh}{\cyrchar\cyri}{\cyrchar\cyrch} {\cyrchar\cyrk}{\cyrchar\cyrv}a{\cyrchar\cyrd}pa{\cyrchar\cyrt}o{\cyrchar\cyrv} {\cyrchar\cyri} {\cyrchar\cyrn}e{\cyrchar\cyrk}o{\cyrchar\cyrt}op{\cyrchar\cyrery}e {\cyrchar\cyri}{\cyrchar\cyrz} e{\cyrchar\cyrg}o {\cyrchar\cyrv}ap{\cyrchar\cyri}a{\cyrchar\cyrn}{\cyrchar\cyrt}o{\cyrchar\cyrv} {\cyrchar\cyrm}o{\cyrchar\cyrg}y{\cyrchar\cyrshch}{\cyrchar\cyri}{\cyrchar\cyrch} pe{\cyrchar\cyrsh}{\cyrchar\cyri}{\cyrchar\cyrt}{\cyrchar\cyrsftsn} {\cyrchar\cyrp}po{\cyrchar\cyrb}{\cyrchar\cyrl}e{\cyrchar\cyrm}y {\cyrchar\cyrk}oppe{\cyrchar\cyrl}{\cyrchar\cyri}po{\cyrchar\cyrv}a{\cyrchar\cyrn}{\cyrchar\cyrn}{\cyrchar\cyrery}{\cyrchar\cyrch} oc{\cyrchar\cyrt}a{\cyrchar\cyrt}{\cyrchar\cyrk}o{\cyrchar\cyrv}, {\cyrchar\cyrk}a{\cyrchar\cyrk} {\cyrchar\cyrn}a {\cyrchar\cyrp}p{\cyrchar\cyri}{\cyrchar\cyrm}ep {\cyrchar\cyrp}o{\cyrchar\cyrv}{\cyrchar\cyrt}op{\cyrchar\cyri}{\cyrchar\cyrt}e{\cyrchar\cyrl}{\cyrchar\cyrsftsn}{\cyrchar\cyrn}{\cyrchar\cyrery}e {\cyrchar\cyrn}a{\cyrchar\cyri}{\cyrchar\cyrm}e{\cyrchar\cyrn}{\cyrchar\cyrsftsn}{\cyrchar\cyrsh}{\cyrchar\cyri}e {\cyrchar\cyrk}{\cyrchar\cyrv}a{\cyrchar\cyrd}pa{\cyrchar\cyrt}{\cyrchar\cyrery}, {\cyrchar\cyrm}e{\cyrchar\cyrt}o{\cyrchar\cyrd} {\cyrchar\cyrn}a{\cyrchar\cyri}{\cyrchar\cyrb}o{\cyrchar\cyrl}{\cyrchar\cyrsftsn}{\cyrchar\cyrsh}e{\cyrchar\cyrg}o {\cyrchar\cyrp}pa{\cyrchar\cyrv}{\cyrchar\cyrd}o{\cyrchar\cyrp}o{\cyrchar\cyrd}o{\cyrchar\cyrd}{\cyrchar\cyri}{\cyrchar\cyrya}, {\cyrchar\cyrm}e{\cyrchar\cyrt}o{\cyrchar\cyrd} {\cyrchar\cyrv}{\cyrchar\cyrl}{\cyrchar\cyri}{\cyrchar\cyrya}{\cyrchar\cyryu}{\cyrchar\cyrshch}e{\cyrchar\cyrishrt} {\cyrchar\cyrn}cpe{\cyrchar\cyrm}e{\cyrchar\cyrn}{\cyrchar\cyrn}o{\cyrchar\cyrishrt}, {\cyrchar\cyrp}p{\cyrchar\cyri}{\cyrchar\cyrn}{\cyrchar\cyrc}{\cyrchar\cyri}{\cyrchar\cyrp} {\cyrchar\cyrp}o{\cyrchar\cyrd}c{\cyrchar\cyrch}{\"e}{\cyrchar\cyrt}a. He{\cyrchar\cyrd}a{\cyrchar\cyrv}{\cyrchar\cyrn}o {\cyrchar\cyrn}e{\cyrchar\cyrl}{\cyrchar\cyri}{\cyrchar\cyrn}e{\cyrchar\cyrishrt}{\cyrchar\cyrn}oe coc{\cyrchar\cyrt}o{\cyrchar\cyrya}{\cyrchar\cyrn}{\cyrchar\cyri}e, o{\cyrchar\cyrp}o{\cyrchar\cyrz}{\cyrchar\cyrn}a{\cyrchar\cyrv}a{\cyrchar\cyrn}{\cyrchar\cyri}e {\cyrchar\cyrv} {\cyrchar\cyrk}o{\cyrchar\cyrn}{\cyrchar\cyrt}ype {\cyrchar\cyri} o{\cyrchar\cyrp}o{\cyrchar\cyrz}{\cyrchar\cyrn}a{\cyrchar\cyrv}a{\cyrchar\cyrn}{\cyrchar\cyri}e {\cyrchar\cyrv} {\cyrchar\cyrd}e{\cyrchar\cyrishrt}c{\cyrchar\cyrt}{\cyrchar\cyrv}{\cyrchar\cyri}{\cyrchar\cyrt}e{\cyrchar\cyrl}{\cyrchar\cyrsftsn}{\cyrchar\cyrn}o{\cyrchar\cyrm} {\cyrchar\cyrv}pe{\cyrchar\cyrm}e{\cyrchar\cyrn}{\cyrchar\cyri} {\cyrchar\cyrp}p{\cyrchar\cyri}{\cyrchar\cyrv}e{\cyrchar\cyrl}{\cyrchar\cyri} {\cyrchar\cyrk} {\cyrchar\cyrz}{\cyrchar\cyrn}a{\cyrchar\cyrch}{\cyrchar\cyri}{\cyrchar\cyrt}e{\cyrchar\cyrl}{\cyrchar\cyrsftsn}{\cyrchar\cyrn}{\cyrchar\cyrery}{\cyrchar\cyrm} pa{\cyrchar\cyrz}{\cyrchar\cyrv}{\cyrchar\cyri}{\cyrchar\cyrt}{\cyrchar\cyri}{\cyrchar\cyrya}{\cyrchar\cyrm} {\cyrchar\cyrk}o{\cyrchar\cyrt}op{\cyrchar\cyrery}e o{\cyrchar\cyrp}{\cyrchar\cyri}c{\cyrchar\cyrery}{\cyrchar\cyrv}a{\cyrchar\cyryu}{\cyrchar\cyrt}c{\cyrchar\cyrya} c{\cyrchar\cyri}c{\cyrchar\cyrt}e{\cyrchar\cyrm}a{\cyrchar\cyrt}{\cyrchar\cyri}{\cyrchar\cyrch}ec{\cyrchar\cyrk}{\cyrchar\cyri}{\cyrchar\cyrm} o{\cyrchar\cyrb}pa{\cyrchar\cyrz}o{\cyrchar\cyrm}. {\cyrchar\CYRP}p{\cyrchar\cyri}{\cyrchar\cyrv}e{\cyrchar\cyrd}e{\cyrchar\cyrn}o c{\cyrchar\cyrv}{\cyrchar\cyrery}{\cyrchar\cyrsh}e 230 pe{\cyrchar\cyrf}epe{\cyrchar\cyrn}{\cyrchar\cyrc}{\cyrchar\cyri}{\cyrchar\cyrishrt} o{\cyrchar\cyrt}{\cyrchar\cyrn}oc{\cyrchar\cyrya}{\cyrchar\cyrshch}{\cyrchar\cyri}{\cyrchar\cyrch}c{\cyrchar\cyrya} {\cyrchar\cyrg}{\cyrchar\cyrl}a{\cyrchar\cyrv}{\cyrchar\cyrn}{\cyrchar\cyrery}{\cyrchar\cyrm} o{\cyrchar\cyrb}pa{\cyrchar\cyrz}o{\cyrchar\cyrm} {\cyrchar\cyrk} {\cyrchar\cyrn}e{\cyrchar\cyrd}a{\cyrchar\cyrv}{\cyrchar\cyrn}{\cyrchar\cyrery}{\cyrchar\cyrm} {\cyrchar\cyrp}y{\cyrchar\cyrb}{\cyrchar\cyrl}{\cyrchar\cyri}{\cyrchar\cyrk}a{\cyrchar\cyrc}{\cyrchar\cyri}{\cyrchar\cyrya}{\cyrchar\cyrm}. {\cyrchar\CYRP}p{\cyrchar\cyri}{\cyrchar\cyrl}o{\cyrchar\cyrzh}e{\cyrchar\cyrn}{\cyrchar\cyri}{\cyrchar\cyrya} co{\cyrchar\cyrd}ep{\cyrchar\cyrzh}a{\cyrchar\cyrt} c{\cyrchar\cyrv}o{\cyrchar\cyrd}{\cyrchar\cyrk}y {\cyrchar\cyrp}p{\cyrchar\cyri}{\cyrchar\cyrn}{\cyrchar\cyrc}{\cyrchar\cyri}{\cyrchar\cyrp}o{\cyrchar\cyrv} o{\cyrchar\cyrc}e{\cyrchar\cyrn}{\cyrchar\cyrk}{\cyrchar\cyri} {\cyrchar\cyrp}apa{\cyrchar\cyrm}e{\cyrchar\cyrt}po{\cyrchar\cyrv} {\cyrchar\cyri} {\cyrchar\cyrb}o{\cyrchar\cyrl}ee {\cyrchar\cyrd}e{\cyrchar\cyrt}a{\cyrchar\cyrl}{\cyrchar\cyrsftsn}{\cyrchar\cyrn}oe o{\cyrchar\cyrp}{\cyrchar\cyri}ca{\cyrchar\cyrn}{\cyrchar\cyri}e {\cyrchar\cyrp}p{\cyrchar\cyri}{\cyrchar\cyrm}epa o{\cyrchar\cyrc}e{\cyrchar\cyrn}{\cyrchar\cyrk}{\cyrchar\cyri} {\cyrchar\cyrp}p{\cyrchar\cyri} {\cyrchar\cyrp}o{\cyrchar\cyrm}o{\cyrchar\cyrshch}{\cyrchar\cyri} {\cyrchar\cyrn}a{\cyrchar\cyri}{\cyrchar\cyrm}e{\cyrchar\cyrn}{\cyrchar\cyrsftsn}{\cyrchar\cyrsh}{\cyrchar\cyri}{\cyrchar\cyrch} {\cyrchar\cyrk}{\cyrchar\cyrv}a{\cyrchar\cyrd}pa{\cyrchar\cyrt}o{\cyrchar\cyrv}.},
  file = {C:\Users\benja\Zotero\storage\VN3U7SF2\0005109871900598.html}
}

@article{atmehDynamicNeuralNetwork2016,
  title = {A {{Dynamic Neural Network}} with {{Feedback}} for {{Trajectory Generation}}},
  author = {Atmeh, Ghassan and Subbarao, Kamesh},
  year = {2016},
  journal = {IFAC-PapersOnLine},
  volume = {49},
  number = {1},
  pages = {367--372},
  issn = {24058963},
  doi = {10.1016/j.ifacol.2016.03.081},
  urldate = {2019-05-17},
  langid = {english}
}

@article{attanasioAutonomySurgicalRobotics2021,
  title = {Autonomy in {{Surgical Robotics}}},
  author = {Attanasio, Aleks and Scaglioni, Bruno and Momi, Elena De and Fiorini, Paolo and Valdastri, Pietro},
  year = {2021},
  month = may,
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  volume = {4},
  number = {Volume 4, 2021},
  pages = {651--679},
  publisher = {Annual Reviews},
  issn = {2573-5144},
  doi = {10.1146/annurev-control-062420-090543},
  urldate = {2024-04-12},
  abstract = {This review examines the dichotomy between automatic and autonomous behaviors in surgical robots, maps the possible levels of autonomy of these robots, and describes the primary enabling technologies that are driving research in this field. It is organized in five main sections that cover increasing levels of autonomy. At level 0, where the bulk of commercial platforms are, the robot has no decision autonomy. At level 1, the robot can provide cognitive and physical assistance to the surgeon, while at level 2, it can autonomously perform a surgical task. Level 3 comes with conditional autonomy, enabling the robot to plan a task and update planning during execution. Finally, robots at level 4 can plan and execute a sequence of surgical tasks autonomously.},
  langid = {english}
}

@mastersthesis{aumannFlexibleGenerationMovements2019,
  title = {Flexible {{Generation}} of {{Movements}} for {{Elderly Care Robotics}}},
  author = {Aumann, Florian},
  year = {2019},
  address = {Karlsruhe},
  school = {Karlsruhe Institute of Technology}
}

@article{avilagarcezConnectionistInductiveLearning1999,
  title = {The {{Connectionist Inductive Learning}} and {{Logic Programming System}}},
  author = {Avila Garcez, Artur S. and Zaverucha, Gerson},
  year = {1999},
  month = jul,
  journal = {Applied Intelligence},
  volume = {11},
  number = {1},
  pages = {59--77},
  issn = {1573-7497},
  doi = {10.1023/A:1008328630915},
  urldate = {2022-04-02},
  abstract = {This paper presents the Connectionist Inductive Learning and Logic Programming System (C-IL2P). C-IL2P is a new massively parallel computational model based on a feedforward Artificial Neural Network that integrates inductive learning from examples and background knowledge, with deductive learning from Logic Programming. Starting with the background knowledge represented by a propositional logic program, a translation algorithm is applied generating a neural network that can be trained with examples. The results obtained with this refined network can be explained by extracting a revised logic program from it. Moreover, the neural network computes the stable model of the logic program inserted in it as background knowledge, or learned with the examples, thus functioning as a parallel system for Logic Programming. We have successfully applied C-IL2P to two real-world problems of computational biology, specifically DNA sequence analyses. Comparisons with the results obtained by some of the main neural, symbolic, and hybrid inductive learning systems, using the same domain knowledge, show the effectiveness of C-IL2P.},
  langid = {english}
}

@book{Azad.2011,
  title = {Computer Vision: {{Das}} Praxisbuch},
  author = {Azad, Pedram and Gockel, Tilo and Dillmann, R{\"u}diger},
  year = {2011},
  edition = {3. Aufl.},
  publisher = {Elektor-Verl.},
  address = {Aachen},
  isbn = {978-3-89576-165-2},
  price = {EUR 42.80}
}

@inproceedings{azmatClosingSkillsGap2020,
  title = {Closing the {{Skills Gap}} in the {{Era}} of {{Industrial Digitalisation}}},
  booktitle = {2020 {{IEEE Conference}} on {{Industrial Cyberphysical Systems}} ({{ICPS}})},
  author = {Azmat, Freeha and Ahmed, Bilal and Colombo, Walter and Harrison, Robert},
  year = {2020},
  month = jun,
  volume = {1},
  pages = {365--370},
  doi = {10.1109/ICPS48405.2020.9274788},
  urldate = {2024-01-26},
  abstract = {Manufacturing industry is in the midst of reshaping modern manufacturing by employing digital technologies due to its promising results. More and more companies are marching towards migrating their product offering and production infrastructure into Industry 4.0-compliant solutions. In this new era of industrial scenario, companies will need technicians and workers with digital skills mainly having abilities and competences in the Industry 4.0 Key Enabling Technologies (KET). However, lack of digital skills in the UK is considered perhaps the most serious obstacle to the practical implementation and investment in manufacturing digitalization technologies and is thus a major contributing factor to the UK productivity gap. Aiming to bridge the gap between the implementation of Industry 4.0 and educational organizations that focus on this field, the study presented here focuses on the development of a degree apprenticeship program in Digital technology solutions (DTS). The DTS program is developed in collaboration with leading UK industry partners to acquaint apprentices with appropriate knowledge that can help them to contribute towards digital transformation of their businesses.},
  keywords = {Business,Companies,Degree apprenticeship,Industrial Cyber Physical systems,Industries,Industry 4.0,Knowledge engineering,Productivity,skills gap,Software,Training},
  file = {C:\Users\benja\Zotero\storage\YGEJY8ZM\9274788.html}
}

@article{backOverviewEvolutionaryAlgorithms1993,
  title = {An Overview of Evolutionary Algorithms for Parameter Optimization},
  author = {B{\"a}ck, Thomas and Schwefel, Hans-Paul},
  year = {1993},
  month = mar,
  journal = {Evolutionary Computation},
  volume = {1},
  number = {1},
  pages = {1--23},
  issn = {1063-6560},
  doi = {10.1162/evco.1993.1.1.1},
  urldate = {2023-05-01},
  abstract = {Three main streams of evolutionary algorithms (EAs), probabilistic optimization algorithms based on the model of natural evolution, are compared in this article: evolution strategies (ESs), evolutionary programming (EP), and genetic algorithms (GAs). The comparison is performed with respect to certain characteristic components of EAs: the representation scheme of object variables, mutation, recombination, and the selection operator. Furthermore, each algorithm is formulated in a high-level notation as an instance of the general, unifying basic algorithm, and the fundamental theoretical results on the algorithms are presented. Finally, after presenting experimental results for three test functions representing a unimodal and a multimodal case as well as a step function with discontinuities, similarities and differences of the algorithms are elaborated, and some hints to open research questions are sketched.}
}

@inproceedings{bagnellIntegratedSystemAutonomous2012,
  title = {An Integrated System for Autonomous Robotics Manipulation},
  booktitle = {2012 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Bagnell, J. Andrew and Cavalcanti, Felipe and Cui, Lei and Galluzzo, Thomas and Hebert, Martial and Kazemi, Moslem and Klingensmith, Matthew and Libby, Jacqueline and Liu, Tian Yu and Pollard, Nancy and Pivtoraiko, Mihail and Valois, Jean-Sebastien and Zhu, Ranqi},
  year = {2012},
  month = oct,
  pages = {2955--2962},
  issn = {2153-0866},
  doi = {10.1109/IROS.2012.6385888},
  urldate = {2024-04-25},
  abstract = {We describe the software components of a robotics system designed to autonomously grasp objects and perform dexterous manipulation tasks with only high-level supervision. The system is centered on the tight integration of several core functionalities, including perception, planning and control, with the logical structuring of tasks driven by a Behavior Tree architecture. The advantage of the implementation is to reduce the execution time while integrating advanced algorithms for autonomous manipulation. We describe our approach to 3-D perception, real-time planning, force compliant motions, and audio processing. Performance results for object grasping and complex manipulation tasks of in-house tests and of an independent evaluation team are presented.},
  keywords = {Computer architecture,Grasping,Planning,Robot sensing systems,Software,Software algorithms},
  file = {C:\Users\benja\Zotero\storage\K9NXF7CC\6385888.html}
}

@inproceedings{bahlHierarchicalNeuralDynamic2021,
  title = {Hierarchical {{Neural Dynamic Policies}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVII}}},
  author = {Bahl, Shikhar and Gupta, Abhinav and Pathak, Deepak},
  year = {2021},
  month = jul,
  volume = {17},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-7-8},
  file = {C:\Users\benja\Zotero\storage\QDC4ZG72\p023.html}
}

@inproceedings{bahlNeuralDynamicPolicies2020,
  title = {Neural {{Dynamic Policies}} for {{End-to-End Sensorimotor Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bahl, Shikhar and Mukadam, Mustafa and Gupta, Abhinav and Pathak, Deepak},
  year = {2020},
  volume = {33},
  pages = {5058--5069},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-04-28},
  abstract = {The current dominant paradigm in sensorimotor control, whether imitation or reinforcement learning, is to train policies directly in raw action spaces such as torque, joint angle, or end-effector position. This forces the agent to make decision at each point in training, and hence, limits the scalability to continuous, high-dimensional, and long-horizon tasks. In contrast, research in classical robotics has, for a long time, exploited dynamical systems as a policy representation to learn robot behaviors via demonstrations. These techniques, however, lack the flexibility and generalizability provided by deep learning or deep reinforcement learning and have remained under-explored in such settings. In this work, we begin to close this gap and embed dynamics structure into deep neural network-based policies by reparameterizing action spaces with differential equations. We propose Neural Dynamic Policies (NPDs) that make predictions in trajectory distribution space as opposed to prior policy learning methods where action represents the raw control space. The embedded structure allows us to perform end-to-end policy learning under both reinforcement and imitation learning setups. We show that NDPs achieve better or comparable performance to state-of-the-art approaches on many robotic control tasks using both reward-based training and demonstrations. Project video and code are available at: https://shikharbahl.github.io/neural-dynamic-policies/.}
}

@article{baigNaturalLanguageSQL2022,
  title = {Natural {{Language}} to {{SQL Queries}}: {{A Review}}},
  shorttitle = {Natural {{Language}} to {{SQL Queries}}},
  author = {Baig, Mirza Shahzaib and Imran, Azhar and Yasin, Amanullah and Butt, Abdul Haleem and Khan, Muhammad Imran},
  year = {2022},
  month = feb,
  journal = {International Journal of Innovations in Science \& Technology},
  volume = {4},
  number = {1},
  pages = {147--162},
  issn = {2709-6130},
  urldate = {2024-09-04},
  abstract = {The relational database is the way of maintaining, storing, and accessing structured data but in order to access the data in that database the queries need to be translated in the format of SQL queries. Using natural language rather than SQL has introduced the advancement of a new kind of handling strategy called Natural Language Interface to Database frameworks (NLIDB).~ NLIDB is a stage towards the turn of events of clever data set frameworks (IDBS) to upgrade the clients in performing adaptable questioning in data sets. A model that can deduce relational database queries from natural language. Advanced neural algorithms synthesize the end-to-end SQL to text relation which results in the accuracy of 80\% on the publicly available datasets. In this paper, we reviewed the existing framework and compared them based on the aggregation classifier, select column pointer, and the clause pointer. Furthermore, we discussed the role of semantic parsing and neural algorithm's contribution in predicting the aggregation, column pointer, and clause pointer.~ In particular, people with limited background knowledge are unable to access databases with ease. Using natural language interfaces for relational databases is the solution to make natural language to SQL queries.~ This paper presents a review of the existing framework to process natural language to SQL queries and we will also cover some of the speech to SQL model in discussion section, in order to understand their framework and to highlight the limitations in the existing models.},
  copyright = {Copyright (c) 2022 50Sea},
  langid = {english},
  keywords = {Database Management System (DBMS),Intelligent Database System (IDBS),Natural Language Interface for Databases (NLIDB),Natural Language Processing,Structured Query Language (SQL),text to relational database},
  file = {C:\Users\benja\Zotero\storage\HIBM46Q8\Baig et al. - 2022 - Natural Language to SQL Queries A Review.pdf}
}

@inproceedings{bairdOnestepNeuralNetwork2005,
  title = {One-Step Neural Network Inversion with {{PDF}} Learning and Emulation},
  booktitle = {Proceedings. 2005 {{IEEE International Joint Conference}} on {{Neural Networks}}, 2005.},
  author = {Baird, L. and Smalenberger, D. and Ingkiriwang, S.},
  year = {2005},
  volume = {2},
  pages = {966--971},
  publisher = {IEEE},
  address = {Montreal, Que., Canada},
  doi = {10.1109/IJCNN.2005.1555983},
  urldate = {2019-05-17},
  abstract = {We present two new types of neural networks (both of which can be trained with ordinary error backpropagation) and we present a new algorithm for learning a probability density function (pdf) from example vectors. It is normally difficult to invert a neural network, but for the new bijective neural network, it is efficient to find an input producing any desired output, and such an input is guaranteed to exist and to be unique. Furthermore, it can be used as one component in building a pdf neural network, which is a neural network with a nonnegative output, and for which it is guaranteed that the integral of the output is exactly 1.0 (as in a pdf function). Both of these can be used for supervised learning using standard error backpropagation. Finally, the new pdf learning algorithm is capable of using those networks to learn a pdf given i.i.d. samples drawn from that pdf, and to then generate new vectors from the learned pdf. This, in turn, allows inversion of a function with non-unique inverses, where each inverse is generated with just a single evaluation of the network.},
  isbn = {978-0-7803-9048-5},
  langid = {english}
}

@article{Bandara2013,
  title = {Distributed, {{Multi-User}}, {{Multi-Application}}, and {{Multi-Sensor Data Fusion}} over {{Named Data Networks}}},
  author = {Bandara, H.M.N. Dilum and Jayasumana, Anura P.},
  year = {2013},
  month = nov,
  journal = {Computer Networks},
  volume = {57},
  number = {16},
  pages = {3235--3248},
  publisher = {Elsevier},
  issn = {1389-1286},
  doi = {10.1016/J.COMNET.2013.07.033},
  urldate = {2018-05-11},
  abstract = {Named Data Networking (NDN) routes data based on their application-layer content names enabling location independence, in-network caching, and enhanced security. A proof-of-concept solution is presented that demonstrates the applicability of NDN for multi-user, multi-application, and multi-sensor data-fusion systems. The system consists of a collaborative network of weather radars name data based on their geographic location and weather feature (e.g., reflectivity of clouds and wind velocity). This enables end users to specify an area of interest for a particular weather feature while being oblivious to the placement of radars and associated computing facilities. Conversely, the data-fusion system can also use its knowledge about the underlying system to decide the best sensing and data processing strategies. Such sensor-independent names also enhance resilience, enable processing data close to the source, and benefit from NDN features such as in-network caching and duplicate query suppression, consequently reducing the bandwidth requirements of the entire data-fusion system. The solution is implemented as an overlaid NDN enabling the benefits of both the NDN and overlay networks. Simulation-based analysis using reflectivity data from an actual weather event showed 84\% reduction in peak bandwidth consumption of radars and 95\% reduction in peak query resolution latency.}
}

@article{banerjeeTakingRecoveriesTask2020,
  title = {Taking {{Recoveries}} to {{Task}}: {{Recovery-Driven Development}} for {{Recipe-based Robot Tasks}}},
  shorttitle = {Taking {{Recoveries}} to {{Task}}},
  author = {Banerjee, Siddhartha and Daruna, Angel and Kent, David and Liu, Weiyu and Balloch, Jonathan and Jain, Abhinav and Krishnan, Akshay and Rana, Muhammad Asif and Ravichandar, Harish and Shah, Binit and Shrivatsav, Nithin and Chernova, Sonia},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.10386 [cs]},
  eprint = {2001.10386},
  primaryclass = {cs},
  urldate = {2021-03-03},
  abstract = {Robot task execution when situated in real-world environments is fragile. As such, robot architectures must rely on robust error recovery, adding non-trivial complexity to highly-complex robot systems. To handle this complexity in development, we introduce Recovery-Driven Development (RDD), an iterative task scripting process that facilitates rapid task and recovery development by leveraging hierarchical specification, separation of nominal task and recovery development, and situated testing. We validate our approach with our challenge-winning mobile manipulator software architecture developed using RDD for the FetchIt! Challenge at the IEEE 2019 International Conference on Robotics and Automation. We attribute the success of our system to the level of robustness achieved using RDD, and conclude with lessons learned for developing such systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\A53NS7LV\2001.html}
}

@inproceedings{bansakLearningRandomDistributional2024,
  title = {Learning {{Under Random Distributional Shifts}}},
  booktitle = {Proceedings of {{The}} 27th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Bansak, Kirk C. and Paulson, Elisabeth and Rothenhaeusler, Dominik},
  year = {2024},
  month = apr,
  pages = {3943--3951},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-14},
  abstract = {Many existing approaches for generating predictions in settings with distribution shift model distribution shifts as adversarial or low-rank in suitable representations. In various real-world settings, however, we might expect shifts to arise through the superposition of many small and random changes in the population and environment. Thus, we consider a class of random distribution shift models that capture arbitrary changes in the underlying covariate space, and dense, random shocks to the relationship between the covariates and the outcomes. In this setting, we characterize the benefits and drawbacks of several alternative prediction strategies: the standard approach that directly predicts the long-term outcomes of interest, the proxy approach that directly predicts shorter-term proxy outcomes, and a hybrid approach that utilizes both the long-term policy outcome and (shorter-term) proxy outcome(s). We show that the hybrid approach is robust to the strength of the distribution shift and the proxy relationship. We apply this method to datasets in two high-impact domains: asylum-seeker placement and early childhood education. In both settings, we find that the proposed approach results in substantially lower mean-squared error than current approaches.},
  langid = {english}
}

@article{barbecho-jimboRemoteOperationCeCi2023,
  title = {Remote {{Operation}} of {{CeCi Social Robot}}},
  author = {{Barbecho-Jimbo}, Edisson and {Vallejo-Ram{\'i}rez}, David and {Cobos-Torres}, Juan-Carlos and Angulo, Cecilio and {Flores-V{\'a}zquez}, Carlos},
  year = {2023},
  month = feb,
  journal = {Robotics},
  volume = {12},
  number = {1},
  pages = {19},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2218-6581},
  doi = {10.3390/robotics12010019},
  urldate = {2023-04-11},
  abstract = {This paper presents a validation methodology for a remote system with its objective focused on a social robot. The research process starts with the customization of an application for smartphones, achieving a simple method of connection and attachment to the robot. This customization allows remote operation of the robot's movements and an additional level of autonomy for the displacements in previously known locations. One of several teleoperations methods is the direct teleoperations method, which is used in master--slave control mode via a wireless network. Next, the article focuses on proposing a validation methodology for social robot applications design. Under this approach, two tests are performed to validate the designed application. The first one seeks to find the response speed of the communication between the robot and the mobile device wherein 10 devices with different characteristics and capabilities are used. This test is critical since a delay outside the allowable range invalidates the use of the application. The second test measures the application's usability through a user survey, which allows for determining the preferences that people may have when using this type of application. This second test is essential to consider the overall acceptability of the social robot.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {android,mobile robot,remote control,remote operation,ROS,social robot}
}

@article{Barnes.2013,
  title = {{{2D}}/{{3D}} Shape Manipulation, {{3D}} Printing: {{Courtesy}} of Slides: {{Olga}} Sorkine},
  author = {Barnes, Connelly},
  year = {2013},
  address = {University of Virginia}
}

@inproceedings{barraganSACHETSSemiAutonomousCognitive2021,
  title = {{{SACHETS}}: {{Semi-Autonomous Cognitive Hybrid Emergency Teleoperated Suction}}},
  shorttitle = {{{SACHETS}}},
  booktitle = {2021 30th {{IEEE International Conference}} on {{Robot}} \& {{Human Interactive Communication}} ({{RO-MAN}})},
  author = {Barragan, Juan Antonio and Chanci, Daniela and Yu, Denny and Wachs, Juan P.},
  year = {2021},
  month = aug,
  pages = {1243--1248},
  issn = {1944-9437},
  doi = {10.1109/RO-MAN50785.2021.9515517},
  urldate = {2024-04-12},
  abstract = {Blood suction and irrigation are among the most critical support tasks in robotic-assisted minimally invasive surgery (RMIS). Usually, suction/irrigation tools are controlled by a surgical assistant to maintain a clear view of the surgical field. Thus, the assistant's contribution to other emergency support tasks is limited. Similarly, when the surgical assistant is not available to perform the blood suction, the leading surgeon must take over this task, which in a complex surgical procedure can result in an unnecessary increment in the cognitive load. To alleviate this problem, we have developed a semi-autonomous robotic suction assistant, which was integrated with a Da Vinci Research Kit (DVRK). At the heart of the algorithm, there is an autonomous control based on a deep learning model to segment and identify the location of blood accumulations. This system provides automatic suction allowing the leading surgeon to focus exclusively on the main task through the control of key instruments of the robot. We conducted a user study to evaluate the user's workload demands and performance while doing a surgical task under two modalities: (1) autonomous suction action and (2) a surgeon-controlled-suction. Our results indicate that users working with the autonomous system completed the task 161 seconds faster than in the surgeon-controlled-suction modality. Furthermore, the autonomous modality led to a lower percentage of bleeding in the surgical field and workload demands on the users (p-value{$<$}0.05). These results show how leveraging state-of-the-art AI algorithms can reduce cognitive demands and enhance performance.},
  keywords = {Cognitive workload,Collaboration,Human robotic interaction,Image segmentation,Instruments,Robot vision systems,Robotic surgery,Semantics,Semi-autonomous assistant,Task analysis,Tools},
  file = {C:\Users\benja\Zotero\storage\LEA37TQ7\9515517.html}
}

@article{barraquandRobotMotionPlanning1991,
  title = {Robot {{Motion Planning}}: {{A Distributed Representation Approach}}},
  shorttitle = {Robot {{Motion Planning}}},
  author = {Barraquand, J{\'e}r{\^o}me and Latombe, Jean-Claude},
  year = {1991},
  month = dec,
  journal = {The International Journal of Robotics Research},
  volume = {10},
  number = {6},
  pages = {628--649},
  publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  doi = {10.1177/027836499101000604},
  urldate = {2024-04-04},
  abstract = {We propose a new approach to robot path planning that consists of building and searching a graph connecting the local minima of a potential function defined over the robot's configuration space. A planner based on this approach has been implemented. This planner is consider ably faster than previous path planners and solves prob lems for robots with many more degrees of freedom (DOFs). The power of the planner derives both from the "good" properties of the potential function and from the efficiency of the techniques used to escape the local min ima of this function. The most powerful of these tech niques is a Monte Carlo technique that escapes local min ima by executing Brownian motions. The overall approach is made possible by the systematic use of distributed rep resentations (bitmaps) for the robot's work space and configuration space. We have experimented with the plan ner using several computer-simulated robots, including rigid objects with 3 DOFs (in 2D work space) and 6 DOFs (in 3D work space) and manipulator arms with 8, 10, and 31 DOFs (in 2D and 3D work spaces). Some of the most significant experiments are reported in this article.},
  langid = {english}
}

@article{barrosSurveyUserInterfaces2009,
  title = {A {{Survey}} of {{User Interfaces}} for {{Robot Teleoperation}}},
  author = {Barros, P. D. and Linderman, R.},
  year = {2009},
  journal = {undefined},
  urldate = {2021-09-16},
  abstract = {Semantic Scholar extracted view of \&quot;A Survey of User Interfaces for Robot Teleoperation\&quot; by P. D. Barros et al.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\2P43MUI9\53c9114200f6f7fc266abf83be8b7d457aed112e.html}
}

@incollection{bartlettVapnikChervonenkisDimensionNeural2003,
  title = {Vapnik-{{Chervonenkis Dimension}} of {{Neural Nets}}},
  booktitle = {The {{Handbook}} of {{Brain Theory}} and {{Neural Networks}}},
  author = {Bartlett, Peter L and Maass, Wolfgang},
  year = {2003},
  edition = {2nd},
  pages = {1188--1192},
  publisher = {MIT Press},
  address = {Cambridge},
  langid = {english}
}

@inproceedings{batemanHeterogeneousOntologiesHybrid2017,
  title = {Heterogeneous {{Ontologies}} and {{Hybrid Reasoning}} for {{Service Robotics}}: {{The EASE Framework}}},
  shorttitle = {Heterogeneous {{Ontologies}} and {{Hybrid Reasoning}} for {{Service Robotics}}},
  booktitle = {Third {{Iberian Robotics Conference}}},
  author = {Bateman, John and Beetz, Michael and Be\&szlig;ler, Daniel and Bozcuoglu, Asil Kaan and Pomarlan, Mihai},
  year = {2017},
  file = {C:\Users\benja\Zotero\storage\MULY4UMZ\bibtexbrowser.html}
}

@inproceedings{batesOnlineSimultaneousLearning2017,
  title = {On-Line Simultaneous Learning and Recognition of Everyday Activities from Virtual Reality Performances},
  booktitle = {2017 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Bates, Tamas and {Ramirez-Amaro}, Karinne and Inamura, Tetsunari and Cheng, Gordon},
  year = {2017},
  month = sep,
  pages = {3510--3515},
  issn = {2153-0866},
  doi = {10.1109/IROS.2017.8206193},
  abstract = {Capturing realistic human behaviors is essential to learn human models that can later be transferred to robots. Recent improvements in virtual reality (VR) head-mounted displays provide a viable way to collect natural examples of human behavior without the difficulties often associated with capturing performances in a physical environment. We present a realistic, cluttered, VR environment for experimentation with household tasks paired with a semantic extraction and reasoning system able to utilize data collected in real-time and apply ontology-based reasoning to learn and classify activities performed in VR. The system performs continuous segmentation of the motions of users' hands and simultaneously classifies known actions while learning new ones on demand. The system then constructs a graph of all related activities in the environment through its observations, extracting the task space utilized by observed users during their performance. The action recognition and learning system was able to maintain a high degree of accuracy of around 92\% while dealing with a more complex and realistic environment compared to earlier work in both physical and virtual spaces.},
  keywords = {Avatars,Cognition,Motion segmentation,Semantics,Training},
  file = {C:\Users\benja\Zotero\storage\E39XAPLZ\8206193.html}
}

@inproceedings{bateuxTrainingDeepNeural2018,
  title = {Training {{Deep Neural Networks}} for {{Visual Servoing}}},
  booktitle = {{{ICRA}}},
  author = {Bateux, Quentin and Marchand, Eric and Leitner, Jurgen and Chaumette, Francois and Corke, Peter},
  year = {2018},
  month = may,
  pages = {3307--3314},
  doi = {10.1109/ICRA.2018.8461068},
  urldate = {2019-10-10},
  abstract = {We present a deep neural network-based method to perform high-precision, robust and real-time 6 DOF positioning tasks by visual servoing. A convolutional neural network is fine-tuned to estimate the relative pose between the current and desired images and a pose-based visual servoing control law is considered to reach the desired pose. The paper describes how to efficiently and automatically create a dataset used to train the network. We show that this enables the robust handling of various perturbations (occlusions and lighting variations). We then propose the training of a scene-agnostic network by feeding in both the desired and current images into a deep network. The method is validated on a 6 DOF robot.},
  isbn = {978-1-5386-3081-5},
  langid = {english}
}

@article{battertonLikertScaleWhat2017,
  title = {The {{Likert Scale}}. {{What It Is}} and {{How To Use It}}},
  author = {Batterton, Katherine A. and Hale, Kimberly N.},
  year = {2017},
  journal = {Phalanx},
  volume = {50},
  number = {2},
  eprint = {26296382},
  eprinttype = {jstor},
  pages = {32--39},
  publisher = {Military Operations Research Society},
  issn = {0195-1920},
  urldate = {2024-03-22}
}

@techreport{BauchspeicheldrusenkrebsPankreaskarzinom2023,
  title = {Bauchspeicheldr{\"u}senkrebs ({{Pankreaskarzinom}})},
  year = {2023},
  month = jul,
  number = {ICD-10 C25},
  institution = {Robert Koch Institut},
  urldate = {2024-04-12},
  file = {C:\Users\benja\Zotero\storage\YXP3VGTX\bauchspeicheldruesenkrebs_node.html}
}

@inproceedings{Bauer,
  title = {The {{Mobile Patient}}: {{Wireless Distributed Sensor Networks}} for {{Patient Monitoring}} and {{Care}}},
  booktitle = {Proceedings 2000 {{IEEE EMBS International Conference}} on {{Information Technology Applications}} in {{Biomedicine}}},
  author = {Bauer, P. and Sichitiu, M. and Istepanian, R. and Premaratne, K.},
  pages = {17--21},
  publisher = {IEEE},
  doi = {10.1109/ITAB.2000.892341},
  urldate = {2018-05-11},
  isbn = {0-7803-6449-X}
}

@article{bauerHumanRobotCollaborationSurvey2008,
  title = {Human-{{Robot Collaboration}}: A {{Survey}}.},
  shorttitle = {Human-{{Robot Collaboration}}},
  author = {Bauer, Andrea and Wollherr, Dirk and Buss, Martin},
  year = {2008},
  month = mar,
  journal = {International Journal of Humanoid Robotics},
  volume = {5},
  pages = {47--66},
  doi = {10.1142/S0219843608001303},
  abstract = {As robots are gradually leaving highly structured factory environments and moving into human populated environments, they need to possess more complex cognitive abilities. They do not only have to operate efficiently and safely in natural, populated environments, but also be able to achieve higher levels of cooperation and communication with humans. Human--robot collaboration (HRC) is a research field with a wide range of applications, future scenarios, and potentially a high economic impact. HRC is an interdisciplinary research area comprising classical robotics, cognitive sciences, and psychology. This paper gives a survey of the state of the art of HRC. Established methods for intention estimation, action planning, joint action, and machine learning are presented together with existing guidelines to hardware design. This paper is meant to provide the reader with a good overview of technologies and methods for HRC.}
}

@incollection{Bay.2006,
  title = {{{SURF}}: {{Speeded}} up Robust Features},
  booktitle = {Computer Vision -- {{ECCV}} 2006},
  author = {Bay, Herbert and Tuytelaars, Tinne and {van Gool}, Luc},
  editor = {Leonardis, Ale{\v s} and Bischof, Horst and Pinz, Axel},
  year = {2006},
  series = {Lecture Notes in Computer Science},
  volume = {3951},
  pages = {404--417},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11744023_32},
  bookpagination = {page},
  isbn = {978-3-540-33832-1}
}

@article{baydinAutomaticDifferentiationMachine2018,
  title = {Automatic {{Differentiation}} in {{Machine Learning}}: {{A Survey}}},
  shorttitle = {Automatic {{Differentiation}} in {{Machine Learning}}},
  author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {153},
  pages = {1--43},
  issn = {1533-7928},
  urldate = {2020-10-19},
  file = {C:\Users\benja\Zotero\storage\DIK5UCQE\17-468.html}
}

@patent{Bayer.1976,
  title = {Color Imaging Array},
  author = {Bayer, B.},
  year = {1976},
  number = {US3971065}
}

@article{Bayramoglu.2016,
  title = {Comparison of {{3D}} Local and Global Descriptors for Similarity Retrieval of Range Data},
  author = {Bayramoglu, Neslihan and Alatan, A. Ayd{\i}n},
  year = {2016},
  journal = {Neurocomputing},
  volume = {184},
  pages = {13--27},
  issn = {09252312},
  doi = {10.1016/j.neucom.2015.08.105},
  pagination = {page}
}

@article{bechtleLeveragingForwardModel2020,
  title = {Leveraging {{Forward Model Prediction Error}} for {{Learning Control}}},
  author = {Bechtle, Sarah and Hammoud, Bilal and Rai, Akshara and Meier, Franziska and Righetti, Ludovic},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.03859 [cs]},
  eprint = {2011.03859},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {Learning for model based control can be sample-efficient and generalize well, however successfully learning models and controllers that represent the problem at hand can be challenging for complex tasks. Using inaccurate models for learning can lead to sub-optimal solutions, that are unlikely to perform well in practice. In this work, we present a learning approach which iterates between model learning and data collection and leverages forward model prediction error for learning control. We show how using the controller's prediction as input to a forward model can create a differentiable connection between the controller and the model, allowing us to formulate a loss in the state space. This lets us include forward model prediction error during controller learning and we show that this creates a loss objective that significantly improves learning on different motor control tasks. We provide empirical and theoretical results that show the benefits of our method and present evaluations in simulation for learning control on a 7 DoF manipulator and an underactuated 12 DoF quadruped. We show that our approach successfully learns controllers for challenging motor control tasks involving contact switching.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  annotation = {recommend},
  file = {C:\Users\benja\Zotero\storage\4M8B8MI4\2011.html}
}

@book{beckFirstOrderMethodsOptimization2017,
  title = {First-{{Order Methods}} in {{Optimization}}},
  author = {Beck, Amir},
  year = {2017},
  month = sep,
  publisher = {{SIAM-Society for Industrial and Applied Mathematics}},
  address = {Philadelphia, PA, USA},
  abstract = {The primary goal of this book is to provide a self-contained, comprehensive study of the main rst-order methods that are frequently used in solving large-scale problems. First-order methods exploit information on values and gradients/subgradients (but not Hessians) of the functions composing the model under consideration. With the increase in the number of applications that can be modeled as large or even huge-scale optimization problems, there has been a revived interest in using simple methods that require low iteration cost as well as low memory storage. The author has gathered, reorganized, and synthesized (in a unified manner) many results that are currently scattered throughout the literature, many of which cannot be typically found in optimization books. First-Order Methods in Optimization offers comprehensive study of first-order methods with the theoretical foundations; provides plentiful examples and illustrations; emphasizes rates of convergence and complexity analysis of the main first-order methods used to solve large-scale problems; and covers both variables and functional decomposition methods. Audience: This book is intended primarily for researchers and graduate students in mathematics, computer sciences, and electrical and other engineering departments. Readers with a background in advanced calculus and linear algebra, as well as prior knowledge in the fundamentals of optimization (some convex analysis, optimality conditions, and duality), will be best prepared for the material.},
  isbn = {978-1-61197-498-0}
}

@article{beerFrameworkLevelsRobot2014,
  title = {Toward a Framework for Levels of Robot Autonomy in Human-Robot Interaction},
  author = {Beer, Jenay M. and Fisk, Arthur D. and Rogers, Wendy A.},
  year = {2014},
  month = jul,
  journal = {Journal of human-robot interaction},
  volume = {3},
  number = {2},
  pages = {74--99},
  issn = {2163-0364},
  doi = {10.5898/JHRI.3.2.Beer},
  urldate = {2021-02-05},
  abstract = {A critical construct related to human-robot interaction (HRI) is autonomy, which varies widely across robot platforms. Levels of robot autonomy (LORA), ranging from teleoperation to fully autonomous systems, influence the way in which humans and robots may interact with one another. Thus, there is a need to understand HRI by identifying variables that influence -- and are influenced by -- robot autonomy. Our overarching goal is to develop a framework for levels of robot autonomy in HRI. To reach this goal, the framework draws links between HRI and human-automation interaction, a field with a long history of studying and understanding human-related variables. The construct of autonomy is reviewed and redefined within the context of HRI. Additionally, the framework proposes a process for determining a robot's autonomy level, by categorizing autonomy along a 10-point taxonomy. The framework is intended to be treated as guidelines to determine autonomy, categorize the LORA along a qualitative taxonomy, and consider which HRI variables (e.g., acceptance, situation awareness, reliability) may be influenced by the LORA.},
  pmcid = {PMC5656240},
  pmid = {29082107}
}

@misc{beetzCRAMCognitiveArchitecture2023,
  title = {The {{CRAM Cognitive Architecture}} for {{Robot Manipulation}} in {{Everyday Activities}}},
  author = {Beetz, Michael and Kazhoyan, Gayane and Vernon, David},
  year = {2023},
  month = apr,
  number = {arXiv:2304.14119},
  eprint = {2304.14119},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.14119},
  urldate = {2023-08-28},
  abstract = {This paper presents a hybrid robot cognitive architecture, CRAM, that enables robot agents to accomplish everyday manipulation tasks. It addresses five key challenges that arise when carrying out everyday activities. These include (i) the underdetermined nature of task specification, (ii) the generation of context-specific behavior, (iii) the ability to make decisions based on knowledge, experience, and prediction, (iv) the ability to reason at the levels of motions and sensor data, and (v) the ability to explain actions and the consequences of these actions. We explore the computational foundations of the CRAM cognitive model: the self-programmability entailed by physical symbol systems, the CRAM plan language, generalized action plans and implicit-to-explicit manipulation, generative models, digital twin knowledge representation \& reasoning, and narrative-enabled episodic memories. We describe the structure of the cognitive architecture and explain the process by which CRAM transforms generalized action plans into parameterized motion plans. It does this using knowledge and reasoning to identify the parameter values that maximize the likelihood of successfully accomplishing the action. We demonstrate the ability of a CRAM-controlled robot to carry out everyday activities in a kitchen environment. Finally, we consider future extensions that focus on achieving greater flexibility through transformational learning and metacognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\YMYZCYKX\2304.html}
}

@inproceedings{beetzCRAMCognitiveRobot2010,
  title = {{{CRAM}} --- {{A Cognitive Robot Abstract Machine}} for Everyday Manipulation in Human Environments},
  booktitle = {2010 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Beetz, Michael and M{\"o}senlechner, Lorenz and Tenorth, Moritz},
  year = {2010},
  month = oct,
  pages = {1012--1017},
  issn = {2153-0866},
  doi = {10.1109/IROS.2010.5650146},
  abstract = {This paper describes CRAM (Cognitive Robot Abstract Machine) as a software toolbox for the design, the implementation, and the deployment of cognition-enabled autonomous robots performing everyday manipulation activities. CRAM equips autonomous robots with lightweight reasoning mechanisms that can infer control decisions rather than requiring the decisions to be preprogrammed. This way CRAM-programmed autonomous robots are much more flexible, reliable, and general than control programs that lack such cognitive capabilities. CRAM does not require the whole domain to be stated explicitly in an abstract knowledge base. Rather, it grounds symbolic expressions in the knowledge representation into the perception and actuation routines and into the essential data structures of the control programs. In the accompanying video, we show complex mobile manipulation tasks performed by our household robot that were realized using the CRAM infrastructure.},
  keywords = {Cognition,Computer architecture,Kernel,Robot control,Robot sensing systems,Trajectory},
  file = {C:\Users\benja\Zotero\storage\NF5PA5TW\5650146.html}
}

@inproceedings{beetzKnowRob202nd2018,
  title = {{{KnowRob}} 2.0 - {{A}} 2nd {{Generation Knowledge Processing Framework}} for {{Cognition-Enabled Robotic Agents}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Beetz, Michael and Bessler, Daniel and Haidu, Andrei and Pomarlan, Mihai and Bozcuoglu, Asil Kaan and Bartels, Georg},
  year = {2018},
  month = may,
  pages = {512--519},
  doi = {10.1109/ICRA.2018.8460964},
  urldate = {2019-08-05},
  abstract = {In this paper we present KNOWROB2, a second generation knowledge representation and reasoning framework for robotic agents. KNOWROB2 is an extension and partial redesign of KNOWROB, currently one of the most advanced knowledge processing systems for robots that has enabled them to successfully perform complex manipulation tasks such as making pizza, conducting chemical experiments, and setting tables. The knowledge base appears to be a conventional firstorder time interval logic knowledge base, but it exists to a large part only virtually: many logical expressions are constructed on demand from data structures of the control program, computed through robotics algorithms including ones for motion planning and solving inverse kinematics problems, and log data stored in noSQL databases. Novel features and extensions of KNOWROB2 substantially increase the capabilities of robotic agents of acquiring open-ended manipulation skills and competence, reasoning about how to perform manipulation actions more realistically, and acquiring commonsense knowledge.},
  isbn = {978-1-5386-3081-5},
  langid = {english}
}

@inproceedings{beetzRoboSherlockUnstructuredInformation2015,
  title = {{{RoboSherlock}}: {{Unstructured}} Information Processing for Robot Perception},
  shorttitle = {{{RoboSherlock}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Beetz, Michael and {B{\'a}lint-Bencz{\'e}di}, Ferenc and Blodow, Nico and Nyga, Daniel and Wiedemeyer, Thiemo and M{\'a}rton, Zolt{\'a}n-Csaba},
  year = {2015},
  month = may,
  pages = {1549--1556},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2015.7139395},
  abstract = {We present RoboSherlock, an open source software framework for implementing perception systems for robots performing human-scale everyday manipulation tasks. In RoboSherlock, perception and interpretation of realistic scenes is formulated as an unstructured information management (UIM) problem. The application of the UIM principle supports the implementation of perception systems that can answer task-relevant queries about objects in a scene, boost object recognition performance by combining the strengths of multiple perception algorithms, support knowledge-enabled reasoning about objects and enable automatic and knowledge-driven generation of processing pipelines. We demonstrate the potential of the proposed framework by three feasibility studies of systems for real-world scene perception that have been built on top of RoboSherlock.},
  keywords = {Cognition,Containers,Engines,Robot sensing systems,Shape},
  file = {C:\Users\benja\Zotero\storage\P5WKZVHI\7139395.html}
}

@inproceedings{behlAutoSimulateQuicklyLearning2020,
  title = {{{AutoSimulate}}: ({{Quickly}}) {{Learning Synthetic Data Generation}}},
  shorttitle = {{{AutoSimulate}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2020: 16th {{European Conference}}, {{Glasgow}}, {{UK}}, {{August}} 23--28, 2020, {{Proceedings}}, {{Part XXII}}},
  author = {Behl, Harkirat Singh and Baydin, Atilim G{\"u}ne{\c s} and Gal, Ran and Torr, Philip H. S. and Vineet, Vibhav},
  year = {2020},
  month = aug,
  pages = {255--271},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-030-58542-6_16},
  urldate = {2024-05-23},
  abstract = {Simulation is increasingly being used for generating large labelled datasets in many machine learning problems. Recent methods have focused on adjusting simulator parameters with the goal of maximising accuracy on a validation task, usually relying on REINFORCE-like gradient estimators. However these approaches are very expensive as they treat the entire data generation, model training, and validation pipeline as a black-box and require multiple costly objective evaluations at each iteration. We propose an efficient alternative for optimal synthetic data generation, based on a novel differentiable approximation of the objective. This allows us to optimize the simulator, which may be non-differentiable, requiring only one objective evaluation at each iteration with a little overhead. We demonstrate on a state-of-the-art photorealistic renderer that the proposed method finds the optimal data distribution faster (up to 50{\texttimes}{\texttimes}{$<$}math{$><$}mo{$>\times<$}/mo{$><$}/math{$>$}), with significantly reduced training data generation and better accuracy on real-world test datasets than previous methods.},
  isbn = {978-3-030-58541-9},
  keywords = {Optimization,Rendering,Simulator,Synthetic data,Training data distribution}
}

@inproceedings{beik-mohammadiLearningRiemannianManifolds2021,
  title = {Learning {{Riemannian Manifolds}} for {{Geodesic Motion Skills}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVII}}},
  author = {{Beik-Mohammadi}, Hadi and Hauberg, S{\o}ren and Arvanitidis, Georgios and Neumann, Gerhard and Rozo, Leonel},
  year = {2021},
  month = jul,
  volume = {17},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-7-8}
}

@article{belkinReconcilingModernMachinelearning2019,
  title = {Reconciling Modern Machine-Learning Practice and the Classical Bias--Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year = {2019},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {32},
  pages = {15849--15854},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1903070116},
  urldate = {2024-01-24},
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias--variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias--variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This ``double-descent'' curve subsumes the textbook U-shaped bias--variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.}
}

@article{bellini-leiteDualProcessTheory2024,
  title = {Dual {{Process Theory}} for {{Large Language Models}}: {{An}} Overview of Using {{Psychology}} to Address Hallucination and Reliability Issues},
  shorttitle = {Dual {{Process Theory}} for {{Large Language Models}}},
  author = {{Bellini-Leite}, Samuel C},
  year = {2024},
  month = aug,
  journal = {Adaptive Behavior},
  volume = {32},
  number = {4},
  pages = {329--343},
  publisher = {SAGE Publications Ltd STM},
  issn = {1059-7123},
  doi = {10.1177/10597123231206604},
  urldate = {2024-09-29},
  abstract = {State-of-the-art Large Language Models have recently exhibited extraordinary linguistic abilities which have surprisingly extended to reasoning. However, responses that are unreliable, false, or invented are still a frequent issue. It has been argued that scaling up strategies, as in increasing model size or hardware power, might not be enough to resolve the issue. Recent research has implemented Type 2 strategies (such as Chain-of-Thought and Tree-of-Thought), as strategies that mimic Type 2 reasoning, from Dual Process Theory, to interact with Large Language Models for improved results. The current paper reviews these strategies in light of the Predicting and Reflecting Framework for understanding Dual Process Theory and suggests what Psychology, drawing from research in executive functions, thinking disposition and creativity, can further contribute to possible implementations that address hallucination and reliability issues.},
  langid = {english}
}

@article{beltaSymbolicPlanningControl2007,
  title = {Symbolic Planning and Control of Robot Motion [{{Grand Challenges}} of {{Robotics}}]},
  author = {Belta, Calin and Bicchi, Antonio and Egerstedt, Magnus and Frazzoli, Emilio and Klavins, Eric and Pappas, George J.},
  year = {2007},
  month = mar,
  journal = {IEEE Robotics Automation Magazine},
  volume = {14},
  number = {1},
  pages = {61--70},
  issn = {1558-223X},
  doi = {10.1109/MRA.2007.339624},
  abstract = {In this paper, different research trends that use symbolic techniques for robot motion planning and control are illustrated. As it often happens in new research areas, contributions to this topic started at about the same time by different groups with different emphasis, approaches, and notation. This article tries to describe a framework in which many of the current methods and ideas can be placed and to provide a coherent picture of what the authors want to do, what have they got so far, and what the main missing pieces are. Generally speaking, the aim of symbolic control as is envisioned in this article is to enable the usage of methods of formal logic, languages, and automata theory for solving effectively complex planning problems for robots and teams of robots. The results presented in this article can be divided in two groups: top-down approaches, whereby formal logic tools are employed on rather abstract models of robots; and bottom up approaches, whose aim is to provide means by which such abstractions are possible and effective. The two ends do not quite tie as yet, and much work remains to be done in both directions to obtain generally applicable methods. However, the prospects of symbolic control of robots are definitely promising, and the challenging nature of problems to be solved warrants for the interest of a wide community of researchers},
  keywords = {Automata,Communication system control,Control systems,Mobile robots,Motion control,Motion planning,Robot control,Robot motion,Robot sensing systems,Robotics and automation},
  file = {C:\Users\benja\Zotero\storage\4YPJ3BGK\4141034.html}
}

@article{ben-davidLearnabilityCanBe2019,
  title = {Learnability Can Be Undecidable},
  author = {{Ben-David}, Shai and Hrube{\v s}, Pavel and Moran, Shay and Shpilka, Amir and Yehudayoff, Amir},
  year = {2019},
  month = jan,
  journal = {Nature Machine Intelligence},
  volume = {1},
  number = {1},
  pages = {44--48},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-018-0002-3},
  urldate = {2019-01-10},
  abstract = {The mathematical foundations of machine learning play a key role in the development of the field. They improve our understanding and provide tools for designing new learning paradigms. The advantages of mathematics, however, sometimes come with a cost. G{\"o}del and Cohen showed, in a nutshell, that not everything is provable. Here we show that machine learning shares this fate. We describe simple scenarios where learnability cannot be proved nor refuted using the standard axioms of mathematics. Our proof is based on the fact the continuum hypothesis cannot be proved nor refuted. We show that, in some cases, a solution to the `estimating the maximum' problem is equivalent to the continuum hypothesis. The main idea is to prove an equivalence between learnability and compression.},
  keywords = {Computational science,Statistics}
}

@book{Bengel.2008,
  title = {Masterkurs Parallele Und Verteilte Systeme: {{Grundlagen}} Und Programmierung von Multicoreprozessoren, Multiprozessoren, Cluster Und Grid},
  author = {Bengel, G{\"u}nther and Baun, Christian and Kunze, Marcel and Stucky, Karl-Uwe},
  year = {2008},
  publisher = {Vieweg+Teubner / GWV Fachverlage GmbH Wiesbaden},
  address = {Wiesbaden},
  doi = {10.1007/978-3-8348-9516-5},
  abstract = {Dieses Buch bietet eine systematische Darstellung des Stands der Technik und der aktuellen Entwicklungen auf dem Gebiet des parallelen und verteilten Rechnens. Es stellt alle relevanten Hardwarearchitekturen f{\"u}r Multiprozessoren und Multicoreprozessoren sowie ihre Betriebssysteme bis hin zum Google-Cluster vor. Das parallele Programmieren bildet einen Schwerpunkt des Werkes. Dazu geh{\"o}ren Client-Server-Modelle und Serviceorientierte Architekturen sowie Programmiermodelle f{\"u}r unterschiedliche Speicherarchitekturen. Eine ausf{\"u}hrliche Erl{\"a}uterung von Leistungsma{\ss}en, Parallelisierungstechniken und verteilten Algorithmen zeigt dem Programmierer M{\"o}glichkeiten und Grenzen der Verteilung auf. Methoden der statischen und der dynamischen Rechenlastverteilung sind ebenso enthalten wie moderne Virtualisierungstechniken. Die abschlie{\ss}enden Kapitel {\"u}ber Cluster- und Grid-Computing geben Einblick in die aktuellen Themen des Gebiets und einen Ausblick auf die zuk{\"u}nftigen Entwicklungen.},
  isbn = {978-3-8348-0394-8}
}

@article{bengioAdaptiveImportanceSampling2008,
  title = {Adaptive {{Importance Sampling}} to {{Accelerate Training}} of a {{Neural Probabilistic Language Model}}},
  author = {Bengio, Y. and Senecal, J.-S.},
  year = {2008},
  month = apr,
  journal = {IEEE Transactions on Neural Networks},
  volume = {19},
  number = {4},
  pages = {713--722},
  issn = {1045-9227, 1941-0093},
  doi = {10.1109/TNN.2007.912312},
  urldate = {2020-12-19},
  abstract = {Previous work on statistical language modeling has shown that it is possible to train a feedforward neural network to approximate probabilities over sequences of words, resulting in significant error reduction when compared to standard baseline models based on -grams. However, training the neural network model with the maximum-likelihood criterion requires computations proportional to the number of words in the vocabulary. In this paper, we introduce adaptive importance sampling as a way to accelerate training of the model. The idea is to use an adaptive -gram model to track the conditional distributions produced by the neural network. We show that a very significant speedup can be obtained on standard problems.},
  langid = {english}
}

@article{bengioScheduledSamplingSequence2015,
  title = {Scheduled {{Sampling}} for {{Sequence Prediction}} with {{Recurrent Neural Networks}}},
  author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
  year = {2015},
  month = sep,
  journal = {arXiv:1506.03099 [cs]},
  eprint = {1506.03099},
  primaryclass = {cs},
  urldate = {2020-01-01},
  abstract = {Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{benidisDeepLearningTime2022,
  title = {Deep {{Learning}} for {{Time Series Forecasting}}: {{Tutorial}} and {{Literature Survey}}},
  shorttitle = {Deep {{Learning}} for {{Time Series Forecasting}}},
  author = {Benidis, Konstantinos and Rangapuram, Syama Sundar and Flunkert, Valentin and Wang, Yuyang and Maddix, Danielle and Turkmen, Caner and Gasthaus, Jan and {Bohlke-Schneider}, Michael and Salinas, David and Stella, Lorenzo and Aubet, Fran{\c c}ois-Xavier and Callot, Laurent and Januschowski, Tim},
  year = {2022},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {6},
  pages = {121:1--121:36},
  issn = {0360-0300},
  doi = {10.1145/3533382},
  urldate = {2024-04-13},
  abstract = {Deep learning based forecasting methods have become the methods of choice in many applications of time series prediction or forecasting often outperforming other approaches. Consequently, over the last years, these methods are now ubiquitous in large-scale industrial forecasting applications and have consistently ranked among the best entries in forecasting competitions (e.g., M4 and M5). This practical success has further increased the academic interest to understand and improve deep forecasting methods. In this article we provide an introduction and overview of the field: We present important building blocks for deep forecasting in some depth; using these building blocks, we then survey the breadth of the recent deep forecasting literature.},
  keywords = {forecasting,neural networks,Time series}
}

@article{bereskaMechanisticInterpretabilityAI2024,
  title = {Mechanistic {{Interpretability}} for {{AI Safety}} - {{A Review}}},
  author = {Bereska, Leonard and Gavves, Stratis},
  year = {2024},
  month = apr,
  journal = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  urldate = {2024-09-29},
  abstract = {Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We examine benefits in understanding, control, alignment, and risks such as capability gains and dual-use concerns. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable. For an HTML version of the paper, visit https://leonardbereska.github.io/blog/2024/mechinterpreview/.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\PWACGPJP\Bereska und Gavves - 2024 - Mechanistic Interpretability for AI Safety - A Review.pdf}
}

@book{Berg.2008,
  title = {Computational Geometry},
  author = {{de Berg}, Mark and Cheong, Otfried and {van Kreveld}, Marc and Overmars, Mark},
  year = {2008},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-77974-2},
  isbn = {978-3-540-77973-5}
}

@inproceedings{Bergstrom.2012,
  title = {On-Line Learning of Temporal State Models for Flexible Objects},
  booktitle = {2012 12th {{IEEE-RAS}} International Conference on Humanoid Robots (Humanoids 2012)},
  author = {Bergstrom, N. and Ek, C. H. and Kragic, D. and Yamakawa, Y. and Senoo, T. and Ishikawa, M.},
  year = {2012},
  pages = {712--718},
  publisher = {IEEE},
  doi = {10.1109/HUMANOIDS.2012.6651598},
  bookpagination = {page},
  isbn = {978-1-4673-1369-8}
}

@article{berkenkampBayesianOptimizationSafety2023,
  title = {Bayesian Optimization with Safety Constraints: Safe and Automatic Parameter Tuning in Robotics},
  shorttitle = {Bayesian Optimization with Safety Constraints},
  author = {Berkenkamp, Felix and Krause, Andreas and Schoellig, Angela P.},
  year = {2023},
  month = oct,
  journal = {Machine Learning},
  volume = {112},
  number = {10},
  pages = {3713--3747},
  issn = {1573-0565},
  doi = {10.1007/s10994-021-06019-1},
  urldate = {2024-05-23},
  abstract = {Selecting the right tuning parameters for algorithms is a pravelent problem in machine learning that can significantly affect the performance of algorithms. Data-efficient optimization algorithms, such as Bayesian optimization, have been used to automate this process. During experiments on real-world systems such as robotic platforms these methods can evaluate unsafe parameters that lead to safety-critical system failures and can destroy the system. Recently, a safe Bayesian optimization algorithm, called~SafeOpt, has been developed, which guarantees that the performance of the system never falls below a critical value; that is, safety is defined based on the performance function. However, coupling performance and safety is often not desirable in practice, since they are often opposing objectives. In this paper, we present a generalized algorithm that allows for multiple safety constraints separate from the objective. Given an initial set of safe parameters, the algorithm maximizes performance but only evaluates parameters that satisfy safety for all constraints with high probability. To this end, it carefully explores the parameter space by exploiting regularity assumptions in terms of a Gaussian process prior. Moreover, we show how context variables can be used to safely transfer knowledge to new situations and tasks. We provide a theoretical analysis and demonstrate that the proposed algorithm enables fast, automatic, and safe optimization of tuning parameters in experiments on a quadrotor vehicle.},
  langid = {english},
  keywords = {Bayesian optimization,Reinforcement learning,Robotics,Safe exploration,Safety constraints}
}

@article{berthelotMixMatchHolisticApproach2019,
  title = {{{MixMatch}}: {{A Holistic Approach}} to {{Semi-Supervised Learning}}},
  shorttitle = {{{MixMatch}}},
  author = {Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin},
  year = {2019},
  month = may,
  journal = {arXiv:1905.02249 [cs, stat]},
  eprint = {1905.02249},
  primaryclass = {cs, stat},
  urldate = {2019-05-21},
  abstract = {Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp. We show that MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38\% to 11\%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{beschiCAPIRCIMultimodalSystem2019,
  title = {{{CAPIRCI}}: {{A Multi-modal System}} for {{Collaborative Robot Programming}}},
  shorttitle = {{{CAPIRCI}}},
  booktitle = {End-{{User Development}}},
  author = {Beschi, Sara and Fogli, Daniela and Tampalini, Fabio},
  editor = {Malizia, Alessio and Valtolina, Stefano and Morch, Anders and Serrano, Alan and Stratton, Andrew},
  year = {2019},
  pages = {51--66},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-24781-2_4},
  abstract = {This paper presents CAPIRCI (Chat And Program Industrial Robots through Convenient Interaction), a multi-modal web application supporting end users, with no expertise in computer science, to define and modify tasks to be executed by collaborative robots. The application provides two interaction modalities, the former based on a chat interface, the latter presenting a visual programming language inspired to block-based solutions but tailored to the domain at hand. In order to investigate how different kinds of users may accept and use CAPIRCI, a user study with 20 participants has been carried out. Participants were equally split in expert programmers and non-expert programmers; execution times do not show any significant differences between the two groups, while qualitative data collected through direct observation and interviews provide useful hints and suggestions for system refinement.},
  isbn = {978-3-030-24781-2},
  langid = {english},
  keywords = {Collaborative robot programming,Component-based development,Domain experts,Industrial robotics interfaces,Natural language interfaces}
}

@inproceedings{Besl.1992,
  title = {Method for Registration of 3-{{D}} Shapes},
  booktitle = {Sensor Fusion {{IV}}: {{Control}} Paradigms and Data Structures},
  author = {Besl, Paul J. and McKay, Neil D.},
  editor = {Schenker, Paul S.},
  year = {1992},
  series = {{{SPIE}} Proceedings},
  pages = {586--606},
  publisher = {SPIE},
  doi = {10.1117/12.57955},
  bookpagination = {page}
}

@article{besslerFormalModelAffordances2020,
  title = {A {{Formal Model}} of {{Affordances}} for {{Flexible Robotic Task Execution}}},
  author = {Be{\ss}ler, Daniel and Porzel, Robert and Pomarlan, Mihai and Beetz, Michael and Malaka, Rainer and Bateman, John},
  year = {2020},
  journal = {ECAI 2020},
  pages = {2425--2432},
  publisher = {IOS Press},
  doi = {10.3233/FAIA200374},
  urldate = {2021-04-18},
  file = {C\:\\Users\\benja\\Zotero\\storage\\VTJUK97M\\Be&#223 et al. - 2020 - A Formal Model of Affordances for Flexible Robotic.pdf;C\:\\Users\\benja\\Zotero\\storage\\SXKV7CIL\\55169.html}
}

@incollection{besslerFoundationalModelsManipulation2019,
  title = {Foundational {{Models}} for {{Manipulation Activity Parsing}}},
  booktitle = {Augmented {{Reality}} and {{Virtual Reality}} -- {{Changing Realities}} in a {{Dynamic World}}},
  author = {Be{\ss}ler, Daniel and Porzel, Robert and Mihai, Pomarlan and Beetz, Michael},
  year = {2019},
  publisher = {Springer},
  file = {C:\Users\benja\Zotero\storage\9YF6FNR5\bibtexbrowser.html}
}

@incollection{besslerFoundationsSocioPhysicalModel2021,
  title = {Foundations of the {{Socio-Physical Model}} of {{Activities}} ({{SOMA}}) for {{Autonomous Robotic Agents}}\&lt;Span Ref-Type=\&quot;Fn\&quot; Rid=\&quot;{{FAIA210379}}\_fn001\&quot; Style=\&quot;Display:None\&quot;\&gt; \&lt;Sup\&gt;1\&lt;/Sup\&gt; \&lt;/Span\&gt;},
  shorttitle = {Foundations of the {{Socio-Physical Model}} of {{Activities}} ({{SOMA}}) for {{Autonomous Robotic Agents}}\&lt;Span Ref-Type=\&quot;Fn\&quot; Rid=\&quot;{{FAIA210379}}\_fn001\&quot; Style=\&quot;Display},
  booktitle = {Formal {{Ontology}} in {{Information Systems}}},
  author = {Be{\ss}ler, Daniel and Porzel, Robert and Pomarlan, Mihai and Vyas, Abhijit and H{\"o}ffner, Sebastian and Beetz, Michael and Malaka, Rainer and Bateman, John},
  year = {2021},
  series = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  pages = {159--174},
  publisher = {IOS Press},
  doi = {10.3233/FAIA210379},
  urldate = {2024-09-29},
  file = {C:\Users\benja\Zotero\storage\CM2QIKCN\Be&#223 et al. - 2021 - Foundations of the Socio-Physical Model of Activities (SOMA) for Autonomous Robotic Agents&lt;span r.pdf}
}

@incollection{besslerInformationSystemStorage2019,
  title = {Information {{System}} for {{Storage}}, {{Management}} and {{Usage}} for {{Embodied Intelligent Agents}}},
  booktitle = {Information {{Storage}} from a Multi-Disciplinary Perspective},
  author = {Be{\ss}ler, Daniel and Bozcuoglu, Asil Kaan and Beetz, Michael},
  year = {2019},
  publisher = {Springer},
  file = {C:\Users\benja\Zotero\storage\DWV3JH48\bibtexbrowser.html}
}

@phdthesis{besslerOntologicalRepresentationActivity2022,
  title = {Ontological Representation of Activity Context for Flexible Robot Task Execution},
  author = {Be{\ss}ler, Daniel},
  year = {2022},
  month = sep,
  address = {Bremen, Germany},
  urldate = {2024-04-30},
  abstract = {It has been demonstrated many times that modern robotic platforms can generate competent bodily behavior comparable to the level of humans. However, the implementation of such behavior requires a lot of programming effort, and is often not feasible for the general case, i.e., regardless of the situational context in which the activity is performed. Furthermore, research and industry have an enormous need for intuitive robot programming. This is due to the high complexity of realizing an integrated robot control system, and adapting it to other robots, tasks and environments. The challenge is how a robot control program can be realized that can generate competent behavior depending on characteristics of the robot, the task it executes, and the environment where it operates. One way to approach this problem is to specialize the control program through the context-specific application of abstract knowledge.    In this work, it will be investigated how abstract knowledge, required for flexible and competent robot task execution, can be represented using a formal ontology. To this end, a domain ontology of robot activity context will be proposed. Using this ontology, robots can infer how tasks can be accomplished through movements and interactions with the environment, and how they can improvise to a certain extent to take advantage of action possibilities that objects provide in their environment. Accordingly, it will be shown that parts of the context-specific information required for flexible task execution can be derived from broadly applicable knowledge represented in an ontology. Furthermore, it will be shown that the domain vocabulary yields additional benefits for the representation of knowledge gained through experimentation and simulation. Such knowledge can be leveraged for learning, or be used to inspect the robot's behavior. The latter of which will be demonstrated in this work by means of a case study.},
  copyright = {CC BY 4.0 (Attribution)},
  langid = {english},
  school = {University of Bremen},
  annotation = {Accepted: 2022-10-17T06:25:25Z}
}

@article{besslerOWLenabledAssemblyPlanning,
  title = {{{OWL-enabled Assembly Planning}} for {{Robotic Agents}}},
  author = {Be{\ss}ler, Daniel and Pomarlan, Mihai and Beetz, Michael},
  pages = {9},
  abstract = {Assembly cells run by intelligent robotic agents promise highly flexible product customization without the cost implication product individualization has nowadays. One of the main questions an assembly robot has to answer is which sequence of manipulation actions it should perform to create an assembled product from scattered pieces available. We propose a novel approach to assembly planning that employs Description Logics (DL) to describe what an assembled product should look like, and to plan the next action according to faulty and missing assertions in the robot's beliefs about an ongoing assembly task. To this end we extend the KNOWROB knowledge base with representations and inference rules that enable robots to reason about incomplete assemblies. We show that our approach performs well for large batches of assembly pieces available, as well as for varying structural complexity of assembled products.},
  langid = {english}
}

@inproceedings{besslerOWLenabledAssemblyPlanning2018,
  title = {{{OWL-enabled Assembly Planning}} for {{Robotic Agents}}},
  booktitle = {Proceedings of the 17th {{International Conference}} on {{Autonomous Agents}} and {{MultiAgent Systems}}},
  author = {Be{\ss}ler, Daniel and Pomarlan, Mihai and Beetz, Michael},
  year = {2018},
  month = jul,
  series = {{{AAMAS}} '18},
  pages = {1684--1692},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  address = {Richland, SC},
  urldate = {2021-03-03},
  abstract = {Assembly cells run by intelligent robotic agents promise highly flexible product customization without the cost implication product individualization has nowadays. One of the main questions an assembly robot has to answer is which sequence of manipulation actions it should perform to create an assembled product from scattered pieces available. We propose a novel approach to assembly planning that employs Description Logics (DL) to describe what an assembled product should look like, and to plan the next action according to faulty and missing assertions in the robot's beliefs about an ongoing assembly task. To this end we extend the KnowRob knowledge base with representations and inference rules that enable robots to reason about incomplete assemblies. We show that our approach performs well for large batches of assembly pieces available, as well as for varying structural complexity of assembled products.}
}

@article{betkerImprovingImageGeneration2023,
  title = {Improving {{Image Generation}} with {{Better Captions}}},
  author = {Betker, James and Goh, Gabriel and Jing, Li and Brooks, Tim and Wang, Jianfeng and Li, Linjie and Ouyang, Long and Zhuang, Juntang and Lee, Joyce and Guo, Yufei and Manassra, Wesam and Dhariwal, Prafulla and Chu, Casey and Jiao, Yunxin and Ramesh, Aditya},
  year = {2023},
  abstract = {We show that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions. Existing text-to-image models struggle to follow detailed image descriptions and often ignore words or confuse the meaning of prompts. We hypothesize that this issue stems from noisy and inaccurate image captions in the training dataset. We address this by training a bespoke image captioner and use it to recaption the training dataset. We then train several text-to-image models and find that training on these synthetic captions reliably improves prompt following ability. Finally, we use these findings to build DALL-E 3: a new text-to-image generation system, and benchmark its performance on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors. We publish samples and code for these evaluations so that future research can continue optimizing this important aspect of text-to-image systems.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\2R9ZP8LC\Betker et al. - Improving Image Generation with Better Captions.pdf}
}

@inproceedings{beyretDottoDotExplainableHierarchical2019,
  title = {Dot-to-{{Dot}}: {{Explainable Hierarchical Reinforcement Learning}} for {{Robotic Manipulation}}},
  shorttitle = {Dot-to-{{Dot}}},
  booktitle = {{{IROS}}},
  author = {Beyret, Benjamin and Shafti, Ali and Faisal, A. Aldo},
  year = {2019},
  month = nov,
  pages = {5014--5019},
  issn = {2153-0866},
  doi = {10.1109/IROS40897.2019.8968488},
  abstract = {Robotic systems are ever more capable of automation and fulfilment of complex tasks, particularly with reliance on recent advances in intelligent systems, deep learning and artificial intelligence in general. However, as robots and humans come closer together in their interactions, the matter of interpretability, or explainability of robot decision-making processes for the human grows in importance. A successful interaction and collaboration would only be possible through mutual understanding of underlying representations of the environment and the task at hand. This is currently a challenge in deep learning systems. We present a hierarchical deep reinforcement learning system, consisting of a low-level agent handling the large actions/states space of a robotic system efficiently, by following the directives of a high-level agent which is learning the high-level dynamics of the environment and task. This high-level agent forms a representation of the world and task at hand that is interpretable for a human operator. The method, which we call Dot-to-Dot, is tested on a MuJoCo-based model of the Fetch Robotics Manipulator, as well as a Shadow Hand, to test its performance. Results show efficient learning of complex actions/states spaces by the low-level agent, and an interpretable representation of the task and decision-making process learned by the high-level agent.},
  keywords = {artificial intelligence,decision making,deep reinforcement learning system,Dot-to-Dot,explainability,explainable hierarchical reinforcement,explanation,Fetch Robotics Manipulator,high-level agent,intelligent robotic agent,intelligent robots,intelligent systems,learning (artificial intelligence),low-level agent,manipulators,MuJoCo-based model,multi-agent systems,robot decision-making processes,robotic manipulation,Shadow Hand},
  file = {C:\Users\benja\Zotero\storage\MNFKP98P\8968488.html}
}

@inproceedings{bhardwajDifferentiableGaussianProcess2020,
  title = {Differentiable {{Gaussian Process Motion Planning}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Bhardwaj, Mohak and Boots, Byron and Mukadam, Mustafa},
  year = {2020},
  month = may,
  pages = {10598--10604},
  issn = {2577-087X},
  doi = {10.1109/ICRA40945.2020.9197260},
  urldate = {2024-09-29},
  abstract = {Modern trajectory optimization based approaches to motion planning are fast, easy to implement, and effective on a wide range of robotics tasks. However, trajectory optimization algorithms have parameters that are typically set in advance (and rarely discussed in detail). Setting these parameters properly can have a significant impact on the practical performance of the algorithm, sometimes making the difference between finding a feasible plan or failing at the task entirely. We propose a method for leveraging past experience to learn how to automatically adapt the parameters of Gaussian Process Motion Planning (GPMP) algorithms. Specifically, we propose a differentiable extension to the GPMP2 algorithm, so that it can be trained end-to-end from data. We perform several experiments that validate our algorithm and illustrate the benefits of our proposed learning-based approach to motion planning.},
  keywords = {Artificial intelligence,Automation,Conferences,Gaussian processes,Planning,Robots,Trajectory optimization},
  file = {C\:\\Users\\benja\\Zotero\\storage\\MWUCAAJA\\Bhardwaj et al. - 2020 - Differentiable Gaussian Process Motion Planning.pdf;C\:\\Users\\benja\\Zotero\\storage\\KJDTFPPJ\\9197260.html}
}

@misc{bhatPyttsx32020,
  title = {Pyttsx3},
  shorttitle = {Pyttsx3},
  author = {Bhat, Natesh},
  year = {2020},
  month = jun,
  urldate = {2024-07-25},
  abstract = {Text to Speech (TTS) library for Python 2 and 3. Works without internet connection or delay. Supports multiple TTS engines, including Sapi5, nsss, and espeak.},
  copyright = {OSI Approved :: GNU General Public License v3},
  keywords = {gtts,ivona,offline text to speech,offline tts,pyttsx,pyttsx for python3,pyttsx3,speech,speech synthesis,text to speech,text to speech for python,tts,TTS for python3},
  file = {C:\Users\benja\Zotero\storage\PCEPYUAZ\pyttsx3.html}
}

@article{biedermanRecognitionbycomponentsTheoryHuman1987,
  title = {Recognition-by-Components: {{A}} Theory of Human Image Understanding},
  shorttitle = {Recognition-by-Components},
  author = {Biederman, Irving},
  year = {1987},
  journal = {Psychological Review},
  volume = {94},
  pages = {115--147},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.94.2.115},
  abstract = {The perceptual recognition of objects is conceptualized to be a process in which the image of the input is segmented at regions of deep concavity into an arrangement of simple geometric components. The fundamental assumption of the proposed theory, recognition-by-components (RBC), is that a modest set of generalized-cone components, called geons, can be derived from contrasts of five readily detectable properties of edges in a two-dimensional image. The detection of these properties is generally invariant over viewing position and image quality and consequently allows robust object perception when the image is projected from a novel viewpoint or is degraded. RBC thus provides a principled account of the heretofore undecided relation between the classic principles of perceptual organization and pattern recognition. The results from experiments on the perception of briefly presented pictures by human observers provide empirical support for the theory. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Pattern Discrimination,Perceptual Organization,Pictorial Stimuli,Spatial Imagery,Spatial Organization,Spatial Orientation (Perception)},
  file = {C:\Users\benja\Zotero\storage\ZTENABRN\1987-20898-001.html}
}

@inproceedings{biggsSurveyRobotProgramming2003,
  title = {A {{Survey}} of {{Robot Programming Systems}}},
  author = {Biggs, Georey and MacDonald, Bruce},
  year = {2003},
  urldate = {2024-03-06},
  abstract = {Robots have become signi(cid:12)cantly more powerful and intelligent over the last decade, and are moving in to more service oriented roles. As a result robots will more often be used by people with minimal technical skills and so there is a need for easier to use and more (cid:13)exible programming systems. This paper reviews the current state of the art in robot programming systems. A distinction is made between manual and automatic programming systems. Manual systems require the user/programmer to create the robot program directly, by hand, while automatic systems generate a robot program as a result of interaction between the robot and the human; there are a variety of methods including learning, programming by demonstration and instructive systems.}
}

@article{billardLearningHumanArm2001,
  title = {Learning Human Arm Movements by Imitation:: {{Evaluation}} of a Biologically Inspired Connectionist Architecture},
  shorttitle = {Learning Human Arm Movements by Imitation},
  author = {Billard, Aude and Matari{\'c}, Maja J.},
  year = {2001},
  month = nov,
  journal = {Robotics and Autonomous Systems},
  series = {Humanoid {{Robots}}},
  volume = {37},
  number = {2},
  pages = {145--160},
  issn = {0921-8890},
  doi = {10.1016/S0921-8890(01)00155-5},
  urldate = {2021-09-16},
  abstract = {This paper evaluates a model of human imitation of abstract, two-arm movements. The model consists of a hierarchy of artificial neural networks, which are abstractions of brain regions involved in visuo-motor control. The model is validated in a biomechanical simulation of a 37 degree of freedom (DOF) humanoid. Input to the model are data from human arm movements recorded using video and marker-based tracking systems. Results show a high qualitative and quantitative agreement with human data. The model's reproduction is better or comparable to that of human subjects imitating the same movements.},
  langid = {english},
  keywords = {Artificial neural networks,Imitation,Learning},
  file = {C:\Users\benja\Zotero\storage\PENRURAL\S0921889001001555.html}
}

@incollection{billardRobotProgrammingDemonstration2008,
  title = {Robot {{Programming}} by {{Demonstration}}},
  booktitle = {Springer {{Handbook}} of {{Robotics}}},
  author = {Billard, Aude and Calinon, Sylvain and Dillmann, R{\"u}diger and Schaal, Stefan},
  editor = {Siciliano, Bruno and Khatib, Oussama},
  year = {2008},
  pages = {1371--1394},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-30301-5_60},
  urldate = {2019-05-17},
  isbn = {978-3-540-23957-4 978-3-540-30301-5},
  langid = {english}
}

@inproceedings{bilosNeuralFlowsEfficient2021,
  title = {Neural {{Flows}}: {{Efficient Alternative}} to {{Neural ODEs}}},
  shorttitle = {Neural {{Flows}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bilo{\v s}, Marin and Sommer, Johanna and Rangapuram, Syama Sundar and Januschowski, Tim and G{\"u}nnemann, Stephan},
  year = {2021},
  month = nov,
  urldate = {2024-03-25},
  abstract = {Neural ordinary differential equations describe how values change in time. This is the reason why they gained importance in modeling sequential data, especially when the observations are made at irregular intervals. In this paper we propose an alternative by directly modeling the solution curves - the flow of an ODE - with a neural network. This immediately eliminates the need for expensive numerical solvers while still maintaining the modeling capability of neural ODEs. We propose several flow architectures suitable for different applications by establishing precise conditions on when a function defines a valid flow. Apart from computational efficiency, we also provide empirical evidence of favorable generalization performance via applications in time series modeling, forecasting, and density estimation.},
  langid = {english}
}

@article{bischlHyperparameterOptimizationFoundations2023,
  title = {Hyperparameter Optimization: {{Foundations}}, Algorithms, Best Practices, and Open Challenges},
  shorttitle = {Hyperparameter Optimization},
  author = {Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and Deng, Difan and Lindauer, Marius},
  year = {2023},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {13},
  number = {2},
  pages = {e1484},
  issn = {1942-4795},
  doi = {10.1002/widm.1484},
  urldate = {2024-09-08},
  abstract = {Most machine learning algorithms are configured by a set of hyperparameters whose values must be carefully chosen and which often considerably impact performance. To avoid a time-consuming and irreproducible manual process of trial-and-error to find well-performing hyperparameter configurations, various automatic hyperparameter optimization (HPO) methods---for example, based on resampling error estimation for supervised machine learning---can be employed. After introducing HPO from a general perspective, this paper reviews important HPO methods, from simple techniques such as grid or random search to more advanced methods like evolution strategies, Bayesian optimization, Hyperband, and racing. This work gives practical recommendations regarding important choices to be made when conducting HPO, including the HPO algorithms themselves, performance evaluation, how to combine HPO with machine learning pipelines, runtime improvements, and parallelization. This article is categorized under: Algorithmic Development {$>$} Statistics Technologies {$>$} Machine Learning Technologies {$>$} Prediction},
  copyright = {{\copyright} 2023 The Authors. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals LLC.},
  langid = {english},
  keywords = {automl,hyperparameter optimization,machine learning,model selection,tuning},
  file = {C\:\\Users\\benja\\Zotero\\storage\\XWTNSBGW\\Bischl et al. - 2023 - Hyperparameter optimization Foundations, algorithms, best practices, and open challenges.pdf;C\:\\Users\\benja\\Zotero\\storage\\6VYQ4CCZ\\widm.html}
}

@book{bittencourtModelingDiagnosisFriction2014,
  title = {Modeling and {{Diagnosis}} of {{Friction}} and {{Wear}} in {{Industrial Robots}}},
  author = {Bittencourt, Andr{\'e}},
  year = {2014},
  month = sep,
  doi = {10.3384/diss.diva-109335},
  isbn = {978-91-7519-251-2}
}

@incollection{blaesControlWhatYou2019,
  title = {Control {{What You Can}}: {{Intrinsically Motivated Task-Planning Agent}}},
  shorttitle = {Control {{What You Can}}},
  booktitle = {{{NeurIPS}}},
  author = {Blaes, Sebastian and Vlastelica Pogan{\v c}i{\'c}, Marin and Zhu, Jiajie and Martius, Georg},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {12541--12552},
  urldate = {2020-07-09},
  file = {C:\Users\benja\Zotero\storage\EEL8Z4Z9\9418-control-what-you-can-intrinsically-motivated-task-planning-agent.html}
}

@incollection{blankExploringSymbolicSubsymbolic1992,
  title = {Exploring the Symbolic/Subsymbolic Continuum: {{A}} Case Study of {{RAAM}}},
  shorttitle = {Exploring the Symbolic/Subsymbolic Continuum},
  booktitle = {The Symbolic and Connectionist Paradigms:  {{Closing}} the Gap},
  author = {Blank, Douglas S. and Meeden, Lisa A. and Marshall, James B.},
  year = {1992},
  series = {The Cognitive Science Series:  {{Technical}} Monographs and Edited Collection},
  pages = {113--148},
  publisher = {Lawrence Erlbaum Associates, Inc},
  address = {Hillsdale, NJ, US},
  abstract = {characterized the symbolic and subsymbolic paradigms [within artificial intelligence and cognitive science] as two opposing corners of an abstract space of paradigms / this space . . . has at least three dimensions: representation, composition, and functionality  focuses on the RAAM [Recursive Auto-Associative Memory] model as an exemplar of the subsymbolic paradigm / do not wish to claim that RAAM is an accurate model of human cognition; we simply feel that it provides a clear illustration of the characteristics of the subsymbolic region of the continuum (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  isbn = {978-0-8058-1079-0 978-0-8058-1080-6},
  keywords = {Artificial Intelligence,Associative Memory,Associative Processes,Memory,Models,Symbolism},
  file = {C:\Users\benja\Zotero\storage\5MG4ZKV2\1992-98156-005.html}
}

@inproceedings{blattmannAlignYourLatents2023,
  title = {Align {{Your Latents}}: {{High-Resolution Video Synthesis With Latent Diffusion Models}}},
  shorttitle = {Align {{Your Latents}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Blattmann, Andreas and Rombach, Robin and Ling, Huan and Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis, Karsten},
  year = {2023},
  pages = {22563--22575},
  urldate = {2024-04-27},
  langid = {english}
}

@phdthesis{Bleier.2015,
  type = {Bachelorarbeit},
  title = {Lokalisierung von Schl{\"a}uchen in Daten von {{3D-Kamerasystemen}} Und Modellierung Ihrer Lage Als Kugelkette},
  author = {Bleier, Johannes},
  year = {2015},
  address = {Karslruhe},
  school = {Fakult{\"a}t f{\"u}r Informatik, Institut f{\"u}r Anthropromatik und Robotik / Karlsruher Institut f{\"u}r Technologie}
}

@misc{blondelElementsDifferentiableProgramming2024,
  title = {The {{Elements}} of {{Differentiable Programming}}},
  author = {Blondel, Mathieu and Roulet, Vincent},
  year = {2024},
  month = mar,
  number = {arXiv:2403.14606},
  eprint = {2403.14606},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.14606},
  urldate = {2024-06-25},
  abstract = {Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming. This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible. As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics. This book presents a comprehensive review of the fundamental concepts useful for differentiable programming. We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two. Differentiable programming is not merely the differentiation of programs, but also the thoughtful design of programs intended for differentiation. By making programs differentiable, we inherently introduce probability distributions over their execution, providing a means to quantify the uncertainty associated with program outputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {C:\Users\benja\Zotero\storage\CQZS8R3U\2403.html}
}

@misc{blondelElementsDifferentiableProgramming2024a,
  title = {The {{Elements}} of {{Differentiable Programming}}},
  author = {Blondel, Mathieu and Roulet, Vincent},
  year = {2024},
  month = mar,
  journal = {arXiv.org},
  urldate = {2024-09-06},
  abstract = {Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming. This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible. As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics. This book presents a comprehensive review of the fundamental concepts useful for differentiable programming. We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two. Differentiable programming is not merely the differentiation of programs, but also the thoughtful design of programs intended for differentiation. By making programs differentiable, we inherently introduce probability distributions over their execution, providing a means to quantify the uncertainty associated with program outputs.},
  howpublished = {https://arxiv.org/abs/2403.14606v2},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\RZRFZ9CQ\Blondel und Roulet - 2024 - The Elements of Differentiable Programming.pdf}
}

@inproceedings{bloomfieldTaxonomyComparisonHaptic2003,
  title = {A Taxonomy and Comparison of Haptic Actions for Disassembly Tasks},
  booktitle = {{{IEEE Virtual Reality}}, 2003. {{Proceedings}}.},
  author = {Bloomfield, A. and Deng, Yu and Wampler, J. and Rondot, P. and Harth, D. and McManus, M. and Badler, N.},
  year = {2003},
  month = mar,
  pages = {225--231},
  issn = {1087-8270},
  doi = {10.1109/VR.2003.1191143},
  abstract = {The usefulness of modern day haptics equipment for virtual simulations of actual maintenance actions is examined. In an effort to categorize which areas haptic simulations may be useful, we have developed a taxonomy for haptic actions. This classification has two major dimensions: the general type of action performed and the type of force or torque required. Building upon this taxonomy, we selected three representative tasks from the taxonomy to evaluate in a virtual reality simulation. We conducted a series of human subject experiments to compare user performance and preference on a disassembly task with and without haptic feedback using CyberGlove, Phantom, and SpaceMouse interfaces. Analysis of the simulation runs shows Phantom users learned to accomplish the simulated actions significantly more quickly than did users of the CyberGlove or the SpaceMouse. Moreover, a lack of differences in the post-experiment questionnaire suggests that haptics research should include a measure of actual performance speed or accuracy rather than relying solely on subjective reports of a device's ease of use.},
  keywords = {Analytical models,Data gloves,Feedback,Haptic interfaces,Humans,Imaging phantoms,Taxonomy,Torque,Velocity measurement,Virtual reality},
  file = {C:\Users\benja\Zotero\storage\D3CX96QS\1191143.html}
}

@article{blundellWeightUncertaintyNeural2015,
  title = {Weight {{Uncertainty}} in {{Neural Networks}}},
  author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  year = {2015},
  month = may,
  journal = {arXiv:1505.05424 [cs, stat]},
  eprint = {1505.05424},
  primaryclass = {cs, stat},
  urldate = {2020-12-19},
  abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\AEXG8CLL\1505.html}
}

@article{Bobenko.2007,
  title = {A Discrete {{Laplace}}--{{Beltrami}} Operator for Simplicial Surfaces},
  author = {Bobenko, Alexander I. and Springborn, Boris A.},
  year = {2007},
  journal = {Discrete \& Computational Geometry},
  volume = {38},
  number = {4},
  pages = {740--756},
  issn = {0179-5376},
  doi = {10.1007/s00454-007-9006-1},
  pagination = {page}
}

@article{bobuLESSMoreRethinking2020,
  title = {{{LESS}} Is {{More}}: {{Rethinking Probabilistic Models}} of {{Human Behavior}}},
  shorttitle = {{{LESS}} Is {{More}}},
  author = {Bobu, Andreea and Scobee, Dexter R. R. and Fisac, Jaime F. and Sastry, S. Shankar and Dragan, Anca D.},
  year = {2020},
  month = mar,
  journal = {Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
  eprint = {2001.04465},
  pages = {429--437},
  doi = {10.1145/3319502.3374811},
  urldate = {2020-10-07},
  abstract = {Robots need models of human behavior for both inferring human goals and preferences, and predicting what people will do. A common model is the Boltzmann noisily-rational decision model, which assumes people approximately optimize a reward function and choose trajectories in proportion to their exponentiated reward. While this model has been successful in a variety of robotics domains, its roots lie in econometrics, and in modeling decisions among different discrete options, each with its own utility or reward. In contrast, human trajectories lie in a continuous space, with continuous-valued features that influence the reward function. We propose that it is time to rethink the Boltzmann model, and design it from the ground up to operate over such trajectory spaces. We introduce a model that explicitly accounts for distances between trajectories, rather than only their rewards. Rather than each trajectory affecting the decision independently, similar trajectories now affect the decision together. We start by showing that our model better explains human behavior in a user study. We then analyze the implications this has for robot inference, first in toy environments where we have ground truth and find more accurate inference, and finally for a 7DOF robot arm learning from user demonstrations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\N6QHJC99\2001.html}
}

@article{Bodenhagen.2014,
  title = {An Adaptable Robot Vision System Performing Manipulation Actions with Flexible Objects},
  author = {Bodenhagen, Leon and Fugl, Andreas R. and Jordt, Andreas and Willatzen, Morten and Andersen, Knud A. and Olsen, Martin M. and Koch, Reinhard and Petersen, Henrik G. and Kruger, Norbert},
  year = {2014},
  journal = {IEEE Transactions on Automation Science and Engineering},
  volume = {11},
  number = {3},
  pages = {749--765},
  issn = {1545-5955},
  doi = {10.1109/TASE.2014.2320157},
  pagination = {page}
}

@article{bodenstedt2016image,
  title = {Image-Based Laparoscopic Bowel Measurement},
  author = {Bodenstedt, Sebastian and Wagner, Martin and Mayer, Benjamin and Stemmer, Katherine and Kenngott, Hannes and {M{\"u}ller-Stich}, Beat and Dillmann, R{\"u}diger and Speidel, Stefanie},
  year = {2016},
  journal = {International journal of computer assisted radiology and surgery},
  volume = {11},
  number = {3},
  pages = {407--419},
  publisher = {Springer}
}

@article{bodenstedt2018comparative,
  title = {Comparative Evaluation of Instrument Segmentation and Tracking Methods in Minimally Invasive Surgery},
  author = {Bodenstedt, Sebastian and Allan, Max and Agustinos, Anthony and Du, Xiaofei and {Garcia-Peraza-Herrera}, Luis and Kenngott, Hannes and Kurmann, Thomas and {M{\"u}ller-Stich}, Beat and Ourselin, Sebastien and Pakhomov, Daniil and others},
  year = {2018},
  journal = {arXiv preprint arXiv:1805.02475},
  eprint = {1805.02475},
  archiveprefix = {arXiv}
}

@article{bodenstedtArtificialIntelligenceAssistedSurgery2020,
  title = {Artificial {{Intelligence-Assisted Surgery}}: {{Potential}} and {{Challenges}}},
  shorttitle = {Artificial {{Intelligence-Assisted Surgery}}},
  author = {Bodenstedt, Sebastian and Wagner, Martin and {M{\"u}ller-Stich}, Beat Peter and Weitz, J{\"u}rgen and Speidel, Stefanie},
  year = {2020},
  month = dec,
  journal = {Visceral Medicine},
  volume = {36},
  number = {6},
  pages = {450--455},
  issn = {2297-4725},
  doi = {10.1159/000511351},
  urldate = {2024-06-14},
  abstract = {Background Artificial intelligence (AI) has recently achieved considerable success in different domains including medical applications. Although current advances are expected to impact surgery, up until now AI has not been able to leverage its full potential due to several challenges that are specific to that field. Summary This review summarizes data-driven methods and technologies needed as a prerequisite for different AI-based assistance functions in the operating room. Potential effects of AI usage in surgery will be highlighted, concluding with ongoing challenges to enabling AI for surgery. Key Messages AI-assisted surgery will enable data-driven decision-making via decision support systems and cognitive robotic assistance. The use of AI for workflow analysis will help provide appropriate assistance in the right context. The requirements for such assistance must be defined by surgeons in close cooperation with computer scientists and engineers. Once the existing challenges will have been solved, AI assistance has the potential to improve patient care by supporting the surgeon without replacing him or her.},
  pmcid = {PMC7768095},
  pmid = {33447600}
}

@misc{bodenstedtEndoscopicVisionChallenge2021,
  title = {Endoscopic {{Vision Challenge}} 2021: {{HeiChole Surgical Workflow Analysis}} and {{Full Scene Segmentation}} ({{HeiSurF}})},
  author = {Bodenstedt, Sebastian and Speidel, Stefanie and Wagner, Martin and Chen, Jonathan and Kisilenko, Anna and {M{\"u}ller-Stich}, Beat and {Maier-Hein}, Lena},
  year = {2021},
  month = jan
}

@article{boggessPersonalizedExplanationRobot2021,
  title = {Towards {{Personalized Explanation}} of {{Robot Path Planning}} via {{User Feedback}}},
  author = {Boggess, Kayla and Chen, Shenghui and Feng, Lu},
  year = {2021},
  month = mar,
  journal = {arXiv:2011.00524 [cs]},
  eprint = {2011.00524},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {Prior studies have found that explaining robot decisions and actions helps to increase system transparency, improve user understanding, and enable effective human-robot collaboration. In this paper, we present a system for generating personalized explanations of robot path planning via user feedback. We consider a robot navigating in an environment modeled as a Markov decision process (MDP), and develop an algorithm to automatically generate a personalized explanation of an optimal MDP policy, based on the user preference regarding four elements (i.e., objective, locality, specificity, and corpus). In addition, we design the system to interact with users via answering users' further questions about the generated explanations. Users have the option to update their preferences to view different explanations. The system is capable of detecting and resolving any preference conflict via user interaction. The results of an online user study show that the generated personalized explanations improve user satisfaction, while the majority of users liked the system's capabilities of question-answering and conflict detection/resolution.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\ES673CNY\2011.html}
}

@inproceedings{boletExplorationGlobalOptimization2024,
  title = {An {{Exploration}} of {{Global Optimization Strategies}} for {{Autotuning OpenMP-based Codes}}},
  booktitle = {2024 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Bolet, Gregory and Georgakoudis, Giorgis and Parasyris, Konstantinos and Cameron, Kirk W. and Beckingsale, David and Gamblin, Todd},
  year = {2024},
  month = may,
  pages = {741--750},
  doi = {10.1109/IPDPSW63119.2024.00138},
  urldate = {2024-09-06},
  abstract = {Automatic parameter tuning of parallel codes is ubiquitous in today's HPC environments where the performance portability of said codes is expected to keep pace with the perpetual release of new hardware. With changes in hardware, it is often the case that finding an optimal configuration of these codes is challenging and only further complicated by the high dimensionality or discontinuous topologies of the tuning spaces. Selecting a proper optimization strategy to automatically search these spaces is paramount to minimizing the energy and time spent on exploring sub-optimal configurations. Unfortunately, it is often the case that these optimizers have hyperparameters of their own, which are sensitive and can greatly affect the outcome of quickly converging to, or even finding an optimal code configuration. Much of the existing autotuning literature tends to use particular optimizers without describing their hyperparameter selection, leaving readers to figure out how to configure their optimizer for the best performance. In this work we compare and contrast the popular global optimization strategy of Bayesian Optimization (BO) to two less popular strategies: Particle Swarm Optimization (PSO), and Covariance Matrix Adaptive Evolution Strategy (CMA-ES). We sweep the hyperparameters of these three optimizers in the context of tuning OpenMP hyperparameters of four classic OpenMP programs: BT, FT, HPCG, and Lulesh. Our study compares the long-term search behavior and average time-to-convergence between these three optimization strategies in tuning OpenMP codes. We contribute a detailed study of these strategies and provide deeper insights as to their sensitivities, noting the conditions where each performs well, and hinting at which optimizers require minimal tuning of their hyperparameters for desirable tuning results.},
  keywords = {Bayes methods,bayesian optimization,Codes,covariance matrix adaptive evolution strategy,Distributed processing,Hardware,hpcg,lulesh,nas parallel benchmarks,openmp,particle swarm optimization,Particle swarm optimization,Sensitivity,Topology},
  file = {C:\Users\benja\Zotero\storage\5HTCHUZK\10596452.html}
}

@inproceedings{bonassoIntegratingDeliberationIntelligent1995,
  title = {Integrating {{Deliberation}} in an {{Intelligent Agent Architecture}}},
  booktitle = {Papers from the 1995 {{AAAI Spring Symposium}}},
  author = {Bonasso, R. Peter},
  year = {1995},
  publisher = {AAAI Press},
  urldate = {2024-03-08},
  isbn = {978-0-929280-87-5},
  langid = {american},
  file = {C:\Users\benja\Zotero\storage\2TUKNHHS\0004-ss95-04-004-integrating-deliberation-in-an-intelligent-agent-architecture.html}
}

@inproceedings{bonattiPACTPerceptionActionCausal2022,
  title = {{{PACT}}: {{Perception-Action Causal Transformer}} for {{Autoregressive Robotics Pretraining}}},
  shorttitle = {{{PACT}}},
  booktitle = {{{NeurIPS}} 2022 {{Foundation Models}} for {{Decision Making Workshop}}},
  author = {Bonatti, Rogerio and Vemprala, Sai and Ma, Shuang and Frujeri, Felipe Vieira and Chen, Shuhang and Kapoor, Ashish},
  year = {2022},
  month = nov,
  urldate = {2024-04-29},
  abstract = {Robotics has long been a field riddled with complex systems architectures whose modules and connections, whether traditional or learning-based, require significant human expertise and prior knowledge. Inspired by large pre-trained language models, this work introduces a paradigm for pre-training a general purpose representation that can serve as a starting point for multiple tasks on a given robot. We present the Perception-Action Causal Transformer (PACT), a generative transformer-based architecture that aims to build representations directly from robot data in a self-supervised fashion. Through autoregressive prediction of states and actions over time, our model implicitly encodes dynamics and behaviors for a particular robot. Our experimental evaluation focuses on the domain of mobile agents, where we show that this robot-specific representation can function as a single starting point to achieve distinct tasks such as safe navigation, localization and mapping. We evaluate two form factors: a wheeled robot that uses a LiDAR sensor as perception input (MuSHR), and a simulated agent that uses first-person RGB images (Habitat). We show that finetuning small task-specific networks on top of the larger pretrained model results in significantly better performance compared to training a single model from scratch for all tasks simultaneously, and comparable performance to training a separate large model for each task independently. By sharing a common good-quality representation across tasks we can lower overall model capacity and speed up the real-time deployment of such systems.},
  langid = {english}
}

@incollection{books/ios/p/PresuttiG16,
  title = {Dolce+{{D}}\&{{S Ultralite}} and Its Main Ontology Design Patterns},
  booktitle = {Ontology Engineering with Ontology Design Patterns},
  author = {Presutti, Valentina and Gangemi, Aldo},
  editor = {Hitzler, Pascal and Gangemi, Aldo and Janowicz, Krzysztof and Krisnadhi, Adila and Presutti, Valentina},
  year = {2016},
  series = {Studies on the Semantic Web},
  volume = {25},
  pages = {81--103},
  publisher = {IOS Press},
  added-at = {2024-02-05T00:00:00.000+0100},
  interhash = {981a5c6df48443dc2d0b854fea26e6b5},
  intrahash = {2c1a90c61f4c68e52d1f317de91b47c2},
  isbn = {978-1-61499-676-7},
  keywords = {dblp},
  timestamp = {2024-04-10T12:21:50.000+0200}
}

@article{Bookstein.1989,
  title = {Principal Warps: Thin-Plate Splines and the Decomposition of Deformations},
  author = {Bookstein, F. L.},
  year = {1989},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {11},
  number = {6},
  pages = {567--585},
  issn = {01628828},
  doi = {10.1109/34.24792},
  pagination = {page}
}

@article{Boost.2019,
  title = {Boost},
  author = {Beman, Dawes and Abrahams, David and Rivera, Rene},
  year = {2019},
  lastvisited = {2019-07-22}
}

@misc{borchersHitchhikersGuideMixture2019,
  title = {A {{Hitchhiker}}'s {{Guide}} to {{Mixture Density Networks}}},
  author = {Borchers, Oliver},
  year = {2019},
  month = feb,
  journal = {Medium},
  urldate = {2019-11-22},
  abstract = {Assessing the uncertainty of predictions is elementary for business decisions. Mixture density networks help you to better understand the{\dots}},
  howpublished = {https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca},
  langid = {english}
}

@inproceedings{bosnjakProgrammingDifferentiableForth2017,
  title = {Programming with a {{Differentiable Forth Interpreter}}},
  booktitle = {{{ICML}}},
  author = {Bo{\v s}njak, Matko and Rockt{\"a}schel, Tim and Naradowsky, Jason and Riedel, Sebastian},
  year = {2017},
  month = jul,
  pages = {547--556},
  urldate = {2019-07-16},
  abstract = {Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior ...},
  langid = {english}
}

@incollection{bostromStrategicImplicationsOpenness,
  title = {Strategic {{Implications}} of {{Openness}} in {{AI Development}}},
  booktitle = {Artificial {{Intelligence Safety}} and {{Security}}},
  author = {Bostrom, Nick},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton},
  isbn = {978-0-8153-6982-0}
}

@article{bostromStrategicImplicationsOpenness2017,
  title = {Strategic {{Implications}} of {{Openness}} in {{AI Development}}},
  author = {Bostrom, Nick},
  year = {2017},
  journal = {Global Policy},
  volume = {8},
  number = {2},
  pages = {135--148},
  issn = {1758-5899},
  doi = {10.1111/1758-5899.12403},
  urldate = {2024-01-28},
  abstract = {This paper attempts a preliminary analysis of the global desirability of different forms of openness in AI development (including openness about source code, science, data, safety techniques, capabilities, and goals). Short-term impacts of increased openness appear mostly socially beneficial in expectation. The strategic implications of medium and long-term impacts are complex. The evaluation of long-term impacts, in particular, may depend on whether the objective is to benefit the present generation or to promote a time-neutral aggregate of well-being of future generations. Some forms of openness are plausibly positive on both counts (openness about safety measures, openness about goals). Others (openness about source code, science, and possibly capability) could lead to a tightening of the competitive situation around the time of the introduction of advanced AI, increasing the probability that winning the AI race is incompatible with using any safety method that incurs a delay or limits performance. We identify several key factors that must be taken into account by any well-founded opinion on the matter.},
  copyright = {{\copyright} 2017 The Authors Global Policy published by Durham University and John Wiley \& Sons, Ltd},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\S5P6Z6AM\1758-5899.html}
}

@book{bostromSuperintelligencePathsDangers2016,
  title = {Superintelligence: {{Paths}}, {{Dangers}}, {{Strategies}}},
  shorttitle = {Superintelligence},
  author = {Bostrom, Nick},
  year = {2016},
  month = apr,
  publisher = {Oxford University Press},
  address = {Oxford, New York},
  abstract = {The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. Other animals have stronger muscles or sharper claws, but we have cleverer brains. If machine brains one day come to surpass human brains in general intelligence, then this new superintelligence could become very powerful. As the fate of the gorillas now depends more on us humans than on the gorillas themselves, so the fate of our species then would come to depend on the actions of the machine superintelligence. But we have one advantage: we get to make the first move. Will it be possible to construct a seed AI or otherwise to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation? To get closer to an answer to this question, we must make our way through a fascinating landscape of topics and considerations. Read the book and learn about oracles, genies, singletons; about boxing methods, tripwires, and mind crime; about humanity's cosmic endowment and differential technological development; indirect normativity, instrumental convergence, whole brain emulation and technology couplings; Malthusian economics and dystopian evolution; artificial intelligence, and biological cognitive enhancement, and collective intelligence. This profoundly ambitious and original book picks its way carefully through a vast tract of forbiddingly difficult intellectual terrain. Yet the writing is so lucid that it somehow makes it all seem easy. After an utterly engrossing journey that takes us to the frontiers of thinking about the human condition and the future of intelligent life, we find in Nick Bostrom's work nothing less than a reconceptualization of the essential task of our time.                                                        ,                The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. Other animals have stronger muscles or sharper claws, but we have cleverer brains. If machine brains one day come to surpass human brains in general intelligence, then this new superintelligence could become very powerful. As the fate of the gorillas now depends more on us humans than on the gorillas themselves, so the fate of our species then would come to depend on the actions of the machine superintelligence. But we have one advantage: we get to make the first move. Will it be possible to construct a seed AI or otherwise to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation? To get closer to an answer to this question, we must make our way through a fascinating landscape of topics and considerations. Read the book and learn about oracles, genies, singletons; about boxing methods, tripwires, and mind crime; about humanity's cosmic endowment and differential technological development; indirect normativity, instrumental convergence, whole brain emulation and technology couplings; Malthusian economics and dystopian evolution; artificial intelligence, and biological cognitive enhancement, and collective intelligence. This profoundly ambitious and original book picks its way carefully through a vast tract of forbiddingly difficult intellectual terrain. Yet the writing is so lucid that it somehow makes it all seem easy. After an utterly engrossing journey that takes us to the frontiers of thinking about the human condition and the future of intelligent life, we find in Nick Bostrom's work nothing less than a reconceptualization of the essential task of our time.},
  isbn = {978-0-19-873983-8},
  file = {C:\Users\benja\Zotero\storage\GFGY5U2L\superintelligence-9780198739838.html}
}

@article{Bouaziz.7.4.2014,
  title = {Dynamic {{2D}}/{{3D}} Registration Eurographics Tutorial Slides},
  author = {Bouaziz, Sofien and Tagliasacchi, Andrea and Pauly, Mark},
  year = {2014},
  month = apr,
  series = {Eurographics Tutorial}
}

@article{bouget2017vision,
  title = {Vision-Based and Marker-Less Surgical Tool Detection and Tracking: A Review of the Literature},
  author = {Bouget, David and Allan, Max and Stoyanov, Danail and Jannin, Pierre},
  year = {2017},
  journal = {Medical image analysis},
  volume = {35},
  pages = {633--654},
  publisher = {Elsevier}
}

@article{Bour88,
  title = {Auto-{{Association}} by {{Multilayer Perceptrons}} and {{Singular Value Decomposition}}},
  author = {{Bourlard Herv{\'e}} and Kamp, Yves},
  year = {1988},
  month = sep,
  journal = {Biological Cybernetics},
  volume = {59},
  number = {4},
  pages = {291--294}
}

@article{bousmalisRoboCatSelfImprovingGeneralist2023,
  title = {{{RoboCat}}: {{A Self-Improving Generalist Agent}} for {{Robotic Manipulation}}},
  shorttitle = {{{RoboCat}}},
  author = {Bousmalis, Konstantinos and Vezzani, Giulia and Rao, Dushyant and Devin, Coline Manon and Lee, Alex X. and Villalonga, Maria Bauza and Davchev, Todor and Zhou, Yuxiang and Gupta, Agrim and Raju, Akhil and Laurens, Antoine and Fantacci, Claudio and Dalibard, Valentin and Zambelli, Martina and Martins, Murilo Fernandes and Pevceviciute, Rugile and Blokzijl, Michiel and Denil, Misha and Batchelor, Nathan and Lampe, Thomas and Parisotto, Emilio and Zolna, Konrad and Reed, Scott and Colmenarejo, Sergio G{\'o}mez and Scholz, Jonathan and Abdolmaleki, Abbas and Groth, Oliver and Regli, Jean-Baptiste and Sushkov, Oleg and Roth{\"o}rl, Thomas and Chen, Jose Enrique and Aytar, Yusuf and Barker, David and Ortiz, Joy and Riedmiller, Martin and Springenberg, Jost Tobias and Hadsell, Raia and Nori, Francesco and Heess, Nicolas},
  year = {2023},
  month = sep,
  journal = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  urldate = {2024-04-29},
  abstract = {The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a multi-embodiment, multi-task generalist agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100--1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent's capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks.},
  langid = {english}
}

@article{bousmalisUsingSimulationDomain2017,
  title = {Using {{Simulation}} and {{Domain Adaptation}} to {{Improve Efficiency}} of {{Deep Robotic Grasping}}},
  author = {Bousmalis, Konstantinos and Irpan, Alex and Wohlhart, Paul and Bai, Yunfei and Kelcey, Matthew and Kalakrishnan, Mrinal and Downs, Laura and Ibarz, Julian and Pastor, Peter and Konolige, Kurt and Levine, Sergey and Vanhoucke, Vincent},
  year = {2017},
  month = sep,
  journal = {arXiv:1709.07857 [cs]},
  eprint = {1709.07857},
  primaryclass = {cs},
  urldate = {2019-07-15},
  abstract = {Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the GraspGAN. We show that, by using synthetic data and domain adaptation, we are able to reduce the number of real-world samples needed to achieve a given level of performance by up to 50 times, using only randomly generated simulated objects. We also show that by using only unlabeled real-world data and our GraspGAN methodology, we obtain real-world grasping performance without any real-world labels that is similar to that achieved with 939,777 labeled real-world samples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics}
}

@inproceedings{bousmalisUsingSimulationDomain2018,
  title = {Using {{Simulation}} and {{Domain Adaptation}} to {{Improve Efficiency}} of {{Deep Robotic Grasping}}},
  booktitle = {{{ICRA}}},
  author = {Bousmalis, Konstantinos and Irpan, Alex and Wohlhart, Paul and Bai, Yunfei and Kelcey, Matthew and Kalakrishnan, Mrinal and Downs, Laura and Ibarz, Julian and Pastor, Peter and Konolige, Kurt and Levine, Sergey and Vanhoucke, Vincent},
  year = {2018},
  month = may,
  pages = {4243--4250},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2018.8460875},
  abstract = {Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms can be extremely time-consuming and expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. Unfortunately, models trained purely on simulated data often fail to generalize to the real world. We study how randomized simulated environments and domain adaptation methods can be extended to train a grasping system to grasp novel objects from raw monocular RGB images. We extensively evaluate our approaches with a total of more than 25,000 physical test grasps, studying a range of simulation conditions and domain adaptation methods, including a novel extension of pixel-level domain adaptation that we term the GraspGAN. We show that, by using synthetic data and domain adaptation, we are able to reduce the number of real-world samples needed to achieve a given level of performance by up to 50 times, using only randomly generated simulated objects. We also show that by using only unlabeled real-world data and our GraspGAN methodology, we obtain real-world grasping performance without any real-world labels that is similar to that achieved with 939,777 labeled real-world samples.},
  keywords = {Adaptation models,annotated visual grasping datasets,Cameras,deep robotic grasping,domain adaptation methods,Feature extraction,generative adversial network,GraspGAN,Grasping,grasping system,ground-truth annotations,image colour analysis,manipulators,neurocontrollers,off-the-shelf simulators,pixel-level domain adaptation,randomized simulated environments,raw monocular RGB images,real-world grasping performance,robot vision,Robots,Task analysis,Training},
  file = {C:\Users\benja\Zotero\storage\2NGPJVCP\8460875.html}
}

@misc{boutselisConstrainedSamplingbasedTrajectory2019,
  title = {Constrained {{Sampling-based Trajectory Optimization}} Using {{Stochastic Approximation}}},
  author = {Boutselis, George I. and Wang, Ziyi and Theodorou, Evangelos A.},
  year = {2019},
  month = nov,
  number = {arXiv:1911.04621},
  eprint = {1911.04621},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.04621},
  urldate = {2023-04-23},
  abstract = {We propose a sampling-based trajectory optimization methodology for constrained problems. We extend recent works on stochastic search to deal with box control constraints,as well as nonlinear state constraints for discrete dynamical systems. Regarding the former, our strategy is to optimize over truncated parameterized distributions on control inputs. Furthermore, we show how non-smooth penalty functions can be incorporated into our framework to handle state constraints. Simulations on cartpole and quadcopter show that our approach outperforms previous methods on constrained sampling-based optimization, in terms of quality of solutions and convergence speed.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control},
  file = {C:\Users\benja\Zotero\storage\BWFVDSIZ\1911.html}
}

@inproceedings{boveInvestigatingIntelligibilityPlural2023,
  title = {Investigating the {{Intelligibility}} of {{Plural Counterfactual Examples}} for {{Non-Expert Users}}: An {{Explanation User Interface Proposition}} and {{User Study}}},
  shorttitle = {Investigating the {{Intelligibility}} of {{Plural Counterfactual Examples}} for {{Non-Expert Users}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Bove, Clara and Lesot, Marie-Jeanne and Tijus, Charles Albert and Detyniecki, Marcin},
  year = {2023},
  month = mar,
  series = {{{IUI}} '23},
  pages = {188--203},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3581641.3584082},
  urldate = {2024-02-04},
  abstract = {Plural counterfactual examples have been proposed to explain the prediction of a classifier by offering a user several instances of minimal modifications that may be performed to change the prediction. Yet, such explanations may provide too much information, generating potential confusion for the end-users with no specific knowledge, neither on the machine learning, nor on the application domains. In this paper, we investigate the design of explanation user interfaces for plural counterfactual examples offering comparative analysis features to mitigate this potential confusion and improve the intelligibility of such explanations for non-expert users. We propose an implementation of such an enhanced explanation user interface, illustrating it in a financial scenario related to a loan application. We then present the results of a lab user study conducted with 112 participants to evaluate the effectiveness of having plural examples and of offering comparative analysis principles, both on the objective understanding and satisfaction of such explanations. The results demonstrate the effectiveness of the plural condition, both on objective understanding and satisfaction scores, as compared to having a single counterfactual example. Beside the statistical analysis, we perform a thematic analysis of the participants' responses to the open-response questions, that also shows encouraging results for the comparative analysis features on the objective understanding.},
  isbn = {9798400701061},
  keywords = {explainable AI,human-centered AI methods,interface design,user studies,XAI,XUI}
}

@misc{bowmanMeasuringProgressScalable2022,
  title = {Measuring {{Progress}} on {{Scalable Oversight}} for {{Large Language Models}}},
  author = {Bowman, Samuel R. and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Askell, Amanda and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Olah, Christopher and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and {Tran-Johnson}, Eli and Kernion, Jackson and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lovitt, Liane and Elhage, Nelson and Schiefer, Nicholas and Joseph, Nicholas and Mercado, Noem{\'i} and DasSarma, Nova and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and {Telleen-Lawton}, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and {Hatfield-Dodds}, Zac and Mann, Ben and Kaplan, Jared},
  year = {2022},
  month = nov,
  number = {arXiv:2211.03540},
  eprint = {2211.03540},
  publisher = {arXiv},
  urldate = {2024-03-03},
  abstract = {Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.},
  archiveprefix = {arXiv},
  langid = {english}
}

@misc{bowmanMeasuringProgressScalable2022a,
  title = {Measuring {{Progress}} on {{Scalable Oversight}} for {{Large Language Models}}},
  author = {Bowman, Samuel R. and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Askell, Amanda and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Olah, Christopher and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and {Tran-Johnson}, Eli and Kernion, Jackson and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lovitt, Liane and Elhage, Nelson and Schiefer, Nicholas and Joseph, Nicholas and Mercado, Noem{\'i} and DasSarma, Nova and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and {Telleen-Lawton}, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and {Hatfield-Dodds}, Zac and Mann, Ben and Kaplan, Jared},
  year = {2022},
  month = nov,
  journal = {arXiv.org},
  urldate = {2024-03-03},
  abstract = {Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.},
  howpublished = {https://arxiv.org/abs/2211.03540v2},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\9UA3KB7I\Bowman et al. - 2022 - Measuring Progress on Scalable Oversight for Large.pdf}
}

@article{boydDistributedOptimizationStatistical2011,
  title = {Distributed {{Optimization}} and {{Statistical Learning}} via the {{Alternating Direction Method}} of {{Multipliers}}},
  author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
  year = {2011},
  month = jan,
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {3},
  number = {1},
  pages = {1--122},
  issn = {1935-8237},
  doi = {10.1561/2200000016},
  urldate = {2024-05-23},
  abstract = {Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas--Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for {$\ell$}1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations.}
}

@inproceedings{bozcuogluCloudServiceRobotic2017,
  title = {A Cloud Service for Robotic Mental Simulations},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Bozcuo{\u g}lu, Asil Kaan and Beetz, Michael},
  year = {2017},
  month = may,
  pages = {2653--2658},
  doi = {10.1109/ICRA.2017.7989309},
  abstract = {Robotic agents that do everyday manipulation tasks can hugely benefit from being able to predict consequences of their actions just before the execution. However, such a simulation technique is usually computationally-expensive and may not be achieved with agents' self computing power. For this problem, cloud robotics may offer a solution. Cloud robotics is an emerging field in the intersection of robotics and cloud computing which enables robots to access a greater amount of processing power and storage capacity than it can employ within itself. In this work, we introduce a mental simulation service to, one of the cloud engines, openEASE [1]. Using this service, researchers and robots can describe the world model, the state of the agent and the problem that is being dealt with. In return, it simulates the world and runs a learning algorithm and suggests a solution how the robotic agent can handle the problem. This service does not only offer a free remote access to simulation which is computationally expensive but also thanks to OPEnEASE's rich reasoning techniques these simulated experiments can be reasoned later on using prolog queries.},
  keywords = {cloud computing,cloud engines,cloud robotics,cloud service,Cognition,Computational modeling,control engineering computing,Data models,digital simulation,Engines,everyday manipulation tasks,inference mechanisms,learning (artificial intelligence),learning algorithm,manipulators,mental simulation service,openEASE,OPEnEASE,Planning,PROLOG,prolog queries,query processing,reasoning techniques,robotic agents,robotic mental simulations,Robots,self computing power,storage capacity,Trajectory},
  file = {C:\Users\benja\Zotero\storage\BDE9TAJU\7989309.html}
}

@book{brachmanKnowledgeRepresentationReasoning2004,
  title = {{Knowledge Representation and Reasoning}},
  author = {Brachman, Ronald and Levesque, Hector},
  year = {2004},
  month = jun,
  series = {{The Morgan Kaufmann Series in Artificial Intelligence}},
  edition = {1},
  publisher = {Morgan Kaufmann},
  address = {Amsterdam},
  abstract = {Knowledge representation is at the very core of a radical idea for understanding intelligence. Instead of trying to understand or build brains from the bottom up, its goal is to understand and build intelligent behavior from the top down, putting the focus on what an agent needs to know in order to behave intelligently, how this knowledge can be represented symbolically, and how automated reasoning procedures can make this knowledge available as needed. This landmark text takes the central concepts of knowledge representation developed over the last 50 years and illustrates them in a lucid and compelling way. Each of the various styles of representation is presented in a simple and intuitive form, and the basics of reasoning with that representation are explained in detail. This approach gives readers a solid foundation for understanding the more advanced work found in the research literature. The presentation is clear enough to be accessible to a broad audience, including researchers and practitioners in database management, information retrieval, and object-oriented systems as well as artificial intelligence. This book provides the foundation in knowledge representation and reasoning that every AI practitioner needs.},
  isbn = {978-1-55860-932-7},
  langid = {Englisch}
}

@article{brock2016generative,
  title = {Generative and Discriminative Voxel Modeling with Convolutional Neural Networks},
  author = {Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
  year = {2016},
  journal = {arXiv preprint arXiv:1608.04236},
  eprint = {1608.04236},
  archiveprefix = {arXiv}
}

@inproceedings{brockettRoboticManipulatorsProduct1984,
  title = {Robotic Manipulators and the Product of Exponentials Formula},
  booktitle = {Mathematical {{Theory}} of {{Networks}} and {{Systems}}},
  author = {Brockett, R. W.},
  editor = {Fuhrmann, P. A.},
  year = {1984},
  pages = {120--129},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/BFb0031048},
  abstract = {The manipulation of rigid bodies by manipulators which are motor driven kinematic chains is a fundamental aspect of robotics. In this paper, we discuss the kinematics of such processes and discuss the classification of kinematic chains using ideas from algebra and group theory. Earlier work on the role of Lie groups in mechanisms is contained in Herv{\'e} [7], but the role of Lie algebras is not considered by this author. More relevant (but less group theoretic) is the extensive case-by-case analysis found in Pieper's thesis [9]. In fact, Pieper's work suggests an interesting and rather general problem in Galois theory which is directly related to manipulation. Also of interest is the well-known Baker-Campbell-Hausdorff formula for the derivative of a product of exponentials since such products are of fundamental importance in the study of kinematic programming.},
  isbn = {978-3-540-38826-5},
  langid = {english},
  keywords = {Closed Curf,Galois Group,Joint Angle,Kinematic Chain,Rigid Motion}
}

@book{Bronstein.2009,
  title = {Numerical Geometry of Non-Rigid Shapes},
  author = {Bronstein, Alexander M. and Bronstein, Michael M. and Kimmel, Ron},
  year = {2009},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-0-387-73301-2},
  isbn = {978-0-387-73300-5}
}

@misc{brooksBetterLesson2019,
  title = {A {{Better Lesson}}},
  author = {Brooks, Rodney A.},
  year = {2019},
  month = mar,
  urldate = {2024-01-25},
  langid = {american},
  file = {C:\Users\benja\Zotero\storage\RSY68SFS\a-better-lesson.html}
}

@article{brooksElephantsDontPlay1990,
  title = {Elephants Don't Play Chess},
  author = {Brooks, Rodney A.},
  year = {1990},
  month = jun,
  journal = {Robotics and Autonomous Systems},
  series = {Designing {{Autonomous Agents}}},
  volume = {6},
  number = {1},
  pages = {3--15},
  issn = {0921-8890},
  doi = {10.1016/S0921-8890(05)80025-9},
  urldate = {2024-04-18},
  abstract = {There is an alternative route to Artificial Intelligence that diverges from the directions pursued under that banner for the last thirty some years. The traditional approach has emphasized the abstract manipulation of symbols, whose grounding in physical reality has rarely been achieved. We explore a research methodology which emphasizes ongoing physical interaction with the environment as the primary source of constraint on the design of intelligent systems. We show how this methodology has recently had significant successes on a par with the most successful classical efforts. We outline plausible future work along these lines which can lead to vastly more ambitious systems.},
  keywords = {Artificial Intelligence,Mobile robots,Planning,Situated activity,Subsumption architecture},
  file = {C:\Users\benja\Zotero\storage\EVESMNYJ\S0921889005800259.html}
}

@inproceedings{brookshirePreliminaryResultsSliding2004,
  title = {Preliminary Results in Sliding Autonomy for Assembly by Coordinated Teams},
  booktitle = {2004 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}}) ({{IEEE Cat}}. {{No}}.{{04CH37566}})},
  author = {Brookshire, J. and Singh, S. and Simmons, R.},
  year = {2004},
  month = sep,
  volume = {1},
  pages = {706-711 vol.1},
  doi = {10.1109/IROS.2004.1389435},
  abstract = {We are developing a coordinated team of robots to assemble structures, a task that cannot be performed by any single robot. Even simple operations in this domain require complex interaction between multiple robots and the number of contingencies that must be addressed if the team is to act completely autonomously is prohibitively large. This scenario forces incorporation of a human operator. Ideally we would like a seamless interface between the robots and the operator such that the operator can interact with the system by helping it be more efficient or get out of a stuck condition or performing a task that the robots are not capable of themselves. We use an architecture that implements "sliding autonomy" to accomplish these goals. The system of robots can be fully autonomous as long as all is well. The system is capable of accepting input from the operator at any time, especially when it is unable to recover from a failure. We motivate this scenario with results from an extended series of experiments we have conducted with three robots that work together to dock both ends of a suspended beam. We show the difference in performance between a completely teleoperated system, a fully autonomous system, and one in which sliding autonomy has been incorporated.},
  keywords = {assembly,Collaboration,Control systems,coordinated robot teams,Humans,intelligent robots,man-machine systems,Manufacturing,Monitoring,multi-robot systems,Robot kinematics,robotic assembly,Robotic assembly,Robustness,Silicon compounds,sliding autonomy,suspended beam,Switches,teleoperated system,telerobotics}
}

@misc{brooksOriginRobotArm2021,
  title = {The {{Origin}} of {{Robot Arm Programming Languages}}},
  author = {Brooks, Rodney},
  year = {2021},
  month = feb,
  urldate = {2024-04-15},
  langid = {american},
  file = {C:\Users\benja\Zotero\storage\MM7TF8Y8\1000.html}
}

@techreport{brooksTimeSensitiveNetworkingTheory,
  type = {Whitepaper},
  title = {Time-{{Sensitive Networking}}: {{From Theory}} to {{Implementation}} in {{Industrial Automation}}},
  author = {Brooks, Simon and Uludag, Ecehan},
  institution = {Intel Corporation}
}

@techreport{brossetScalingAIManufacturing2019,
  title = {Scaling {{AI}} in {{Manufacturing}}: {{A Practitioner}}'s {{Perspective}}},
  author = {Brosset, Pascal and Patsko, Sergey and Khadikar, Amol and Thieullent, Anne-Laure and Buvat, Jerome and Khemka, Yashwardhan and Jain, Abhishek},
  year = {2019},
  institution = {Capgemini Research Institute},
  urldate = {2022-05-06},
  file = {C:\Users\benja\Zotero\storage\8K8E5VQS\AI-in-manufacturing-operations.pdf}
}

@article{brownleeSystematicApproachParameter2022,
  title = {A Systematic Approach to Parameter Optimization and Its Application to Flight Schedule Simulation Software},
  author = {Brownlee, Alexander E. I. and Epitropakis, Michael G. and Mulder, Jeroen and Paelinck, Marc and Burke, Edmund K.},
  year = {2022},
  month = aug,
  journal = {Journal of Heuristics},
  volume = {28},
  number = {4},
  pages = {509--538},
  issn = {1572-9397},
  doi = {10.1007/s10732-022-09501-8},
  urldate = {2023-06-01},
  abstract = {Industrial software often has many parameters that critically impact performance. Frequently, these are left in a sub-optimal configuration for a given application because searching over possible configurations is costly and, except for developer instinct, the relationships between parameters and performance are often unclear and complex. While there have been significant advances in automated parameter tuning approaches recently, they are typically black-box. The high-quality solutions produced are returned to the user without explanation. The nature of optimisation means that, often, these solutions are far outside the well-established settings for the software, making it difficult to accept and use them. To address the above issue, a systematic approach to software parameter optimization is presented. Several well-established techniques are followed in sequence, each underpinning the next, with rigorous analysis of the search space. This allows the results to be explainable to both end users and developers, improving confidence in the optimal solutions, particularly where they are counter-intuitive. The process comprises statistical analysis of the parameters; single-objective optimization for each target objective; functional ANOVA to explain trends and inter-parameter interactions; and a multi-objective optimization seeded with the results from the single-objective stage. A case study demonstrates application to business-critical software developed by the international airline Air France-KLM for measuring flight schedule robustness. A configuration is found with a run-time of 80\% that of the tried-and-tested configuration, with no loss in predictive accuracy. The configuration is supplemented with detailed analysis explaining the importance of each parameter, how they interact with each other, how they influence run-time and accuracy, and how the final configuration was reached. In particular, this explains why the configuration included some parameter settings that were outwith the usually recommended range, greatly increasing developer confidence and encouraging adoption of the new configuration.},
  langid = {english},
  keywords = {Explanation,Multi-objective optimization,Optimization,Parameter tuning,Search-based software engineering,Statistical methods}
}

@inproceedings{bruceDeepParameterOptimisation2016,
  title = {Deep {{Parameter Optimisation}} for {{Face Detection Using}} the {{Viola-Jones Algorithm}} in {{OpenCV}}},
  booktitle = {Search {{Based Software Engineering}}},
  author = {Bruce, Bobby R. and Aitken, Jonathan M. and Petke, Justyna},
  editor = {Sarro, Federica and Deb, Kalyanmoy},
  year = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {238--243},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-47106-8_18},
  abstract = {OpenCV is a commonly used computer vision library containing a wide variety of algorithms for the AI community. This paper uses deep parameter optimisation to investigate improvements to face detection using the Viola-Jones algorithm in OpenCV, allowing a trade-off between execution time and classification accuracy. Our results show that execution time can be decreased by 48~\% if a 1.80~\% classification inaccuracy is permitted (compared to 1.04~\% classification inaccuracy of the original, unmodified algorithm). Further execution time savings are possible depending on the degree of inaccuracy deemed acceptable by the user.},
  isbn = {978-3-319-47106-8},
  langid = {english},
  keywords = {Automated parameter tuning,Deep parameter optimisation,Genetic improvement,GI,Multi-objective optimisation,OpenCV,SBSE,Viola-Jones Algorithm}
}

@article{brucknerIntroductionOPCUA2019,
  title = {An {{Introduction}} to {{OPC UA TSN}} for {{Industrial Communication Systems}}},
  author = {Bruckner, Dietmar and St{\u a}nic{\u a}, Marius-Petru and Blair, Richard and Schriegel, Sebastian and Kehrer, Stephan and Seewald, Maik and Sauter, Thilo},
  year = {2019},
  month = jun,
  journal = {Proceedings of the IEEE},
  volume = {107},
  number = {6},
  pages = {1121--1131},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2018.2888703},
  urldate = {2024-08-16},
  abstract = {The industrial communication market is dominated by Ethernet-based fieldbus systems. Although they share similar requirements and market segments, their implementations and ecosystems differ considerably. As a result, end customers and device manufacturers are faced with a multitude of technologies that need to be produced, run, diagnosed, maintained, and kept in stock. Although the availability of products and services is largely satisfactory, dealing with multiple solutions generates high costs and limits IoT capability. This paper introduces Open Platform Communication Unified Architecture Time-Sensitive Networking (OPC UA TSN) as a new technology and presents the current view. This time, the industrial prospects of fulfilling industrial communication requirements while leveraging the cost benefits of standard Ethernet hardware in the midterm are in reach. We anticipate that OPC UA TSN will reveal itself as a game changer in the field of industrial automation, being a candidate for establishing a holistic communication infrastructure from the sensor to the cloud.},
  keywords = {Converged network,Ethernet,IEEE Standards,industrial automation,industrial communication,Industrial communication,industrial Ethernet,Industrial Internet of Things (IIoT),Internet of Things,Manufacturing automation,Open Platform Communication Unified Architecture (OPC UA),real time,Real-time systems,Time-Sensitive Networking (TSN)}
}

@article{brunaIntriguingPropertiesNeural2013,
  title = {Intriguing Properties of Neural Networks},
  author = {Bruna, Joan and Szegedy, Christian and Sutskever, Ilya and Goodfellow, Ian and Zaremba, Wojciech and Fergus, Rob and Erhan, Dumitru},
  year = {2013},
  month = dec,
  urldate = {2022-04-30},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they...},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\7W5FHMZE\forum.html}
}

@article{brunettHoLLiECaresDevelopmentMultiFunctional2024,
  title = {{{HoLLiECares}} -{{Development}} of a {{Multi-Functional Robot}} for {{Professional Care}}},
  author = {Br{\"u}nett, Matthias and Gebert, Anne and Gisa, Kevin and Hermann, Andreas and Lengenfelder, Christian and Roennau, Arne and Schneider, Julian and Schuh, Svea M. and Steffen, Lea},
  year = {2024},
  month = oct,
  journal = {Frontiers in Robotics and AI},
  volume = {11},
  publisher = {Frontiers},
  issn = {2296-9144},
  doi = {10.3389/frobt.2024.1325143},
  urldate = {2024-10-15},
  langid = {english},
  keywords = {Health care robot,human-robot interaction,motion planning,no-code programming,robotic manipulation,service robotics,Smart hospital}
}

@article{bruyninckxSpecificationForceControlledActions1996,
  title = {Specification of {{Force-Controlled Actions}} in the "{{Task Frame Formalism}}" - {{A Synthesis}}},
  author = {Bruyninckx, H. and De Schutter, J.},
  year = {1996},
  month = aug,
  journal = {IEEE Transactions on Robotics and Automation},
  volume = {12},
  number = {4},
  pages = {581--589},
  issn = {2374-958X},
  doi = {10.1109/70.508440},
  urldate = {2024-03-26},
  abstract = {Autonomous robot tasks involving contacts with the environment must be performed under active force control if the geometric uncertainties in the task models are too large to cope with by means of passive compliance only. In practice, task specification of force-controlled actions is closely linked to the task frame formalism (TFF), also known as the compliance frame formalism. The TFF is a very intuitive and controller-independent approach to model a motion constraint, and to specify the desired forces and motions compatible with this constraint. However, it has never been defined clearly and unambiguously, and it cannot cope with all possible constrained motion tasks. This paper provides, for the first time, a formal definition of what makes up a TFF task specification. It gives also a synthesis of which tasks the TFF can cope with, and proposes a generic textual task specification formalism. Finally, it describes an example constrained motion task that the TFF cannot handle.},
  keywords = {Automatic control,Force control,Force sensors,Motion control,Programming profession,Robot control,Robot sensing systems,Robotics and automation,Solid modeling,Uncertainty},
  file = {C:\Users\benja\Zotero\storage\7B43IVK2\508440.html}
}

@inproceedings{buchinaDesignEvaluationEnduser2016,
  title = {Design and Evaluation of an End-User Friendly Tool for Robot Programming},
  booktitle = {2016 25th {{IEEE International Symposium}} on {{Robot}} and {{Human Interactive Communication}} ({{RO-MAN}})},
  author = {Buchina, Nina and Kamel, Sherin and Barakova, Emilia},
  year = {2016},
  month = aug,
  pages = {185--191},
  issn = {1944-9437},
  doi = {10.1109/ROMAN.2016.7745109},
  urldate = {2024-08-15},
  abstract = {End-user programming for robots is becoming an increasingly important topic since robots are being introduced into a wide variety of domains. We propose a design of a web based programming interface that makes it possible for end-users with different backgrounds to program robots using natural language. We used the cognitive dimensions framework to compare the usability of the newly created and the currently employed programming interfaces. The results showed that domain specialists are able to make robot programs more quickly and pleasantly with the proposed interface than with an existing one. Another important finding is that without physical simulation of the robot behaviours, the end-users do not feel confident enough to develop their scenarios in a realistic setting.},
  keywords = {Complexity theory,Natural languages,Programming profession,Robot sensing systems,Visualization}
}

@inproceedings{buchinaNaturalLanguageInterface2019,
  title = {Natural Language Interface for Programming Sensory-Enabled Scenarios for Human-Robot Interaction},
  booktitle = {2019 28th {{IEEE International Conference}} on {{Robot}} and {{Human Interactive Communication}} ({{RO-MAN}})},
  author = {Buchina, Nina G. and Sterkenburg, Paula and Lourens, Tino and Barakova, Emilia I.},
  year = {2019},
  month = oct,
  pages = {1--8},
  issn = {1944-9437},
  doi = {10.1109/RO-MAN46459.2019.8956248},
  urldate = {2024-08-15},
  abstract = {Previous research has shown that robot-mediated therapy may be effective in improving different mental or physical conditions, but this effectiveness strongly depends on how well the therapy can be translated to robot training. The goal of this study is to assist the end-users such as occupational and rehabilitation therapists to create without help of technical professional therapy-specific and sensory-enabled scenarios for the robotic assistant for use in an unstructured environment. The Cognitive Dimension of Notations framework was applied to assess the usability of the programming interface and the Cyclomatic complexity method was used to evaluate the complexity of the created robot scenarios. Eleven therapists with a mean age of 39 years working in the care for persons with visual-and-intellectual disabilities participated. The results show good usability of the interface, as measured via the CDN framework and the cyclomatic complexity analysis showed an increased complexity of the created by the occupational and rehabilitation therapist's scenarios. The participants did not request for very specifically defined behaviors for the robot, and therefore descriptions in natural text can be successfully used for robot programming.},
  keywords = {Cognitive Dimensions of Notations,Cyclomatic Complexity,End-user development for robots,Occupational and rehabilitation therapies,Robot mediated/assisted therapy (RAT),Visual and intellectual disability}
}

@book{bucknerDeepLearningRational2024,
  title = {From {{Deep Learning}} to {{Rational Machines}}: {{What}} the {{History}} of {{Philosophy Can Teach Us}} about the {{Future}} of {{Artificial Intelligence}}},
  shorttitle = {From {{Deep Learning}} to {{Rational Machines}}},
  author = {Buckner, Cameron J.},
  year = {2024},
  month = feb,
  publisher = {Oxford University Press},
  address = {Oxford, New York},
  abstract = {This book provides a framework for thinking about foundational philosophical questions surrounding the use of deep artificial neural networks ("deep learning") to achieve artificial intelligence. Specifically, it links recent breakthroughs to classic works in empiricist philosophy of mind. In recent assessments of deep learning's potential, scientists have cited historical figures from the philosophical debate between nativism and empiricism, which concerns the origins of abstract knowledge. These empiricists were faculty psychologists; that is, they argued that the extraction of abstract knowledge from experience involves the active engagement of psychological faculties such as perception, memory, imagination, attention, and empathy. This book explains how recent deep learning breakthroughs realized some of the most ambitious ideas about these faculties from philosophers such as Aristotle, Ibn Sina (Avicenna), John Locke, David Hume, William James, and Sophie de Grouchy. It illustrates the utility of this interdisciplinary connection by showing how it can provide benefits to both philosophy and computer science: computer scientists can continue to mine the history of philosophy for ideas and aspirational targets to hit, and philosophers can see how some of the historical empiricists' most ambitious speculations can now be realized in specific computational systems.                                                        ,                This book provides a framework for thinking about foundational philosophical questions surrounding the use of deep artificial neural networks ("deep learning") to achieve artificial intelligence. Specifically, it links recent breakthroughs to classic works in empiricist philosophy of mind. In recent assessments of deep learning's potential, scientists have cited historical figures from the philosophical debate between nativism and empiricism, which concerns the origins of abstract knowledge. These empiricists were faculty psychologists; that is, they argued that the extraction of abstract knowledge from experience involves the active engagement of psychological faculties such as perception, memory, imagination, attention, and empathy. This book explains how recent deep learning breakthroughs realized some of the most ambitious ideas about these faculties from philosophers such as Aristotle, Ibn Sina (Avicenna), John Locke, David Hume, William James, and Sophie de Grouchy. It illustrates the utility of this interdisciplinary connection by showing how it can provide benefits to both philosophy and computer science: computer scientists can continue to mine the history of philosophy for ideas and aspirational targets to hit, and philosophers can see how some of the historical empiricists' most ambitious speculations can now be realized in specific computational systems.},
  isbn = {978-0-19-765330-2},
  file = {C:\Users\benja\Zotero\storage\XXFUYIZB\from-deep-learning-to-rational-machines-9780197653302.html}
}

@article{buczAdvancedMethodsPID2018,
  title = {Advanced {{Methods}} of {{PID Controller Tuning}} for {{Specified Performance}}},
  author = {Bucz, {\v S}tefan and Koz{\'a}kov{\'a}, Alena},
  year = {2018},
  month = sep,
  journal = {PID Control for Industrial Processes},
  publisher = {IntechOpen},
  doi = {10.5772/intechopen.76069},
  urldate = {2020-05-12},
  abstract = {This chapter provides a concise survey, classification and historical perspective of practice-oriented methods for designing proportional-integral-derivative (PID) controllers and autotuners showing the persistent demand for PID tuning algorithms that integrate performance requirements into the tuning algorithm. The proposed frequency-domain PID controller design method guarantees closed-loop performance in terms of commonly used time-domain specifications. One of its major benefits is universal applicability for both slow and fast-controlled plants with unknown mathematical model. Special charts called B-parabolas were developed as a practical design tool that enables consistent and systematic shaping of the closed-loop step response with regard to specified performance and dynamics of the uncertain controlled plant.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\35YBH4EV\advanced-methods-of-pid-controller-tuning-for-specified-performance.html}
}

@article{bullConvergenceRatesEfficient2011,
  title = {Convergence {{Rates}} of {{Efficient Global Optimization Algorithms}}},
  author = {Bull, Adam D.},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  number = {88},
  pages = {2879--2904},
  issn = {1533-7928},
  urldate = {2024-05-23},
  abstract = {In the efficient global optimization problem, we minimize an unknown function f, using as few observations f(x) as possible. It can be considered a continuum-armed-bandit problem, with noiseless data, and simple regret. Expected-improvement algorithms are perhaps the most popular methods for solving the problem; in this paper, we provide theoretical results on their asymptotic behaviour. Implementing these algorithms requires a choice of Gaussian-process prior, which determines an associated space of functions, its reproducing-kernel Hilbert space (RKHS). When the prior is fixed, expected improvement is known to converge on the minimum of any function in its RKHS. We provide convergence rates for this procedure, optimal for functions of low smoothness, and describe a modified algorithm attaining optimal rates for smoother functions. In practice, however, priors are typically estimated sequentially from the data. For standard estimators, we show this procedure may never find the minimum of f. We then propose alternative estimators, chosen to minimize the constants in the rate of convergence, and show these estimators retain the convergence rates of a fixed prior.}
}

@article{bullockHandCentricClassificationHuman2013,
  title = {A {{Hand-Centric Classification}} of {{Human}} and {{Robot Dexterous Manipulation}}},
  author = {Bullock, Ian and Ma, Raymond and Dollar, Aaron},
  year = {2013},
  month = apr,
  journal = {Haptics, IEEE Transactions on},
  volume = {6},
  pages = {129--144},
  doi = {10.1109/TOH.2012.53},
  abstract = {This work contributes to the development of a common framework for the discussion and analysis of dexterous manipulation across the human and robotic domains. An overview of previous work is first provided along with an analysis of the tradeoffs between arm and hand dexterity. A hand-centric and motion-centric manipulation classification is then presented and applied in four different ways. It is first discussed how the taxonomy can be used to identify a manipulation strategy. Then, applications for robot hand analysis and engineering design are explained. Finally, the classification is applied to three activities of daily living (ADLs) to distinguish the patterns of dexterous manipulation involved in each task. The same analysis method could be used to predict problem ADLs for various impairments or to produce a representative benchmark set of ADL tasks. Overall, the classification scheme proposed creates a descriptive framework that can be used to effectively describe hand movements during manipulation in a variety of contexts and might be combined with existing object centric or other taxonomies to provide a complete description of a specific manipulation task.}
}

@inproceedings{burghartCognitiveArchitectureHumanoid2006,
  title = {A Cognitive Architecture for a Humanoid Robot: {{A}} First Approach},
  shorttitle = {A Cognitive Architecture for a Humanoid Robot},
  booktitle = {5th {{IEEE-RAS International Conference}} on {{Humanoid Robots}}},
  author = {Burghart, Catherina and Mikut, Ralf and Stiefelhagen, Rainer and Asfour, Tamim and Holzapfel, Hartwig and Steinhaus, Peter and Dillmann, R{\"u}diger},
  year = {2006},
  month = jan,
  volume = {2005},
  pages = {357--362},
  publisher = {IEEE},
  address = {Tsukuba, Japan},
  doi = {10.1109/ICHR.2005.1573593},
  abstract = {Future life pictures humans having intelligent humanoid robotic systems taking part in their everyday life. Thus researchers strive to supply robots with an adequate artificial intelligence in order to achieve a natural and intuitive interaction between human being and robotic system. Within the German Humanoid Project we focus on learning and cooperating multimodal robotic systems. In this paper we present a first cognitive architecture for our humanoid robot: The architecture is a mixture of a hierarchical three-layered form on the one hand and a composition of behaviour-specific modules on the other hand. Perception, learning, planning of actions, motor control, and human-like communication play an important role in the robotic system and are embedded step by step in our architecture},
  isbn = {978-0-7803-9320-2},
  file = {C:\Users\benja\Zotero\storage\J6YSDRNS\Burghart et al. - 2006 - A cognitive architecture for a humanoid robot A first approach.pdf}
}

@article{burkartSurveyExplainabilitySupervised2021,
  title = {A {{Survey}} on the {{Explainability}} of {{Supervised Machine Learning}}},
  author = {Burkart, Nadia and Huber, Marco F.},
  year = {2021},
  month = may,
  journal = {Journal of Artificial Intelligence Research},
  volume = {70},
  pages = {245--317},
  issn = {1076-9757},
  doi = {10.1613/jair.1.12228},
  urldate = {2024-03-04},
  abstract = {Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.}
}

@article{Burnod1992a,
  title = {Visuomotor Transformations Underlying Arm Movements toward Visual Targets: A Neural Network Model of Cerebral Cortical Operations},
  author = {Burnod, Y and Grandguillaume, P and Otto, I and Ferraina, S and Johnson, P B and Caminiti, R},
  year = {1992},
  month = apr,
  journal = {The Journal of Neuroscience},
  volume = {12},
  number = {4},
  eprint = {1556602},
  eprinttype = {pubmed},
  pages = {1435--53},
  urldate = {2018-05-10},
  abstract = {We propose a biologically realistic neural network that computes coordinate transformations for the command of arm reaching movements in 3-D space. This model is consistent with anatomical and physiological data on the cortical areas involved in the command of these movements. Studies of the neuronal activity in the motor (Georgopoulos et al., 1986; Schwartz et al., 1988; Caminiti et al., 1990a) and premotor (Caminiti et al., 1990b, 1991) cortices of behaving monkeys have shown that the activity of individual arm-related neurons is broadly tuned around a preferred direction of movements in 3-D space. Recent data demonstrate that in both frontal areas (Caminiti et al., 1990a,b, 1991) these cell preferred directions rotate with the initial position of the arm. Furthermore, the rotation of the population of preferred directions precisely corresponds to the rotation of the arm in space. The neural network model computes the motor command by combining the visual information about movement trajectory with the kinesthetic information concerning the orientation of the arm in space. The appropriate combination, learned by the network from spontaneous movement, can be approximated by a bilinear operation that can be interpreted as a projection of the visual information on a reference frame that rotates with the arm. This bilinear combination implies that neural circuits converging on a single neuron in the motor and premotor cortices can learn and generalize the appropriate command in a 2-D subspace but not in the whole 3-D space. However, the uniform distribution of cell preferred directions in these frontal areas can explain the computation of the correct solution by a population of cortical neurons. The model is consistent with the existing neurophysiological data and predicts how visual and somatic information can be combined in the different processing steps of the visuomotor transformation subserving visual reaching.},
  pmid = {1556602}
}

@misc{buschBesserEinkaufenMit2021,
  title = {{Besser einkaufen mit K{\"u}nstlicher Intelligenz}},
  author = {Busch, Rainer},
  year = {2021},
  month = may,
  journal = {up2date - Das Onlinemagazin der Universit{\"a}t Bremen},
  urldate = {2023-07-02},
  abstract = {Mithilfe von K{\"u}nstlicher Intelligenz und Robotik wollen Forschende im Projekt ,,Knowledge4Retail`` den Einzelhandel in ein neues Zeitalter katapultieren.},
  howpublished = {https://up2date.uni-bremen.de/forschung/besser-einkaufen-mit-kuenstlicher-intelligenz},
  langid = {ngerman},
  file = {C:\Users\benja\Zotero\storage\E2S9LPML\besser-einkaufen-mit-kuenstlicher-intelligenz.html}
}

@article{bussIntroductionInverseKinematics2004,
  title = {Introduction to Inverse Kinematics with {{Jacobian}} Transpose, Pseudoinverse and Damped Least Squares Methods},
  author = {Buss, Samuel},
  year = {2004},
  month = may,
  journal = {IEEE Transactions in Robotics and Automation},
  volume = {17},
  abstract = {This is a introduction to the Jacobian transpose method, the pseudoinverse method, and the damped least squares methods for inverse kinematics (IK). The mathematical foundations of these methods are presented, with an analysis based on the singular value decomposition.}
}

@article{bustosCORTEXCognitiveRobotics2019,
  title = {The {{CORTEX}} Cognitive Robotics Architecture: {{Use}} Cases},
  shorttitle = {The {{CORTEX}} Cognitive Robotics Architecture},
  author = {Bustos, P. and Manso, L.J. and Bandera, A.J. and Bandera, J.P. and {Garc{\'i}a-Varea}, I. and {Mart{\'i}nez-G{\'o}mez}, J.},
  year = {2019},
  month = jun,
  journal = {Cogn. Syst. Res.},
  volume = {55},
  number = {C},
  pages = {107--123},
  issn = {1389-0417},
  doi = {10.1016/j.cogsys.2019.01.003},
  urldate = {2024-09-18},
  file = {C:\Users\benja\Zotero\storage\BW7V8A5N\Bustos et al. - 2019 - The CORTEX cognitive robotics architecture Use cases.pdf}
}

@article{cadenaPresentFutureSimultaneous2016,
  title = {Past, {{Present}}, and {{Future}} of {{Simultaneous Localization}} and {{Mapping}}: {{Toward}} the {{Robust-Perception Age}}},
  shorttitle = {Past, {{Present}}, and {{Future}} of {{Simultaneous Localization}} and {{Mapping}}},
  author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, Jos{\'e} and Reid, Ian and Leonard, John J.},
  year = {2016},
  month = dec,
  journal = {IEEE Transactions on Robotics},
  volume = {32},
  number = {6},
  pages = {1309--1332},
  issn = {1941-0468},
  doi = {10.1109/TRO.2016.2624754},
  abstract = {Simultaneous localization and mapping (SLAM) consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications and witnessing a steady transition of this technology to industry. We survey the current state of SLAM and consider future directions. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?},
  keywords = {Factor graphs,Graph theory,localization,Localization,mapping,maximum a posteriori estimation,perception,robots,Robustness,sensing,Service robots,simultaneous localization and mapping (SLAM),Simultaneous location and mapping},
  file = {C:\Users\benja\Zotero\storage\PKQS2WJ5\7747236.html}
}

@inproceedings{cakmakDesigningRobotLearners2012,
  title = {Designing Robot Learners That Ask Good Questions},
  booktitle = {2012 7th {{ACM}}/{{IEEE International Conference}} on {{Human-Robot Interaction}} ({{HRI}})},
  author = {Cakmak, Maya and Thomaz, Andrea L.},
  year = {2012},
  month = mar,
  pages = {17--24},
  issn = {2167-2148},
  doi = {10.1145/2157689.2157693},
  urldate = {2024-08-16},
  abstract = {Programming new skills on a robot should take minimal time and effort. One approach to achieve this goal is to allow the robot to ask questions. This idea, called Active Learning, has recently caught a lot of attention in the robotics community. However, it has not been explored from a human-robot interaction perspective. In this paper, we identify three types of questions (label, demonstration and feature queries) and discuss how a robot can use these while learning new skills. Then, we present an experiment on human question asking which characterizes the extent to which humans use these question types. Finally, we evaluate the three question types within a human-robot teaching interaction. We investigate the ease with which different types of questions are answered and whether or not there is a general preference of one type of question over another. Based on our findings from both experiments we provide guidelines for designing question asking behaviors on a robot learner.},
  keywords = {Active Learning,Green products,Humans,Learning from Demonstration,Programming,Robots,Trajectory,USA Councils,Videos}
}

@article{calandraBayesianOptimizationLearning2016,
  title = {Bayesian Optimization for Learning Gaits under Uncertainty},
  author = {Calandra, Roberto and Seyfarth, Andr{\'e} and Peters, Jan and Deisenroth, Marc Peter},
  year = {2016},
  month = feb,
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {76},
  number = {1},
  pages = {5--23},
  issn = {1573-7470},
  doi = {10.1007/s10472-015-9463-9},
  urldate = {2020-10-30},
  abstract = {Designing gaits and corresponding control policies is a key challenge in robot locomotion. Even with a viable controller parametrization, finding near-optimal parameters can be daunting. Typically, this kind of parameter optimization requires specific expert knowledge and extensive robot experiments. Automatic black-box gait optimization methods greatly reduce the need for human expertise and time-consuming design processes. Many different approaches for automatic gait optimization have been suggested to date. However, no extensive comparison among them has yet been performed. In this article, we thoroughly discuss multiple automatic optimization methods in the context of gait optimization. We extensively evaluate Bayesian optimization, a model-based approach to black-box optimization under uncertainty, on both simulated problems and real robots. This evaluation demonstrates that Bayesian optimization is particularly suited for robotic applications, where it is crucial to find a good set of gait parameters in a small number of experiments.},
  langid = {english}
}

@inproceedings{calandraExperimentalComparisonBayesian2014,
  title = {An Experimental Comparison of {{Bayesian}} Optimization for Bipedal Locomotion},
  booktitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Calandra, Roberto and Seyfarth, Andr{\'e} and Peters, Jan and Deisenroth, Marc Peter},
  year = {2014},
  month = may,
  pages = {1951--1958},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2014.6907117},
  urldate = {2024-05-23},
  abstract = {The design of gaits and corresponding control policies for bipedal walkers is a key challenge in robot locomotion. Even when a viable controller parametrization already exists, finding near-optimal parameters can be daunting. The use of automatic gait optimization methods greatly reduces the need for human expertise and time-consuming design processes. Many different approaches to automatic gait optimization have been suggested to date. However, no extensive comparison among them has yet been performed. In this paper, we present some common methods for automatic gait optimization in bipedal locomotion, and analyze their strengths and weaknesses. We experimentally evaluated these gait optimization methods on a bipedal robot, in more than 1800 experimental evaluations. In particular, we analyzed Bayesian optimization in different configurations, including various acquisition functions.},
  keywords = {Bayes methods,Linear programming,Optimization methods,Response surface methodology,Robots,Search problems},
  file = {C:\Users\benja\Zotero\storage\6C5ZCKD8\6907117.html}
}

@article{calinonLearningReproductionGestures2010,
  title = {Learning and {{Reproduction}} of {{Gestures}} by {{Imitation}}},
  author = {Calinon, Sylvain and D'halluin, Florent and Sauser, Eric L. and Caldwell, Darwin G. and Billard, Aude G.},
  year = {2010},
  month = jun,
  journal = {IEEE Robotics Automation Magazine},
  volume = {17},
  number = {2},
  pages = {44--54},
  issn = {1558-223X},
  doi = {10.1109/MRA.2010.936947},
  abstract = {We presented and evaluated an approach based on HMM, GMR, and dynamical systems to allow robots to acquire new skills by imitation. Using HMM allowed us to get rid of the explicit time dependency that was considered in our previous work [12], by encapsulating precedence information within the statistical representation. In the context of separated learning and reproduction processes, this novel formulation was systematically evaluated with respect to our previous approach, LWR [20], LWPR [21], and DMPs [13]. We finally presented applications on different kinds of robots to highlight the flexibility of the proposed approach in three different learning by imitation scenarios.},
  keywords = {Character generation,control engineering education,dynamical system,Education,Educational robots,Encoding,Feeds,Gaussian mixture regression,Gaussian processes,gesture recognition,gesture reproduction,hidden Markov model,hidden Markov models,Hidden Markov models,humanoid robots,Humanoid robots,Humans,imitation learning,information encapsulation,learning (artificial intelligence),robot learning,robot programming,Robot programming,Robustness,statistical representation},
  file = {C:\Users\benja\Zotero\storage\U6AFPAFY\5480475.html}
}

@book{Callister.2013,
  title = {Materialwissenschaften Und Werkstofftechnik: {{Eine}} Einf{\"u}hrung},
  author = {Callister, William D. and Rethwisch, David G. and Scheffler, Michael},
  year = {2013},
  series = {Wiley {{VCH}} Lehrbuchkollektion 1},
  edition = {1. Aufl.},
  publisher = {Wiley-VCH},
  address = {Weinheim},
  doi = {10.1002/3527682864},
  abstract = {Erstmals ins Deutsche {\"u}bersetztes Standardwerk der Materialwissenschaften und Werkstofftechnik. Die englischsprachige Ausgabe erscheint seit 1985 bereits in 8. Auflage. Die {\"U}bersetzung wurde durchgehend an die deutschsprachige Universit{\"a}ts- und Forschungslandschaft angepasst. Wo n{\"o}tig, wurden Anpassungen und {\"A}nderungen bzw. Erg{\"a}nzungen durchgef{\"u}hrt, so z.B. bei Werkstoffbezeichnungen und deren Umwandlung in das EN- und DIN-Normensystem, ebenso bei Symbolen und Formeln. Im Gegensatz zur Originalausgabe liegen in dieser {\"U}bersetzung alle Kapitel in gedruckter Form vor. F{\"u}r Studenten und Dozenten bietet der Verlag auf seiner Homepage weitere Informationen und Lehrmaterialien. Im Anhang u.a. ein Glossar und eine Zusammenstellung von Werkstoffeigenschaften. Das {\"u}bersichtlich gestaltete und gut illustrierte Werk richtet sich f{\"u}r Studenten an Fachhochschule und Universit{\"a}t und bietet auch weiteren Fachrichtungen (Physik, Chemie, Maschinenbau etc.) eine umfassende Einf{\"u}hrung in die Grundlagen. Vom Verlag in {\"a}hnlichem Layout s. auch Werkstoffwissenschaft (ID-G 31/11). (3) (LK/SH: Preisler) Erstmals ins Deutsche {\"u}bersetztes Standardwerk der Materialwissenschaften und Werkstofftechnik f{\"u}r den Einsatz an Fachhochschule und Universit{\"a}t. (LK/SH: Preisler)},
  isbn = {978-3-527-33007-2}
}

@article{calvo-guiradoAdaptiveInverseControl2014,
  title = {Adaptive {{Inverse Control Using}} an {{Online Learning Algorithm}} for {{Neural Networks}}},
  author = {{Calvo-Guirado}, Jos{\'e} Luis and {Fontenla-Romero}, Oscar and {P{\'e}rez-S{\'a}nchez}, Beatriz and {Guijarro-Berdi{\~n}as}, Bertha},
  year = {2014},
  month = oct,
  journal = {INFORMATICA,},
  volume = {25},
  number = {3},
  pages = {401--414},
  issn = {08684952},
  doi = {10.15388/Informatica.2014.20},
  urldate = {2019-05-17},
  abstract = {We propose an adaptive inverse control scheme, which employs a neural network for the system identification phase and updates its weights in online mode. The theoretical basis of the method is given and its performance is illustrated by means of its application to different control problems showing that our proposal is able to overcome the problems generated by dynamic nature of the process or by physical changes of the system which originate important modifications in the process. A comparative experimental study is presented in order to show the more stable behavior of the proposed method in several working ranks.},
  langid = {english}
}

@inproceedings{calzolariSingularityMapsSpace2020,
  title = {Singularity {{Maps}} of {{Space Robots}} and Their {{Application}} to {{Gradient-based Trajectory Planning}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVI}}},
  author = {Calzolari, Davide and Lampariello, Roberto and Giordano, Alessandro Massimo},
  year = {2020},
  month = jul,
  volume = {16},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-6-1},
  file = {C:\Users\benja\Zotero\storage\92KSWAK5\p015.html}
}

@article{caniotAdaptedPepper2020,
  title = {Adapted {{Pepper}}},
  author = {Caniot, Maxime and Bonnet, Vincent and Busy, Maxime and Labaye, Thierry and Besombes, Michel and Courtois, Sebastien and Lagrue, Edouard},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.03648 [cs]},
  eprint = {2009.03648},
  primaryclass = {cs},
  urldate = {2021-02-03},
  abstract = {One of the main issue in robotics is the lack of embedded computational power. Recently, state of the art algorithms providing a better understanding of the surroundings (Object detection, skeleton tracking, etc.) are requiring more and more computational power. The lack of embedded computational power is more significant in mass-produced robots because of the difficulties to follow the increasing computational requirements of state of the art algorithms. The integration of an additional GPU allows to overcome this lack of embedded computational power. We introduce in this paper a prototype of Pepper with an embedded GPU, but also with an additional 3D camera on the head of the robot and plugged to the late GPU. This prototype, called Adapted Pepper, was built for the European project called MuMMER (MultiModal Mall Entertainment Robot) in order to embed algorithms like OpenPose, YOLO or to process sensors information and, in all cases, avoid network dependency for deported computation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C\:\\Users\\benja\\Zotero\\storage\\GAGEYG45\\Caniot et al. - 2020 - Adapted Pepper.pdf;C\:\\Users\\benja\\Zotero\\storage\\AWUB5ABI\\2009.html}
}

@article{Canny.1986,
  title = {A Computational Approach to Edge Detection},
  author = {Canny, John},
  year = {1986},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-8},
  number = {6},
  pages = {679--698},
  issn = {01628828},
  doi = {10.1109/TPAMI.1986.4767851},
  pagination = {page}
}

@article{caoFormationMechanismDetection2022,
  title = {Formation Mechanism and Detection and Evaluation Methods as Well as Repair Technology of Crack Damage in Fiber-Reinforced Composite Wind Turbine Blade: A Review},
  shorttitle = {Formation Mechanism and Detection and Evaluation Methods as Well as Repair Technology of Crack Damage in Fiber-Reinforced Composite Wind Turbine Blade},
  author = {Cao, Zheng and Li, Shujian and Li, Changping and Li, Pengnan and Ko, Tae Jo},
  year = {2022},
  month = jun,
  journal = {The International Journal of Advanced Manufacturing Technology},
  volume = {120},
  pages = {1--24},
  doi = {10.1007/s00170-022-09230-z},
  abstract = {The development of wind power industry is one of the important ways to solve current energy shortage and reduce carbon emissions. As the key component to capture wind energy, fiber-reinforced composite (FRC) wind turbine blades are subject to complex alternating loads in harsh environments. It is easy to produce various damages during long-term service, such as cracks, fiber spalling, and surface abrasion. Among them, crack is the most serious damage. FRC is a kind of composite material with obvious anisotropy, which is very sensitive to crack damage. The generation and propagation of cracks can easily lead to further failure of the whole blade, resulting in great economic losses and safety risks. How to deal with the damage of wind turbine blades in time has become a thorny problem in the wind power industry. In this paper, the current research status of the crack damage for the FRC wind turbine blade is comprehensively reviewed from the formation mechanism of crack and the detection and evaluation methods as well as the repair technologies. Finally, the development of related technologies has been prospected.}
}

@article{Carli2008,
  title = {Distributed {{Kalman Filtering Based On Consensus Strategies}}},
  author = {Carli, Ruggero and Chiuso, Alessandro and Schenato, Luca and Zampieri, Sandro},
  year = {2008},
  month = may,
  journal = {IEEE Journal on Selected Areas in Communications},
  volume = {26},
  number = {4},
  pages = {622--633},
  issn = {0733-8716},
  doi = {10.1109/JSAC.2008.080505},
  urldate = {2018-05-12}
}

@article{carstensDresdenSurgicalAnatomy2023,
  title = {The {{Dresden Surgical Anatomy Dataset}} for {{Abdominal Organ Segmentation}} in {{Surgical Data Science}}},
  author = {Carstens, Matthias and Rinner, Franziska M. and Bodenstedt, Sebastian and Jenke, Alexander C. and Weitz, J{\"u}rgen and Distler, Marius and Speidel, Stefanie and Kolbinger, Fiona R.},
  year = {2023},
  month = jan,
  journal = {Scientific Data},
  volume = {10},
  number = {1},
  pages = {3},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01719-2},
  urldate = {2024-04-12},
  abstract = {Laparoscopy is an imaging technique that enables minimally-invasive procedures in various medical disciplines including abdominal surgery, gynaecology and urology. To date, publicly available laparoscopic image datasets are mostly limited to general classifications of data, semantic segmentations of surgical instruments and low-volume weak annotations of specific abdominal organs. The Dresden Surgical Anatomy Dataset provides semantic segmentations of eight abdominal organs (colon, liver, pancreas, small intestine, spleen, stomach, ureter, vesicular glands), the abdominal wall and two vessel structures (inferior mesenteric artery, intestinal veins) in laparoscopic view. In total, this dataset comprises 13195 laparoscopic images. For each anatomical structure, we provide over a thousand images with pixel-wise segmentations. Annotations comprise semantic segmentations of single organs and one multi-organ-segmentation dataset including segments for all eleven anatomical structures. Moreover, we provide weak annotations of organ presence for every single image. This dataset markedly expands the horizon for surgical data science applications of computer vision in laparoscopic surgery and could thereby contribute to a reduction of risks and faster translation of Artificial Intelligence into surgical practice.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Endoscopy,Gastrointestinal system,Translational research}
}

@article{cashmoreReplanningSituatedRobots2019,
  title = {Replanning for {{Situated Robots}}},
  author = {Cashmore, Michael and Coles, Andrew and Cserna, Bence and Karpas, Erez and Magazzeni, Daniele and Ruml, Wheeler},
  year = {2019},
  month = jul,
  journal = {Proceedings of the International Conference on Automated Planning and Scheduling},
  volume = {29},
  pages = {665--673},
  issn = {2334-0843},
  urldate = {2021-03-03},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\HSM3WGWH\3534.html}
}

@article{Cava97,
  title = {A {{Solution}} to the {{End-Effector Position Optimisation Problem}} in {{Robotics}} Using {{Neural Networks}}},
  author = {Cavalieri, Salvatore},
  year = {1997},
  journal = {Neural Computing \& Applications},
  volume = {5},
  number = {1},
  pages = {45--57}
}

@inproceedings{cciccek20163d,
  title = {{{3D U-Net}}: Learning Dense Volumetric Segmentation from Sparse Annotation},
  booktitle = {Int. Conf. on Medical Image Computing and Computer-Assisted Intervention},
  author = {{\c C}i{\c c}ek, {\"O}zg{\"u}n and Abdulkadir, Ahmed and Lienkamp, Soeren S and Brox, Thomas and Ronneberger, Olaf},
  year = {2016},
  pages = {424--432},
  organization = {Springer}
}

@misc{CeTIResearchingVarious,
  title = {{{CeTI}} -- {{Researching}} on Various Aspects of {{Tactile Internet}} with {{Human-in-the-Loop}}},
  urldate = {2024-04-12},
  langid = {american},
  file = {C:\Users\benja\Zotero\storage\Q9BCJPTV\ceti.one.html}
}

@article{CGAL.,
  title = {Triangulated Surface Mesh Deformation},
  author = {{CGAL}}
}

@inproceedings{chalkidisLEGALBERTMuppetsStraight2020,
  title = {{{LEGAL-BERT}}: {{The Muppets}} Straight out of {{Law School}}},
  shorttitle = {{{LEGAL-BERT}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Chalkidis, Ilias and Fergadiotis, Manos and Malakasiotis, Prodromos and Aletras, Nikolaos and Androutsopoulos, Ion},
  editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
  year = {2020},
  month = nov,
  pages = {2898--2904},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.findings-emnlp.261},
  urldate = {2024-09-29},
  abstract = {BERT has achieved impressive performance in several NLP tasks. However, there has been limited investigation on its adaptation guidelines in specialised domains. Here we focus on the legal domain, where we explore several approaches for applying BERT models to downstream legal tasks, evaluating on multiple datasets. Our findings indicate that the previous guidelines for pre-training and fine-tuning, often blindly followed, do not always generalize well in the legal domain. Thus we propose a systematic investigation of the available strategies when applying BERT in specialised domains. These are: (a) use the original BERT out of the box, (b) adapt BERT by additional pre-training on domain-specific corpora, and (c) pre-train BERT from scratch on domain-specific corpora. We also propose a broader hyper-parameter search space when fine-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT models intended to assist legal NLP research, computational law, and legal technology applications.},
  file = {C:\Users\benja\Zotero\storage\ZHZ5HTF6\Chalkidis et al. - 2020 - LEGAL-BERT The Muppets straight out of Law School.pdf}
}

@article{changUnderstandingDistributedRepresentations2024,
  title = {Understanding {{Distributed Representations}} of {{Concepts}} in {{Deep Neural Networks}} without {{Supervision}}},
  author = {Chang, Wonjoon and Kwon, Dahee and Choi, Jaesik},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {10},
  pages = {11212--11220},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i10.28999},
  urldate = {2024-04-27},
  abstract = {Understanding intermediate representations of the concepts learned by deep learning classifiers is indispensable for interpreting general model behaviors. Existing approaches to reveal learned concepts often rely on human supervision, such as pre-defined concept sets or segmentation processes. In this paper, we propose a novel unsupervised method for discovering distributed representations of concepts by selecting a principal subset of neurons. Our empirical findings demonstrate that instances with similar neuron activation states tend to share coherent concepts. Based on the observations, the proposed method selects principal neurons that construct an interpretable region, namely a Relaxed Decision Region (RDR), encompassing instances with coherent concepts in the feature space. It can be utilized to identify unlabeled subclasses within data and to detect the causes of misclassifications. Furthermore, the applicability of our method across various layers discloses distinct distributed representations over the layers, which provides deeper insights into the internal mechanisms of the deep learning model.},
  copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {and Transparency,CV: Interpretability,Explainability}
}

@article{chanWeightedLeastnormSolution1995,
  title = {A Weighted Least-Norm Solution Based Scheme for Avoiding Joint Limits for Redundant Joint Manipulators},
  author = {Chan, Tan Fung and Dubey, R.V.},
  year = {1995},
  month = apr,
  journal = {IEEE Transactions on Robotics and Automation},
  volume = {11},
  number = {2},
  pages = {286--292},
  issn = {2374-958X},
  doi = {10.1109/70.370511},
  urldate = {2024-03-31},
  abstract = {It is proposed to use weighted least-norm solution to avoid joint limits for redundant joint manipulators. A comparison is made with the gradient projection method for avoiding joint limits. While the gradient projection method provides the optimal direction for the joint velocity vector within the null space, its magnitude is not unique and is adjusted by a scalar coefficient chosen by trial and error. It is shown in this paper that one fixed value of the scalar coefficient is not suitable even in a small workspace. The proposed manipulation scheme automatically chooses an appropriate magnitude of the self-motion throughout the workspace. This scheme, unlike the gradient projection method, guarantees joint limit avoidance, and also minimizes unnecessary self-motion. It was implemented and tested for real-time control of a seven-degree-of-freedom (7-DOF) Robotics Research Corporation (RRC) manipulator.{$<>$}},
  keywords = {Automatic control,Equations,Jacobian matrices,Kinematics,Manipulators,Null space,Redundancy,Robotics and automation,Testing,Vectors},
  file = {C:\Users\benja\Zotero\storage\RX9KFK3W\370511.html}
}

@book{chaoLanguageSymbolicSystems1968,
  title = {Language and {{Symbolic Systems}}},
  author = {Chao, Yuen Ren},
  year = {1968},
  month = apr,
  edition = {Reissue edition},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  abstract = {This is an introduction to the study of language for the non-specialist or beginner in linguistics. Professor Chao covers the whole field of language and of modern developments in linguistics, with particular emphasis on those aspects which are likely to be most interesting to the layman. Professor Chao emphasises the relationship between language and other aspects of human culture and discusses systems of writing, minority languages and problems of translation in this context. An important part of the book reviews symbolic systems in language, writing and modern communication technology, with applications such as automatic speech, machine translation and related topics. Professor Chao laid the foundations of modern linguistics in China and has been associated with a number of important linguistic projects both in China and in the USA where he has taught for many years. In this book he approaches his subject with clarity, warmth and humour.},
  isbn = {978-0-521-09457-3},
  langid = {english}
}

@article{chaplotLearningExploreUsing2020,
  title = {Learning to {{Explore}} Using {{Active Neural SLAM}}},
  author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Saurabh and Gupta, Abhinav and Salakhutdinov, Ruslan},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.05155 [cs]},
  eprint = {2004.05155},
  primaryclass = {cs},
  urldate = {2022-04-26},
  abstract = {This work presents a modular and hierarchical approach to learn policies for exploring 3D environments, called `Active Neural SLAM'. Our approach leverages the strengths of both classical and learning-based methods, by using analytical path planners with learned SLAM module, and global and local policies. The use of learning provides flexibility with respect to input modalities (in the SLAM module), leverages structural regularities of the world (in global policies), and provides robustness to errors in state estimation (in local policies). Such use of learning within each module retains its benefits, while at the same time, hierarchical decomposition and modular training allow us to sidestep the high sample complexities associated with training end-to-end policies. Our experiments in visually and physically realistic simulated 3D environments demonstrate the effectiveness of our approach over past learning and geometry-based approaches. The proposed model can also be easily transferred to the PointGoal task and was the winning entry of the CVPR 2019 Habitat PointGoal Navigation Challenge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\W9E6D5SW\2004.html}
}

@incollection{Chas93,
  title = {Modeling of {{Robot Dynamics}} by {{Neural Networks}} with {{Dynamic Neurons}}},
  booktitle = {Neural {{Networks}} in {{Robotics}}},
  author = {Chassiakos, Anastasios G and Kosmatopoulos, Elias B and Christodoulou, Manolis A},
  year = {1993},
  pages = {165--176},
  publisher = {Springer Science+Business Media}
}

@article{chaudhuriNeurosymbolicProgramming2021,
  title = {Neurosymbolic {{Programming}}},
  author = {Chaudhuri, Swarat and Ellis, Kevin and Polozov, Oleksandr and Singh, Rishabh and {Solar-Lezama}, Armando and Yue, Yisong},
  year = {2021},
  month = dec,
  journal = {Foundations and Trends{\textregistered} in Programming Languages},
  volume = {7},
  number = {3},
  pages = {158--243},
  publisher = {Now Publishers, Inc.},
  issn = {2325-1107, 2325-1131},
  doi = {10.1561/2500000049},
  urldate = {2022-04-02},
  abstract = {Neurosymbolic Programming},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\GCIZMF5D\PGL-049.html}
}

@inproceedings{chen2016virtual,
  title = {Virtual Fixture Assistance for Needle Passing and Knot Tying},
  booktitle = {2016 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems ({{IROS}})},
  author = {Chen, Zihan and Malpani, Anand and Chalasani, Preetham and Deguet, Anton and Vedula, S Swaroop and Kazanzides, Peter and Taylor, Russell H},
  year = {2016},
  pages = {2343--2350},
  organization = {IEEE}
}

@inproceedings{chen2018encoder,
  title = {Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation},
  booktitle = {Proceedings of the {{European}} Conference on Computer Vision ({{ECCV}})},
  author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = {2018},
  pages = {801--818}
}

@article{chenBatchExplorationExamples2021,
  title = {Batch {{Exploration}} with {{Examples}} for {{Scalable Robotic Reinforcement Learning}}},
  author = {Chen, Annie S. and Nam, HyunJi and Nair, Suraj and Finn, Chelsea},
  year = {2021},
  month = jul,
  journal = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {3},
  eprint = {2010.11917},
  pages = {4401--4408},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2021.3068655},
  urldate = {2021-06-03},
  abstract = {Learning from diverse offline datasets is a promising path towards learning general purpose robotic agents. However, a core challenge in this paradigm lies in collecting large amounts of meaningful data, while not depending on a human in the loop for data collection. One way to address this challenge is through task-agnostic exploration, where an agent attempts to explore without a task-specific reward function, and collect data that can be useful for any downstream task. While these approaches have shown some promise in simple domains, they often struggle to explore the relevant regions of the state space in more challenging settings, such as vision based robotic manipulation. This challenge stems from an objective that encourages exploring everything in a potentially vast state space. To mitigate this challenge, we propose to focus exploration on the important parts of the state space using weak human supervision. Concretely, we propose an exploration technique, Batch Exploration with Examples (BEE), that explores relevant regions of the state-space, guided by a modest number of human provided images of important states. These human provided images only need to be collected once at the beginning of data collection and can be collected in a matter of minutes, allowing us to scalably collect diverse datasets, which can then be combined with any batch RL algorithm. We find that BEE is able to tackle challenging vision-based manipulation tasks both in simulation and on a real Franka robot, and observe that compared to task-agnostic and weakly-supervised exploration techniques, it (1) interacts more than twice as often with relevant objects, and (2) improves downstream task performance when used in conjunction with offline RL.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\49893QB7\2010.html}
}

@article{chenBenchmarkingLargeLanguage2024,
  title = {Benchmarking {{Large Language Models}} in {{Retrieval-Augmented Generation}}},
  author = {Chen, Jiawei and Lin, Hongyu and Han, Xianpei and Sun, Le},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {16},
  pages = {17754--17762},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i16.29728},
  urldate = {2024-06-16},
  abstract = {Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.},
  copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Analysis,and Evaluation of NLP Models,NLP: Interpretability}
}

@article{chenContextAwareSafeReinforcement2020,
  title = {Context-{{Aware Safe Reinforcement Learning}} for {{Non-Stationary Environments}}},
  author = {Chen, Baiming and Liu, Zuxin and Zhu, Jiacheng and Xu, Mengdi and Ding, Wenhao and Li, Liang and Zhao, Ding},
  year = {2020},
  pages = {8},
  abstract = {Safety is a critical concern when deploying reinforcement learning agents for real-world applications. Recently, safe reinforcement learning algorithms have been developed to optimize the agent's performance while avoiding violations of safety constraints. However, few studies have addressed the nonstationary disturbances in the environments, which may cause catastrophic outcomes. In this paper, we propose the contextaware safe reinforcement learning (CASRL) method, a metalearning framework to realize safe adaptation in non-stationary environments. We use a probabilistic latent variable model to achieve fast inference of the posterior environment transition distribution given the context data. Chance constraints of safety are then evaluated with uncertainty-aware trajectory sampling. The high cost of safety violations leads to the rareness of unsafe records in the dataset. We address this issue by enabling prioritized sampling during model training and formulating sim-to-real prior safety constraints during constrained planning. The algorithm is evaluated in realistic safety-critical environments with non-stationary disturbances. Results show that the proposed algorithm significantly outperforms existing baselines in terms of safety and robustness.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\Z5NMWYJU\Z5NMWYJU.pdf}
}

@inproceedings{chenDevelopingHighlevelCognitive2010,
  title = {Developing High-Level Cognitive Functions for Service Robots},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Autonomous Agents}} and {{Multiagent Systems}}: Volume 1 - {{Volume}} 1},
  author = {Chen, Xiaoping and Ji, Jianmin and Jiang, Jiehui and Jin, Guoqiang and Wang, Feng and Xie, Jiongkun},
  year = {2010},
  month = may,
  series = {{{AAMAS}} '10},
  pages = {989--996},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  address = {Richland, SC},
  urldate = {2024-08-15},
  abstract = {The primary target of this work is human-robot collaboration, especially for service robots in complicated application scenarios. Three assumptions and four requirements are identified. State-of-the-art, general-purpose Natural Language Processing (NLP), Commonsense Reasoning (in particular, ASP), and Robotics techniques are integrated in a layered architecture. The architecture and mechanisms have been implemented on a service robot, Ke Jia. Instead of command languages, small limited segments of natural languages are employed in spoken dialog between Ke Jia and its users. The information in the dialog is extracted, classified and transferred into inner representation by Ke Jia's NLP mechanism, and further used autonomously in problem-solving and planning. A series of case study was conducted on Ke Jia with positive results, verifying its ability of acquiring knowledge through spoken dialog with users, autonomous solving problems by virtue of acquired causal knowledge, and autonomous planning for complex tasks.},
  isbn = {978-0-9826571-1-9}
}

@article{chenEnhancingRobotProgram2023,
  title = {Enhancing {{Robot Program Synthesis Through Environmental Context}}},
  author = {Chen, Tianyi and Wang, Qidi and Dong, Zhen and Shen, Liwei and Peng, Xin},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {3881--3893},
  urldate = {2024-04-17},
  langid = {english}
}

@misc{chenEvaluatingLargeLanguage2021,
  title = {Evaluating {{Large Language Models Trained}} on {{Code}}},
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and {Herbert-Voss}, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  year = {2021},
  month = jul,
  number = {arXiv:2107.03374},
  eprint = {2107.03374},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.03374},
  urldate = {2023-10-30},
  abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\CDIFAQA2\2107.html}
}

@book{chenExplainableArtificialIntelligence2023,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}) in {{Manufacturing}}: {{Methodology}}, {{Tools}}, and {{Applications}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}}) in {{Manufacturing}}},
  author = {Chen, Tin-Chih Toly},
  year = {2023},
  month = mar,
  publisher = {Springer Nature},
  abstract = {This book provides a comprehensive overview of the latest developments in Explainable AI (XAI) and its applications in manufacturing. It covers the various methods, tools, and technologies that are being used to make AI more understandable and communicable for factory workers. With the increasing use of AI in manufacturing, there is a growing need to address the limitations of advanced AI methods that are difficult to understand or explain to those without a background in AI. This book addresses this need by providing a systematic review of the latest research and advancements in XAI specifically tailored for the manufacturing industry.The book includes real-world case studies and examples to illustrate the practical applications of XAI in manufacturing. It is a valuable resource for researchers, engineers, and practitioners working in the field of AI and manufacturing.},
  googlebooks = {X1i0EAAAQBAJ},
  isbn = {978-3-031-27961-4},
  langid = {english},
  keywords = {Business & Economics / Production & Operations Management,Computers / Artificial Intelligence / General,Technology & Engineering / General,Technology & Engineering / Industrial Engineering,Technology & Engineering / Manufacturing}
}

@article{chenForgetfulLargeLanguage2023,
  title = {Forgetful {{Large Language Models}}: {{Lessons Learned}} from {{Using LLMs}} in {{Robot Programming}}},
  shorttitle = {Forgetful {{Large Language Models}}},
  author = {Chen, Juo-Tung and Huang, Chien-Ming},
  year = {2023},
  journal = {Proceedings of the AAAI Symposium Series},
  volume = {2},
  number = {1},
  pages = {508--513},
  issn = {2994-4317},
  doi = {10.1609/aaaiss.v2i1.27721},
  urldate = {2024-04-30},
  abstract = {Large language models offer new ways of empowering people to program robot applications-namely, code generation via prompting. However, the code generated by LLMs is susceptible to errors. This work reports a preliminary exploration that empirically characterizes common errors produced by LLMs in robot programming. We categorize these errors into two phases: interpretation and execution. In this work, we focus on errors in execution and observe that they are caused by LLMs being ``forgetful'' of key information provided in user prompts. Based on this observation, we propose prompt engineering tactics designed to reduce errors in execution. We then demonstrate the effectiveness of these tactics with three language models: ChatGPT, Bard, and LLaMA-2. Finally, we discuss lessons learned from using LLMs in robot programming and call for the benchmarking of LLM-powered end-user development of robot applications.},
  copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Prompt Engineering}
}

@article{chengDiffTuneAutoTuningAutoDifferentiation2024,
  title = {{{DiffTune}}: {{Auto-Tuning}} through {{Auto-Differentiation}}},
  shorttitle = {{{DiffTune}}},
  author = {Cheng, Sheng and Kim, Minkyung and Song, Lin and Yang, Chengyu and Jin, Yiquan and Wang, Shenlong and Hovakimyan, Naira},
  year = {2024},
  journal = {IEEE Transactions on Robotics},
  pages = {1--17},
  issn = {1941-0468},
  doi = {10.1109/TRO.2024.3429191},
  urldate = {2024-09-06},
  abstract = {The performance of robots in high-level tasks depends on the quality of their lower-level controller, which requires fine-tuning. However, the intrinsically nonlinear dynamics and controllers make tuning a challenging task when it is done by hand. In this paper, we present DiffTune, a novel, gradient-based automatic tuning framework. We formulate the controller tuning as a parameter optimization problem. Our method unrolls the dynamical system and controller as a computational graph and updates the controller parameters through gradient-based optimization. The gradient is obtained using sensitivity propagation, which is the only method for gradient computation when tuning for a physical system instead of its simulated counterpart. Furthermore, we use {\textbackslash}mathcal L\_1 adaptive control to compensate for the uncertainties (that unavoidably exist in a physical system) such that the gradient is not biased by the unmodelled uncertainties. We validate the DiffTune on a Dubin's car and a quadrotor in challenging simulation environments. In comparison with state-of-the-art auto-tuning methods, DiffTune achieves the best performance in a more efficient manner owing to its effective usage of the first-order information of the system. Experiments on tuning a nonlinear controller for quadrotor show promising results, where DiffTune achieves 3.5x tracking error reduction on an aggressive trajectory in only 10 trials over a 12-dimensional controller parameter space.},
  keywords = {Computational modeling,Optimization,Quadrotors,Robots,Sensitivity,Tuning,Uncertainty},
  file = {C:\Users\benja\Zotero\storage\F2WH32Y6\Cheng et al. - 2024 - DiffTune Auto-Tuning through Auto-Differentiation.pdf}
}

@misc{chenGenAugRetargetingBehaviors2023,
  title = {{{GenAug}}: {{Retargeting}} Behaviors to Unseen Situations via {{Generative Augmentation}}},
  shorttitle = {{{GenAug}}},
  author = {Chen, Zoey and Kiami, Sho and Gupta, Abhishek and Kumar, Vikash},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06671},
  eprint = {2302.06671},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.06671},
  urldate = {2023-02-17},
  abstract = {Robot learning methods have the potential for widespread generalization across tasks, environments, and objects. However, these methods require large diverse datasets that are expensive to collect in real-world robotics settings. For robot learning to generalize, we must be able to leverage sources of data or priors beyond the robot's own experience. In this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data, can serve as such a data source. We show that despite these generative models being trained on largely non-robotics data, they can serve as effective ways to impart priors into the process of robot learning in a way that enables widespread generalization. In particular, we show how pre-trained generative models can serve as effective tools for semantically meaningful data augmentation. By leveraging these pre-trained models for generating appropriate "semantic" data augmentations, we propose a system GenAug that is able to significantly improve policy generalization. We apply GenAug to tabletop manipulation tasks, showing the ability to re-target behavior to novel scenarios, while only requiring marginal amounts of real-world data. We demonstrate the efficacy of this system on a number of object manipulation problems in the real world, showing a 40\% improvement in generalization to novel scenes and objects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\52RH4BZL\2302.html}
}

@article{chengHumanAwareRobotTask2021,
  title = {Human-{{Aware Robot Task Planning Based}} on a {{Hierarchical Task Model}}},
  author = {Cheng, Yujiao and Sun, Liting and Tomizuka, Masayoshi},
  year = {2021},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {2},
  pages = {1136--1143},
  issn = {2377-3766},
  doi = {10.1109/LRA.2021.3056370},
  urldate = {2024-04-23},
  abstract = {When robots work with humans for collaborative task, they need to plan their actions while taking humans' actions into account. However, due to the complexity of the tasks and stochastic nature of human collaborators, it is quite challenging for the robot to efficiently collaborate with the humans. To address this challenge, in this letter, we first propose an algorithm to automatically construct a hierarchical task model from single-agent demonstrations. The hierarchical task model explicitly captures the sequential and parallel relationships of the task at all levels of abstraction. We then propose an optimization-based planner, which exploits the parallel relationships and prioritizes actions that are parallel to the humans' actions. In such a way, potential spatial interfaces can be avoided, task completion time can be reduced, and human's satisfaction level can be improved. We conducted simulations of a robot arm collaborating with a human for several collaborative tasks. The comparison results with several baselines proved that our proposed planner is better in terms of efficiency, safety and human satisfaction.},
  keywords = {assembly,Collaboration,Fans,human-centered robotics,Industrial robots,Planning,Robots,Safety,Service robots,Task analysis,task planning},
  file = {C:\Users\benja\Zotero\storage\58XFE389\9345470.html}
}

@inproceedings{chenGRIPGenerativeRobust2019,
  title = {{{GRIP}}: {{Generative Robust Inference}} and {{Perception}} for {{Semantic Robot Manipulation}} in {{Adversarial Environments}}},
  shorttitle = {{{GRIP}}},
  booktitle = {{{IROS}}},
  author = {Chen, Xiaotong and Chen, Rui and Sui, Zhiqiang and Ye, Zhefan and Liu, Yanqi and Bahar, R. Iris and Jenkins, Odest Chadwicke},
  year = {2019},
  month = nov,
  pages = {3988--3995},
  issn = {2153-0866},
  doi = {10.1109/IROS40897.2019.8967983},
  abstract = {Recent advancements have led to a proliferation of machine learning systems used to assist humans in a wide range of tasks. However, we are still far from accurate, reliable, and resource-efficient operations of these systems. For robot perception, convolutional neural networks (CNNs) for object detection and pose estimation are recently coming into widespread use. However, neural networks are known to suffer from overfitting during the training process and are less robust under unforeseen conditions (which makes them especially vulnerable to adversarial scenarios). In this work, we propose Generative Robust Inference and Perception (GRIP) as a two-stage object detection and pose estimation system that aims to combine the relative strengths of discriminative CNNs and generative inference methods to achieve robust estimation. Our results show that a second stage of sample-based generative inference is able to recover from false object detections by CNNs, and produce robust estimations in adversarial conditions. We demonstrate the efficacy of GRIP robustness through comparison with state-of-the-art learning-based pose estimators and pick-and-place manipulation in dark and cluttered environments.},
  keywords = {adversarial conditions,adversarial environments,convolutional neural nets,convolutional neural networks,discriminative CNNs,estimation system,false object detections,generative inference methods,generative robust inference,GRIP,learning (artificial intelligence),machine learning systems,object detection,pose estimation,resource-efficient operations,robot perception,robot vision,robust estimation,sample-based generative inference,semantic robot manipulation,two-stage object detection},
  file = {C:\Users\benja\Zotero\storage\9QYDG46H\8967983.html}
}

@article{chengRMPflowComputationalGraph2019,
  title = {{{RMPflow}}: {{A Computational Graph}} for {{Automatic Motion Policy Generation}}},
  shorttitle = {{{RMPflow}}},
  author = {Cheng, Ching-An and Mukadam, Mustafa and Issac, Jan and Birchfield, Stan and Fox, Dieter and Boots, Byron and Ratliff, Nathan},
  year = {2019},
  month = apr,
  journal = {arXiv:1811.07049 [cs]},
  eprint = {1811.07049},
  primaryclass = {cs},
  urldate = {2020-01-08},
  abstract = {We develop a novel policy synthesis algorithm, RMPflow, based on geometrically consistent transformations of Riemannian Motion Policies (RMPs). RMPs are a class of reactive motion policies designed to parameterize non-Euclidean behaviors as dynamical systems in intrinsically nonlinear task spaces. Given a set of RMPs designed for individual tasks, RMPflow can consistently combine these local policies to generate an expressive global policy, while simultaneously exploiting sparse structure for computational efficiency. We study the geometric properties of RMPflow and provide sufficient conditions for stability. Finally, we experimentally demonstrate that accounting for the geometry of task policies can simplify classically difficult problems, such as planning through clutter on high-DOF manipulation systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {C:\Users\benja\Zotero\storage\29YHEICE\1811.html}
}

@inproceedings{chenHardwareConditionedPolicies2018,
  title = {Hardware {{Conditioned Policies}} for {{Multi-robot Transfer Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Tao and Murali, Adithyavairavan and Gupta, Abhinav},
  year = {2018},
  pages = {9355--9366},
  address = {USA},
  urldate = {2019-07-14},
  abstract = {Deep reinforcement learning could be used to learn dexterous robotic policies but it is challenging to transfer them to new robots with vastly different hardware properties. It is also prohibitively expensive to learn a new policy from scratch for each robot hardware due to the high sample complexity of modern state-of-the-art algorithms. We propose a novel approach called Hardware Conditioned Policies where we train a universal policy conditioned on a vector representation of robot hardware. We considered robots in simulation with varied dynamics, kinematic structure, kinematic lengths and degrees-of-freedom. First, we use the kinematic structure directly as the hardware encoding and show great zero-shot transfer to completely novel robots not seen during training. For robots with lower zero-shot success rate, we also demonstrate that fine-tuning the policy network is significantly more sample-efficient than training a model from scratch. In tasks where knowing the agent dynamics is important for success, we learn an embedding for robot hardware and show that policies conditioned on the encoding of hardware tend to generalize and transfer well. Videos of experiments are available at: https://sites.google.com/view/robot-transfer-hcp.}
}

@inproceedings{chenLearningGeneralizableRobotic2021,
  title = {Learning {{Generalizable Robotic Reward Functions}} from ``{{In-The-Wild}}'' {{Human Videos}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVII}}},
  author = {Chen, Annie S. and Nair, Suraj and Finn, Chelsea},
  year = {2021},
  month = jul,
  volume = {17},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-7-8}
}

@article{chenMonocularHumanPose2020,
  title = {Monocular {{Human Pose Estimation}}: {{A Survey}} of {{Deep Learning-based Methods}}},
  shorttitle = {Monocular {{Human Pose Estimation}}},
  author = {Chen, Yucheng and Tian, Yingli and He, Mingyi},
  year = {2020},
  month = mar,
  journal = {Computer Vision and Image Understanding},
  volume = {192},
  eprint = {2006.01423},
  pages = {102897},
  issn = {10773142},
  doi = {10.1016/j.cviu.2019.102897},
  urldate = {2021-01-26},
  abstract = {Vision-based monocular human pose estimation, as one of the most fundamental and challenging problems in computer vision, aims to obtain posture of the human body from input images or video sequences. The recent developments of deep learning techniques have been brought significant progress and remarkable breakthroughs in the field of human pose estimation. This survey extensively reviews the recent deep learning-based 2D and 3D human pose estimation methods published since 2014. This paper summarizes the challenges, main frameworks, benchmark datasets, evaluation metrics, performance comparison, and discusses some promising future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\benja\Zotero\storage\R8CTK29Q\2006.html}
}

@inproceedings{chenNeuralOrdinaryDifferential2018,
  title = {Neural {{Ordinary Differential Equations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-25},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.}
}

@article{chenOnlineControlMetaoptimization2023,
  title = {Online {{Control}} for {{Meta-optimization}}},
  author = {Chen, Xinyi and Hazan, Elad},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {36768--36780},
  urldate = {2024-09-07},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\H8EVMK7Q\Chen und Hazan - 2023 - Online Control for Meta-optimization.pdf}
}

@inproceedings{chenPreTrainedImageProcessing2021,
  title = {Pre-{{Trained Image Processing Transformer}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Chen, Hanting and Wang, Yunhe and Guo, Tianyu and Xu, Chang and Deng, Yiping and Liu, Zhenhua and Ma, Siwei and Xu, Chunjing and Xu, Chao and Gao, Wen},
  year = {2021},
  pages = {12299--12310},
  urldate = {2021-06-21},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\L3IM7IK9\Chen_Pre-Trained_Image_Processing_Transformer_CVPR_2021_paper.html}
}

@inproceedings{chenSearchingEfficientMultiscale2018,
  title = {Searching for Efficient Multi-Scale Architectures for Dense Image Prediction},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Chen, Liang-Chieh and Collins, Maxwell D. and Zhu, Yukun and Papandreou, George and Zoph, Barret and Schroff, Florian and Adam, Hartwig and Shlens, Jonathon},
  year = {2018},
  month = dec,
  series = {{{NIPS}}'18},
  pages = {8713--8724},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2023-03-06},
  abstract = {The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7\% on Cityscapes (street scene parsing), 71.3\% on PASCAL-Person-Part (person-part segmentation), and 87.9\% on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.}
}

@inproceedings{chenTOMNetLearningTransparent2018,
  title = {{{TOM-Net}}: {{Learning Transparent Object Matting From}} a {{Single Image}}},
  shorttitle = {{{TOM-Net}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Chen, Guanying and Han, Kai and Wong, Kwan-Yee K.},
  year = {2018},
  pages = {9233--9241},
  urldate = {2023-03-10}
}

@inproceedings{chernovaEvolutionaryApproachGait2004,
  title = {An Evolutionary Approach to Gait Learning for Four-Legged Robots},
  booktitle = {2004 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}}) ({{IEEE Cat}}. {{No}}.{{04CH37566}})},
  author = {Chernova, S. and Veloso, M.},
  year = {2004},
  month = sep,
  volume = {3},
  pages = {2562-2567 vol.3},
  doi = {10.1109/IROS.2004.1389794},
  abstract = {Developing fast gaits for legged robots is a difficult task that requires optimizing parameters in a highly irregular, multidimensional space. In the past, walk optimization for quadruped robots, namely the Sony AIBO robot, was done by handtuning the parameterized gaits. In addition to requiring a lot of time and human expertise, this process produced sub-optimal results. Several recent projects have focused on using machine learning to automate the parameter search. Algorithms utilizing Powell's minimization method and policy gradient reinforcement learning have shown significant improvement over previous walk optimization results. In this paper we present a new algorithm for walk optimization based on an evolutionary approach. Unlike previous methods, our algorithm does not attempt to approximate the gradient of the multidimensional space. This makes it more robust to noise in parameter evaluations and avoids prematurely converging to local optima, a problem encountered by both of the previously suggested algorithms. Our evolutionary algorithm matches the best previous learning method, achieving several different walks of high quality. Furthermore, the best learned walks represent an impressive 20\% improvement over our own best hand-tuned walks.},
  keywords = {Humans,Legged locomotion,Machine learning,Machine learning algorithms,Minimization methods,Multidimensional systems,Optimization methods,Orbital robotics,Robotics and automation,Robots},
  file = {C:\Users\benja\Zotero\storage\9B4ZMMMG\1389794.html}
}

@inproceedings{chiDiffusionPolicyVisuomotor2023,
  title = {Diffusion {{Policy}}: {{Visuomotor Policy Learning}} via {{Action Diffusion}}},
  shorttitle = {Diffusion {{Policy}}},
  booktitle = {Robotics: {{Science}} and {{Systems XIX}}},
  author = {Chi, Cheng and Feng, Siyuan and Du, Yilun and Xu, Zhenjia and Cousineau, Eric and Burchfiel, Benjamin CM and Song, Shuran},
  year = {2023},
  month = jul,
  volume = {19},
  urldate = {2024-04-28},
  isbn = {978-0-9923747-9-2}
}

@article{childGeneratingLongSequences2019,
  title = {Generating {{Long Sequences}} with {{Sparse Transformers}}},
  author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.10509 [cs, stat]},
  eprint = {1904.10509},
  primaryclass = {cs, stat},
  urldate = {2020-12-15},
  abstract = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n {\textbackslash}sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\UD35ZIUV\1904.html}
}

@article{chiouMixedInitiativeVariableAutonomy2020,
  title = {Mixed-{{Initiative}} Variable Autonomy for Remotely Operated Mobile Robots},
  author = {Chiou, Manolis and Hawes, Nick and Stolkin, Rustam},
  year = {2020},
  month = oct,
  journal = {arXiv:1911.04848 [cs]},
  eprint = {1911.04848},
  primaryclass = {cs},
  urldate = {2021-02-05},
  abstract = {This paper presents an Expert-guided Mixed-Initiative Control Switcher (EMICS) for remotely operated mobile robots. The EMICS enables switching between different levels of autonomy during task execution initiated by either the human operator and/or the EMICS. The EMICS is evaluated in two disaster response inspired experiments, one with a simulated robot and test arena, and one with a real robot in a realistic environment. Analyses from the two experiments provide evidence that: a) Human-Initiative (HI) systems outperform systems with single modes of operation, such as pure teleoperation, in navigation tasks; b) in the context of the simulated robot experiment, Mixed-Initiative (MI) systems provide improved performance in navigation tasks, improved operator performance in cognitive demanding secondary tasks, and improved operator workload compared to HI. Results also reinforce previous human-robot interaction evidence regarding the importance of the operator's personality traits and their trust in the autonomous system. Lastly, our experiment on a physical robot provides empirical evidence that identify two major challenges for MI control: a) the design of context-aware MI control systems; and b) the conflict for control between the robot's MI control system and the operator. Insights regarding these challenges are discussed and ways to tackle them are proposed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\SNP95ANZ\1911.html}
}

@inproceedings{chiouPrincipledStudyVariable2015,
  title = {Towards the {{Principled Study}} of {{Variable Autonomy}} in {{Mobile Robots}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}}},
  author = {Chiou, M. and Hawes, N. and Stolkin, R. and Shapiro, K. L. and Kerlin, J. R. and Clouter, A.},
  year = {2015},
  month = oct,
  pages = {1053--1059},
  doi = {10.1109/SMC.2015.190},
  abstract = {Safety critical and demanding tasks (e.g. Search and rescue or hazardous environments inspection), can benefit from robotic systems that offer a spectrum of control modes. These can range from direct teleoperation to full autonomy. This paper describes a pilot-study experiment in which a variable autonomy robot completes a navigation task. It explores the comparative performances of the human-robot system at different autonomy levels under different sets of conditions. This is done from a Mixed-Initiative system investigation perspective. Sensor noise was added to degrade robot performance, while a secondary task induced varying degrees of additional workload on the human operator. Carrying out these experiments and analyzing the initial results, has highlighted the profound complexities of designing tasks, conditions, and performance metrics which are: principled, eliminate confounding factors, and yield scientifically rigorous insights into the intricacies of a collaborative system that combines both human and robot intelligences. A key contribution of this paper is to describe the lessons learned from attempting these experiments, and to suggest a variety of guidelines for other researchers to consider when designing experiments in this context.},
  keywords = {Artificial intelligence,cooperative control,demanding tasks,direct teleoperation,dynamic autonomy,human-robot interaction,human-robot system,Human-Robot-Interaction (HRI),Mixed-initiative,mixed-initiative system,mobile robots,Mobile robots,Navigation,noise,Robot sensing systems,safety critical tasks,safety-critical software,sensor noise,Switches,task analysis,telerobotics,variable autonomy}
}

@inproceedings{choLearningPhraseRepresentations2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder}}--{{Decoder}} for {{Statistical Machine Translation}}},
  booktitle = {{{EMNLP}}},
  author = {Cho, Kyunghyun and {van Merri{\"e}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  month = oct,
  pages = {1724--1734},
  address = {Doha, Qatar},
  doi = {10.3115/v1/D14-1179},
  urldate = {2020-07-03}
}

@inproceedings{cholletXceptionDeepLearning2017,
  title = {Xception: {{Deep Learning With Depthwise Separable Convolutions}}},
  shorttitle = {Xception},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Chollet, Francois},
  year = {2017},
  pages = {1251--1258},
  urldate = {2023-02-28}
}

@incollection{Christensen.2016,
  title = {Sensing and Estimation},
  booktitle = {Springer Handbook of Robotics},
  author = {Christensen, Henrik I. and Hager, Gregory D.},
  editor = {Siciliano, Bruno and Khatib, Oussama},
  year = {2016},
  volume = {167},
  pages = {91--112},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-32552-1_5},
  bookpagination = {page},
  isbn = {978-3-319-32550-7}
}

@misc{christianoAISafetyVs2021,
  title = {{{AI}} ``Safety'' vs ``Control'' vs ``Alignment''},
  author = {Christiano, Paul},
  year = {2021},
  month = may,
  journal = {AI Alignment},
  urldate = {2024-03-03},
  abstract = {Defining what I mean by ``AI safety,'' ``AI control,'' and ``value alignment.''},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\2WH8CB8R\ai-safety-vs-control-vs-alignment-2a4b42a863cc.html}
}

@inproceedings{christianoDeepReinforcementLearning2017,
  title = {Deep {{Reinforcement Learning}} from {{Human Preferences}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-10-30},
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. Our approach separates learning the goal from learning the behavior to achieve it. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on about 0.1\% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.}
}

@inproceedings{chromikHumanXAIInteractionReview2021,
  title = {Human-{{XAI Interaction}}: {{A Review}} and {{Design Principles}} for {{Explanation User Interfaces}}},
  shorttitle = {Human-{{XAI Interaction}}},
  booktitle = {Human-{{Computer Interaction}} -- {{INTERACT}} 2021},
  author = {Chromik, Michael and Butz, Andreas},
  editor = {Ardito, Carmelo and Lanzilotti, Rosa and Malizia, Alessio and Petrie, Helen and Piccinno, Antonio and Desolda, Giuseppe and Inkpen, Kori},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {619--640},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-85616-8_36},
  abstract = {The interdisciplinary field of explainable artificial intelligence (XAI) aims to foster human understanding of black-box machine learning models through explanation-generating methods. Although the social sciences suggest that explanation is a social and iterative process between an explainer and an explainee, explanation user interfaces and their user interactions have not been systematically explored in XAI research yet. Therefore, we review prior XAI research containing explanation user interfaces for ML-based intelligent systems and describe different concepts of interaction. Further, we present observed design principles for interactive explanation user interfaces. With our work, we inform designers of XAI systems about human-centric ways to tailor their explanation user interfaces to different target audiences and use cases.},
  isbn = {978-3-030-85616-8},
  langid = {english},
  keywords = {Explainable AI,Explanation user interfaces,Interaction design,Literature review}
}

@article{Chui.2003,
  title = {A New Point Matching Algorithm for Non-Rigid Registration},
  author = {Chui, Haili and Rangarajan, Anand},
  year = {2003},
  journal = {Computer Vision and Image Understanding},
  volume = {89},
  number = {2-3},
  pages = {114--141},
  issn = {10773142},
  doi = {10.1016/S1077-3142(03)00009-2},
  pagination = {page}
}

@inproceedings{cicek3DUNetLearning2016,
  title = {{{3D U-Net}}: {{Learning Dense Volumetric Segmentation}} from {{Sparse Annotation}}},
  shorttitle = {{{3D U-Net}}},
  booktitle = {Medical {{Image Computing}} and {{Computer-Assisted Intervention}} -- {{MICCAI}} 2016},
  author = {{\c C}i{\c c}ek, {\"O}zg{\"u}n and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf},
  editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
  year = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {424--432},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-46723-8_49},
  abstract = {This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases.},
  isbn = {978-3-319-46723-8},
  langid = {english},
  keywords = {3D,Biomedical volumetric image segmentation,Convolutional neural networks,Fully-automated,Semi-automated,Sparse annotation,Xenopus kidney}
}

@inproceedings{cieslakWindTurbineInspectionMaintenance2023,
  title = {Wind-{{Turbine Inspection}}, {{Maintenance}} and {{Repair Robotic System}}},
  booktitle = {{{ASME Turbo Expo}} 2023: {{Turbomachinery Technical Conference}} and {{Exposition}}},
  author = {Cieslak, Chris and Shah, Aksat and Clark, Brodie and Childs, Peter},
  year = {2023},
  month = sep,
  publisher = {American Society of Mechanical Engineers Digital Collection},
  doi = {10.1115/GT2023-101713},
  urldate = {2024-07-27},
  abstract = {Abstract. World-wide wind energy capacity continues to grow with 1870 TW h in 2021, with annual additions of 113 GW in 2020 and 59 GW in 2019 with over 90\% of the growth as a result of offshore wind farm installations. To attain 2050 net zero targets annual capacity additions of 250 GW are expected. Wind turbine assets require regular inspection and maintenance to ensure performance, compliance to standards and insurance requirements. A range of approaches are used in inspection and maintenance including airborne and platform mounted technologies, robots, as well as accessing the blades directly with rope assisted technicians. In addition to reviewing the merits and challenges associated with a range of approaches to offshore horizontal axis wind turbine maintenance this paper describes the design, trialling and in-service operation of a hexapod robot, along with options for its deployment. The robot is capable of deploying a wide range of measuring equipment and maintenance tools. The hexapod gait employed enables the chassis to be an end-effector, avoiding the use of traditional robot arms and the multiple degrees of freedom associated with the robot legs enable the robot to walk on both convex and concave surfaces as well as straddle a leading-edge profile on a wind turbine aerofoil. The robots have demonstrated functionality through multiple blade walks and tasks such as lightning protection system verification, inspections and blade surface treatments. The robot has the capability to work independently or alongside rope-assisted technicians.},
  langid = {english}
}

@inproceedings{Cignoni.2008,
  title = {{{MeshLab}}: An Open-Source Mesh Processing Tool},
  booktitle = {Eurographics Italian Chapter Conference},
  author = {Cignoni, Paolo and Callieri, Marco and Corsini, Massimiliano and Dellepiane, Matteo and Ganovelli, Fabio and Ranzuglia, Guido},
  editor = {{Vittorio Scarano} and {Rosario De Chiara} and {Ugo Erra}},
  year = {2008},
  publisher = {The Eurographics Association},
  doi = {10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136},
  isbn = {978-3-905673-68-5}
}

@inproceedings{cimpoiDescribingTexturesWild2014,
  title = {Describing {{Textures}} in the {{Wild}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea},
  year = {2014},
  month = jun,
  pages = {3606--3613},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2014.461},
  abstract = {Patterns and textures are key characteristics of many natural objects: a shirt can be striped, the wings of a butterfly can be veined, and the skin of an animal can be scaly. Aiming at supporting this dimension in image understanding, we address the problem of describing textures with semantic attributes. We identify a vocabulary of forty-seven texture terms and use them to describe a large dataset of patterns collected "in the wild". The resulting Describable Textures Dataset (DTD) is a basis to seek the best representation for recognizing describable texture attributes in images. We port from object recognition to texture recognition the Improved Fisher Vector (IFV) and Deep Convolutional-network Activation Features (DeCAF), and show that surprisingly, they both outperform specialized texture descriptors not only on our problem, but also in established material recognition datasets. We also show that our describable attributes are excellent texture descriptors, transferring between datasets and tasks, in particular, combined with IFV and DeCAF, they significantly outperform the state-of-the-art by more than 10\% on both FMD and KTH-TIPS-2b benchmarks. We also demonstrate that they produce intuitive descriptions of materials and Internet images.},
  keywords = {attribute,convolutional neural network,Fisher Vector,Image color analysis,Internet,material,Materials,Object recognition,recognition,texture,Vectors,Visualization,Vocabulary},
  file = {C:\Users\benja\Zotero\storage\MI5SJ6HS\6909856.html}
}

@article{cioflanMSRANASMultiScaleResourceAware2020,
  title = {{{MS-RANAS}}: {{Multi-Scale Resource-Aware Neural Architecture Search}}},
  shorttitle = {{{MS-RANAS}}},
  author = {Cioflan, Cristian and Timofte, Radu},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.13940 [cs]},
  eprint = {2009.13940},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {Neural Architecture Search (NAS) has proved effective in offering outperforming alternatives to handcrafted neural networks. In this paper we analyse the benefits of NAS for image classification tasks under strict computational constraints. Our aim is to automate the design of highly efficient deep neural networks, capable of offering fast and accurate predictions and that could be deployed on a low-memory, low-power system-on-chip. The task thus becomes a three-party trade-off between accuracy, computational complexity, and memory requirements. To address this concern, we propose Multi-Scale Resource-Aware Neural Architecture Search (MS-RANAS). We employ a one-shot architecture search approach in order to obtain a reduced search cost and we focus on an anytime prediction setting. Through the usage of multiple-scaled features and early classifiers, we achieved state-of-the-art results in terms of accuracy-speed trade-off.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,I.4.9,I.5.4},
  file = {C:\Users\benja\Zotero\storage\H6LT5DIK\2009.html}
}

@misc{CLHSSection1415,
  title = {{{CLHS}}: {{Section}} 1.4.1.5: {{Designators}}},
  journal = {Common Lisp HyperSpec},
  urldate = {2023-02-14},
  howpublished = {http://clhs.lisp.se/Body/01\_dae.htm},
  file = {C:\Users\benja\Zotero\storage\TP59DFV2\01_dae.html}
}

@misc{CloudCompare,
  title = {{{CloudCompare}}}
}

@misc{CobotsFuerOberflaechenbehandlung2020,
  title = {{Cobots f{\"u}r die Oberfl{\"a}chenbehandlung von Werkst{\"u}cken}},
  year = {2020},
  month = jun,
  journal = {Robotik und Produktion},
  urldate = {2020-10-07},
  abstract = {Immer mehr Betriebe automatisieren Teile ihrer Produktionslinien. Dabei kommen zunehmend auch kollaborierende Roboter zum Einsatz. Durch spezielle Sensoren verf{\"u}gen sie mittlerweile {\"u}ber ein hohes Ma{\ss} an Feinf{\"u}hligkeit und k{\"o}nnen so auch T{\"a}tigkeiten aus{\"u}ben, die zuvor als nicht automatisierbar galten.},
  chapter = {Mensch/Roboter-Kollaboration},
  langid = {ngerman},
  file = {C:\Users\benja\Zotero\storage\WAN4MN8Z\mit-dem-noetigen-feingefuehl.html}
}

@book{CognitiveRobotics2022,
  title = {Cognitive {{Robotics}}},
  year = {2022},
  month = may,
  doi = {10.7551/mitpress/13780.001.0001},
  urldate = {2022-08-01},
  abstract = {The current state of the art in cognitive robotics, covering the challenges of building AI-powered intelligent robots inspired by natural cognitive systems. A n},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\5JTQVBL6\Cognitive-Robotics.html}
}

@article{colemanReducingBarrierEntry2014,
  title = {Reducing the {{Barrier}} to {{Entry}} of {{Complex Robotic Software}}: A {{MoveIt}}! {{Case Study}}},
  shorttitle = {Reducing the {{Barrier}} to {{Entry}} of {{Complex Robotic Software}}},
  author = {Coleman, David T. and Sucan, Ioan A. and Chitta, Sachin and Correll, Nikolaus},
  year = {2014},
  publisher = {Universit{\dbend} degli studi di Bergamo},
  doi = {10.6092/JOSER_2014_05_01_P3},
  urldate = {2023-01-31},
  abstract = {Developing robot agnostic software frameworks involves synthesizing the disparate fields of robotic theory and software engineering while simultaneously accounting for a large variability in hardware designs and control paradigms. As the capabilities of robotic software frameworks increase, the setup difficulty and learning curve for new users also increase. If the entry barriers for configuring and using the software on robots is too high, even the most powerful of frameworks are useless. A growing need exists in robotic software engineering to aid users in getting started with, and customizing, the software framework as necessary for particular robotic applications. In this paper a case study is presented for the best practices found for lowering the barrier of entry in the MoveIt! framework, an open-source tool for mobile manipulation in ROS, that allows users to 1) quickly get basic motion planning functionality with minimal initial setup, 2) automate its configuration and optimization, and 3) easily customize its components. A graphical interface that assists the user in configuring MoveIt! is the cornerstone of our approach, coupled with the use of an existing standardized robot model for input, automatically generated robot-specific configuration files, and a plugin-based architecture for extensibility. These best practices are summarized into a set of barrier to entry design principles applicable to other robotic software. The approaches for lowering the entry barrier are evaluated by usage statistics, a user survey, and compared against our design objectives for their effectiveness to users.},
  langid = {english}
}

@book{colledanchiseBehaviorTreesRobotics2018,
  title = {Behavior {{Trees}} in {{Robotics}} and {{AI}}: {{An Introduction}}},
  shorttitle = {Behavior {{Trees}} in {{Robotics}} and {{AI}}},
  author = {Colledanchise, Michele and {\"O}gren, Petter},
  year = {2018},
  month = jul,
  publisher = {CRC Press},
  address = {Boca Raton},
  doi = {10.1201/9780429489105},
  abstract = {Behavior Trees (BTs) provide a way to structure the behavior of an artificial agent such as a robot or a non-player character in a computer game. Traditional design methods, such as finite state machines, are known to produce brittle behaviors when complexity increases, making it very hard to add features without breaking existing functionality. BTs were created to address this very problem, and enables the creation of systems that are both modular and reactive. Behavior Trees in Robotics and AI: An Introduction provides a broad introduction as well as an in-depth exploration of the topic, and is the first comprehensive book on the use of BTs. This book introduces the subject of BTs from simple topics, such as semantics and design principles, to complex topics, such as learning and task planning. For each topic, the authors provide a set of examples, ranging from simple illustrations to realistic complex behaviors, to enable the reader to successfully combine theory with practice. Starting with an introduction to BTs, the book then describes how BTs relate to, and in many cases, generalize earlier switching structures, or control architectures. These ideas are then used as a foundation for a set of efficient and easy to use design principles. The book then presents a set of important extensions and provides a set of tools for formally analyzing these extensions using a state space formulation of BTs.  With the new analysis tools, the book then formalizes the descriptions of how BTs generalize earlier approaches and shows how BTs can be automatically generated using planning and learning. The final part of the book provides an extended set of tools to capture the behavior of Stochastic BTs, where the outcomes of actions are described by probabilities. These tools enable the computation of both success probabilities and time to completion. This book targets a broad audience, including both students and professionals interested in modeling complex behaviors for robots, game characters, or other AI agents. Readers can choose at which depth and pace they want to learn the subject, depending on their needs and background.},
  isbn = {978-0-429-48910-5}
}

@article{collierImplementingNeuralTuring2018,
  title = {Implementing {{Neural Turing Machines}}},
  author = {Collier, Mark and Beel, Joeran},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.08518 [cs, stat]},
  eprint = {1807.08518},
  primaryclass = {cs, stat},
  urldate = {2020-04-27},
  abstract = {Neural Turing Machines (NTMs) are an instance of Memory Augmented Neural Networks, a new class of recurrent neural networks which decouple computation from memory by introducing an external memory unit. NTMs have demonstrated superior performance over Long Short-Term Memory Cells in several sequence learning tasks. A number of open source implementations of NTMs exist but are unstable during training and/or fail to replicate the reported performance of NTMs. This paper presents the details of our successful implementation of a NTM. Our implementation learns to solve three sequential learning tasks from the original NTM paper. We find that the choice of memory contents initialization scheme is crucial in successfully implementing a NTM. Networks with memory contents initialized to small constant values converge on average 2 times faster than the next best memory contents initialization scheme.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{Colo17,
  title = {Industrial {{Cyberphysical Systems}}},
  author = {Colombo, Armando W and Karnouskos, Stamatis and Kaynak, Okyay and Shi, Yang and Yin, Shen},
  year = {2017}
}

@inproceedings{combaliaUncertaintyEstimationDeep2020,
  title = {Uncertainty {{Estimation}} in {{Deep Neural Networks}} for {{Dermoscopic Image Classification}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Combalia, Marc and Hueto, Ferran and Puig, Susana and Malvehy, Josep and Vilaplana, Veronica},
  year = {2020},
  month = jun,
  pages = {3211--3220},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPRW50498.2020.00380},
  urldate = {2020-12-19},
  abstract = {The high performance of machine learning algorithms for the task of skin lesion classification has been shown over the past few years. However, real-world implementations are still scarce. One of the reasons could be that most methods do not quantify the uncertainty in the predictions and are not able to detect data that is anomalous or significantly different from that used in training, which may lead to a lack of confidence in the automated diagnosis or errors in the interpretation of results. In this work, we explore the use of uncertainty estimation techniques and metrics for deep neural networks based on Monte-Carlo sampling and apply them to the problem of skin lesion classification on data from ISIC Challenges 2018 and 2019. Our results show that uncertainty metrics can be successfully used to detect difficult and out-of-distribution samples.},
  isbn = {978-1-72819-360-1},
  langid = {english}
}

@inproceedings{coninckLearningGraspArbitrary2019,
  title = {Learning to {{Grasp Arbitrary Household Objects}} from a {{Single Demonstration}}},
  booktitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Coninck, Elias De and Verbelen, Tim and Molle, Pieter Van and Simoens, Pieter and IDLab, Bart Dhoedt},
  year = {2019},
  month = nov,
  pages = {2372--2377},
  issn = {2153-0866},
  doi = {10.1109/IROS40897.2019.8967638},
  abstract = {Upon the advent of Industry 4.0, collaborative robotics and intelligent automation gain more and more traction for enterprises to improve their production processes. In order to adapt to this trend, new programming, learning and collaborative techniques are investigated. Program-bydemonstration is one of the techniques that aim to reduce the burden of manually programming collaborative robots. However, this is often limited to teaching to grasp at a certain position, rather than grasping a certain object. In this paper, we propose a method that learns to grasp an arbitrary object from visual input. While other learning-based approaches for robotic grasping require collecting a large dataset, manually or automatically labeled in a real or simulated world, our approach requires a single demonstration. We present results on grasping various objects with the Franka Panda collaborative robot after capturing a single image from a wrist mounted RGB camera. From this image we learn a robot controller with a convolutional neural network to adapt to changes in the object's position and rotation with less than 5 minutes of training time on a NVIDIA Titan X GPU, achieving over 90\% grasp success rate.},
  file = {C:\Users\benja\Zotero\storage\89GGE7HR\8967638.html}
}

@inproceedings{connellDynamicPathPlanning2017,
  title = {Dynamic Path Planning and Replanning for Mobile Robots Using {{RRT}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author = {Connell, D. and La, H. M.},
  year = {2017},
  month = oct,
  pages = {1429--1434},
  doi = {10.1109/SMC.2017.8122814},
  abstract = {It is necessary for a mobile robot to be able to efficiently plan a path from its starting, or current location to a desired goal location. This is a trivial task when the environment is static. However, the operational environment of the robot is rarely static, and it often has many moving obstacles. The robot may encounter one, or many of these unknown and unpredictable moving obstacles. The robot will need to decide how to proceed when one of these obstacles is obstructing it's path. A method of dynamic replanning using RRT* is presented. The robot will modify it's current plan when an unknown random moving obstacle obstructs the path. Various experimental results show the effectiveness of the proposed method.},
  keywords = {collision avoidance,Collision avoidance,current location,desired goal location,dynamic path planning,dynamic path replanning,Heuristic algorithms,mobile robot,mobile robots,operational environment,path planning,Path planning,Planning,Robot sensing systems,RRT*,trivial task,unknown moving obstacles,unknown random moving obstacle,unpredictable moving obstacles,Vegetation},
  file = {C:\Users\benja\Zotero\storage\SAAHUXQJ\8122814.html}
}

@article{cooperTractabilityExplainingClassifier2023,
  title = {Tractability of Explaining Classifier Decisions},
  author = {Cooper, Martin C. and {Marques-Silva}, Jo{\~a}o},
  year = {2023},
  month = mar,
  journal = {Artificial Intelligence},
  volume = {316},
  pages = {103841},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2022.103841},
  urldate = {2024-03-04},
  abstract = {Explaining decisions is at the heart of explainable AI. We investigate the computational complexity of providing a formally-correct and minimal explanation of a decision taken by a classifier. In the case of threshold (i.e. score-based) classifiers, we show that a complexity dichotomy follows from the complexity dichotomy for languages of cost functions. In particular, submodular classifiers allow tractable explanation of positive decisions, but not negative decisions (assuming P{$\neq$}NP). This is an example of the possible asymmetry between the complexity of explaining positive and negative decisions of a particular classifier. Nevertheless, there are large families of classifiers for which explaining both positive and negative decisions is tractable, such as monotone or modular (e.g. linear) classifiers. We extend the characterisation of tractable cases to constrained classifiers (when there are constraints on the possible input vectors) and to the search for contrastive rather than abductive explanations. Indeed, we show that tractable classes coincide for abductive and contrastive explanations in the constrained or unconstrained settings. We show the intractability of returning a set of k diverse explanations even for linear classifiers and k=2. Finding a minimum-cardinality explanation is tractable for the family of modular classifiers, i.e. when the score function is the sum of unary functions, but becomes intractable when any non-modular function is also allowed.},
  keywords = {Explanations,Machine learning,Tractability,Weighted constraint satisfaction},
  file = {C:\Users\benja\Zotero\storage\8NIXJ36K\S0004370222001813.html}
}

@inproceedings{cordtsCityscapesDatasetSemantic2016,
  title = {The {{Cityscapes Dataset}} for {{Semantic Urban Scene Understanding}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  year = {2016},
  pages = {3213--3223},
  urldate = {2023-03-06}
}

@misc{CoronaBeschleunigtDigitalisierung,
  title = {{Corona beschleunigt Digitalisierung und Automatisierung}},
  journal = {all-electronics},
  urldate = {2022-01-10},
  abstract = {Nach einer Umfrage der Wirtschaftspr{\"u}fungsgesellschaft Ernst \& Young (EY) im M{\"a}rz unter weltweit 2.900 Entscheidern in {\"u}berwiegend gro{\ss}en Unternehmen, davon 145 aus Deutschland, kommt in diesen Wochen der Corona-Krise alles auf den Pr{\"u}fstand.},
  howpublished = {https://www.all-electronics.de/markt/corona-beschleunigt-digitalisierung-und-automatisierung.html},
  langid = {ngerman},
  file = {C:\Users\benja\Zotero\storage\QSC32N2G\corona-beschleunigt-digitalisierung-und-automatisierung.html}
}

@inproceedings{corsiFormalVerificationNeural2021,
  title = {Formal Verification of Neural Networks for Safety-Critical Tasks in Deep Reinforcement Learning},
  booktitle = {Proceedings of the {{Thirty-Seventh Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Corsi, Davide and Marchesini, Enrico and Farinelli, Alessandro},
  year = {2021},
  month = dec,
  pages = {333--343},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2022-04-30},
  abstract = {In the last years, neural networks achieved groundbreaking successes in a wide variety of applications. However, for safety critical tasks, such as robotics and healthcare, it is necessary to provide some specific guarantees before the deployment in a real world context. Even in these scenarios, where high cost equipment and human safety are involved, the evaluation of the models is usually performed with the standard metrics (i.e., cumulative reward or success rate). In this paper, we introduce a novel metric for the evaluation of models in safety critical tasks, the violation rate. We build our work upon the concept of formal verification for neural networks, providing a new formulation for the safety properties that aims to ensure that the agent always makes rational decisions. To perform this evaluation, we present ProVe (Property Verifier), a novel approach based on the interval algebra, designed for the analysis of our novel behavioral properties. We apply our method to different domains (i.e., mapless navigation for mobile robots, trajectory generation for manipulators, and the standard ACAS benchmark). Results show that the violation rate computed by ProVe provides a good evaluation for the safety of trained models.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\AT87E8US\Corsi et al. - 2021 - Formal verification of neural networks for safety-.pdf}
}

@inproceedings{cosierUnifyingVariationalFramework2024,
  title = {A {{Unifying Variational Framework}} for {{Gaussian Process Motion Planning}}},
  booktitle = {Proceedings of {{The}} 27th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Cosier, Lucas C. and Iordan, Rares and Zwane, Sicelukwanda N. T. and Franzese, Giovanni and Wilson, James T. and Deisenroth, Marc and Terenin, Alexander and Bekiroglu, Yasemin},
  year = {2024},
  month = apr,
  pages = {1315--1323},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-09-06},
  abstract = {To control how a robot moves, motion planning algorithms must compute paths in high-dimensional state spaces while accounting for physical constraints related to motors and joints, generating smooth and stable motions, avoiding obstacles, and preventing collisions. A motion planning algorithm must therefore balance competing demands, and should ideally incorporate uncertainty to handle noise, model errors, and facilitate deployment in complex environments. To address these issues, we introduce a framework for robot motion planning based on variational Gaussian processes, which unifies and generalizes various probabilistic-inference-based motion planning algorithms, and connects them with optimization-based planners. Our framework provides a principled and flexible way to incorporate equality-based, inequality-based, and soft motion-planning constraints during end-to-end training, is straightforward to implement, and provides both interval-based and Monte-Carlo-based uncertainty estimates. We conduct experiments using different environments and robots, comparing against baseline approaches based on the feasibility of the planned paths, and obstacle avoidance quality. Results show that our proposed approach yields a good balance between success rates and path quality.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\TMAVTL9D\Cosier et al. - 2024 - A Unifying Variational Framework for Gaussian Process Motion Planning.pdf}
}

@article{costanzoManipulationPlanningControl2020,
  title = {Manipulation {{Planning}} and {{Control}} for {{Shelf Replenishment}}},
  author = {Costanzo, Marco and Stelter, Simon and Natale, Ciro and Pirozzi, Salvatore and Bartels, Georg and Maldonado, Alexis and Beetz, Michael},
  year = {2020},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {2},
  pages = {1595--1601},
  issn = {2377-3766},
  doi = {10.1109/LRA.2020.2969179},
  abstract = {Manipulation planning and control are relevant building blocks of a robotic system and their tight integration is a key factor to improve robot autonomy and allows robots to perform manipulation tasks of increasing complexity, such as those needed in the in-store logistics domain. Supermarkets contain a large variety of objects to be placed on the shelf layers with specific constraints, doing this with a robot is a challenge and requires a high dexterity. However, an integration of reactive grasping control and motion planning can allow robots to perform such tasks even with grippers with limited dexterity. The main contribution of the letter is a novel method for planning manipulation tasks to be executed using a reactive control layer that provides more control modalities, i.e., slipping avoidance and controlled sliding. Experiments with a new force/tactile sensor equipping the gripper of a mobile manipulator show that the approach allows the robot to successfully perform manipulation tasks unfeasible with a standard fixed grasp.},
  keywords = {Control systems,Force,Grippers,manipulation planning,Motion and path planning,Planning,Robot sensing systems,Task analysis},
  file = {C:\Users\benja\Zotero\storage\BT53NXNT\8968346.html}
}

@misc{covariantCovariantBrain2024,
  type = {Company Website},
  title = {Covariant {{Brain}}},
  author = {{Covariant}},
  year = {2024},
  journal = {Covariant},
  urldate = {2024-09-18},
  abstract = {Covariant builds and delivers Robotics Foundation Models into the real world, meeting the reliability and flexibility required by the world's leading retailers and logistics providers.},
  howpublished = {https://covariant.ai/covariant-brain/},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\KWJ7IRJE\covariant-brain.html}
}

@misc{covariantRFM1WorldModel2024,
  title = {{{RFM-1}}: {{A}} World Model That Understands Physics},
  shorttitle = {{{RFM-1}}},
  author = {{Covariant}},
  year = {2024},
  month = mar,
  urldate = {2024-07-20},
  abstract = {In robotics, millimeters can be the difference between ``completely unusable'' and ``superhuman.'' The fine-grained predictions that RFM-1 provides can help prevent costly mistakes in the real world and further optimize robotic operations to maximize throughput.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\2IUEAKNV\rfm-1-a-world-model-that-understands-physics.html}
}

@book{craigIntroductionRoboticsMechanics2004,
  title = {{Introduction to Robotics: Mechanics and Control}},
  shorttitle = {{Introduction to Robotics}},
  author = {Craig, John J.},
  year = {2004},
  month = jul,
  edition = {3},
  publisher = {Pearson},
  address = {Upper Saddle River, N.J},
  abstract = {Now in its third edition, Introduction to Robotics by John J. Craig provides readers with real-world practicality with underlying theory presented. With one half of the material from traditional mechanical engineering material, one fourth control theoretical material, and one fourth computer science, the book covers rigid-body transformations, forward and inverse positional kinematics, velocities and Jacobians of linkages, dynamics, linear control, non-linear control, force control methodologies, mechanical design aspects and programming of robots.  For engineers.},
  isbn = {978-0-201-54361-2},
  langid = {Englisch}
}

@book{craikNatureExplanation1967,
  title = {The {{Nature}} of {{Explanation}}},
  author = {Craik, K. J. W.},
  year = {1967},
  month = oct,
  publisher = {CUP Archive},
  abstract = {In his brilliant and tragically brief career, Kenneth Craik anticipated certain ideas which since his death in 1945 have found wide acceptance. As one of the first to realise that machines share with the brain certain principles of functioning, Craik was a pioneer in the development of physiological psychology and cybernetics. Craik published only one complete work of any length, this essay on The Nature of Explanation. Here he considers thought as a term for the conscious working of a highly complex machine, viewing the brain as a calculating machine which can model or parallel external events, a process that is the basic feature of thought and explanation. He applies this view to a number of psychological and philosophical problems (such as paradox and illusion) and suggests possible experiments to test his theory. This book is of interest to those concerned with the concepts of brain and mind.},
  googlebooks = {wT04AAAAIAAJ},
  isbn = {978-0-521-09445-0},
  langid = {english},
  keywords = {Psychology / Cognitive Psychology & Cognition,Psychology / General}
}

@inproceedings{Cretu.2010,
  title = {Deformable Object Segmentation and Contour Tracking in Image Sequences Using Unsupervised Networks},
  booktitle = {2010 Canadian Conference on Computer and Robot Vision},
  author = {Cretu, Ana-Maria and Petriu, Emil M. and Payeur, Pierre and Khalil, Fouad F.},
  year = {2010},
  pages = {277--284},
  publisher = {IEEE},
  doi = {10.1109/CRV.2010.43},
  bookpagination = {page},
  isbn = {978-1-4244-6963-5}
}

@article{cubricDriversBarriersSocial2020,
  title = {Drivers, Barriers and Social Considerations for {{AI}} Adoption in Business and Management: {{A}} Tertiary Study},
  shorttitle = {Drivers, Barriers and Social Considerations for {{AI}} Adoption in Business and Management},
  author = {Cubric, Marija},
  year = {2020},
  month = aug,
  journal = {Technology in Society},
  volume = {62},
  pages = {101257},
  issn = {0160-791X},
  doi = {10.1016/j.techsoc.2020.101257},
  urldate = {2024-09-08},
  abstract = {The number of academic papers in the area of Artificial Intelligence (AI) and its applications across business and management domains has risen significantly in the last decade, and that rise has been followed by an increase in the number of systematic literature reviews. The aim of this study is to provide an overview of existing systematic reviews in this growing area of research and to synthesise the findings related to drivers, barriers and social implications of the AI adoption in business and management. The methodology used for this tertiary study is based on Kitchenham and Charter's guidelines [14], resulting in a selection of 30 reviews published between 2005 and 2019 which are reporting results of 2021 primary studies. These reviews cover the AI adoption across various business sectors (healthcare, information technology, energy, agriculture, apparel industry, engineering, smart cities, tourism and transport), management and business functions (HR, customer services, supply chain, health and safety, project management, decision-support, systems management and technology adoption). While the drivers for the AI adoption in these areas are mainly economic, the barriers are related to the technical aspects (e.g. availability of data, reusability of models) as well as the social considerations such as, increased dependence on non-humans, job security, lack of knowledge, safety, trust and lack of multiple stakeholders'perspectives. Very few reviews outside of the healthcare management domain consider human, organisational and wider societal factors of the AI adoption. In addition to increased focus on social implications of AI, the reviews are recommending more rigorous evaluation, increased use of hybrid solutions (AI and non-AI) and multidisciplinary approach to AI design and evaluation. Furthermore, this study found that there is a lack of systematic reviews in some of the early AI adoption sectors such as financial industry and retail.},
  keywords = {Artificial intelligence,Business,Machine learning,Management,Systematic literature review,Tertiary study},
  file = {C\:\\Users\\benja\\Zotero\\storage\\TQ9KZW7Z\\Cubric - 2020 - Drivers, barriers and social considerations for AI adoption in business and management A tertiary s.pdf;C\:\\Users\\benja\\Zotero\\storage\\MV4FHS5L\\S0160791X19307171.html}
}

@article{cuomoScientificMachineLearning2022,
  title = {Scientific {{Machine Learning Through Physics}}--{{Informed Neural Networks}}: {{Where}} We Are and {{What}}'s {{Next}}},
  shorttitle = {Scientific {{Machine Learning Through Physics}}--{{Informed Neural Networks}}},
  author = {Cuomo, Salvatore and Di Cola, Vincenzo Schiano and Giampaolo, Fabio and Rozza, Gianluigi and Raissi, Maziar and Piccialli, Francesco},
  year = {2022},
  month = jul,
  journal = {Journal of Scientific Computing},
  volume = {92},
  number = {3},
  pages = {88},
  issn = {1573-7691},
  doi = {10.1007/s10915-022-01939-z},
  urldate = {2024-04-30},
  abstract = {Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs. This novel methodology has arisen as a multi-task learning framework in which a NN must fit observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla PINN, as well as many other variants, such as physics-constrained neural networks (PCNN), variational hp-VPINN, and conservative PINN (CPINN). The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.},
  langid = {english},
  keywords = {Deep Neural Networks,Nonlinear equations,Numerical methods,Partial Differential Equations,Physics-Informed Neural Networks,Scientific Machine Learning,Uncertainty}
}

@inproceedings{Curless.1996,
  title = {A Volumetric Method for Building Complex Models from Range Images},
  booktitle = {Proceedings of the 23rd Annual Conference on {{Computer}} Graphics and Interactive Techniques - {{SIGGRAPH}} '96},
  author = {Curless, Brian and Levoy, Marc},
  editor = {Fujii, John},
  year = {1996},
  pages = {303--312},
  publisher = {ACM Press},
  address = {New York, New York, USA},
  doi = {10.1145/237170.237269},
  bookpagination = {page},
  isbn = {0-89791-746-4}
}

@article{cutting-decelleISO15531MANDATE2007,
  title = {{{ISO}} 15531 {{MANDATE}}: {{A Product-process-resource}} Based {{Approach}} for {{Managing Modularity}} in {{Production Management}}},
  shorttitle = {{{ISO}} 15531 {{MANDATE}}},
  author = {{Cutting-Decelle}, A.F. and Young, R.I.M. and Michel, J.J. and Grangel, R. and Le Cardinal, J. and Bourey, J.P.},
  year = {2007},
  month = jun,
  journal = {Concurrent Engineering},
  volume = {15},
  number = {2},
  pages = {217--235},
  publisher = {SAGE Publications Ltd STM},
  issn = {1063-293X},
  doi = {10.1177/1063293X07079329},
  urldate = {2024-03-08},
  abstract = {Managing modularity and commonality in product development more and more needs modularity and commonality in the production process, with the objectives of reducing manufacturing costs, time to market and improving quality. A critical issue is the way of managing data, information and knowledge: data most of the time structured according to data models, often using proprietary formats, leading to consistency problems for the exchanges. The use of international standards is a good way of improving quality of the information systems used in production management, since they facilitate interoperability of the software tools used. They also contribute to the integration of the production process in a product life cycle management-based approach. This study presents the ISO 15531 MANDATE standard for the exchanges of industrial manufacturing management data. In terms of industrial maturity, MANDATE is a new standard, whose development is based on research work done by the authors and whose parts have not reached the IS status (necessary for sake of stability) at the same time. For this reason, the different models proposed by the standard have not been implemented altogether at the same time. Indeed numerous standards do exist in the domain of production information management, however the information models proposed are not always compatible in between them, the vocabulary used is not defined in the same way even though the terms used are the same: ontology-based approaches are sometimes necessary to find the common `essence' of the information handled, but they can be integrated in software interfaces, thus making easier to convey a higher level of semantics in the exchanges. This study presents one of those approaches, defined in the INTEROP NoE EC funded project.},
  langid = {english}
}

@misc{Cybe10,
  title = {Cyber-{{Physical Systems}} ({{CPS}})},
  year = {2010},
  publisher = {National Science Foundation},
  urldate = {2017-06-23}
}

@article{Cybe89,
  title = {Approximation by {{Superpositions}} of a {{Sigmoidal Function}}},
  author = {Cybenko, George},
  year = {1989},
  number = {2},
  pages = {303--314}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, George},
  year = {1989},
  journal = {Mathematics of Control, Signals and Systems},
  volume = {2},
  pages = {303--314},
  langid = {english}
}

@article{Dagum.1998,
  title = {{{OpenMP}}: An Industry Standard {{API}} for Shared-Memory Programming},
  author = {Dagum, L. and Menon, R.},
  year = {1998},
  journal = {IEEE Computational Science and Engineering},
  volume = {5},
  number = {1},
  pages = {46--55},
  issn = {10709924},
  doi = {10.1109/99.660313},
  lastvisited = {2019-07-22},
  pagination = {page}
}

@book{Dahmen.2008,
  title = {Numerik F{\"u}r Ingenieure Und Naturwissenschaftler},
  author = {Dahmen, Wolfgang and Reusken, Arnold},
  year = {2008},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-76493-9},
  isbn = {978-3-540-76492-2}
}

@article{daiPlanningJerkOptimizedTrajectory2020,
  title = {Planning {{Jerk-Optimized Trajectory With Discrete Time Constraints}} for {{Redundant Robots}}},
  author = {Dai, C. and Lefebvre, S. and Yu, K.-M. and Geraedts, J. M. P. and Wang, C. C. L.},
  year = {2020},
  month = oct,
  journal = {IEEE Transactions on Automation Science and Engineering},
  volume = {17},
  number = {4},
  pages = {1711--1724},
  issn = {1558-3783},
  doi = {10.1109/TASE.2020.2974771},
  abstract = {We present a method for effectively planning the motion trajectory of robots in manufacturing tasks, the tool paths of which are usually complex and have a large number of discrete time constraints as waypoints. Kinematic redundancy also exists in these robotic systems. The jerk of motion is optimized in our trajectory planning method at the meanwhile of fabrication process to improve the quality of fabrication. Our method is based on a sampling strategy and consists of two major parts. After determining an initial path by graph search, a greedy algorithm is adopted to optimize a path by locally applying adaptive filers in the regions with large jerks. The filtered result is obtained by numerical optimization. In order to achieve efficient computation, an adaptive sampling method is developed for learning a collision-indication function that is represented as a support-vector machine. Applications in robot-assisted 3-D printing are given in this article to demonstrate the functionality of our approach. Note to Practitioners-In robot-assisted manufacturing applications, robotic arms are employed to realize the motion of workpieces (or machining tools) specified as a sequence of waypoints with the positions of tool tip and the tool orientations constrained. The required degree of freedom (DOF) is often less than the robotic hardware system (e.g., a robotic arm has six-DOF). Specifically, rotations of the workpiece around the axis of a tool can be arbitrary (see Fig. 1 for an example). By using this redundancy, i.e., there are many possible poses of a robotic arm to realize a given waypoint, the trajectory of robots can be optimized to consider the performance of motion in velocity, acceleration, and jerk in the joint space. In addition, when fabricating complex models, each tool path can have a large amount of waypoints. It is crucial for a motion planning algorithm to compute a smooth and collision-free trajectory of robot to improve the fabrication quality. The time taken by the planning algorithm should not significantly lengthen the total manufacturing time; ideally, it would remain hidden as computing motions for a layer can be done while the previous layer is printing. The method presented in this article provides an efficient framework to tackle this problem. The framework has been well tested on our robot-assisted additive manufacturing system to demonstrate its effectiveness and can be generally applied to other robot-assisted manufacturing systems.},
  keywords = {adaptive sampling method,Collision avoidance,collision-indication function,computing motions,control engineering computing,discrete time constraints,Discrete time constraints,Discrete-time systems,Fabrication,fabrication process,graph search,greedy algorithm,greedy algorithms,Greedy algorithms,industrial robots,kinematic redundancy,learning (artificial intelligence),machine tools,machining,machining tools,manufacturing systems,motion planning algorithm,motion trajectory,numerical optimization,optimisation,planning jerk-optimized trajectory,Redundancy,redundant manipulators,redundant robots,robot-assisted 3-D printing,robot-assisted additive manufacturing system,robotic arm,robotic fabrication,robotic hardware system,Robots,sampling methods,support vector machines,Support vector machines,support-vector machine,three-dimensional printing,Trajectory,trajectory control,trajectory planning,trajectory planning method},
  file = {C:\Users\benja\Zotero\storage\KE8F6X57\9025760.html}
}

@misc{dalrympleGuaranteedSafeAI2024,
  title = {Towards {{Guaranteed Safe AI}}: {{A Framework}} for {{Ensuring Robust}} and {{Reliable AI Systems}}},
  shorttitle = {Towards {{Guaranteed Safe AI}}},
  author = {Dalrymple, David "davidad" and Skalse, Joar and Bengio, Yoshua and Russell, Stuart and Tegmark, Max and Seshia, Sanjit and Omohundro, Steve and Szegedy, Christian and Goldhaber, Ben and Ammann, Nora and Abate, Alessandro and Halpern, Joe and Barrett, Clark and Zhao, Ding and {Zhi-Xuan}, Tan and Wing, Jeannette and Tenenbaum, Joshua},
  year = {2024},
  month = jul,
  number = {arXiv:2405.06624},
  eprint = {2405.06624},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.06624},
  urldate = {2024-09-22},
  abstract = {Ensuring that AI systems reliably and robustly avoid harmful or dangerous behaviours is a crucial challenge, especially for AI systems with a high degree of autonomy and general intelligence, or systems used in safety-critical contexts. In this paper, we will introduce and define a family of approaches to AI safety, which we will refer to as guaranteed safe (GS) AI. The core feature of these approaches is that they aim to produce AI systems which are equipped with high-assurance quantitative safety guarantees. This is achieved by the interplay of three core components: a world model (which provides a mathematical description of how the AI system affects the outside world), a safety specification (which is a mathematical description of what effects are acceptable), and a verifier (which provides an auditable proof certificate that the AI satisfies the safety specification relative to the world model). We outline a number of approaches for creating each of these three core components, describe the main technical challenges, and suggest a number of potential solutions to them. We also argue for the necessity of this approach to AI safety, and for the inadequacy of the main alternative approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\benja\\Zotero\\storage\\LHGTZBRC\\Dalrymple et al. - 2024 - Towards Guaranteed Safe AI A Framework for Ensuring Robust and Reliable AI Systems.pdf;C\:\\Users\\benja\\Zotero\\storage\\U6EYE46P\\2405.html}
}

@article{dasariRoboNetLargeScaleMultiRobot2019,
  title = {{{RoboNet}}: {{Large-Scale Multi-Robot Learning}}},
  shorttitle = {{{RoboNet}}},
  author = {Dasari, Sudeep and Ebert, Frederik and Tian, Stephen and Nair, Suraj and Bucher, Bernadette and Schmeckpeper, Karl and Singh, Siddharth and Levine, Sergey and Finn, Chelsea},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.11215 [cs]},
  eprint = {1910.11215},
  primaryclass = {cs},
  urldate = {2019-11-03},
  abstract = {Robot learning has emerged as a promising tool for taming the complexity and diversity of the real world. Methods based on high-capacity models, such as deep networks, hold the promise of providing effective generalization to a wide range of open-world environments. However, these same methods typically require large amounts of diverse training data to generalize effectively. In contrast, most robotic learning experiments are small-scale, single-domain, and single-robot. This leads to a frequent tension in robotic learning: how can we learn generalizable robotic controllers without having to collect impractically large amounts of data for each separate experiment? In this paper, we propose RoboNet, an open database for sharing robotic experience, which provides an initial pool of 15 million video frames, from 7 different robot platforms, and study how it can be used to learn generalizable models for vision-based robotic manipulation. We combine the dataset with two different learning algorithms: visual foresight, which uses forward video prediction models, and supervised inverse models. Our experiments test the learned algorithms' ability to work across new objects, new tasks, new scenes, new camera viewpoints, new grippers, or even entirely new robots. In our final experiment, we find that by pre-training on RoboNet and fine-tuning on data from a held-out Franka or Kuka robot, we can exceed the performance of a robot-specific training approach that uses 4x-20x more data. For videos and data, see the project webpage: https://www.robonet.wiki/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\B4RB86RE\1910.html}
}

@inproceedings{dastiderRETROReactiveTrajectory2024,
  title = {{{RETRO}}: {{Reactive Trajectory Optimization}} for {{Real-Time Robot Motion Planning}} in {{Dynamic Environments}}},
  shorttitle = {{{RETRO}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Dastider, Apan and Fang, Hao and Lin, Mingjie},
  year = {2024},
  month = may,
  pages = {8764--8770},
  doi = {10.1109/ICRA57147.2024.10610542},
  urldate = {2024-09-06},
  abstract = {Reactive trajectory optimization for robotics presents formidable challenges, demanding the rapid generation of purposeful robot motion in complex and swiftly changing dynamic environments. While much existing research predominantly addresses robotic motion planning with predefined objectives, emerging problems in robotic trajectory optimization frequently involve dynamically evolving objectives and stochastic motion dynamics. However, effectively addressing such reactive trajectory optimization challenges for robot manipulators proves difficult due to inefficient, high-dimensional trajectory representations and a lack of consideration for time optimization.In response, we introduce a novel trajectory optimization framework called RETRO. RETRO employs adaptive optimization techniques that span both spatial and temporal dimensions. As a result, it achieves a remarkable computing complexity of O(T2.4)+O(Tn2), a significant improvement over the traditional application of DDP, which leads to a complexity of O(n4) when reasonable time step sizes are used. To evaluate RETRO's performance in terms of error, we conducted a comprehensive analysis of its regret bounds, comparing it to an Oracle value function obtained through an Oracle trajectory optimization algorithm. Our analytical findings demonstrate that RETRO's total regret can be upper-bounded by a function of the chosen time step size. Moreover, our approach delivers smoothly optimized robot trajectories within the joint space, offering flexibility and adaptability for various tasks. It can seamlessly integrate task-specific requirements such as collision avoidance while maintaining real-time control rates. We validate the effectiveness of our framework through extensive simulations and real-world robot experiments in closed-loop manipulation scenarios.For further details and supplementary materials, please visit: https://sites.google.com/view/retro-optimal-control/home},
  keywords = {Collision avoidance,Complexity theory,Dynamics,Planning,Real-time systems,Robot motion,Time factors},
  file = {C:\Users\benja\Zotero\storage\4IPVILXF\10610542.html}
}

@article{dautenhahnSociallyIntelligentRobots2007,
  title = {Socially Intelligent Robots: Dimensions of Human--Robot Interaction},
  shorttitle = {Socially Intelligent Robots},
  author = {Dautenhahn, Kerstin},
  year = {2007},
  month = feb,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  publisher = {The Royal SocietyLondon},
  doi = {10.1098/rstb.2006.2004},
  urldate = {2024-07-22},
  abstract = {Social intelligence in robots has a quite recent history in artificial intelligence and robotics. However, it has become increasingly apparent that social and interactive skills are necessary requirements in many application areas and contexts where ...},
  copyright = {{\copyright} 2007 The Royal Society},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\PLUNP8SE\rstb.2006.html}
}

@inproceedings{davidorRobotProgrammingGenetic1990,
  title = {Robot Programming with a Genetic Algorithm},
  booktitle = {{{COMPEURO}}'90: {{Proceedings}} of the 1990 {{IEEE International Conference}} on {{Computer Systems}} and {{Software Engineering}} - {{Systems Engineering Aspects}} of {{Complex Computerized Systems}}},
  author = {Davidor, Y.},
  year = {1990},
  month = may,
  pages = {186--191},
  doi = {10.1109/CMPEUR.1990.113625},
  abstract = {Robot systems are an archetype of complex process control systems where valid production programs may have any number ({$>$}2) of process commands which are difficult to optimize with classical methods. Robot trajectories can be programmed automatically by a genetic algorithm, providing the algorithm will consider the order and varying lengths of trajectories. A genetic, adaptive, heuristic algorithm which is designed to optimize robot trajectories is described. Though the model presented is a general model for redundant structures and could represent any n-link structures, it was applied to a 3-link structure. The performance of the genetic algorithm shows characteristic improvements when compared with that of a hill-climb and random search algorithm.{$<>$}},
  keywords = {Control systems,Genetic algorithms,Manufacturing,Mathematics,Optimal control,Optimization methods,Organisms,Process control,Robot programming,Robotics and automation},
  file = {C:\Users\benja\Zotero\storage\WNAYA3DK\113625.html}
}

@article{davisSolvingInverseProblems1997,
  title = {Solving Inverse Problems by {{Bayesian}} Neural Network Iterative Inversion with Ground Truth Incorporation},
  author = {Davis, D. T. and {Jenq-Neng Hwang}},
  year = {1997},
  month = nov,
  journal = {IEEE Transactions on Signal Processing},
  volume = {45},
  number = {11},
  pages = {2749--2757},
  issn = {1053-587X},
  doi = {10.1109/78.650101},
  abstract = {Neural networks have long been applied to inverse parameter retrieval problems. The literature documents a development from the use of neural networks as explicit inverses to neural network iterative inversion (NNII) and, finally, to Bayesian neural network iterative inversion (BNNII), which adds a Bayesian superstructure to NNII. Inverse problems have been often considered ill posed, i.e. the statement of the problem does not thoroughly constrain the solution space. BNNII takes advantage of this lack of information by adding additional informative constraints to the problem solution using Bayesian methodology. This paper extends BNNII, showing how ground truth information, information regarding the particular parameter contour under reconstruction, and information regarding the underlying physical process, can be seamlessly added to the problem solution. Remote sensing problems afford opportunities for inclusion of ground truth information, prior probabilities, noise distributions, and other informative constraints within a Bayesian probabilistic framework. We apply these Bayesian methods to a synthetic remote sensing problem, showing that the addition of ground truth information, which is naturally included through Bayesian modeling, provides a significant performance improvement.},
  keywords = {Bayes methods,Bayesian methods,Bayesian neural network iterative inversion,Bayesian superstructure,Context modeling,geophysical signal processing,ground truth incorporation,ground truth information,Image reconstruction,Information processing,inverse parameter retrieval problems,inverse problems,Inverse problems,iterative methods,NASA,Neural networks,noise distributions,Noise measurement,parameter contour,parameter estimation,prior probabilities,Probability distribution,reconstruction,remote sensing,Remote sensing,solution space}
}

@misc{dawson-haggertyTrimesh2020,
  title = {Trimesh},
  author = {{Dawson-Haggerty}, Michael},
  year = {2020},
  month = oct,
  urldate = {2020-10-12},
  abstract = {Python library for loading and using triangular meshes.},
  copyright = {MIT License         ,                 MIT License},
  keywords = {geometry,mesh,python,triangular-meshes}
}

@inproceedings{DBLP:conf/corl/FlorenceMT18,
  title = {Dense Object Nets: {{Learning}} Dense Visual Object Descriptors by and for Robotic Manipulation},
  booktitle = {{{CoRL}}},
  author = {Florence, Peter R. and Manuelli, Lucas and Tedrake, Russ},
  year = {2018},
  series = {Proceedings of Machine Learning Research},
  volume = {87},
  pages = {373--385},
  publisher = {PMLR},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/corl/FlorenceMT18.bib},
  timestamp = {Wed, 03 Apr 2019 18:17:24 +0200}
}

@inproceedings{DBLP:conf/icsym/Dijkstra77,
  title = {Programming: {{From Craft}} to {{Scientific Discipline}}},
  booktitle = {Proceedings of the {{International}}                   {{Computing Symposium}} 1977},
  author = {Dijkstra, Edsger W.},
  editor = {Morlet, E. and Ribbens, D.},
  year = {1977},
  month = apr,
  pages = {23--30},
  publisher = {North-Holland},
  address = {Li{\`e}ge, Belgium},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Tue, 01 Apr 2003 10:45:57 +0200}
}

@article{DBLP:journals/corr/abs-2107-02295,
  title = {A Review of Explainable Artificial Intelligence in Manufacturing},
  author = {Sofianidis, Georgios and Rozanec, Joze M. and Mladenic, Dunja and Kyriazis, Dimosthenis},
  year = {2021},
  journal = {CoRR},
  volume = {abs/2107.02295},
  eprint = {2107.02295},
  archiveprefix = {arXiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/abs-2107-02295.bib},
  timestamp = {Wed, 07 Jul 2021 15:23:11 +0200}
}

@book{deaconSymbolicSpeciesCoevolution1998,
  title = {{The Symbolic Species: The Co-evolution of Language and the Brain}},
  shorttitle = {{The Symbolic Species}},
  author = {Deacon, Terrence W.},
  year = {1998},
  month = apr,
  edition = {Illustrated Edition},
  publisher = {W. W. Norton \& Company},
  address = {New York, NY},
  abstract = {This revolutionary book provides fresh answers to long-standing questions of human origins and consciousness. Drawing on his breakthrough research in comparative neuroscience, Terrence Deacon offers a wealth of insights into the significance of symbolic thinking: from the co-evolutionary exchange between language and brains over two million years of hominid evolution to the ethical repercussions that followed man's newfound access to other people's thoughts and emotions.Informing these insights is a new understanding of how Darwinian processes underlie the brain's development and function as well as its evolution. In contrast to much contemporary neuroscience that treats the brain as no more or less than a computer, Deacon provides a new clarity of vision into the mechanism of mind. It injects a renewed sense of adventure into the experience of being human.},
  isbn = {978-0-393-31754-1},
  langid = {Englisch}
}

@inproceedings{dean-leonRoboticTechnologiesFast2016,
  title = {Robotic Technologies for Fast Deployment of Industrial Robot Systems},
  booktitle = {{{IECON}} 2016 - 42nd {{Annual Conference}} of the {{IEEE Industrial Electronics Society}}},
  author = {{Dean-Leon}, Emmanuel and {Ramirez-Amaro}, Karinne and Bergner, Florian and Dianov, Ilya and Lanillos, Pablo and Cheng, Gordon},
  year = {2016},
  month = oct,
  pages = {6900--6907},
  doi = {10.1109/IECON.2016.7793823},
  abstract = {The development of breakthrough technologies helps the deployment of robotic systems in the industry. The implementation and integration of such technologies will improve productivity, flexibility and competitiveness, in diverse industrial settings specially for small and medium enterprises. In this paper we present a framework that integrates three novel technologies, namely safe robot arms with multi-modal and auto-calibrated sensing skin, a robot control framework to generate dynamic behaviors fusing multiple sensor signals, and an intuitive and fast teaching by demonstration method that segments and recognizes the robot activities on-line based on re-usable semantic descriptions. In order to validate our framework, these technologies are integrated in a industrial setting to sort and pack fruits. We demonstrate that our presented framework enables a standard industrial robotic system to be flexible, modular and adaptable to different production requirements.},
  keywords = {Collision avoidance,Education,Programming,Robot sensing systems,Service robots,Skin},
  file = {C:\Users\benja\Zotero\storage\5RFBCXIG\7793823.html}
}

@inproceedings{deavilabelbute-peresEndtoEndDifferentiablePhysics2018,
  title = {End-to-{{End Differentiable Physics}} for {{Learning}} and {{Control}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{de Avila Belbute-Peres}, Filipe and Smith, Kevin and Allen, Kelsey and Tenenbaum, Josh and Kolter, J. Zico},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-06-24},
  abstract = {We present a differentiable physics engine that can be integrated as a module in deep neural networks for end-to-end learning.  As a result, structured physics knowledge can be embedded into larger systems, allowing them, for example, to match observations by performing precise simulations, while achieves high sample efficiency.  Specifically, in this paper we demonstrate how to perform backpropagation analytically through a physical simulator defined via a linear complementarity problem.  Unlike traditional finite difference methods, such gradients can be computed analytically, which allows for greater flexibility of the engine. Through experiments in diverse domains, we highlight the system's ability to learn physical parameters from data, efficiently match and simulate observed visual behavior, and readily enable control via gradient-based planning methods. Code for the engine and experiments is included with the paper.}
}

@article{debFastElitistMultiobjective2002,
  title = {A Fast and Elitist Multiobjective Genetic Algorithm: {{NSGA-II}}},
  shorttitle = {A Fast and Elitist Multiobjective Genetic Algorithm},
  author = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  year = {2002},
  month = apr,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {6},
  number = {2},
  pages = {182--197},
  issn = {1941-0026},
  doi = {10.1109/4235.996017},
  abstract = {Multi-objective evolutionary algorithms (MOEAs) that use non-dominated sorting and sharing have been criticized mainly for: (1) their O(MN/sup 3/) computational complexity (where M is the number of objectives and N is the population size); (2) their non-elitism approach; and (3) the need to specify a sharing parameter. In this paper, we suggest a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic Algorithm II), which alleviates all of the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN/sup 2/) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best N solutions (with respect to fitness and spread). Simulation results on difficult test problems show that NSGA-II is able, for most problems, to find a much better spread of solutions and better convergence near the true Pareto-optimal front compared to the Pareto-archived evolution strategy and the strength-Pareto evolutionary algorithm - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint nonlinear problem, are compared with another constrained multi-objective optimizer, and the much better performance of NSGA-II is observed.},
  keywords = {Associate members,Computational complexity,Computational modeling,Constraint optimization,Decision making,Diversity reception,Evolutionary computation,Genetic algorithms,Sorting,Testing},
  file = {C:\Users\benja\Zotero\storage\KE28M4LQ\996017.html}
}

@article{degeusPartawarePanopticSegmentation2021,
  title = {Part-Aware {{Panoptic Segmentation}}},
  author = {{de Geus}, Daan and Meletis, Panagiotis and Lu, Chenyang and Wen, Xiaoxiao and Dubbelman, Gijs},
  year = {2021},
  month = jun,
  journal = {CVPR 2021},
  eprint = {2106.06351},
  urldate = {2022-04-26},
  abstract = {In this work, we introduce the new scene understanding task of Part-aware Panoptic Segmentation (PPS), which aims to understand a scene at multiple levels of abstraction, and unifies the tasks of scene parsing and part parsing. For this novel task, we provide consistent annotations on two commonly used datasets: Cityscapes and Pascal VOC. Moreover, we present a single metric to evaluate PPS, called Part-aware Panoptic Quality (PartPQ). For this new task, using the metric and annotations, we set multiple baselines by merging results of existing state-of-the-art methods for panoptic segmentation and part segmentation. Finally, we conduct several experiments that evaluate the importance of the different levels of abstraction in this single task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\benja\Zotero\storage\NHN3MFZ8\2106.html}
}

@article{degraveDifferentiablePhysicsEngine2019,
  title = {A {{Differentiable Physics Engine}} for {{Deep Learning}} in {{Robotics}}},
  author = {Degrave, Jonas and Hermans, Michiel and Dambre, Joni and Wyffels, Francis},
  year = {2019},
  journal = {Frontiers in Neurorobotics},
  volume = {13},
  publisher = {Frontiers},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2019.00006},
  urldate = {2020-10-18},
  abstract = {An important field in robotics is the optimization of controllers. Currently, robots are often treated as a black box in this optimization process, which is the reason why derivative-free optimization methods such as evolutionary algorithms or reinforcement learning are omnipresent. When gradient-based methods are used, models are kept small or rely on finite difference approximations for the Jacobian. This method quickly grows expensive with increasing numbers of parameters, such as found in deep learning. We propose an implementation of a modern physics engine, which can differentiate control parameters. This engine is implemented for both CPU and GPU. Firstly, this paper shows how such an engine speeds up the optimization process, even for small problems. Furthermore, it explains why this is an alternative approach to deep Q-learning, for using deep learning in robotics. Finally, we argue that this is a big step for deep learning in robotics, as it opens up new possibilities to optimize robots, both in hardware and software.},
  langid = {english},
  keywords = {backpropagation,deep learning,Differential physics engine,Robotics,Simulation Technology}
}

@article{deisenrothGaussianProcessesDataEfficient2015,
  title = {Gaussian {{Processes}} for {{Data-Efficient Learning}} in {{Robotics}} and {{Control}}},
  author = {Deisenroth, Marc Peter and Fox, Dieter and Rasmussen, Carl Edward},
  year = {2015},
  month = feb,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {37},
  number = {2},
  eprint = {1502.02860},
  pages = {408--423},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2013.218},
  urldate = {2020-04-05},
  abstract = {Autonomous learning has been a promising direction in control and robotics for more than a decade since data-driven learning allows to reduce the amount of engineering knowledge, which is otherwise required. However, autonomous reinforcement learning (RL) approaches typically require many interactions with the system to learn controllers, which is a practical limitation in real systems, such as robots, where many interactions can be impractical and time consuming. To address this problem, current learning approaches typically require task-specific knowledge in form of expert demonstrations, realistic simulators, pre-shaped policies, or specific knowledge about the underlying dynamics. In this article, we follow a different approach and speed up learning by extracting more information from data. In particular, we learn a probabilistic, non-parametric Gaussian process transition model of the system. By explicitly incorporating model uncertainty into long-term planning and controller learning our approach reduces the effects of model errors, a key problem in model-based learning. Compared to state-of-the art RL our model-based policy search method achieves an unprecedented speed of learning. We demonstrate its applicability to autonomous learning in real robot and control tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning}
}

@inproceedings{deisenrothLearningControlLowCost2012,
  title = {Learning to {{Control}} a {{Low-Cost Manipulator Using Data-Efficient Reinforcement Learning}}},
  booktitle = {{{RSS}}},
  author = {Deisenroth, Mark Peter and Rasmussen, Carl Edward and Fox, Dieter},
  editor = {{Durrant-Whyte}, H. and Roy, N. and Abbeel, P.},
  year = {2012},
  urldate = {2019-07-14},
  abstract = {Over the last years, there has been substantial progress in robust manipulation in unstructured environments. The long-term goal of our work is to get away from precise, but very expensive robotic systems and to develop affordable, potentially imprecise, self-adaptive manipulator systems that can interactively perform tasks such as playing with children. In this paper, we demonstrate how a low-cost off-the-shelf robotic system can learn closed-loop policies for a stacking task in only a handful of trials---from scratch. Our manipulator is inaccurate and provides no pose feedback. For learning a controller in the work space of a Kinect-style depth camera, we use a model-based reinforcement learning technique. Our learning method is data efficient, reduces model bias, and deals with several noise sources in a principled way during long-term planning. We present a way of incorporating state-space constraints into the learning process and analyze the learning gain by exploiting the sequential structure of the stacking task.},
  isbn = {978-0-262-30596-9}
}

@article{deitsClarifyingCommandsInformationtheoretic2013,
  title = {Clarifying Commands with Information-Theoretic Human-Robot Dialog},
  author = {Deits, Robin and Tellex, Stefanie and Thaker, Pratiksha and Simeonov, Dimitar and Kollar, Thomas and Roy, Nicholas},
  year = {2013},
  month = jun,
  journal = {J. Hum.-Robot Interact.},
  volume = {2},
  number = {2},
  pages = {58--79},
  doi = {10.5898/JHRI.2.2.Deits},
  urldate = {2024-08-16},
  abstract = {Our goal is to improve the efficiency and effectiveness of natural language communication between humans and robots. Human language is frequently ambiguous, and a robot's limited sensing makes complete understanding of a statement even more difficult. To address these challenges, we describe an approach for enabling a robot to engage in clarifying dialog with a human partner, just as a human might do in a similar situation. Given an unconstrained command from a human operator, the robot asks one or more questions and receives natural language answers from the human. We apply an information-theoretic approach to choosing questions for the robot to ask. Specifically, we choose the type and subject of questions in order to maximize the reduction in Shannon entropy of the robot's mapping between language and entities in the world. Within the framework of the G3 graphical model, we derive a method to estimate this entropy reduction, choose the optimal question to ask, and merge the information gained from the human operator's answer. We demonstrate that this improves the accuracy of command understanding over prior work while asking fewer questions as compared to baseline question-selection strategies.}
}

@incollection{demersInverseKinematicsDextrous1997,
  title = {Inverse {{Kinematics}} of {{Dextrous Manipulators}}},
  booktitle = {Neural {{Systems}} for {{Robotics}}},
  author = {DeMers, David and {Kreutz-Delgado}, Kenneth},
  editor = {Omidvar, Omid and {van der Smagt}, Patrick},
  year = {1997},
  month = jan,
  pages = {75--116},
  publisher = {Academic Press},
  address = {Boston},
  doi = {10.1016/B978-0-08-092509-7.50008-7},
  urldate = {2024-03-31},
  abstract = {Manipulators with extra, or redundant, degrees of freedom are capable of dextrous motion. However, control of such manipulators is difficult because the inverse problem; i.e., the choice of a set of control variables which positions the manipulator at a desired target location, is underdetermined. This chapter has two main points. First, it provides a taxonomy of approaches to solving the inverse problem for dextrous manipulators which outlines the relationship between the standard robotics approaches and approaches using neural networks. Second, this chapter analyzes the topological structure of the kinematics problem, showing that the inverse kinematic structure generically decomposes into a finite number of solution branches, each corresponding roughly to the notion of ``posture,'' such that each solution branch can be described as the product of a one-to-one inverse map and a set of free parameters describing the redundancy. Using these geometric insights, one can then generate direct inverse functions for dextrous manipulators, with which the free parameters can be used flexibly during operation to optimize any user-specified side constraint.},
  isbn = {978-0-08-092509-7},
  file = {C:\Users\benja\Zotero\storage\RPJY2FXV\B9780080925097500087.html}
}

@article{demirIndustry50HumanRobot2019,
  title = {Industry 5.0 and {{Human-Robot Co-working}}},
  author = {Demir, Kadir Alpaslan and D{\"o}ven, G{\"o}zde and Sezen, B{\"u}lent},
  year = {2019},
  month = jan,
  journal = {Procedia Computer Science},
  series = {3rd {{WORLD CONFERENCE ON TECHNOLOGY}}, {{INNOVATION AND ENTREPRENEURSHIP}}"{{INDUSTRY}} 4.0 {{FOCUSED INNOVATION}}, {{TECHNOLOGY}}, {{ENTREPRENEURSHIP AND MANUFACTURE}}" {{June}} 21-23, 2019},
  volume = {158},
  pages = {688--695},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2019.09.104},
  urldate = {2022-05-06},
  abstract = {According to many, we are at the brink of the fourth industrial revolution. The theme of Industry 4.0 is "Smart Manufacturing for the Future". Now, some futurists even discuss what the fifth industrial revolution's theme will be. There are a few visions for Industry 5.0. One emerging theme is human-robot co-working. In recent years, we have seen significant advancements in robotics and artificial intelligence (AI) research. Today, there are robots for various purposes at affordable prices in the market. It is not long before we closely interact with robots in our lives and workplaces. Testing autonomous cars in traffic is a promising example of this upcoming trend. There are companies having an employee record for robots or AI applications. While there are many studies on human-robot collaboration for low-level tasks with a focus on robot development, we lack studies focusing on organizational issues emerging from human-robot co-working. In this study, we discuss the possible issues related to human-robot co-working from the organizational and human employee's perspective. We believe the issues identified in this study will be the focus of many upcoming organizational robotics research studies.},
  langid = {english},
  keywords = {Human-Robot Co-working,Human-Robot Coordination,Industry 4.0,Industry 5.0,Organizational Behaviour,Organizational Robotics,Robots in Organizations},
  file = {C:\Users\benja\Zotero\storage\2PBNZFM4\S1877050919312748.html}
}

@article{denavitKinematicNotationLowerPair1955,
  title = {A {{Kinematic Notation}} for {{Lower-Pair Mechanisms Based}} on {{Matrices}}},
  author = {Denavit, J. and Hartenberg, R. S.},
  year = {1955},
  month = jun,
  journal = {Journal of Applied Mechanics},
  volume = {22},
  number = {2},
  pages = {215--221},
  issn = {0021-8936},
  doi = {10.1115/1.4011045},
  urldate = {2024-03-31},
  abstract = {A symbolic notation devised by Reuleaux to describe mechanisms did not recognize the necessary number of variables needed for complete description. A reconsideration of the problem leads to a symbolic notation which permits the complete description of the kinematic properties of all lower-pair mechanisms by means of equations. The symbolic notation also yields a method for studying lower-pair mechanisms by means of matrix algebra; two examples of application to space mechanisms are given.},
  file = {C:\Users\benja\Zotero\storage\P782P6LE\A-Kinematic-Notation-for-Lower-Pair-Mechanisms.html}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  month = jun,
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {C:\Users\benja\Zotero\storage\QSTJBTGV\5206848.html}
}

@article{deschutterConstraintbasedTaskSpecification2007,
  title = {Constraint-Based {{Task Specification}} and {{Estimation}} for {{Sensor-Based Robot Systems}} in the {{Presence}} of {{Geometric Uncertainty}}},
  author = {De Schutter, Joris and De Laet, Tinne and Rutgeerts, Johan and Decr{\'e}, Wilm and Smits, Ruben and Aertbeli{\"e}n, Erwin and Claes, Kasper and Bruyninckx, Herman},
  year = {2007},
  month = may,
  journal = {The International Journal of Robotics Research},
  volume = {26},
  number = {5},
  pages = {433--455},
  publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  doi = {10.1177/027836490707809107},
  urldate = {2021-11-15},
  abstract = {This paper introduces a systematic constraint-based approach to specify complex tasks of general sensor-based robot systems consisting of rigid links and joints. The approach integrates both instantaneous task specification and estimation of geometric uncertainty in a unified framework. Major components are the use of feature coordinates, defined with respect to object and feature frames, which facilitate the task specification, and the introduction of uncertainty coordinates to model geometric uncertainty. While the focus of the paper is on task specification, an existing velocity- based control scheme is reformulated in terms of these feature and uncertainty coordinates. This control scheme compensates for the effect of time varying uncertainty coordinates. Constraint weighting results in an invariant robot behavior in case of conflicting constraints with heterogeneous units. The approach applies to a large variety of robot systems (mobile robots, multiple robot systems, dynamic human-robot interaction, etc.), various sensor systems, and different robot tasks. Ample simulation and experimental results are presented.},
  langid = {english},
  keywords = {constraint-based programming,estimation,geometric uncertainty,task specification}
}

@article{desmaraisReview3DHuman2020,
  title = {A Review of {{3D}} Human Pose Estimation Algorithms for Markerless Motion Capture},
  author = {Desmarais, Yann and Mottet, Denis and Slangen, Pierre and Montesinos, Philippe},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.06449 [cs]},
  eprint = {2010.06449},
  primaryclass = {cs},
  urldate = {2021-01-26},
  abstract = {Human pose estimation (HPE) in 3D is an active research field that have many applications in entertainment, health and sport science, robotics. In the last five years markerless motion captures techniques have seen their average error decrease from more than 10cm to less than 2cm today. This evolution is mainly driven by the improvements in 2D pose estimation task that benefited from the use of convolutional networks. However with the multiplication of different approaches it can be difficult to identify what is more adapted to the specifics of any applications. We suggest to classify existing methods with a taxonomy based on the performance criteria of accuracy, speed and robustness. We review more than twenty methods from the last three years. Additionally we analyze the metrics, benchmarks and structure of the different pose estimation systems and propose several direction for future research. We hope to offer a good introduction to 3D markerless pose estimation as well as discussing the leading contemporary algorithms.},
  archiveprefix = {arXiv},
  keywords = {68T45,Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\benja\Zotero\storage\486EPLI2\2010.html}
}

@misc{dettmersBitsAndBytes2023,
  type = {Software Repository},
  title = {{{BitsAndBytes}}},
  author = {Dettmers, Tim},
  year = {2023},
  month = oct,
  journal = {BitsAndBytes},
  urldate = {2023-10-11},
  abstract = {8-bit CUDA functions for PyTorch},
  copyright = {MIT},
  howpublished = {https://github.com/TimDettmers/bitsandbytes}
}

@misc{dettmersQLoRAEfficientFinetuning2023,
  title = {{{QLoRA}}: {{Efficient Finetuning}} of {{Quantized LLMs}}},
  shorttitle = {{{QLoRA}}},
  author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  year = {2023},
  month = may,
  number = {arXiv:2305.14314},
  eprint = {2305.14314},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14314},
  urldate = {2023-10-11},
  abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\3RRQ3VP7\2305.html}
}

@inproceedings{devabhaktuniNeuralNetworkTrainingDriven2000,
  title = {Neural {{Network Training-Driven Adaptive Sampling Algorithm}} for {{Microwave Modeling}}},
  booktitle = {2000 30th {{European Microwave Conference}}},
  author = {Devabhaktuni, V. K. and Zhang, Q.},
  year = {2000},
  month = oct,
  pages = {1--4},
  doi = {10.1109/EUMA.2000.338591},
  abstract = {We present a neural network training-driven adaptive sampling algorithm for efficient generation of training and test data. The proposed approach makes microwave data generation an integral part of model development/training. For user-specified model accuracy, the algorithm periodically communicates with the neural network training process and automatically determines the number of samples required and their distribution in the model input space. The algorithm has an inherent ability to distinguish nonlinear and smooth regions of model behavior. Consequently, more samples are generated in nonlinear regions improving model accuracy, and redundant data is avoided in smooth regions reducing model development cost.},
  keywords = {Adaptive systems,Costs,Electronic equipment testing,MESFETs,Microstrip components,Microwave devices,Microwave generation,Neural networks,Predictive models,Sampling methods}
}

@inproceedings{devlinRobustFillNeuralProgram2017,
  title = {{{RobustFill}}: {{Neural Program Learning}} under {{Noisy I}}/{{O}}},
  shorttitle = {{{RobustFill}}},
  booktitle = {{{ICML}}},
  author = {Devlin, Jacob and Uesato, Jonathan and Bhupatiraju, Surya and Singh, Rishabh and Mohamed, Abdel-rahman and Kohli, Pushmeet},
  year = {2017},
  month = jul,
  pages = {990--998},
  urldate = {2019-07-16},
  abstract = {The problem of automatically generating a computer program from some specification has been studied since the early days of AI. Recently, two competing approaches for `automatic program learning' h...},
  langid = {english}
}

@incollection{diabOntologyFailureInterpretation2019,
  title = {An {{Ontology}} for {{Failure Interpretation}} in {{Automated Planning}} and {{Execution}}},
  booktitle = {Fourth {{Iberian Robotics Conference}}},
  author = {Diab, Mohammed and Pomarlan, Mihai and Be\&szlig;ler, Daniel and Abkari, Aliakbar and Rossel, Jan and Bateman, John and Beetz, Michael},
  year = {2019},
  file = {C:\Users\benja\Zotero\storage\W8Y5NN83\bibtexbrowser.html}
}

@incollection{diabOntologyFailureInterpretation2019a,
  title = {An {{Ontology}} for {{Failure Interpretation}} in {{Automated Planning}} and {{Execution}}},
  booktitle = {Fourth {{Iberian Robotics Conference}}},
  author = {Diab, Mohammed and Pomarlan, Mihai and Be\&szlig;ler, Daniel and Abkari, Aliakbar and Rossel, Jan and Bateman, John and Beetz, Michael},
  year = {2019},
  file = {C:\Users\benja\Zotero\storage\7N2GY4CM\bibtexbrowser.html}
}

@article{diabPMKAKnowledgeProcessing2019,
  title = {{{PMK-A Knowledge Processing Framework}} for {{Autonomous Robotics Perception}} and {{Manipulation}}},
  author = {Diab, Mohammed and Akbari, Ali and Din, Ud and Rosell, Jan},
  year = {2019},
  month = mar,
  journal = {Sensors},
  volume = {19},
  doi = {10.3390/s19051166},
  abstract = {Autonomous indoor service robots are supposed to accomplish tasks, like serve a cup, which involve manipulation actions. Particularly, for complex manipulation tasks which are subject to geometric constraints, spatial information and a rich semantic knowledge about objects, types, and functionality are required, together with the way in which these objects can be manipulated. In this line, this paper presents an ontological-based reasoning framework called Perception and Manipulation Knowledge (PMK) that includes: (1) the modeling of the environment in a standardized way to provide common vocabularies for information exchange in human-robot or robot-robot collaboration, (2) a sensory module to perceive the objects in the environment and assert the ontological knowledge, (3) an evaluation-based analysis of the situation of the objects in the environment, in order to enhance the planning of manipulation tasks. The paper describes the concepts and the implementation of PMK, and presents an example demonstrating the range of information the framework can provide for autonomous robots.},
  file = {C:\Users\benja\Zotero\storage\QKR4HMXG\Diab et al. - 2019 - PMK-A Knowledge Processing Framework for Autonomou.pdf}
}

@article{Diamond2017a,
  title = {Rapid Target Foraging with Reach or Gaze: {{The}} Hand Looks Further Ahead than the Eye},
  author = {Diamond, Jonathan S. and Wolpert, Daniel M. and Flanagan, J. Randall},
  editor = {Faisal, Aldo A},
  year = {2017},
  month = jul,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {7},
  pages = {e1005504},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005504},
  urldate = {2018-05-10},
  abstract = {Real-world tasks typically consist of a series of target-directed actions and often require choices about which targets to act on and in what order. Such choice behavior can be assessed from an optimal foraging perspective whereby target selection is shaped by a balance between rewards and costs. Here we evaluated such decision-making in a rapid movement foraging task. On a given trial, participants were presented with 15 targets of varying size and value and were instructed to harvest as much reward as possible by either moving a handle to the targets (hand task) or by briefly fixating them (eye task). The short trial duration enabled participants to harvest about half the targets, ensuring that total reward was due to choice behavior. We developed a probabilistic model to predict target-by-target harvesting choices that considered the rewards and movement-related costs (i.e., target distance and size) associated with the current target as well as future targets. In the hand task, in comparison to the eye task, target choice was more strongly influenced by movement-related costs and took into account a greater number of future targets, consistent with the greater costs associated with arm movement. In both tasks, participants exhibited near-optimal behaviour and in a constrained version of the hand task in which choices could only be based on target positions, participants consistently chose among the shortest movement paths. Our results demonstrate that people can rapidly and effectively integrate values and movement-related costs associated with current and future targets when sequentially harvesting targets.}
}

@inproceedings{dianovExtractingGeneralTask2016,
  title = {Extracting General Task Structures to Accelerate the Learning of New Tasks},
  booktitle = {2016 {{IEEE-RAS}} 16th {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  author = {Dianov, Ilya and {Ram{\'i}rez-Amaro}, Karinne and Lanillos, Pablo and {Dean-Leon}, Emmanuel and Bergner, Florian and Cheng, Gordon},
  year = {2016},
  month = nov,
  pages = {802--807},
  issn = {2164-0580},
  doi = {10.1109/HUMANOIDS.2016.7803365},
  abstract = {Teaching a robot new tasks through kinesthetic demonstrations can be a long and complicated process. For example, a human has to demonstrate a new ``pick and place'' task each time the object or the target location has changed. However, obtaining the abstract representation of such task can significantly reduce the learning time as the human only has to teach the necessary parameters required for the successful execution, e.g. the location of an object. In this work, we present a framework which allows to extract general task structures which together with the obtained knowledge can improve and accelerate the teaching of new tasks. Additionally, our framework exploits the semantic similarities between task parameters in order to infer the possible structure of unknown tasks. Our proposed method utilises symbolic representations of tasks combined with an ontology which makes it applicable to different environments in various domains. We analysed our framework in an orange sorting scenario and a cleaning scenario to demonstrate that it allows reducing the time required for teaching from 136.3 to 53 seconds (61.12\%) and from 48.7 to 21 seconds (56.87\%) respectively compared to learning only by kinesthetic demonstrations.},
  keywords = {Acceleration,Education,Hidden Markov models,Inference algorithms,Ontologies,Robots,Semantics}
}

@article{diehlAutomatedGenerationRobotic2021,
  title = {Automated {{Generation}} of {{Robotic Planning Domains}} from {{Observations}}},
  author = {Diehl, Maximilian and Paxton, Chris and {Ramirez-Amaro}, Karinne},
  year = {2021},
  journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS51168.2021.9636781},
  abstract = {This paper introduces a novel method for generating executable plans from using one single demonstration with a 92\% success rate, and 100\% when the information from all demonstrations are included, even for previously unseen stacking goals. Automated planning enables robots to find plans to achieve complex, long-horizon tasks, given a planning domain. This planning domain consists of a list of actions, with their associated preconditions and effects, and is usually manually defined by a human expert, which is very time-consuming or even infeasible. In this paper, we introduce a novel method for generating this domain automatically from human demonstrations. First, we automatically segment and recognize the different observed actions from human demonstrations. From these demonstrations, the relevant preconditions and effects are obtained, and the associated planning operators are generated. Finally, a sequence of actions that satisfies a user-defined goal can be planned using a symbolic planner. The generated plan is executed in a simulated environment by the TIAGo robot. We tested our method on a dataset of 12 demonstrations collected from three different participants. The results show that our method is able to generate executable plans from using one single demonstration with a 92\% success rate, and 100\% when the information from all demonstrations are included, even for previously unseen stacking goals.}
}

@article{diehlOptimizingRobotPlanning2021,
  title = {Optimizing Robot Planning Domains to Reduce Search Time for Long-Horizon Planning},
  author = {Diehl, Maximilian and Paxton, Chris and {Ramirez-Amaro}, Karinne},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.05397 [cs]},
  eprint = {2111.05397},
  primaryclass = {cs},
  urldate = {2021-11-12},
  abstract = {We have recently introduced a system that automatically generates robotic planning operators from human demonstrations. One feature of our system is the operator count, which keeps track of the application frequency of every operator within the demonstrations. In this extended abstract, we show that we can use the count to slim down domains with the goal of decreasing the search time for long-horizon planning goals. The conceptual idea behind our approach is that we would like to prioritize operators that have occurred more often in the demonstrations over those that were not observed so frequently. We, therefore, propose to limit the domain only to the most popular operators. If this subset of operators is not sufficient to find a plan, we iteratively expand this subset of operators. We show that this significantly reduces the search time for long-horizon planning goals.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\NRY74Q74\2111.html}
}

@misc{DigitalizationIndustryTwins,
  title = {Digitalization in Industry: {{Twins}} with Potential},
  shorttitle = {Digitalization in Industry},
  journal = {siemens.com Global Website},
  urldate = {2021-04-14},
  abstract = {The digital twin has long since established itself in industry, where it's revolutionizing processes along the entire value chain. It creates a consistent improvement in efficiency, minimizes failure rates, shortens development cycles, and opens up new business opportunities: In other words, it creates a lasting competitive edge},
  howpublished = {https://new.siemens.com/global/en/company/stories/industry/the-digital-twin.html},
  langid = {english}
}

@misc{DigitalTwinPerformance,
  type = {Newton\_ps-Detail},
  title = {The Digital Twin of Performance in the Automotive Industry},
  journal = {siemens.com Global Website},
  urldate = {2021-04-14},
  abstract = {Continuous improvements of production and product},
  howpublished = {https://new.siemens.com/global/en/markets/automotive-manufacturing/digital-twin-performance.html},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\CNQQVAPK\digital-twin-performance.html}
}

@article{DigitalTwinsIndustrial,
  title = {Digital {{Twins}} for {{Industrial Applications}}},
  pages = {19},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\XAK4VJKD\Digital Twins for Industrial Applications.pdf}
}

@misc{dijkstraEWD316Short1971,
  title = {{{EWD}} 316: {{A Short Introduction}} to the {{Art}} of {{Programming}}},
  shorttitle = {Ewd 316},
  author = {Dijkstra, Edsger W.},
  year = {1971},
  urldate = {2024-03-07},
  abstract = {Semantic Scholar extracted view of "Ewd 316: a short introduction to the art of programming" by E. Dijkstra}
}

@article{Dill10,
  title = {Advances in {{Robot Programming}} by {{Demonstration}}},
  author = {Dillmann, R{\"u}diger and Asfour, Tamim and Do, Martin and J{\"a}kel, Rainer and Kasper, Alexander and Azad, Pedram and Ude, Ale{\v s} and {Schmidt-Rohr}, Sven R and L{\"o}sch, Martin},
  year = {2010},
  volume = {24},
  number = {4},
  pages = {295--303}
}

@misc{dinevDifferentiableOptimalControl2022,
  title = {Differentiable {{Optimal Control}} via {{Differential Dynamic Programming}}},
  author = {Dinev, Traiko and Mastalli, Carlos and Ivan, Vladimir and Tonneau, Steve and Vijayakumar, Sethu},
  year = {2022},
  month = sep,
  number = {arXiv:2209.01117},
  eprint = {2209.01117},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.01117},
  urldate = {2024-06-24},
  abstract = {Robot design optimization, imitation learning and system identification share a common problem which requires optimization over robot or task parameters at the same time as optimizing the robot motion. To solve these problems, we can use differentiable optimal control for which the gradients of the robot's motion with respect to the parameters are required. We propose a method to efficiently compute these gradients analytically via the differential dynamic programming (DDP) algorithm using sensitivity analysis (SA). We show that we must include second-order dynamics terms when computing the gradients. However, we do not need to include them when computing the motion. We validate our approach on the pendulum and double pendulum systems. Furthermore, we compare against using the derivatives of the iterative linear quadratic regulator (iLQR), which ignores these second-order terms everywhere, on a co-design task for the Kinova arm, where we optimize the link lengths of the robot for a target reaching task. We show that optimizing using iLQR gradients diverges as ignoring the second-order dynamics affects the computation of the derivatives. Instead, optimizing using DDP gradients converges to the same optimum for a range of initial designs allowing our formulation to scale to complex systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\XHAZULAD\2209.html}
}

@article{dinevSparsityInducingOptimalControl2021,
  title = {Sparsity-{{Inducing Optimal Control}} via {{Differential Dynamic Programming}}},
  author = {Dinev, Traiko and Merkt, Wolfgang and Ivan, Vladimir and Havoutis, Ioannis and Vijayakumar, Sethu},
  year = {2021},
  month = mar,
  journal = {arXiv:2011.07325 [cs]},
  eprint = {2011.07325},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {Optimal control is a popular approach to synthesize highly dynamic motion. Commonly, \$L\_2\$ regularization is used on the control inputs in order to minimize energy used and to ensure smoothness of the control inputs. However, for some systems, such as satellites, the control needs to be applied in sparse bursts due to how the propulsion system operates. In this paper, we study approaches to induce sparsity in optimal control solutions -- namely via smooth \$L\_1\$ and Huber regularization penalties. We apply these loss terms to state-of-the-art DDP-based solvers to create a family of sparsity-inducing optimal control methods. We analyze and compare the effect of the different losses on inducing sparsity, their numerical conditioning, their impact on convergence, and discuss hyperparameter settings. We demonstrate our method in simulation and hardware experiments on canonical dynamics systems, control of satellites, and the NASA Valkyrie humanoid robot. We provide an implementation of our method and all examples for reproducibility on GitHub.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\NH9GVE63\2011.html}
}

@article{Dip2009,
  title = {Genetic Algorithm-Based Optimal Bipedal Walking Gait Synthesis Considering Tradeoff between Stability Margin and Speed},
  author = {Dip, Goswami and Prahlad, Vadakkepat and Kien, Phung Duc},
  year = {2009},
  journal = {Robotica},
  volume = {27},
  number = {3},
  pages = {355--365},
  publisher = {Cambridge University Press}
}

@mastersthesis{dittus3DLokalisierungUnd2019,
  title = {{3D Lokalisierung und Tracking benutzerdefinierter Punkte auf Oberfl{\"a}chen nicht-rigider Objekte mit Hilfe von RGB-D Punktwolken}},
  author = {Dittus, Sven},
  year = {2019},
  address = {Karlsruhe},
  langid = {ngerman},
  school = {Karlsruhe Institute of Technology}
}

@inproceedings{dittusLocalizationTrackingUserDefined2021,
  title = {Localization and {{Tracking}} of {{User-Defined Points}} on {{Deformable Objects}} for {{Robotic Manipulation}}},
  booktitle = {{{IEEE ICRA Workshop}} on {{Representing}} and {{Manipulating Deformable Objects}}},
  author = {Dittus, Sven and Alt, Benjamin and Hermann, Andreas and Katic, Darko and J{\"a}kel, Rainer and Fleischer, J{\"u}rgen},
  year = {2021},
  month = may,
  eprint = {2105.09067},
  publisher = {IEEE},
  address = {Xi'an, China},
  urldate = {2021-06-02},
  abstract = {This paper introduces an efficient procedure to localize user-defined points on the surface of deformable objects and track their positions in 3D space over time. To cope with a deformable object's infinite number of DOF, we propose a discretized deformation field, which is estimated during runtime using a multi-step non-linear solver pipeline. The resulting high-dimensional energy minimization problem describes the deviation between an offline-defined reference model and a pre-processed camera image. An additional regularization term allows for assumptions about the object's hidden areas and increases the solver's numerical stability. Our approach is capable of solving the localization problem online in a data-parallel manner, making it ideally suitable for the perception of non-rigid objects in industrial manufacturing processes.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics,my,workshop},
  file = {C\:\\Users\\benja\\Zotero\\storage\\LJ2NYCCB\\Dittus et al. - 2021 - Localization and Tracking of User-Defined Points o.pdf;C\:\\Users\\benja\\Zotero\\storage\\FKPUV3WL\\2105.html}
}

@inproceedings{doceaLaparoscopicLiverNavigation2022,
  title = {A {{Laparoscopic Liver Navigation Pipeline}} with {{Minimal Setup Requirements}}},
  booktitle = {2022 {{IEEE Biomedical Circuits}} and {{Systems Conference}} ({{BioCAS}})},
  author = {Docea, Reuben and Pfeiffer, Micha and M{\"u}ller, Jan and Krug, Katja and Hardner, Matthias and Riedel, Paul and Menzel, Martin and Kolbinger, Fiona R. and Frohneberg, Laura and Weitz, J{\"u}rgen and Speidel, Stefanie},
  year = {2022},
  month = oct,
  pages = {578--582},
  issn = {2163-4025},
  doi = {10.1109/BioCAS54905.2022.9948587},
  urldate = {2024-04-12},
  abstract = {In the case of liver tumor resections, Minimally Invasive Surgery (MIS) poses several benefits over open surgery. However, MIS makes navigating the surgical scene considerably more challenging, which hampers the realization of its full potential. Many Image Guidance Navigation Systems (IGS) have been proposed to overcome these challenges. The majority of these depend on optical tracking systems, whose additional setup overhead is a barrier to clinical translation. In this paper we put forward an IGS prototype which eliminates the need for optical tracking, and additionally incorporates a user-oriented camera calibration method which is more reliable and faster than typical checkerboard methods. We lastly make publicly available the core system modules for 3D reconstruction and rigid registration.},
  keywords = {Augmented reality,Biomedical optical imaging,Computer assisted navigation,Computer assisted surgery,Image guided surgery,Laparoscopic liver resection,Liver,Machine vision,Minimally invasive surgery,Navigation,Operating systems,Pipelines,Robot Operating System,Robotic liver surgery,Three-dimensional displays},
  file = {C:\Users\benja\Zotero\storage\3WSBMDWQ\9948587.html}
}

@article{doceaSimultaneousLocalisationMapping2021,
  title = {Simultaneous Localisation and Mapping for Laparoscopic Liver Navigation : A Comparative Evaluation Study},
  shorttitle = {Simultaneous Localisation and Mapping for Laparoscopic Liver Navigation},
  author = {Docea, Reuben and Pfeiffer, Micha and Bodenstedt, Sebastian and Kolbinger, Fiona R. and H{\"o}ller, Lukas and Wittig, Ines and Hoffmann, Ralf-Thorsten and Troost, Esther G. C. and Riediger, Carina and Weitz, J{\"u}rgen and Speidel, Stefanie},
  year = {2021},
  month = feb,
  volume = {11598},
  pages = {115980B},
  doi = {10.1117/12.2582121},
  urldate = {2024-04-12},
  abstract = {Computer-Assisted Surgery (CAS) aids the surgeon by enriching the surgical scene with additional information in order to improve patient outcome. One such aid may be the superimposition of important structures (such as blood vessels and tumors) over a laparoscopic image stream. In liver surgery, this may be achieved by creating a dense map of the abdominal environment surrounding the liver, registering a preoperative model (CT scan) to the liver within this map, and tracking the relative pose of the camera. Thereby, known structures may be rendered into images from the camera perspective. This intraoperative map of the scene may be constructed, and the relative pose of the laparoscope camera estimated, using Simultaneous Localisation and Mapping (SLAM). The intraoperative scene poses unique challenges, such as: homogeneous surface textures, sparse visual features, specular reflections and camera motions specific to laparoscopy. This work compares the efficacies of two state-of the-art SLAM systems in the context of laparoscopic surgery, on a newly collected phantom dataset with ground truth trajectory and surface data. The SLAM systems chosen contrast strongly in implementation: one sparse and feature-based, ORB-SLAM3,1\{3 and one dense and featureless, ElasticFusion.4 We find that ORB-SLAM3 greatly outperforms ElasticFusion in trajectory estimation and is more stable on sequences from laparoscopic surgeries. However, when extended to give a dense output, ORB-SLAM3 performs surface reconstruction comparably to ElasticFusion. Our evaluation of these systems serves as a basis for expanding the use of SLAM algorithms in the context of laparoscopic liver surgery and Minimally Invasive Surgery (MIS) more generally.\vphantom\}},
  annotation = {ADS Bibcode: 2021SPIE11598E..0BD}
}

@inproceedings{dodgeHowExpertsIt2018,
  title = {How the {{Experts Do It}}: {{Assessing}} and {{Explaining Agent Behaviors}} in {{Real-Time Strategy Games}}},
  shorttitle = {How the {{Experts Do It}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Dodge, Jonathan and Penney, Sean and Hilderbrand, Claudia and Anderson, Andrew and Burnett, Margaret},
  year = {2018},
  month = apr,
  series = {{{CHI}} '18},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3173574.3174136},
  urldate = {2024-02-04},
  abstract = {How should an AI-based explanation system explain an agent's complex behavior to ordinary end users who have no background in AI? Answering this question is an active research area, for if an AI-based explanation system could effectively explain intelligent agents' behavior, it could enable the end users to understand, assess, and appropriately trust (or distrust) the agents attempting to help them. To provide insights into this question, we turned to human expert explainers in the real-time strategy domain --"shoutcasters"-- to understand (1) how they foraged in an evolving strategy game in real time, (2) how they assessed the players' behaviors, and (3) how they constructed pertinent and timely explanations out of their insights and delivered them to their audience. The results provided insights into shoutcasters' foraging strategies for gleaning information necessary to assess and explain the players; a characterization of the types of implicit questions shoutcasters answered; and implications for creating explanations by using the patterns and abstraction levels these human experts revealed.},
  isbn = {978-1-4503-5620-6},
  keywords = {explainable ai,information foraging,intelligent agents,rts games,starcraft},
  file = {C:\Users\benja\Zotero\storage\T5TSXX4Y\Dodge et al. - 2018 - How the Experts Do It Assessing and Explaining Ag.pdf}
}

@incollection{donaldMultiStepStrategies1989,
  title = {Multi-{{Step Strategies}}},
  booktitle = {Error {{Detection}} and {{Recovery}} in {{Robotics}}},
  editor = {Donald, Bruce R.},
  year = {1989},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {107--219},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/BFb0039643},
  urldate = {2021-03-03},
  isbn = {978-0-387-34784-4},
  langid = {english},
  keywords = {Forward Projection,Model Error,Motion Strategy,Pure Rotation,Pure Translation}
}

@inproceedings{donaldPlanningMultistepError1988,
  title = {Planning Multistep Error Detection and Recovery Strategies},
  booktitle = {1988 {{IEEE International Conference}} on {{Robotics}} and {{Automation Proceedings}}},
  author = {Donald, B. R.},
  year = {1988},
  month = apr,
  pages = {892-897 vol.2},
  doi = {10.1109/ROBOT.1988.12173},
  abstract = {The author describes techniques for planning multistep error detection and recovery strategies for robots operating in the presence of uncertainty. He introduces two approaches for their synthesis: the push-forward algorithm and failure mode analysis. He has implemented the theory in the form of a planner, called LIMITED, in the domain of planar assemblies.{$<>$}},
  keywords = {Artificial intelligence,assembling,Assembly,Computer errors,Contracts,Error correction,error recovery strategies,failure mode analysis,Laboratories,multistep error detection,planar assemblies,push-forward algorithm,Robot sensing systems,Robotics and automation,robots,Strategic planning,Uncertainty},
  file = {C:\Users\benja\Zotero\storage\YXEY6WWY\12173.html}
}

@article{Donc15,
  title = {Evolutionary {{Robotics}}: {{What}}, {{Why}}, and {{Where}} To},
  author = {Doncieux, Stephane and Bredeche, Nicolas and Mouret, Jean-Baptiste and Eiben, Agoston E},
  year = {2015},
  journal = {Frontiers in Robotics and AI},
  volume = {2},
  pages = {4}
}

@article{doNeuralCircuitsSymbolic2021,
  title = {Neural {{Circuits}} and {{Symbolic Processing}}},
  author = {Do, Quan and Hasselmo, Michael E.},
  year = {2021},
  month = dec,
  journal = {Neurobiology of learning and Memory},
  volume = {186},
  pages = {107552},
  issn = {1074-7427},
  doi = {10.1016/j.nlm.2021.107552},
  urldate = {2024-04-21},
  abstract = {The ability to use symbols is a defining feature of human intelligence. However, neuroscience has yet to explain the fundamental neural circuit mechanisms for flexibly representing and manipulating abstract concepts. This article will review the research on neural models for symbolic processing. The review first focuses on the question of how symbols could possibly be represented in neural circuits. The review then addresses how neural symbolic representations could be flexibly combined to meet a wide range of reasoning demands. Finally, the review assesses the research on program synthesis and proposes that the most flexible neural representation of symbolic processing would involve the capacity to rapidly synthesize neural operations analogous to lambda calculus to solve complex cognitive tasks.},
  pmcid = {PMC10121157},
  pmid = {34763073}
}

@inproceedings{dongMotionPlanningProbabilistic2016,
  title = {Motion {{Planning}} as {{Probabilistic Inference}} Using {{Gaussian Processes}} and {{Factor Graphs}}},
  booktitle = {Robotics: {{Science}} and {{Systems}}},
  author = {Dong, Jing and Mukadam, Mustafa and Dellaert, Frank and Boots, Byron},
  year = {2016},
  month = jun,
  doi = {10.15607/RSS.2016.XII.001}
}

@phdthesis{dornhegeTaskPlanningHighLevel,
  title = {{Task Planning for High-Level Robot Control}},
  author = {Dornhege, Christian},
  address = {Freiburg},
  langid = {ngerman},
  school = {Universit{\"a}t Freiburg},
  file = {C:\Users\benja\Zotero\storage\GYHVLAPW\Dornhege - Task Planning for High-Level Robot Control.pdf}
}

@phdthesis{dornhegeTaskPlanningHighlevel2015,
  title = {Task Planning for High-Level Robot Control},
  author = {Dornhege, Christian},
  year = {2015},
  month = jan,
  abstract = {Making intelligent decisions is an essential capability of any robotic system. A reasoning component that solves this problem must combine all available skills of a robot to solve a complex task. This thesis investigates task planning as such a reasoning mechanism. The strength of automated planning lies in the fact that a planner only requires a description of a robot's skills and the goal to reach. From this it computes action sequences for arbitrary situations. To apply planning techniques to robotics we must ensure that these are able to deal with the continuous and geometric nature of real-world tasks. Therefore the first half of the thesis describes planning tasks that integrate external reasoners into the planning process. We consider planning operators to have a symbolic and geometric aspects. While planners deal very well with the former, symbolic task descriptions are not expressive enough for the latter. We introduce the concept of semantic attachments that connect a symbolic predicate like "is the cup graspable" with an external reasoner that computes this query. These allow us, for example, to describe and plan for mobile manipulation tasks soundly. Another issue is that we cannot describe geometrical choices such as where to place an object on a table as planning tasks are required to be finite. To solve this we generate different instantiations of planning operators reflecting different options during the planning process by an external procedure. This made it necessary to develop a new search algorithm for planning with infinite branching factors. This new algorithm outperforms classical search algorithms on manipulation planning tasks. The second half of the thesis investigates techniques for integrating task planning into robotic systems. First, we formulate properties of real-world tasks, e.g., unexpected action outcomes or uncertainty about the world. To tackle these we follow the concept of continual planning, where the planner is embedded in an observation, monitoring and replanning loop. We state what kind of simplifications we make to be able to apply our planner to real-world scenarios and specifically address under which assumptions such a system is guaranteed to reach a desired goal. We demonstrate this by implementing a complex mobile manipulation system. Finally we investigate multi-robot coverage search in 3d. Here, the robots have to observe a known three-dimensional environment as quickly as possible with their sensors. This is a challenging robotics task especially in 3d scenarios that also contains a task planning problem. First, we generate a set of high-quality view poses. A subset of these have to be visited in a short amount of time to cover the search area. We introduce greedy and planning based methods to solve this problem and compare these algorithms in simulation and real-world experiments. The main result of this thesis is our task planner Temporal Fast Downward with Modules (TFD/M) that extends a classical planner with the aforementioned planning techniques. This is used in our continual planning infrastructure that allows to embed this planner into a robotic system. Our evaluation in simulation and real-world experiments shows that task planning is a viable solution for high-level decision making in robotics. The techniques developed in this thesis are essential for that.},
  file = {C:\Users\benja\Zotero\storage\QMCMV8HR\Dornhege - Task Planning for High-Level Robot Control.pdf}
}

@inproceedings{dosovitskiyImageWorth16x162020,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2020},
  month = oct,
  urldate = {2024-01-05},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  langid = {english}
}

@article{Dou.2016,
  title = {{{Fusion4D}}},
  author = {Dou, Mingsong and Taylor, Jonathan and Kohli, Pushmeet and Tankovich, Vladimir and Izadi, Shahram and Khamis, Sameh and Degtyarev, Yury and Davidson, Philip and Fanello, Sean Ryan and Kowdle, Adarsh and Escolano, Sergio Orts and Rhemann, Christoph and Kim, David},
  year = {2016},
  journal = {ACM Transactions on Graphics},
  volume = {35},
  number = {4},
  pages = {1--13},
  issn = {07300301},
  doi = {10.1145/2897824.2925969},
  pagination = {page}
}

@article{Dou.2017,
  title = {Motion2fusion},
  author = {Dou, Mingsong and Davidson, Philip and Fanello, Sean Ryan and Khamis, Sameh and Kowdle, Adarsh and Rhemann, Christoph and Tankovich, Vladimir and Izadi, Shahram},
  year = {2017},
  journal = {ACM Transactions on Graphics},
  volume = {36},
  number = {6},
  pages = {1--16},
  issn = {07300301},
  doi = {10.1145/3130800.3130801},
  pagination = {page}
}

@misc{Dragbot2020,
  title = {Drag\&bot},
  year = {2020},
  month = may,
  journal = {drag\&bot},
  urldate = {2020-05-12},
  abstract = {Easy Robot Programming  Operating industrial robots like a smartphone  Watch Video},
  howpublished = {https://www.dragandbot.com/},
  langid = {american},
  file = {C:\Users\benja\Zotero\storage\GYRQS9LC\www.dragandbot.com.html}
}

@article{driessDeepVisualHeuristics,
  title = {Deep {{Visual Heuristics}}: {{Learning Feasibility}} of {{Mixed-Integer Programs}} for {{Manipulation Planning}}},
  author = {Driess, Danny and Oguz, Ozgur and Ha, Jung-Su and Toussaint, Marc},
  pages = {7},
  abstract = {In this paper, we propose a deep neural network that predicts the feasibility of a mixed-integer program from visual input for robot manipulation planning. Integrating learning into task and motion planning is challenging, since it is unclear how the scene and goals can be encoded as input to the learning algorithm in a way that enables to generalize over a variety of tasks in environments with changing numbers of objects and goals. To achieve this, we propose to encode the scene and the target object directly in the image space.},
  langid = {english}
}

@inproceedings{driessDeepVisualHeuristics2020,
  title = {Deep {{Visual Heuristics}}: {{Learning Feasibility}} of {{Mixed-Integer Programs}} for {{Manipulation Planning}}},
  shorttitle = {Deep {{Visual Heuristics}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Driess, Danny and Oguz, Ozgur and Ha, Jung-Su and Toussaint, Marc},
  year = {2020},
  month = may,
  pages = {9563--9569},
  issn = {2577-087X},
  doi = {10.1109/ICRA40945.2020.9197291},
  urldate = {2024-04-25},
  abstract = {In this paper, we propose a deep neural network that predicts the feasibility of a mixed-integer program from visual input for robot manipulation planning. Integrating learning into task and motion planning is challenging, since it is unclear how the scene and goals can be encoded as input to the learning algorithm in a way that enables to generalize over a variety of tasks in environments with changing numbers of objects and goals. To achieve this, we propose to encode the scene and the target object directly in the image space.Our experiments show that our proposed network generalizes to scenes with multiple objects, although during training only two objects are present at the same time. By using the learned network as a heuristic to guide the search over the discrete variables of the mixed-integer program, the number of optimization problems that have to be solved to find a feasible solution or to detect infeasibility can greatly be reduced.},
  keywords = {Grasping,Neural networks,Planning,Robot sensing systems,Search problems,Task analysis},
  file = {C:\Users\benja\Zotero\storage\E4SJCKV4\9197291.html}
}

@article{driessDeepVisualReasoning2020,
  title = {Deep {{Visual Reasoning}}: {{Learning}} to {{Predict Action Sequences}} for {{Task}} and {{Motion Planning}} from an {{Initial Scene Image}}},
  shorttitle = {Deep {{Visual Reasoning}}},
  author = {Driess, Danny and Ha, Jung-Su and Toussaint, Marc},
  year = {2020},
  month = jul,
  journal = {Robotics: Science and Systems XVI},
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10.15607/RSS.2020.XVI.003},
  urldate = {2024-09-29},
  abstract = {In this paper, we propose a deep convolutional recurrent neural network that predicts action sequences for task and motion planning (TAMP) from an initial scene image. Typical TAMP problems are formalized by combining reasoning on a symbolic, discrete level (e.g. first-order logic) with continuous motion planning such as nonlinear trajectory optimization. Due to the great combinatorial complexity of possible discrete action sequences, a large number of optimization/motion planning problems have to be solved to find a solution, which limits the scalability of these approaches.  To circumvent this combinatorial complexity, we develop a neural network which, based on an initial image of the scene, directly predicts promising discrete action sequences such that ideally only one motion planning problem has to be solved to find a solution to the overall TAMP problem. A key aspect is that our method generalizes to scenes with many and varying number of objects, although being trained on only two objects at a time. This is possible by encoding the objects of the scene in images as input to the neural network, instead of a fixed feature vector. Results show runtime improvements of several magnitudes. Video: this https URL},
  isbn = {9780992374761},
  file = {C:\Users\benja\Zotero\storage\UETFBQGQ\Driess et al. - 2020 - Deep Visual Reasoning Learning to Predict Action Sequences for Task and Motion Planning from an Ini.pdf}
}

@inproceedings{driessLearningGeometricReasoning2021,
  title = {Learning {{Geometric Reasoning}} and {{Control}} for {{Long-Horizon Tasks}} from {{Visual Input}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Driess, Danny and Ha, Jung-Su and Tedrake, Russ and Toussaint, Marc},
  year = {2021},
  month = may,
  pages = {14298--14305},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9560934},
  urldate = {2024-10-20},
  abstract = {Long-horizon manipulation tasks require joint reasoning over a sequence of discrete actions and their associated continuous control parameters. While Task and Motion Planning (TAMP) approaches are capable of generating motion plans that account for this joint reasoning, they usually assume full knowledge about the environment (e.g. in terms of shapes, poses of objects) and often require computation times not suitable for real-time control.To overcome this, we propose a learning framework where a high-level reasoning network predicts, based on an image of the scene, a sequence of discrete actions and the parameter values of their associated low-level controllers. These controllers are parameterized in terms of a learned energy function, leading to time-invariant controllers for each phase. We train the whole framework end-to-end using a dataset of TAMP solutions computed using Logic Geometric Programming. A key feature is that the reasoning network determines the parameters of the controllers jointly, such that the overall task can be solved. Despite having no explicit representation of the geometry nor pose of the objects in the scene, our network is still able to accomplish geometrically precise manipulation tasks, including handovers and an accurate pointing task where the parameters of early actions are tightly coupled with those of later actions. Video: https://youtu.be/AcPWRTkr3\_g},
  keywords = {Cognition,Handover,Knowledge engineering,Programming,Robot sensing systems,Shape,Visualization},
  file = {C:\Users\benja\Zotero\storage\GY8LRYNG\Driess et al. - 2021 - Learning Geometric Reasoning and Control for Long-.pdf}
}

@inproceedings{driessPaLMEEmbodiedMultimodal2023,
  title = {{{PaLM-E}}: An Embodied Multimodal Language Model},
  shorttitle = {{{PaLM-E}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
  year = {2023},
  month = jul,
  series = {{{ICML}}'23},
  volume = {202},
  pages = {8469--8488},
  publisher = {JMLR.org},
  address = {Honolulu, Hawaii, USA},
  urldate = {2024-01-05},
  abstract = {Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g. for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multimodal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internetscale language, vision, and visual-language domains. Our largest model with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.}
}

@article{dsilvaSurveyAutomatedTechniques2008,
  title = {A {{Survey}} of {{Automated Techniques}} for {{Formal Software Verification}}},
  author = {D'Silva, Vijay and Kroening, Daniel and Weissenbacher, Georg},
  year = {2008},
  month = jul,
  journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {27},
  number = {7},
  pages = {1165--1178},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2008.923410},
  urldate = {2024-04-19},
  abstract = {The quality and the correctness of software are often the greatest concern in electronic systems. Formal verification tools can provide a guarantee that a design is free of specific flaws. This paper surveys algorithms that perform automatic static analysis of software to detect programming errors or prove their absence. The three techniques considered are static analysis with abstract domains, model checking, and bounded model checking. A short tutorial on these techniques is provided, highlighting their differences when applied to practical problems. This paper also surveys tools implementing these techniques and describes their merits and shortcomings.},
  keywords = {Algorithm design and analysis,Automatic programming,Automatic testing,Bounded model checking (BMC),Formal verification,Hardware,model checking,Performance analysis,predicate abstraction,Software algorithms,Software performance,Software quality,Software systems,software verification,static analysis},
  file = {C:\Users\benja\Zotero\storage\87SAELRI\4544862.html}
}

@article{Duan2017a,
  title = {One-{{Shot Imitation Learning}}},
  author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly C. and Ho, Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
  year = {2017},
  month = mar,
  eprint = {1703.07326},
  urldate = {2018-08-19},
  abstract = {Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning. Specifically, we consider the setting where there is a very large set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. The use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks. Videos available at https://bit.ly/nips2017-oneshot .},
  archiveprefix = {arXiv}
}

@article{duanOneShotImitationLearning2017,
  title = {One-{{Shot Imitation Learning}}},
  author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly C. and Ho, Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.07326 [cs]},
  eprint = {1703.07326},
  primaryclass = {cs},
  urldate = {2019-05-17},
  abstract = {Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning. Specifically, we consider the setting where there is a very large set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. The use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks. Videos available at https://bit.ly/nips2017-oneshot .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\6BMZ9SP5\1703.html}
}

@inproceedings{duanOneshotImitationLearning2017a,
  title = {One-Shot Imitation Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly and Ho, Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
  year = {2017},
  month = dec,
  pages = {1087--1098},
  urldate = {2020-06-30},
  abstract = {Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning. Specifically, we consider the setting where there is a very large (maybe infinite) set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained such that when it takes as input the first demonstration demonstration and a state sampled from the second demonstration, it should predict the action corresponding to the sampled state. At test time, a full demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. Our experiments show that the use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks.},
  isbn = {978-1-5108-6096-4}
}

@article{duAutoTunedSimtoRealTransfer2021,
  title = {Auto-{{Tuned Sim-to-Real Transfer}}},
  author = {Du, Yuqing and Watkins, Olivia and Darrell, Trevor and Abbeel, Pieter and Pathak, Deepak},
  year = {2021},
  month = may,
  journal = {arXiv:2104.07662 [cs]},
  eprint = {2104.07662},
  primaryclass = {cs},
  urldate = {2021-06-01},
  abstract = {Policies trained in simulation often fail when transferred to the real world due to the `reality gap' where the simulator is unable to accurately capture the dynamics and visual properties of the real world. Current approaches to tackle this problem, such as domain randomization, require prior knowledge and engineering to determine how much to randomize system parameters in order to learn a policy that is robust to sim-to-real transfer while also not being too conservative. We propose a method for automatically tuning simulator system parameters to match the real world using only raw RGB images of the real world without the need to define rewards or estimate state. Our key insight is to reframe the auto-tuning of parameters as a search problem where we iteratively shift the simulation system parameters to approach the real-world system parameters. We propose a Search Param Model (SPM) that, given a sequence of observations and actions and a set of system parameters, predicts whether the given parameters are higher or lower than the true parameters used to generate the observations. We evaluate our method on multiple robotic control tasks in both sim-to-sim and sim-to-real transfer, demonstrating significant improvement over naive domain randomization. Project videos and code at https://yuqingd.github.io/autotuned-sim2real/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\Z9IIQA6K\2104.html}
}

@article{Dube12,
  title = {Task {{Time Optimization}} of a {{Robot Manipulator}} Using {{Artificial Neural Network}} and {{Genetic Algorithm}}},
  author = {Dubey, Akash Dutt and Mishra, Ravi B and Jha, Arun K},
  year = {2012},
  volume = {51},
  number = {13},
  pages = {26--33}
}

@misc{dubeyLlama3Herd2024,
  title = {The {{Llama}} 3 {{Herd}} of {{Models}}},
  author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and {Al-Dahle}, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and {Garcia-Olano}, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and {van der Linde}, Jelmer and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and {El-Arini}, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and {Rantala-Yeary}, Lauren and {van der Maaten}, Laurens and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and {de Oliveira}, Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Duchenne, Olivier and {\c C}elebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Tan, Xiaoqing Ellen and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Grattafiori, Aaron and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Vaughan, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Franco, Annie and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and De Paola, Beto and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Wyatt, Danny and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Ozgenel, Firat and Caggioni, Francesco and Guzm{\'a}n, Francisco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Thattai, Govind and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Laptev, Nikolay Pavlovich and Dong, Ning and Zhang, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Maheswari, Rohan and Howes, Russ and Rinott, Ruty and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Albiero, V{\'i}tor and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang, Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Hao, Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei},
  year = {2024},
  month = aug,
  number = {arXiv:2407.21783},
  eprint = {2407.21783},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.21783},
  urldate = {2024-08-26},
  abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\benja\\Zotero\\storage\\385ZLZFI\\Dubey et al. - 2024 - The Llama 3 Herd of Models.pdf;C\:\\Users\\benja\\Zotero\\storage\\3K9EWDXY\\2407.html}
}

@article{duburcqOnlineTrajectoryPlanning2020,
  title = {Online {{Trajectory Planning Through Combined Trajectory Optimization}} and {{Function Approximation}}: {{Application}} to the {{Exoskeleton Atalante}}},
  shorttitle = {Online {{Trajectory Planning Through Combined Trajectory Optimization}} and {{Function Approximation}}},
  author = {Duburcq, Alexis and Chevaleyre, Yann and Bredeche, Nicolas and Bo{\'e}ris, Guilhem},
  year = {2020},
  month = mar,
  journal = {arXiv:1910.00514 [cs]},
  eprint = {1910.00514},
  primaryclass = {cs},
  urldate = {2020-06-30},
  abstract = {Autonomous robots require online trajectory planning capability to operate in the real world. Efficient offline trajectory planning methods already exist, but are computationally demanding, preventing their use online. In this paper, we present a novel algorithm called Guided Trajectory Learning that learns a function approximation of solutions computed through trajectory optimization while ensuring accurate and reliable predictions. This function approximation is then used online to generate trajectories. This algorithm is designed to be easy to implement, and practical since it does not require massive computing power. It is readily applicable to any robotics systems and effortless to set up on real hardware since robust control strategies are usually already available. We demonstrate the computational performance of our algorithm on flat-foot walking with the self-balanced exoskeleton Atalante.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\V5TY9RGB\1910.html}
}

@inproceedings{duenserInteractiveRoboticManipulation2018,
  title = {Interactive {{Robotic Manipulation}} of {{Elastic Objects}}},
  booktitle = {{{IROS}}},
  author = {Duenser, Simon and Bern, James M. and Poranne, Roi and Coros, Stelian},
  year = {2018},
  month = oct,
  pages = {3476--3481},
  publisher = {IEEE},
  address = {Madrid},
  doi = {10.1109/IROS.2018.8594291},
  urldate = {2019-10-10},
  abstract = {In this paper, we address the challenge of robotic manipulation of elastically deforming objects. To this end, we model elastic objects using the Finite Element Method. Through a quasi-static assumption, we leverage sensitivity analysis to mathematically model how changes in the robot's configuration affect the deformed shape of the object being manipulated. This enables an interactive, simulation-based control methodology, wherein user-specified deformations for the elastic objects are automatically mapped to joint angle commands. The optimization formulation we introduce is general, operates directly within a robot's workspace and can readily incorporate joint limits as well as collision avoidance between the links. We validate our control methodology on a YuMi R IRB 14000, which we use to manipulate a variety of elastic objects.},
  isbn = {978-1-5386-8094-0},
  langid = {english}
}

@article{dulebaComparisonJacobianbasedMethods2013,
  title = {A Comparison of {{Jacobian-based}} Methods of Inverse Kinematics for Serial Robot Manipulators},
  author = {Dul{\k e}ba, Ignacy and Opa{\l}ka, Micha{\l}},
  year = {2013},
  month = jun,
  journal = {International Journal of Applied Mathematics and Computer Science},
  volume = {23},
  number = {2},
  pages = {373--382},
  urldate = {2024-03-31},
  abstract = {The objective of this paper is to present and make a comparative study of several inverse kinematics methods for serial manipulators, based on the Jacobian matrix. Besides the well-known Jacobian transpose and Jacobian pseudo-inverse methods, three others, borrowed from numerical analysis, are presented. Among them, two approximation methods avoid the explicit manipulability matrix inversion, while the third one is a slightly modified version of the Levenberg-Marquardt method (mLM). Their comparison is based on the evaluation of a short distance approaching the goal point and on their computational complexity. As the reference method, the Jacobian pseudo-inverse is utilized. Simulation results reveal that the modified Levenberg-Marquardt method is promising, while the first order approximation method is reliable and requires mild computational costs. Some hints are formulated concerning the application of Jacobian-based methods in practice.},
  langid = {english}
}

@book{dumasMethodologyJointStiffness2010,
  title = {A Methodology for Joint Stiffness Identification of Serial Robots},
  author = {Dumas, Claire and Caro, St{\'e}phane and Ch{\'e}rif, Medhi and Garnier, Sebastien and Furet, Beno{\^i}t},
  year = {2010},
  month = oct,
  journal = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
  pages = {469},
  doi = {10.1109/IROS.2010.5652140},
  abstract = {This paper presents a new methodology for joint stiffness identification of serial robots. This methodology aims at evaluating all joint stiffness values responsible for both translational and rotational displacements of the robot end-effector subject to an external wrench (force and torque). The links of the robot are supposed to be quite stiffer than the joints and not known as it is usually the case with industrial serial robots. The robustness of the identification method and the sensitivity of the results to measurement errors and number of experimental tests are also analyzed. The Kuka KR240-2 robot is used as an illustrative example through the paper.}
}

@misc{dunefskyTranscodersEnableFinegrained2024,
  title = {Transcoders Enable Fine-Grained Interpretable Circuit Analysis for Language Models},
  author = {Dunefsky, Jacob and Chlenski, Philippe and Nanda, Neel},
  year = {2024},
  month = apr,
  urldate = {2024-05-02},
  abstract = {Summary  * We present a method for performing circuit analysis on language models using "transcoders," an occasionally-discussed variant of SAEs tha{\dots}},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\63ZSXYVW\transcoders-enable-fine-grained-interpretable-circuit.html}
}

@article{Dutagaci.2012,
  title = {Evaluation of {{3D}} Interest Point Detection Techniques via Human-Generated Ground Truth},
  author = {Dutagaci, Helin and Cheung, Chun Pan and Godil, Afzal},
  year = {2012},
  journal = {The Visual Computer},
  volume = {28},
  number = {9},
  pages = {901--917},
  issn = {0178-2789},
  doi = {10.1007/s00371-012-0746-4},
  pagination = {page}
}

@article{dwivediArtificialIntelligenceAI2021,
  title = {Artificial {{Intelligence}} ({{AI}}): {{Multidisciplinary}} Perspectives on Emerging Challenges, Opportunities, and Agenda for Research, Practice and Policy},
  shorttitle = {Artificial {{Intelligence}} ({{AI}})},
  author = {Dwivedi, Yogesh K. and Hughes, Laurie and Ismagilova, Elvira and Aarts, Gert and Coombs, Crispin and Crick, Tom and Duan, Yanqing and Dwivedi, Rohita and Edwards, John and Eirug, Aled and Galanos, Vassilis and Ilavarasan, P. Vigneswara and Janssen, Marijn and Jones, Paul and Kar, Arpan Kumar and Kizgin, Hatice and Kronemann, Bianca and Lal, Banita and Lucini, Biagio and Medaglia, Rony and {Le Meunier-FitzHugh}, Kenneth and {Le Meunier-FitzHugh}, Leslie Caroline and Misra, Santosh and Mogaji, Emmanuel and Sharma, Sujeet Kumar and Singh, Jang Bahadur and Raghavan, Vishnupriya and Raman, Ramakrishnan and Rana, Nripendra P. and Samothrakis, Spyridon and Spencer, Jak and Tamilmani, Kuttimani and Tubadji, Annie and Walton, Paul and Williams, Michael D.},
  year = {2021},
  month = apr,
  journal = {International Journal of Information Management},
  volume = {57},
  pages = {101994},
  issn = {0268-4012},
  doi = {10.1016/j.ijinfomgt.2019.08.002},
  urldate = {2022-04-05},
  abstract = {As far back as the industrial revolution, significant development in technical innovation has succeeded in transforming numerous manual tasks and processes that had been in existence for decades where humans had reached the limits of physical capacity. Artificial Intelligence (AI) offers this same transformative potential for the augmentation and potential replacement of human tasks and activities within a wide range of industrial, intellectual and social applications. The pace of change for this new AI technological age is staggering, with new breakthroughs in algorithmic machine learning and autonomous decision-making, engendering new opportunities for continued innovation. The impact of AI could be significant, with industries ranging from: finance, healthcare, manufacturing, retail, supply chain, logistics and utilities, all potentially disrupted by the onset of AI technologies. The study brings together the collective insight from a number of leading expert contributors to highlight the significant opportunities, realistic assessment of impact, challenges and potential research agenda posed by the rapid emergence of AI within a number of domains: business and management, government, public sector, and science and technology. This research offers significant and timely insight to AI technology and its impact on the future of industry and society in general, whilst recognising the societal and industrial influence on pace and direction of AI development.},
  langid = {english},
  keywords = {AI,Artificial intelligence,Cognitive computing,Expert systems,Machine learning,Research agenda}
}

@article{ebertVisualForesightModelBased2018,
  title = {Visual {{Foresight}}: {{Model-Based Deep Reinforcement Learning}} for {{Vision-Based Robotic Control}}},
  shorttitle = {Visual {{Foresight}}},
  author = {Ebert, Frederik and Finn, Chelsea and Dasari, Sudeep and Xie, Annie and Lee, Alex and Levine, Sergey},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.00568 [cs]},
  eprint = {1812.00568},
  primaryclass = {cs},
  urldate = {2019-06-27},
  abstract = {Deep reinforcement learning (RL) algorithms can learn complex robotic skills from raw sensory inputs, but have yet to achieve the kind of broad generalization and applicability demonstrated by deep learning methods in supervised domains. We present a deep RL method that is practical for real-world robotics tasks, such as robotic manipulation, and generalizes effectively to never-before-seen tasks and objects. In these settings, ground truth reward signals are typically unavailable, and we therefore propose a self-supervised model-based approach, where a predictive model learns to directly predict the future from raw sensory readings, such as camera images. At test time, we explore three distinct goal specification methods: designated pixels, where a user specifies desired object manipulation tasks by selecting particular pixels in an image and corresponding goal positions, goal images, where the desired goal state is specified with an image, and image classifiers, which define spaces of goal states. Our deep predictive models are trained using data collected autonomously and continuously by a robot interacting with hundreds of objects, without human supervision. We demonstrate that visual MPC can generalize to never-before-seen objects---both rigid and deformable---and solve a range of user-defined object manipulation tasks using the same model.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics}
}

@article{ebertVisualForesightModelBased2018a,
  title = {Visual {{Foresight}}: {{Model-Based Deep Reinforcement Learning}} for {{Vision-Based Robotic Control}}},
  shorttitle = {Visual {{Foresight}}},
  author = {Ebert, Frederik and Finn, Chelsea and Dasari, Sudeep and Xie, Annie and Lee, Alex and Levine, Sergey},
  year = {2018},
  eprint = {1812.00568},
  urldate = {2020-07-12},
  abstract = {Deep reinforcement learning (RL) algorithms can learn complex robotic skills from raw sensory inputs, but have yet to achieve the kind of broad generalization and applicability demonstrated by deep learning methods in supervised domains. We present a deep RL method that is practical for real-world robotics tasks, such as robotic manipulation, and generalizes effectively to never-before-seen tasks and objects. In these settings, ground truth reward signals are typically unavailable, and we therefore propose a self-supervised model-based approach, where a predictive model learns to directly predict the future from raw sensory readings, such as camera images. At test time, we explore three distinct goal specification methods: designated pixels, where a user specifies desired object manipulation tasks by selecting particular pixels in an image and corresponding goal positions, goal images, where the desired goal state is specified with an image, and image classifiers, which define spaces of goal states. Our deep predictive models are trained using data collected autonomously and continuously by a robot interacting with hundreds of objects, without human supervision. We demonstrate that visual MPC can generalize to never-before-seen objects---both rigid and deformable---and solve a range of user-defined object manipulation tasks using the same model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\T2BHN435\1812.html}
}

@article{edmondsTaleTwoExplanations2019,
  title = {A Tale of Two Explanations: {{Enhancing}} Human Trust by Explaining Robot Behavior},
  shorttitle = {A Tale of Two Explanations},
  author = {Edmonds, Mark and Gao, Feng and Liu, Hangxin and Xie, Xu and Qi, Siyuan and Rothrock, Brandon and Zhu, Yixin and Wu, Ying Nian and Lu, Hongjing and Zhu, Song-Chun},
  year = {2019},
  month = dec,
  journal = {Science Robotics},
  volume = {4},
  number = {37},
  publisher = {Science Robotics},
  issn = {2470-9476},
  doi = {10.1126/scirobotics.aay4663},
  urldate = {2020-07-01},
  abstract = {The ability to provide comprehensive explanations of chosen actions is a hallmark of intelligence. Lack of this ability impedes the general acceptance of AI and robot systems in critical tasks. This paper examines what forms of explanations best foster human trust in machines and proposes a framework in which explanations are generated from both functional and mechanistic perspectives. The robot system learns from human demonstrations to open medicine bottles using (i) an embodied haptic prediction model to extract knowledge from sensory feedback, (ii) a stochastic grammar model induced to capture the compositional structure of a multistep task, and (iii) an improved Earley parsing algorithm to jointly leverage both the haptic and grammar models. The robot system not only shows the ability to learn from human demonstrators but also succeeds in opening new, unseen bottles. Using different forms of explanations generated by the robot system, we conducted a psychological experiment to examine what forms of explanations best foster human trust in the robot. We found that comprehensive and real-time visualizations of the robot's internal decisions were more effective in promoting human trust than explanations based on summary text descriptions. In addition, forms of explanation that are best suited to foster trust do not necessarily correspond to the model components contributing to the best task performance. This divergence shows a need for the robotics community to integrate model components to enhance both task execution and human trust in machines. Forms of explanation that are best suited to foster trust do not necessarily correspond to those components contributing to the best task performance. Forms of explanation that are best suited to foster trust do not necessarily correspond to those components contributing to the best task performance.},
  chapter = {Research Article},
  copyright = {Copyright {\copyright} 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  langid = {english}
}

@article{eiterComputingDiscreteFrechet,
  title = {Computing {{Discrete Fr{\'e}chet Distance}}},
  author = {Eiter, Thomas and Mannila, Heikki},
  pages = {8},
  abstract = {The Fr{\textasciiacute}echet distance between two curves in a metric space is a measure of the similarity between the curves. We present a discrete variation of this measure. It provides good approximations of the continuous measure and can be efficiently computed using a simple algorithm. We also consider variants of discrete Fr{\textasciiacute}echet distance, and find an interesting connection to measuring distance between theories.},
  langid = {english}
}

@inproceedings{ekvallLearningTaskModels2006,
  title = {Learning {{Task Models}} from {{Multiple Human Demonstrations}}},
  booktitle = {{{ROMAN}} 2006 - {{The}} 15th {{IEEE International Symposium}} on {{Robot}} and {{Human Interactive Communication}}},
  author = {Ekvall, Staffan and Kragic, Danica},
  year = {2006},
  month = sep,
  pages = {358--363},
  issn = {1944-9437},
  doi = {10.1109/ROMAN.2006.314460},
  urldate = {2024-04-24},
  abstract = {In this paper, we present a novel method for learning robot tasks from multiple demonstrations. Each demonstrated task is decomposed into subtasks that allow for segmentation and classification of the input data. The demonstrated tasks are then merged into a flexible task model, describing the task goal and its constraints. The two main contributions of the paper are the state generation and contraints identification methods. We also present a task level planner, that is used to assemble a task plan at run-time, allowing the robot to choose the best strategy depending on the current world state},
  keywords = {Computer vision,Education,Educational robots,Human robot interaction,Postal services,Robot programming,Robot sensing systems,Robot vision systems,Robotic assembly,Runtime},
  file = {C:\Users\benja\Zotero\storage\M8HEQ383\4107834.html}
}

@article{elbanhawiSamplingBasedRobotMotion2014,
  title = {Sampling-{{Based Robot Motion Planning}}: {{A Review}}},
  shorttitle = {Sampling-{{Based Robot Motion Planning}}},
  author = {Elbanhawi, M. and Simic, M.},
  year = {2014},
  journal = {IEEE Access},
  volume = {2},
  pages = {56--77},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2014.2302442},
  abstract = {Motion planning is a fundamental research area in robotics. Sampling-based methods offer an efficient solution for what is otherwise a rather challenging dilemma of path planning. Consequently, these methods have been extended further away from basic robot planning into further difficult scenarios and diverse applications. A comprehensive survey of the growing body of work in sampling-based planning is given here. Simulations are executed to evaluate some of the proposed planners and highlight some of the implementation details that are often left unspecified. An emphasis is placed on contemporary research directions in this field. We address planners that tackle current issues in robotics. For instance, real-life kinodynamic planning, optimal planning, replanning in dynamic environments, and planning under uncertainty are discussed. The aim of this paper is to survey the state of the art in motion planning and to assess selected planners, examine implementation details and above all shed a light on the current challenges in motion planning and the promising approaches that will potentially overcome those problems.},
  keywords = {autonomous robots,dynamic environments,Heuristic algorithms,kinodynamic planning,Measurement,mobile robots,motion,optimal replanning,path,path planning,Path planning,Planning,PRM,randomization,robot dynamics,robot kinematics,Robot sensing systems,RRT,sampling,sampling methods,sampling-based robot motion planning,uncertain systems,uncertainty,Vegetation}
}

@misc{elhageToyModelsSuperposition2022,
  title = {Toy {{Models}} of {{Superposition}}},
  author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and {Hatfield-Dodds}, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
  year = {2022},
  month = sep,
  journal = {arXiv e-prints},
  doi = {10.48550/arXiv.2209.10652},
  urldate = {2024-04-27},
  abstract = {Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as 'polysemanticity' which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in "superposition." We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We also discuss potential implications for mechanistic interpretability.},
  keywords = {Computer Science - Machine Learning},
  annotation = {ADS Bibcode: 2022arXiv220910652E}
}

@inproceedings{elhihiHierarchicalRecurrentNeural1995,
  title = {Hierarchical {{Recurrent Neural Networks}} for {{Long-term Dependencies}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {El Hihi, Salah and Bengio, Yoshua},
  year = {1995},
  pages = {493--499},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  urldate = {2019-07-21},
  abstract = {We have already shown that extracting long-term dependencies from sequential data is difficult, both for determimstic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Experiments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs.}
}

@article{ellefsenHowMixtureDensity2019,
  title = {How Do {{Mixture Density RNNs Predict}} the {{Future}}?},
  author = {Ellefsen, Kai Olav and Martin, Charles Patrick and Torresen, Jim},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.07859 [cs, stat]},
  eprint = {1901.07859},
  primaryclass = {cs, stat},
  urldate = {2019-11-22},
  abstract = {Gaining a better understanding of how and what machine learning systems learn is important to increase confidence in their decisions and catalyze further research. In this paper, we analyze the predictions made by a specific type of recurrent neural network, mixture density RNNs (MD-RNNs). These networks learn to model predictions as a combination of multiple Gaussian distributions, making them particularly interesting for problems where a sequence of inputs may lead to several distinct future possibilities. An example is learning internal models of an environment, where different events may or may not occur, but where the average over different events is not meaningful. By analyzing the predictions made by trained MD-RNNs, we find that their different Gaussian components have two complementary roles: 1) Separately modeling different stochastic events and 2) Separately modeling scenarios governed by different rules. These findings increase our understanding of what is learned by predictive MD-RNNs, and open up new research directions for further understanding how we can benefit from their self-organizing model decomposition.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{ellisLearningInferGraphics2018,
  title = {Learning to {{Infer Graphics Programs}} from {{Hand-Drawn Images}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ellis, Kevin and Ritchie, Daniel and {Solar-Lezama}, Armando and Tenenbaum, Josh},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-04-02}
}

@article{ellisWriteExecuteAssess2019,
  title = {Write, {{Execute}}, {{Assess}}: {{Program Synthesis}} with a {{REPL}}},
  shorttitle = {Write, {{Execute}}, {{Assess}}},
  author = {Ellis, Kevin and Nye, Maxwell and Pu, Yewen and Sosa, Felix and Tenenbaum, Josh and {Solar-Lezama}, Armando},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.04604 [cs]},
  eprint = {1906.04604},
  primaryclass = {cs},
  urldate = {2020-04-27},
  abstract = {We present a neural program synthesis approach integrating components which write, execute, and assess code to navigate the search space of possible programs. We equip the search process with an interpreter or a read-eval-print-loop (REPL), which immediately executes partially written programs, exposing their semantics. The REPL addresses a basic challenge of program synthesis: tiny changes in syntax can lead to huge changes in semantics. We train a pair of models, a policy that proposes the new piece of code to write, and a value function that assesses the prospects of the code written so-far. At test time we can combine these models with a Sequential Monte Carlo algorithm. We apply our approach to two domains: synthesizing text editing programs and inferring 2D and 3D graphics programs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages,Computer Science - Software Engineering},
  file = {C:\Users\benja\Zotero\storage\MV67E5FB\1906.html}
}

@article{Elma90,
  title = {Finding {{Structure}} in {{Time}}},
  author = {Elman, Jeffrey L},
  year = {1990},
  volume = {14},
  number = {2},
  pages = {179--211}
}

@article{elmaraghyAutomaticRobotProgram1992,
  title = {Automatic Robot Program Synthesis for Assembly},
  author = {ElMaraghy, H. A. and Rondeau, J. M.},
  year = {1992},
  month = mar,
  journal = {Robotica},
  volume = {10},
  number = {2},
  pages = {113--123},
  issn = {1469-8668, 0263-5747},
  doi = {10.1017/S0263574700007530},
  urldate = {2024-04-17},
  abstract = {This paper describes a revised version of ROBOPLAN, a goal-oriented robot task planning system for automatic generation, decomposition and execution of high-level robot plans for assembly. It emphasizes its new features, i.e., modularity, formal definition of the task, robust plan synthesis, and execution of each assembly step. A task definition language allows a formal description of the robot universe and the assembly task to be input to ROBOPLAN. The expert task planner is a non-linear backward chaining problem solver, using a goal driven depth-first strategy. The implemented search strategy has been tested in the assembly domain, but it could be used in other domains where planning is needed. The motion planner provides a non-optimal, safe robot trajectory; collision free path planning has not been included yet. A robot executable code is generated for each assembly step and monitored in real time. The error detection and recovery capability of the system is rather limited at present, since no sensors are used. The initial implementation of the system has been tested and evaluated on the assembly of a DC motor. The potential of extending this planning framework to other applications is also discussed.},
  langid = {english},
  keywords = {Assembly,Automatic synthesis,Roboplan,Robot}
}

@article{elzaatariCobotProgrammingCollaborative2019,
  title = {Cobot Programming for Collaborative Industrial Tasks: {{An}} Overview},
  shorttitle = {Cobot Programming for Collaborative Industrial Tasks},
  author = {El Zaatari, Shirine and Marei, Mohamed and Li, Weidong and Usman, Zahid},
  year = {2019},
  month = jun,
  journal = {Robotics and Autonomous Systems},
  volume = {116},
  pages = {162--180},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2019.03.003},
  urldate = {2022-05-06},
  abstract = {Collaborative robots (cobots) have been increasingly adopted in industries to facilitate human--robot collaboration. Despite this, it is challenging to program cobots for collaborative industrial tasks as the programming has two distinct elements that are difficult to implement: (1) an intuitive element to ensure that the operations of a cobot can be composed or altered dynamically by an operator, and (2) a human-aware element to support cobots in producing flexible and adaptive behaviours dependent on human partners. In this area, some research works have been carried out recently, but there is a lack of a systematic summary on the subject. In this paper, an overview of collaborative industrial scenarios and programming requirements for cobots to implement effective collaboration is given. Then, detailed reviews on cobot programming, which are categorised into communication, optimisation, and learning, are conducted. Additionally, a significant gap between cobot programming implemented in industry and in research is identified, and research that works towards bridging this gap is pinpointed. Finally, the future directions of cobots for industrial collaborative scenarios are outlined, including potential points of extension and improvement.},
  langid = {english},
  keywords = {Cobot,Human-awareness,Human-robot collaboration,Intuitive programming},
  file = {C:\Users\benja\Zotero\storage\7EINZVN2\S092188901830602X.html}
}

@misc{endovis_ch,
  title = {Endoscopic Vision Challenge},
  author = {Speidel, Stefanie}
}

@misc{endovis_sch_sceneseg,
  title = {Endoscopic Vision Challenge 2021 - Full Scene Segmentation},
  author = {Bodenstedt, Sebastian and M{\"u}ller, Beat and Kisilenko, Anna and Chen, Jonathan and Wagner, Martin and Speidel, Stefanie and {Maier-Hein}, Lena}
}

@article{endrawisEfficientSelfSupervisedData2021,
  title = {Efficient {{Self-Supervised Data Collection}} for {{Offline Robot Learning}}},
  author = {Endrawis, Shadi and Leibovich, Gal and Jacob, Guy and Novik, Gal and Tamar, Aviv},
  year = {2021},
  month = may,
  journal = {arXiv:2105.04607 [cs]},
  eprint = {2105.04607},
  primaryclass = {cs},
  urldate = {2021-06-02},
  abstract = {A practical approach to robot reinforcement learning is to first collect a large batch of real or simulated robot interaction data, using some data collection policy, and then learn from this data to perform various tasks, using offline learning algorithms. Previous work focused on manually designing the data collection policy, and on tasks where suitable policies can easily be designed, such as random picking policies for collecting data about object grasping. For more complex tasks, however, it may be difficult to find a data collection policy that explores the environment effectively, and produces data that is diverse enough for the downstream task. In this work, we propose that data collection policies should actively explore the environment to collect diverse data. In particular, we develop a simple-yet-effective goal-conditioned reinforcement-learning method that actively focuses data collection on novel observations, thereby collecting a diverse data-set. We evaluate our method on simulated robot manipulation tasks with visual inputs and show that the improved diversity of active data collection leads to significant improvements in the downstream learning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,I.2.10,I.2.6,I.2.9},
  file = {C:\Users\benja\Zotero\storage\VPVMMQFI\2105.html}
}

@inproceedings{englertCombinedOptimizationReinforcement2016,
  title = {Combined {{Optimization}} and {{Reinforcement Learning}} for {{Manipulation Skills}}},
  booktitle = {Robotics: {{Science}} and {{Systems XII}}},
  author = {Englert, Peter and Toussaint, Marc},
  year = {2016},
  month = jun,
  volume = {12},
  urldate = {2020-10-31},
  isbn = {978-0-262-70114-3},
  file = {C:\Users\benja\Zotero\storage\AWANYR8N\p33.html}
}

@article{englertLearningManipulationSkills2018,
  title = {Learning Manipulation Skills from a Single Demonstration},
  author = {Englert, Peter and Toussaint, Marc},
  year = {2018},
  month = jan,
  journal = {The International Journal of Robotics Research},
  volume = {37},
  number = {1},
  pages = {137--154},
  publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  doi = {10.1177/0278364917743795},
  urldate = {2020-10-30},
  abstract = {We consider the scenario where a robot is demonstrated a manipulation skill once and should then use only a few trials on its own to learn to reproduce, optimize, and generalize that same skill. A manipulation skill is generally a high-dimensional policy. To achieve the desired sample efficiency, we need to exploit the inherent structure in this problem. With our approach, we propose to decompose the problem into analytically known objectives, such as motion smoothness, and black-box objectives, such as trial success or reward, depending on the interaction with the environment. The decomposition allows us to leverage and combine (i) constrained optimization methods to address analytic objectives, (ii) constrained Bayesian optimization to explore black-box objectives, and (iii) inverse optimal control methods to eventually extract a generalizable skill representation. The algorithm is evaluated on a synthetic benchmark experiment and compared with state-of-the-art learning methods. We also demonstrate the performance on real-robot experiments with a PR2.},
  langid = {english}
}

@inproceedings{envallDifferentiableTaskAssignment2023,
  title = {Differentiable {{Task Assignment}} and {{Motion Planning}}},
  booktitle = {2023 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Envall, Jimmy and Poranne, Roi and Coros, Stelian},
  year = {2023},
  month = oct,
  pages = {2049--2056},
  issn = {2153-0866},
  doi = {10.1109/IROS55552.2023.10341602},
  urldate = {2024-09-07},
  abstract = {Task and motion planning is one of the key problems in robotics today. It is often formulated as a discrete task allocation problem combined with continuous motion planning. Many existing approaches to TAMP involve explicit descriptions of task primitives that cause discrete changes in the kinematic relationship between the actor and the objects. In this work we propose an alternative, fully differentiable approach which supports a large number of TAMP problem instances. Rather than explicitly enumerating task primitives, actions are instead represented implicitly as part of the solution to a nonlinear optimization problem. We focus on decision making for robotic manipulators, specifically for pick and place tasks, and explore the efficacy of the model through a number of simulated experiments including multiple robots, objects and interactions with the environment. We also show several possible extensions.},
  keywords = {Decision making,Kinematics,Manipulators,Optimization,Planning,Resource management,Task analysis},
  file = {C\:\\Users\\benja\\Zotero\\storage\\YJQGUAYF\\Envall et al. - 2023 - Differentiable Task Assignment and Motion Planning.pdf;C\:\\Users\\benja\\Zotero\\storage\\239QYM9U\\10341602.html}
}

@article{erasmusWhatInterpretability2021,
  title = {What Is {{Interpretability}}?},
  author = {Erasmus, Adrian and Brunet, Tyler D. P. and Fisher, Eyal},
  year = {2021},
  journal = {Philosophy \& Technology},
  volume = {34},
  number = {4},
  pages = {833--862},
  issn = {2210-5433},
  doi = {10.1007/s13347-020-00435-2},
  urldate = {2023-03-12},
  abstract = {We argue that artificial networks are explainable and offer a novel theory of interpretability. Two sets of conceptual questions are prominent in theoretical engagements with artificial neural networks, especially in the context of medical artificial intelligence: (1) Are networks explainable, and if so, what does it mean to explain the output of a network? And (2) what does it mean for a network to be interpretable? We argue that accounts of ``explanation'' tailored specifically to neural networks have ineffectively reinvented the wheel. In response to (1), we show how four familiar accounts of explanation apply to neural networks as they would to any scientific phenomenon. We diagnose the confusion about explaining neural networks within the machine learning literature as an equivocation on ``explainability,'' ``understandability'' and ``interpretability.'' To remedy this, we distinguish between these notions, and answer (2) by offering a theory and typology of interpretation in machine learning. Interpretation is something one does to an explanation with the aim of producing another, more understandable, explanation. As with explanation, there are various concepts and methods involved in interpretation: Total or Partial, Global or Local, and Approximative or Isomorphic. Our account of ``interpretability'' is consistent with uses in the machine learning literature, in keeping with the philosophy of explanation and understanding, and pays special attention to medical artificial intelligence systems.},
  pmcid = {PMC8654716},
  pmid = {34966640}
}

@article{erhanWhyDoesUnsupervised2010,
  title = {Why {{Does Unsupervised Pre-training Help Deep Learning}}?},
  author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
  year = {2010},
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {19},
  pages = {625--660},
  issn = {1533-7928},
  urldate = {2021-06-21},
  file = {C:\Users\benja\Zotero\storage\A4PVC4TV\erhan10a.html}
}

@inproceedings{ernstMH1ComputeroperatedMechanical1962,
  title = {{{MH-1}}, a Computer-Operated Mechanical Hand},
  booktitle = {Proceedings of the {{May}} 1-3, 1962, Spring Joint Computer Conference},
  author = {Ernst, Heinrich A.},
  year = {1962},
  month = may,
  series = {{{AIEE-IRE}} '62 ({{Spring}})},
  pages = {39--51},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1460833.1460839},
  urldate = {2024-04-15},
  abstract = {MH-1 is a motorized and sensitized servomanipulator operated by the TX-O computer at the Massachusetts Institute of Technology. It serves as an experimental vehicle to explore the feasibility of direct relations between a digital computer and the physical world with which this computer is concerned. Usually, a human interpreter stands between the computer and the physical world. Instead, the TX-O computer in the MH-1 system is programmed to perform by itself some of the functions normally assigned to the human intermediary; namely, to perceive the world, to appreciate it, and to determine a reasonable course of action after a goal has been specified for the hand. The data processing tools used are, rather than numerical operations on quantitative signals, pattern recognition and simulation of higher cognitive processes such as awareness and understanding. This paper describes some of the experiments performed with MH-1 and the mechanisms upon which the capabilities of MH-1 are based.},
  isbn = {978-1-4503-7875-8}
}

@inproceedings{espiauFormalVerificationRobotics1996,
  title = {Formal {{Verification}} in {{Robotics}}: {{Why}} and {{How}}?},
  shorttitle = {Formal {{Verification}} in {{Robotics}}},
  booktitle = {Robotics {{Research}}},
  author = {Espiau, B. and Kapellos, K. and Jourdan, M.},
  editor = {Giralt, Georges and Hirzinger, Gerhard},
  year = {1996},
  pages = {225--236},
  publisher = {Springer},
  address = {London},
  doi = {10.1007/978-1-4471-1021-7_26},
  abstract = {A mobile robot aimed to operate in an hazardous environment is a typical example of critical system. We mean here that, for such a system, like for a satellite, any repairing or recovery operation, even a mission reconfiguration, which would involve the intervention of a human operator is always costly, often difficult and sometimes impossible. This is why such systems should be at least provided with capacities of on-line adaption, like self replanning or sensor-based control. However, this is not sufficient and we have to be sure, as far as possible, that the system will behave correctly, before launching. More precisely, once a mission has been defined, we would like to verify that:its specifications are correct, i.e. that they correspond to the desired goals,its programming conforms to specifications,the constraints induced by real-time and implementation issues do not disturb its behavior.},
  isbn = {978-1-4471-1021-7},
  langid = {english}
}

@article{evangelouApproachTaskAction2021,
  title = {An {{Approach}} for {{Task}} and {{Action Planning}} in {{Human}}--{{Robot Collaborative Cells Using AI}}},
  author = {Evangelou, George and Dimitropoulos, Nikos and Michalos, George and Makris, Sotiris},
  year = {2021},
  month = jan,
  journal = {Procedia CIRP},
  series = {8th {{CIRP Conference}} of {{Assembly Technology}} and {{Systems}}},
  volume = {97},
  pages = {476--481},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2020.08.006},
  urldate = {2024-04-24},
  abstract = {Human--Robot Collaborative (HRC) workcells aim to elevate the conventional industrial lines, by enabling seamless interaction between operators and machines. Shifting away from standard (mainly static) workcells, with operators and machines following strictly defined limits and schedules, the new era of manufacturing introduces versatile workspaces, shared between the manufacturing resources. The non-deterministic workflow of such workspaces raises concerns over planning of task and actions among the resources, as the workcell is now a dynamic environment. Following these demands, this study introduces a solution in scheduling and assignment of assembly tasks to both human and robot resources, aiming for effective emergence of alternative task-resource assignment sequences. The proposed framework is optimized to provide a human-centered approach on the assembly line and alleviate human operators from exhaustive or unsafe tasks, while accounting for environment changes, such as positioning of resources and parts. This tool was used in a use case from the industrial modules manufacturing sector. {\copyright} 2020 The Authors, Published by Elsevier B.V. Peer review under the responsibility of the scientific committee of CIRP},
  keywords = {Action-planning,AI,HRC,Reconfigurable assembly lines,Scheduling},
  file = {C:\Users\benja\Zotero\storage\9L6LLGBD\S2212827120314931.html}
}

@article{evansLearningExplanatoryRules2018,
  title = {Learning Explanatory Rules from Noisy Data},
  author = {Evans, Richard and Grefenstette, Edward},
  year = {2018},
  month = jan,
  journal = {Journal of Artificial Intelligence Research},
  volume = {61},
  number = {1},
  pages = {1--64},
  issn = {1076-9757},
  abstract = {Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous over\_tting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data--which is not necessarily easily obtained--that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework, which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.}
}

@misc{ExplicitImplicitModels,
  title = {Explicit versus {{Implicit Models}}: {{What}} Are Good {{Languages}} for {{Modeling}}? {\textbar} {{SE}}@{{RWTH}}},
  urldate = {2024-04-17},
  howpublished = {https://www.se-rwth.de/sosym/Explicit-versus-implicit-models/},
  file = {C:\Users\benja\Zotero\storage\T2VFY5JI\Explicit-versus-implicit-models.html}
}

@article{fakhrhosseiniUserAdoptionIntelligent2024,
  title = {User {{Adoption}} of {{Intelligent Environments}}: {{A Review}} of {{Technology Adoption Models}}, {{Challenges}}, and {{Prospects}}},
  shorttitle = {User {{Adoption}} of {{Intelligent Environments}}},
  author = {FakhrHosseini, Shabnam and Chan, Kathryn and Lee, Chaiwoo and Jeon, Myounghoon and Son, Heesuk and Rudnik, John and Coughlin, Joseph},
  year = {2024},
  month = feb,
  journal = {International Journal of Human--Computer Interaction},
  volume = {40},
  number = {4},
  pages = {986--998},
  publisher = {Taylor \& Francis},
  issn = {1044-7318},
  doi = {10.1080/10447318.2022.2118851},
  urldate = {2024-09-08},
  abstract = {Recent technological advancements have enabled the development of smarter (more automated) and more intelligent (adaptable) environments. To understand what factors lead users to reject or adopt Intelligent Environments (IEs), we reviewed nine prominent technology adoption theories. We conducted a literature review to investigate the acceptance and adoption of different types of IEs. We found that perceived usefulness, ease of use, perceived control or self-efficacy, affect and enjoyment, and perceived risks are the common factors across the studies explaining the adoption of IEs. However, shortcomings in the design and methods of the reviewed studies present major concerns in the generalizability and application of existing theories to emerging IEs. We identify eight lacunae in the existing literature and propose a new conceptual model for explaining the adoption of IEs. Through this study, we contribute to the formulation of the theoretical background for the successful introduction of IEs and their integration into users' everyday life.},
  file = {C:\Users\benja\Zotero\storage\RZQELWD3\FakhrHosseini et al. - 2024 - User Adoption of Intelligent Environments A Review of Technology Adoption Models, Challenges, and P.pdf}
}

@article{fanEmbodiedIntelligenceManufacturing2024,
  title = {Embodied Intelligence in Manufacturing: Leveraging Large Language Models for Autonomous Industrial Robotics},
  shorttitle = {Embodied Intelligence in Manufacturing},
  author = {Fan, Haolin and Liu, Xuan and Fuh, Jerry Ying Hsi and Lu, Wen Feng and Li, Bingbing},
  year = {2024},
  month = jan,
  journal = {Journal of Intelligent Manufacturing},
  issn = {1572-8145},
  doi = {10.1007/s10845-023-02294-y},
  urldate = {2024-01-11},
  abstract = {This paper delves into the potential of Large Language Model (LLM) agents for industrial robotics, with an emphasis on autonomous design, decision-making, and task execution within manufacturing contexts. We propose a comprehensive framework that includes three core components: (1) matches manufacturing tasks with process parameters, emphasizing the challenges in LLM agents' understanding of human-imposed constraints; (2) autonomously designs tool paths, highlighting the LLM agents' proficiency in planar tasks and challenges in 3D spatial tasks; and (3) integrates embodied intelligence within industrial robotics simulations, showcasing the adaptability of LLM agents like GPT-4. Our experimental results underscore the distinctive performance of the GPT-4 agent, especially in Component 3, where it is outstanding in task planning and achieved a success rate of 81.88\% across 10 samples in task completion. In conclusion, our study accentuates the transformative potential of LLM agents in industrial robotics and suggests specific avenues, such as visual semantic control and real-time feedback loops, for their enhancement.},
  langid = {english},
  keywords = {Autonomous design,Decision-making,Embodied intelligence,Industrial robotics,Large language models (LLMs) agents}
}

@article{fanEnglishcentricMultilingualMachine2021,
  title = {Beyond English-Centric Multilingual Machine Translation},
  author = {Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and {El-Kishky}, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and Goyal, Naman and Birch, Tom and Liptchinsky, Vitaliy and Edunov, Sergey and Grave, Edouard and Auli, Michael and Joulin, Armand},
  year = {2021},
  month = jan,
  journal = {The Journal of Machine Learning Research},
  volume = {22},
  number = {1},
  pages = {107:4839--107:4886},
  issn = {1532-4435},
  abstract = {Existing work in translation demonstrated the potential of massively multilingual machine translation by training a single model able to translate between any pair of languages. However, much of this work is English-Centric, training only on data which was translated from or to English. While this is supported by large sources of training data, it does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation model that can translate directly between any pair of 100 languages. We build and open-source a training data set that covers thousands of language directions with parallel data, created through large-scale mining. Then, we explore how to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly translating between non-English directions while performing competitively to the best single systems from the Workshop on Machine Translation (WMT). We open-source our scripts so that others may reproduce the data, evaluation, and final M2M- 100 model},
  keywords = {bitext mining,many-to-many,model scaling,multilingual machine translation,neural networks}
}

@inproceedings{fangDiscoveringGeneralizableSkills2021,
  title = {Discovering {{Generalizable Skills}} via {{Automated Generation}} of {{Diverse Tasks}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVII}}},
  author = {Fang, Kuan and Zhu, Yuke and Savarese, Silvio and {Fei-Fei}, Li},
  year = {2021},
  month = jul,
  volume = {17},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-7-8},
  file = {C:\Users\benja\Zotero\storage\NAF9R7DK\p010.html}
}

@article{fangRoboticTeleoperationSystems2017,
  title = {Robotic Teleoperation Systems Using a Wearable Multimodal Fusion Device},
  author = {Fang, Bin and Sun, Fuchun and Liu, Huaping and Guo, Di and Chen, Wendan and Yao, Guodong},
  year = {2017},
  month = jul,
  journal = {International Journal of Advanced Robotic Systems},
  volume = {14},
  pages = {1--11},
  doi = {10.1177/1729881417717057},
  abstract = {Teleoperation is of great importance in the area of robotics especially when people's presence at the robot working space is unavailable. It provides an alternative to employ human intelligence in the control of the robot remotely. We establish robotic teleoperation systems with a wearable multimodal fusion device. The device is integrated with 18 low-cost inertial and magnetic measurement units, which cover all segments of the arm and hand. The multimodal fusion algorithm based on extended Kalman filter is deduced to determine the orientations and positions of each segment. Then, the robotic teleoperation systems using the proposed device are designed. The novel teleoperation schemes can be applied for 11DOF robotic arm-hand system and 10DOF robotic arm-hand system, in which the operator's fingers are used for robotic hand teleoperation, and the arms with palm are used for robotic arm teleoperation. Meanwhile, the proposed robotic teleoperation systems are fully realized with a user-friendly human-machine interaction interface. Finally, a series of experiments are conducted with our robotic teleoperation system successfully.}
}

@article{fanInverseNetSolvingInverse2017,
  title = {{{InverseNet}}: {{Solving Inverse Problems}} with {{Splitting Networks}}},
  shorttitle = {{{InverseNet}}},
  author = {Fan, Kai and Wei, Qi and Wang, Wenlin and Chakraborty, Amit and Heller, Katherine},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.00202 [cs]},
  eprint = {1712.00202},
  primaryclass = {cs},
  urldate = {2019-05-17},
  abstract = {We propose a new method that uses deep learning techniques to solve the inverse problems. The inverse problem is cast in the form of learning an end-to-end mapping from observed data to the ground-truth. Inspired by the splitting strategy widely used in regularized iterative algorithm to tackle inverse problems, the mapping is decomposed into two networks, with one handling the inversion of the physical forward model associated with the data term and one handling the denoising of the output from the former network, i.e., the inverted version, associated with the prior/regularization term. The two networks are trained jointly to learn the end-to-end mapping, getting rid of a two-step training. The training is annealing as the intermediate variable between these two networks bridges the gap between the input (the degraded version of output) and output and progressively approaches to the ground-truth. The proposed network, referred to as InverseNet, is flexible in the sense that most of the existing end-to-end network structure can be leveraged in the first network and most of the existing denoising network structure can be used in the second one. Extensive experiments on both synthetic data and real datasets on the tasks, motion deblurring, super-resolution, and colorization, demonstrate the efficiency and accuracy of the proposed method compared with other image processing algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\benja\Zotero\storage\J9NKQS32\1712.html}
}

@misc{fanucamericacorporation2023CollaborativeRobot2023,
  title = {2023 {{Collaborative Robot Brochure}}},
  author = {{FANUC America Corporation}},
  year = {2023},
  urldate = {2024-08-08}
}

@techreport{fanucamericacorporationFANUCAmericaCorporation2014,
  title = {{{FANUC America Corporation SYSTEM R-30iA}} and {{R-30iB Controller KAREL Reference Manual}}},
  author = {{FANUC America Corporation}},
  year = {2014},
  number = {MARRC75KR07091E Rev H},
  address = {Rochester Hills, Michigan},
  institution = {FANUC America Corporation}
}

@misc{FanucLVC,
  title = {Fanuc {{LVC}}}
}

@misc{FanucROBOGUIDE,
  title = {Fanuc {{ROBOGUIDE}}},
  urldate = {2021-03-24},
  abstract = {Discover how FANUC ROBOGUIDE simulates both the robot's motion and application commands for faster setups and more flexibility.},
  howpublished = {https://www.fanuc.eu/de/en/robots/accessories/roboguide},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\4W666FDX\roboguide.html}
}

@article{fedulloComprehensiveReviewTime2022,
  title = {A {{Comprehensive Review}} on {{Time Sensitive Networks}} with a {{Special Focus}} on {{Its Applicability}} to {{Industrial Smart}} and {{Distributed Measurement Systems}}},
  author = {Fedullo, Tommaso and Morato, Alberto and Tramarin, Federico and Rovati, Luigi and Vitturi, Stefano},
  year = {2022},
  month = jan,
  journal = {Sensors},
  volume = {22},
  number = {4},
  pages = {1638},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s22041638},
  urldate = {2024-08-17},
  abstract = {The groundbreaking transformations triggered by the Industry 4.0 paradigm have dramatically reshaped the requirements for control and communication systems within the factory systems of the future. The aforementioned technological revolution strongly affects industrial smart and distributed measurement systems as well, pointing to ever more integrated and intelligent equipment devoted to derive accurate measurements. Moreover, as factory automation uses ever wider and complex smart distributed measurement systems, the well-known Internet of Things (IoT) paradigm finds its viability also in the industrial context, namely Industrial IoT (IIoT). In this context, communication networks and protocols play a key role, directly impacting on the measurement accuracy, causality, reliability and safety. The requirements coming both from Industry 4.0 and the IIoT, such as the coexistence of time-sensitive and best effort traffic, the need for enhanced horizontal and vertical integration, and interoperability between Information Technology (IT) and Operational Technology (OT), fostered the development of enhanced communication subsystems. Indeed, established technologies, such as Ethernet and Wi-Fi, widespread in the consumer and office fields, are intrinsically non-deterministic and unable to support critical traffic. In the last years, the IEEE 802.1 Working Group defined an extensive set of standards, comprehensively known as Time Sensitive Networking (TSN), aiming at reshaping the Ethernet standard to support for time-, mission- and safety-critical traffic. In this paper, a comprehensive overview of the TSN Working Group standardization activity is provided, while contextualizing TSN within the complex existing industrial technological panorama, particularly focusing on industrial distributed measurement systems. In particular, this paper has to be considered a technical review of the most important features of TSN, while underlining its applicability to the measurement field. Furthermore, the adoption of TSN within the Wi-Fi technology is addressed in the last part of the survey, since wireless communication represents an appealing opportunity in the industrial measurement context. In this respect, a test case is presented, to point out the need for wirelessly connected sensors networks. In particular, by reviewing some literature contributions it has been possible to show how wireless technologies offer the flexibility necessary to support advanced mobile IIoT applications.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Ethernet,IIoT,Industry 4.0,IoT,smart distributed measurement systems,TSN,Wi-Fi}
}

@book{feldmanMoleculeMetaphorNeural2006,
  title = {{From Molecule to Metaphor: A Neural Theory of Language}},
  shorttitle = {{From Molecule to Metaphor}},
  author = {Feldman, Jerome A.},
  year = {2006},
  month = jun,
  edition = {1},
  publisher = {Bradford Books},
  address = {Cambridge, Mass.},
  abstract = {In From Molecule to Metaphor, Jerome Feldman proposes a theory of language and thought that treats language not as an abstract symbol system but as a human biological ability that can be studied as a function of the brain, as vision and motor control are studied. This theory, he writes, is a "bridging theory" that works from extensive knowledge at two ends of a causal chain to explicate the links between. Although the cognitive sciences are revealing much about how our brains produce language and thought, we do not yet know exactly how words are understood or have any methodology for finding out. Feldman develops his theory in computer simulations -- formal models that suggest ways that language and thought may be realized in the brain. Combining key findings and theories from biology, computer science, linguistics, and psychology, Feldman synthesizes a theory by exhibiting programs that demonstrate the required behavior while remaining consistent with the findings from all disciplines.After presenting the essential results on language, learning, neural computation, the biology of neurons and neural circuits, and the mind/brain, Feldman introduces specific demonstrations and formal models of such topics as how children learn their first words, words for abstract and metaphorical concepts, understanding stories, and grammar (including "hot-button" issues surrounding the innateness of human grammar). With this accessible, comprehensive book Feldman offers readers who want to understand how our brains create thought and language a theory of language that is intuitively plausible and also consistent with existing scientific data at all levels.},
  isbn = {978-0-262-06253-4},
  langid = {Englisch}
}

@inproceedings{fenielloProgramSynthesisExamples2014,
  title = {Program Synthesis by Examples for Object Repositioning Tasks},
  booktitle = {2014 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Feniello, Ashley and Dang, Hao and Birchfield, Stan},
  year = {2014},
  month = sep,
  pages = {4428--4435},
  issn = {2153-0866},
  doi = {10.1109/IROS.2014.6943189},
  abstract = {We address the problem of synthesizing human-readable computer programs for robotic object repositioning tasks based on human demonstrations. A stack-based domain specific language (DSL) is introduced for object repositioning tasks, and a learning algorithm is proposed to synthesize a program in this DSL based on human demonstrations. Once the synthesized program has been learned, it can be rapidly verified and refined in the simulator via further demonstrations if necessary, then finally executed on an actual robot to accomplish the corresponding learned tasks in the physical world. By performing demonstrations on a novel tablet interface, the time required for teaching is greatly reduced compared with using a real robot. Experiments show a variety of object repositioning tasks such as sorting, kitting, and packaging can be programmed using this approach.},
  keywords = {Color,Compounds,DSL,Packaging,Robots,Sorting,Training},
  file = {C:\Users\benja\Zotero\storage\6Y85NZUP\6943189.html}
}

@misc{FernzugriffServiceUnd,
  title = {{Fernzugriff, Service und {\"U}berwachung f{\"u}r Industrieroboter \& Cobots}},
  journal = {IXON},
  urldate = {2023-04-11},
  abstract = {Eine leistungsstarke IoT-L{\"o}sung f{\"u}r (kollaborative) Roboter, um mehr Flexibilit{\"a}t, Einblicke und Kontrolle von {\"u}berall her zu schaffen.  Entdecken Sie, wie einfach die Fernwartung und Zustands{\"u}berwachung mit IXON geworden ist.},
  howpublished = {https://www.ixon.cloud/de/wissensdatenbank/fernzugriff-service-und-uberwachung-fur-industrieroboter-cobots},
  langid = {ngerman}
}

@article{ferrariDeathLaparoscopy2024,
  title = {The Death of Laparoscopy},
  author = {Ferrari, Davide and Violante, Tommaso and Novelli, Marco and Starlinger, Patrick P. and Smoot, Rory L. and Reisenauer, Janani S. and Larson, David W.},
  year = {2024},
  month = mar,
  journal = {Surgical Endoscopy},
  issn = {1432-2218},
  doi = {10.1007/s00464-024-10774-2},
  urldate = {2024-04-12},
  abstract = {The introduction of laparoscopy in 1989 revolutionized surgical practices, reducing post-operative complications, and enhancing outcomes. Despite its benefits, limitations in laparoscopic tools have led to continued use of open surgery. Robotic-assisted surgery emerged to address these limitations, but its adoption trends and potential impact on open and laparoscopic surgery require analysis.},
  langid = {english},
  keywords = {Abdominal procedures,Future predictions,Laparoscopy,Minimally invasive surgery (MIS),Robotic surgery,Trends}
}

@inproceedings{ferrarioHowExplainabilityContributes2022,
  title = {How {{Explainability Contributes}} to {{Trust}} in {{AI}}},
  booktitle = {Proceedings of the 2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Ferrario, Andrea and Loi, Michele},
  year = {2022},
  month = jun,
  series = {{{FAccT}} '22},
  pages = {1457--1466},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3531146.3533202},
  urldate = {2024-09-09},
  abstract = {We provide a philosophical explanation of the relation between artificial intelligence (AI) explainability and trust in AI, providing a case for expressions, such as ``explainability fosters trust in AI,'' that commonly appear in the literature. This explanation relates the justification of the trustworthiness of an AI with the need to monitor it during its use. We discuss the latter by referencing an account of trust, called ``trust as anti-monitoring,'' that different authors contributed developing. We focus our analysis on the case of medical AI systems, noting that our proposal is compatible with internalist and externalist justifications of trustworthiness of medical AI and recent accounts of warranted contractual trust. We propose that ``explainability fosters trust in AI'' if and only if it fosters justified and warranted paradigmatic trust in AI, i.e., trust in the presence of the justified belief that the AI is trustworthy, which, in turn, causally contributes to rely on the AI in the absence of monitoring. We argue that our proposed approach can intercept the complexity of the interactions between physicians and medical AI systems in clinical practice, as it can distinguish between cases where humans hold different beliefs on the trustworthiness of the medical AI and exercise varying degrees of monitoring on them. Finally, we apply our account to user's trust in AI, where, we argue, explainability does not contribute to trust. By contrast, when considering public trust in AI as used by a human, we argue, it is possible for explainability to contribute to trust. Our account can explain the apparent paradox that in order to trust AI, we must trust AI users not to trust AI completely. Summing up, we can explain how explainability contributes to justified trust in AI, without leaving a reliabilist framework, but only by redefining the trusted entity as an AI-user dyad.},
  isbn = {978-1-4503-9352-2}
}

@article{feurerInitializingBayesianHyperparameter2015,
  title = {Initializing {{Bayesian Hyperparameter Optimization}} via {{Meta-Learning}}},
  author = {Feurer, Matthias and Springenberg, Jost and Hutter, Frank},
  year = {2015},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {29},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v29i1.9354},
  urldate = {2024-09-07},
  abstract = {Model selection and hyperparameter optimization is crucial in applying machine learning to a novel dataset. Recently, a subcommunity of machine learning has focused on solving this problem with Sequential Model-based Bayesian Optimization (SMBO), demonstrating substantial successes in many applications. However, for computationally expensive algorithms the overhead of hyperparameter optimization can still be prohibitive. In this paper we mimic a strategy human domain experts use: speed up optimization by starting from promising configurations that performed well on similar datasets. The resulting initialization technique integrates naturally into the generic SMBO framework and can be trivially applied to any SMBO method. To validate our approach, we perform extensive experiments with two established SMBO frameworks (Spearmint and SMAC) with complementary strengths; optimizing two machine learning frameworks on 57 datasets. Our initialization procedure yields mild improvements for low-dimensional hyperparameter optimization and substantially improves the state of the art for the more complex combined algorithm selection and hyperparameter optimization problem.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {Sequential Model-based Optimization},
  file = {C:\Users\benja\Zotero\storage\8X3GS5V6\Feurer et al. - 2015 - Initializing Bayesian Hyperparameter Optimization via Meta-Learning.pdf}
}

@article{fikesStripsNewApproach1971,
  title = {Strips: {{A New Approach}} to the {{Application}} of {{Theorem Proving}} to {{Problem Solving}}},
  shorttitle = {Strips},
  author = {Fikes, Richard E. and Nilsson, Nils J.},
  year = {1971},
  month = dec,
  journal = {Artificial Intelligence},
  volume = {2},
  number = {3-4},
  pages = {189--208},
  issn = {00043702},
  doi = {10.1016/0004-3702(71)90010-5},
  urldate = {2019-06-30},
  abstract = {We describe a newproblem solver called STRIPS that attempts to find a sequence of operators in a spcce o f world models to transform a given initial world model into a model in which a given goal formula can be proven to be true. STRIPS represents a world n,{\textasciitilde}del as an arbitrary collection o f first-order predicate calculus formulas and is designed to work with .models consisting of large numbers o f formulas. It employs a resolution theorem prover to answer questions o fparticular models and uses means-ends analysis to guide it to the desired goal-satisfying model.},
  langid = {english}
}

@inproceedings{finnDeepVisualForesight2017,
  title = {Deep Visual Foresight for Planning Robot Motion},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Finn, Chelsea and Levine, Sergey},
  year = {2017},
  month = may,
  pages = {2786--2793},
  doi = {10.1109/ICRA.2017.7989324},
  urldate = {2024-09-29},
  abstract = {A key challenge in scaling up robot learning to many skills and environments is removing the need for human supervision, so that robots can collect their own data and improve their own performance without being limited by the cost of requesting human feedback. Model-based reinforcement learning holds the promise of enabling an agent to learn to predict the effects of its actions, which could provide flexible predictive models for a wide range of tasks and environments, without detailed human supervision. We develop a method for combining deep action-conditioned video prediction models with model-predictive control that uses entirely unlabeled training data. Our approach does not require a calibrated camera, an instrumented training set-up, nor precise sensing and actuation. Our results show that our method enables a real robot to perform nonprehensile manipulation - pushing objects - and can handle novel objects not seen during training.},
  keywords = {Data models,Predictive models,Robot kinematics,Robot sensing systems,Three-dimensional displays,Visualization},
  file = {C\:\\Users\\benja\\Zotero\\storage\\G7BRF462\\Finn und Levine - 2017 - Deep visual foresight for planning robot motion.pdf;C\:\\Users\\benja\\Zotero\\storage\\MQI2ICH8\\7989324.html}
}

@book{Finnegan.2006,
  title = {{{ACM SIGGRAPH}} 2006 Papers on - {{SIGGRAPH}} '06},
  editor = {Finnegan, John and Dorsey, Julie},
  year = {2006},
  publisher = {ACM Press},
  address = {New York, New York, USA},
  doi = {10.1145/1179352},
  isbn = {1-59593-364-6}
}

@inproceedings{finnModelAgnosticMetaLearningFast2017,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  year = {2017},
  month = jul,
  pages = {1126--1135},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-29},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\XCYQY8JW\Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf}
}

@inproceedings{finnOneShotVisualImitation2017,
  title = {One-{{Shot Visual Imitation Learning}} via {{Meta-Learning}}},
  booktitle = {Proceedings of the 1st {{Annual Conference}} on {{Robot Learning}}},
  author = {Finn, Chelsea and Yu, Tianhe and Zhang, Tianhao and Abbeel, Pieter and Levine, Sergey},
  year = {2017},
  month = oct,
  pages = {357--368},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-15},
  abstract = {In order for a robot to be a generalist that can perform a wide range of jobs, it must be able to acquire a wide variety of skills quickly and efficiently in complex unstructured environments. High-capacity models such as deep neural networks can enable a robot to represent complex skills, but learning each skill from scratch then becomes infeasible. In this work, we present a meta-imitation learning method that enables a robot to learn how to learn more efficiently, allowing it to acquire new skills from just a single demonstration. Unlike prior methods for one-shot imitation, our method can scale to raw pixel inputs and requires data from significantly fewer prior tasks for effective learning of new skills. Our experiments on both simulated and real robot platforms demonstrate the ability to learn new tasks, end-to-end, from a single visual demonstration.},
  langid = {english}
}

@inproceedings{finnProbabilisticModelAgnosticMetaLearning2018,
  title = {Probabilistic {{Model-Agnostic Meta-Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Finn, Chelsea and Xu, Kelvin and Levine, Sergey},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-06-15},
  abstract = {Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems. We also show how reasoning about ambiguity can also be used for downstream active learning problems.}
}

@article{fioriniConceptsTrendsAutonomy2022,
  title = {Concepts and {{Trends}} n {{Autonomy}} for {{Robot-Assisted Surgery}}},
  author = {Fiorini, Paolo and Goldberg, Ken Y. and Liu, Yunhui and Taylor, Russell H.},
  year = {2022},
  month = jul,
  journal = {Proceedings of the IEEE. Institute of Electrical and Electronics Engineers},
  volume = {110},
  number = {7},
  pages = {993--1011},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2022.3176828},
  urldate = {2024-04-12},
  abstract = {Surgical robots have been widely adopted with over 4000 robots being used in practice daily. However, these are telerobots that are fully controlled by skilled human surgeons. Introducing ``surgeon-assist''---some forms of autonomy---has the potential to reduce tedium and increase consistency, analogous to driver-assist functions for lanekeeping, cruise control, and parking. This article examines the scientific and technical backgrounds of robotic autonomy in surgery and some ethical, social, and legal implications. We describe several autonomous surgical tasks that have been automated in laboratory settings, and research concepts and trends.},
  pmcid = {PMC7613181},
  pmid = {35911127}
}

@article{fischerICubHRISoftwareFramework2018,
  title = {{{iCub-HRI}}: {{A Software Framework}} for {{Complex Human}}--{{Robot Interaction Scenarios}} on the {{iCub Humanoid Robot}}},
  shorttitle = {{{iCub-HRI}}},
  author = {Fischer, Tobias and Puigb{\`o}, Jordi-Ysard and Camilleri, Daniel and Nguyen, Phuong D. H. and {Moulin-Frier}, Cl{\'e}ment and Lall{\'e}e, St{\'e}phane and Metta, Giorgio and Prescott, Tony J. and Demiris, Yiannis and Verschure, Paul F. M. J.},
  year = {2018},
  month = mar,
  journal = {Frontiers in Robotics and AI},
  volume = {5},
  publisher = {Frontiers},
  issn = {2296-9144},
  doi = {10.3389/frobt.2018.00022},
  urldate = {2024-09-18},
  abstract = {{$<$}p{$>$}Generating complex, human-like behavior in a humanoid robot like the iCub requires the integration of a wide range of open source components and a scalable cognitive architecture. Hence, we present the iCub-HRI library which provides convenience wrappers for components related to perception (object recognition, agent tracking, speech recognition, and touch detection), object manipulation (basic and complex motor actions), and social interaction (speech synthesis and joint attention) exposed as a C++ library with bindings for Java (allowing to use iCub-HRI within Matlab) and Python. In addition to previously integrated components, the library allows for simple extension to new components and rapid prototyping by adapting to changes in interfaces between components. We also provide a set of modules which make use of the library, such as a high-level knowledge acquisition module and an action recognition module. The proposed architecture has been successfully employed for a complex human--robot interaction scenario involving the acquisition of language capabilities, execution of goal-oriented behavior and expression of a verbal narrative of the robot's experience in the world. Accompanying this paper is a tutorial which allows a subset of this interaction to be reproduced. The architecture is aimed at researchers familiarizing themselves with the iCub ecosystem, as well as expert users, and we expect the library to be widely used in the iCub community.{$<$}/p{$>$}},
  langid = {english},
  keywords = {C++,human-robot interaction,iCub Humanoid,Java,python,Robotics,software architecture,YARP},
  file = {C:\Users\benja\Zotero\storage\GFPNFHIW\Fischer et al. - 2018 - iCub-HRI A Software Framework for Complex HumanRobot Interaction Scenarios on the iCub Humanoid Ro.pdf}
}

@article{fischingerHobbitCareRobot2016,
  title = {Hobbit, a Care Robot Supporting Independent Living at Home: {{First}} Prototype and Lessons Learned},
  shorttitle = {Hobbit, a Care Robot Supporting Independent Living at Home},
  author = {Fischinger, David and Einramhof, Peter and Papoutsakis, Konstantinos and Wohlkinger, Walter and Mayer, Peter and Panek, Paul and Hofmann, Stefan and Koertner, Tobias and Weiss, Astrid and Argyros, Antonis and Vincze, Markus},
  year = {2016},
  month = jan,
  journal = {Robotics and Autonomous Systems},
  series = {Assistance and {{Service Robotics}} in a {{Human Environment}}},
  volume = {75},
  pages = {60--78},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2014.09.029},
  urldate = {2020-08-21},
  abstract = {One option to address the challenge of demographic transition is to build robots that enable aging in place. Falling has been identified as the most relevant factor to cause a move to a care facility. The Hobbit project combines research from robotics, gerontology, and human--robot interaction to develop a care robot which is capable of fall prevention and detection as well as emergency detection and handling. Moreover, to enable daily interaction with the robot, other functions are added, such as bringing objects, offering reminders, and entertainment. The interaction with the user is based on a multimodal user interface including automatic speech recognition, text-to-speech, gesture recognition, and a graphical touch-based user interface. We performed controlled laboratory user studies with a total of 49 participants (aged 70 plus) in three EU countries (Austria, Greece, and Sweden). The collected user responses on perceived usability, acceptance, and affordability of the robot demonstrate a positive reception of the robot from its target user group. This article describes the principles and system components for navigation and manipulation in domestic environments, the interaction paradigm and its implementation in a multimodal user interface, the core robot tasks, as well as the results from the user studies, which are also reflected in terms of lessons we learned and we believe are useful to fellow researchers.},
  langid = {english},
  keywords = {Care robot for independent living,Robots for elderly,Social robotics},
  file = {C:\Users\benja\Zotero\storage\VMUXFJN8\S0921889014002140.html}
}

@article{Fischler.1981,
  title = {Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography},
  author = {Fischler, Martin A. and Bolles, Robert C.},
  year = {1981},
  journal = {Communications of the ACM},
  volume = {24},
  number = {6},
  pages = {381--395},
  issn = {00010782},
  doi = {10.1145/358669.358692},
  pagination = {page}
}

@article{fischlerRandomSampleConsensus1981,
  title = {Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography},
  shorttitle = {Random Sample Consensus},
  author = {Fischler, Martin A. and Bolles, Robert C.},
  year = {1981},
  month = jun,
  journal = {Communications of the ACM},
  volume = {24},
  number = {6},
  pages = {381--395},
  issn = {0001-0782},
  doi = {10.1145/358669.358692},
  urldate = {2023-03-02},
  abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
  keywords = {automated cartography,camera calibration,image matching,location determination,model fitting,scene analysis}
}

@article{flanaganControlStrategiesObject2006,
  title = {Control Strategies in Object Manipulation Tasks},
  author = {Flanagan, J Randall and Bowman, Miles C and Johansson, Roland S},
  year = {2006},
  month = dec,
  journal = {Current Opinion in Neurobiology},
  series = {Motor Systems / {{Neurobiology}} of Behaviour},
  volume = {16},
  number = {6},
  pages = {650--659},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2006.10.005},
  urldate = {2024-03-08},
  abstract = {The remarkable manipulative skill of the human hand is not the result of rapid sensorimotor processes, nor of fast or powerful effector mechanisms. Rather, the secret lies in the way manual tasks are organized and controlled by the nervous system. At the heart of this organization is prediction. Successful manipulation requires the ability both to predict the motor commands required to grasp, lift, and move objects and to predict the sensory events that arise as a consequence of these commands.},
  file = {C:\Users\benja\Zotero\storage\YR2SLTUD\S0959438806001450.html}
}

@article{fleXStructuresGmbHKaiserslautern.2013,
  title = {{{IPS}} Cable Simulation},
  author = {{fleXStructures GmbH Kaiserslautern}},
  year = {2013},
  lastvisited = {2019-06-28}
}

@article{florenceDenseObjectNets,
  title = {Dense {{Object Nets}}: {{Learning Dense Visual Object Descriptors By}} and {{For Robotic Manipulation}}},
  author = {Florence, Peter R and Manuelli, Lucas and Tedrake, Russ},
  pages = {13},
  abstract = {What is the right object representation for manipulation? We would like robots to visually perceive scenes and learn an understanding of the objects in them that (i) is task-agnostic and can be used as a building block for a variety of manipulation tasks, (ii) is generally applicable to both rigid and non-rigid objects, (iii) takes advantage of the strong priors provided by 3D vision, and (iv) is entirely learned from self-supervision. This is hard to achieve with previous methods: much recent work in grasping does not extend to grasping specific objects or other tasks, whereas task-specific learning may require many trials to generalize well across object configurations or other tasks. In this paper we present Dense Object Nets, which build on recent developments in self-supervised dense descriptor learning, as a consistent object representation for visual understanding and manipulation. We demonstrate they can be trained quickly (approximately 20 minutes) for a wide variety of previously unseen and potentially non-rigid objects. We additionally present novel contributions to enable multi-object descriptor learning, and show that by modifying our training procedure, we can either acquire descriptors which generalize across classes of objects, or descriptors that are distinct for each object instance. Finally, we demonstrate the novel application of learned dense descriptors to robotic manipulation. We demonstrate grasping of specific points on an object across potentially deformed object configurations, and demonstrate using class general descriptors to transfer specific grasps across objects in a class.},
  langid = {english}
}

@article{Flynn.1972,
  title = {Some Computer Organizations and Their Effectiveness},
  author = {Flynn, Michael J.},
  year = {1972},
  journal = {IEEE Transactions on Computers},
  volume = {C-21},
  number = {9},
  pages = {948--960},
  issn = {0018-9340},
  doi = {10.1109/TC.1972.5009071},
  pagination = {page}
}

@book{fodorLanguageThought1980,
  title = {The {{Language}} of {{Thought}}},
  author = {Fodor, Jerry A.},
  year = {1980},
  month = jan,
  publisher = {Harvard University Press},
  address = {Cambridge, Mass},
  abstract = {Book by Fodor, Jerry A.},
  isbn = {978-0-674-51030-2},
  langid = {english}
}

@book{Fossati.2013,
  title = {Consumer Depth Cameras for Computer Vision},
  editor = {Fossati, Andrea and Gall, Juergen and Grabner, Helmut and Ren, Xiaofeng and Konolige, Kurt},
  year = {2013},
  publisher = {Springer London},
  address = {London},
  doi = {10.1007/978-1-4471-4640-7},
  isbn = {978-1-4471-4639-1}
}

@article{fossPreventingUnrecoverableFailures2007,
  title = {Preventing {{Unrecoverable Failures}} through {{Precautionary Planning}}},
  author = {Foss, Janae and Onder, Nilufer and Smith, David},
  year = {2007},
  month = jan,
  journal = {Proceedings of the ICAPS'07 Workshop on Moving Planning and Scheduling Systems into the Real World},
  abstract = {Traditional approaches to dealing with uncertainty in planning have focused on finding plans that prevent all potential failures. Though such plans are robust, their creation is computationally expensive. This model of planning does not capture the facts that 1) many times the most likely branch of execution succeeds and 2) even when that branch fails, replanning during execu- tion frequently provides an alternate path to the goal. In reality, the only failures that need to be planned for before execution are those that are unrecoverable, thereby preventing achievement of the goals. We have developed a framework called Precautionary Planning that combines interleaved planning and execution with limited contingency planning. Precautionary Planning adopts the view that contingency planning should be a last resort and is not desirable when replanning is pos- sible. In this framework, a robust initial plan is gener- ated using a fast deterministic planner. Next, the plan is analyzed to find potential points of failure, which are identified as recoverable or unrecoverable. Recoverable failures are left in the plan and are repaired through replanning at execution time. For each unrecoverable failure, an attempt is made to improve the chances of recovery, by adding "precautionary" steps such as tak- ing along extra supplies or tools that would allow re- covery if the failure occurs.}
}

@article{foughaliFormalVerificationRealTime2022,
  title = {Formal {{Verification}} of {{Real-Time Autonomous Robots}}: {{An Interdisciplinary Approach}}},
  shorttitle = {Formal {{Verification}} of {{Real-Time Autonomous Robots}}},
  author = {Foughali, Mohammed and Zuepke, Alexander},
  year = {2022},
  month = apr,
  journal = {Frontiers in Robotics and AI},
  volume = {9},
  publisher = {Frontiers},
  issn = {2296-9144},
  doi = {10.3389/frobt.2022.791757},
  urldate = {2024-04-16},
  abstract = {Due to the severe consequences of their possible failure, robotic systems must be rigorously verified as to guarantee that their behavior is correct and safe. Such verification, carried out on a model, needs to cover various behavioral properties (e.g., safety and liveness), but also, given the timing constraints of robotic missions, real-time properties (e.g., schedulability and bounded response). In addition, in order to obtain valid and useful verification results, the model must faithfully represent the underlying robotic system and should therefore take into account all possible behaviors of the robotic software under the actual hardware and OS constraints. These requirements put the rigorous verification of robotic systems at the intersection of at least three communities: the robotic community, the formal methods community, and the real-time systems community. Verifying robotic systems is thus a complex, interdisciplinary task that involves a number of disciplines/techniques (e.g., model checking, schedulability analysis, component-based design) and faces a number of challenges (e.g., formalization, scalability). For instance, the use of formal verification (formal methods community) is hindered by the state-space explosion problem, whereas schedulability analysis (real-time systems) is not suitable for behavioral properties. Moreover, current real-time implementations of robotic software are limited in terms of predictability and efficiency, leading to e.g., unnecessary latencies. This is flagrant, in particular, at the level of locking protocols in robotic software. Such situation may benefit from theoretical and practical findings of the real-time systems community. In this paper, we propose an interdisciplinary approach that, by joining forces of the different communities, provides a scalable and unified means to efficiently implement and rigorously verify real-time robots. First, we propose a scalable two-step verification solution that combines formal methods and schedulability analysis to verify both behavioral and real-time properties. Second, we devise a new multi-resource locking mechanism that is efficient, predictable and suitable for real-time robots, and show how it improves the latter's real-time behavior. In both cases, we show, using a real drone example, how our approach compares favorably to the literature. This paper is a major extension of the RTCSA 2020 publication ``A Two-Step Hybrid Approach for Verifying Real-time Robotic Systems''.},
  langid = {english},
  keywords = {Formal methods (FMs),Locking protocols,Real-time systems (RTSs),Robotics,timed automata (TA)}
}

@inproceedings{foxPlanStabilityReplanning2006,
  title = {Plan Stability: Replanning versus Plan Repair},
  shorttitle = {Plan Stability},
  booktitle = {Proceedings of the {{Sixteenth International Conference}} on {{International Conference}} on {{Automated Planning}} and {{Scheduling}}},
  author = {Fox, Maria and Gerevini, Alfonso and Long, Derek and Serina, Ivan},
  year = {2006},
  month = jun,
  series = {{{ICAPS}}'06},
  pages = {212--221},
  publisher = {AAAI Press},
  address = {Cumbria, UK},
  urldate = {2021-03-03},
  abstract = {The ultimate objective in planning is to construct plans for execution. However, when a plan is executed in a real environment it can encounter differences between the expected and actual context of execution. These differences can manifest as divergences between the expected and observed states of the world, or as a change in the goals to be achieved by the plan. In both cases, the old plan must be replaced with a new one. In replacing the plan an important consideration is plan stability. We compare two alternative strategies for achieving the stable repair of a plan: one is simply to replan from scratch and the other is to adapt the existing plan to the new context. We present arguments to support the claim that plan stability is a valuable property. We then propose an implementation, based on LPG, of a plan repair strategy that adapts a plan to its new context. We demonstrate empirically that our plan repair strategy achieves more stability than replanning and can produce repaired plans more efficiently than replanning.},
  isbn = {978-1-57735-270-9}
}

@article{fragapanePlanningControlAutonomous2021,
  title = {Planning and Control of Autonomous Mobile Robots for Intralogistics: {{Literature}} Review and Research Agenda},
  shorttitle = {Planning and Control of Autonomous Mobile Robots for Intralogistics},
  author = {Fragapane, Giuseppe and {de Koster}, Ren{\'e} and Sgarbossa, Fabio and Strandhagen, Jan Ola},
  year = {2021},
  month = oct,
  journal = {European Journal of Operational Research},
  volume = {294},
  number = {2},
  pages = {405--426},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2021.01.019},
  urldate = {2024-04-24},
  abstract = {Autonomous mobile robots (AMR) are currently being introduced in many intralogistics operations, like manufacturing, warehousing, cross-docks, terminals, and hospitals. Their advanced hardware and control software allow autonomous operations in dynamic environments. Compared to an automated guided vehicle (AGV) system in which a central unit takes control of scheduling, routing, and dispatching decisions for all AGVs, AMRs can communicate and negotiate independently with other resources like machines and systems and thus decentralize the decision-making process. Decentralized decision-making allows the system to react dynamically to changes in the system state and environment. These developments have influenced the traditional methods and decision-making processes for planning and control. This study identifies and classifies research related to the planning and control of AMRs in intralogistics. We provide an extended literature review that highlights how AMR technological advances affect planning and control decisions. We contribute to the literature by introducing an AMR planning and control framework to guide managers in the decision-making process, thereby supporting them to achieve optimal performance. Finally, we propose an agenda for future research within this field.},
  keywords = {Autonomous mobile robots,Literature review,Logistics,Planning and control,Research agenda},
  file = {C:\Users\benja\Zotero\storage\CVV8BMRZ\S0377221721000217.html}
}

@article{frankAlgorithmQuadraticProgramming1956,
  title = {An Algorithm for Quadratic Programming},
  author = {Frank, Marguerite and Wolfe, Philip},
  year = {1956},
  journal = {Naval Research Logistics Quarterly},
  volume = {3},
  number = {1-2},
  pages = {95--110},
  issn = {1931-9193},
  doi = {10.1002/nav.3800030109},
  urldate = {2024-05-21},
  copyright = {Copyright {\copyright} 1956 Wiley Periodicals, Inc., A Wiley Company},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\77FZ8LRB\nav.html}
}

@article{frankLearningObjectDeformation2014,
  title = {Learning Object Deformation Models for Robot Motion Planning},
  author = {Frank, Barbara and Stachniss, Cyrill and Schmedding, R{\"u}diger and Teschner, Matthias and Burgard, Wolfram},
  year = {2014},
  month = aug,
  journal = {Robotics and Autonomous Systems},
  volume = {62},
  number = {8},
  pages = {1153--1174},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2014.04.005},
  urldate = {2022-08-18},
  abstract = {In this paper, we address the problem of robot navigation in environments with deformable objects. The aim is to include the costs of object deformations when planning the robot's motions and trade them off against the travel costs. We present our recently developed robotic system that is able to acquire deformation models of real objects. The robot determines the elasticity parameters by physical interaction with the object and by establishing a relation between the applied forces and the resulting surface deformations. The learned deformation models can then be used to perform physically realistic finite element simulations. This allows the planner to evaluate robot trajectories and to predict the costs of object deformations. Since finite element simulations are time-consuming, we furthermore present an approach to approximate object-specific deformation cost functions by means of Gaussian process regression. We present two real-world applications of our motion planner for a wheeled robot and a manipulation robot. As we demonstrate in real-world experiments, our system is able to estimate appropriate deformation parameters of real objects that can be used to predict future deformations. We show that our deformation cost approximation improves the efficiency of the planner by several orders of magnitude.},
  langid = {english},
  keywords = {Deformation models,Mobile robots,Motion planning,Parameter estimation,Robot navigation},
  file = {C:\Users\benja\Zotero\storage\2577JKCE\S0921889014000797.html}
}

@article{frankoDesignMultiRobotSystem2020,
  title = {Design of a {{Multi-Robot System}} for {{Wind Turbine Maintenance}}},
  author = {Franko, Josef and Du, Shengzhi and Kallweit, Stephan and Duelberg, Enno and Engemann, Heiko},
  year = {2020},
  month = jan,
  journal = {Energies},
  volume = {13},
  number = {10},
  pages = {2552},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1996-1073},
  doi = {10.3390/en13102552},
  urldate = {2024-07-27},
  abstract = {The maintenance of wind turbines is of growing importance considering the transition to renewable energy. This paper presents a multi-robot-approach for automated wind turbine maintenance including a novel climbing robot. Currently, wind turbine maintenance remains a manual task, which is monotonous, dangerous, and also physically demanding due to the large scale of wind turbines. Technical climbers are required to work at significant heights, even in bad weather conditions. Furthermore, a skilled labor force with sufficient knowledge in repairing fiber composite material is rare. Autonomous mobile systems enable the digitization of the maintenance process. They can be designed for weather-independent operations. This work contributes to the development and experimental validation of a maintenance system consisting of multiple robotic platforms for a variety of tasks, such as wind turbine tower and rotor blade service. In this work, multicopters with vision and LiDAR sensors for global inspection are used to guide slower climbing robots. Light-weight magnetic climbers with surface contact were used to analyze structure parts with non-destructive inspection methods and to locally repair smaller defects. Localization was enabled by adapting odometry for conical-shaped surfaces considering additional navigation sensors. Magnets were suitable for steel towers to clamp onto the surface. A friction-based climbing ring robot (SMART--- Scanning, Monitoring, Analyzing, Repair and Transportation) completed the set-up for higher payload. The maintenance period could be extended by using weather-proofed maintenance robots. The multi-robot-system was running the Robot Operating System (ROS). Additionally, first steps towards machine learning would enable maintenance staff to use pattern classification for fault diagnosis in order to operate safely from the ground in the future.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {climbing robot,condition monitoring,low cost,odometry on wind turbines,weather independent operations,wind turbine maintenance}
}

@inproceedings{fransMetaLearningShared2018,
  title = {Meta {{Learning Shared Hierarchies}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
  year = {2018},
  month = feb,
  urldate = {2022-05-01},
  abstract = {learn hierarchal sub-policies through end-to-end training over a distribution of tasks},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\F87W8EZU\forum.html}
}

@article{freschiTechnicalReviewVinci2013,
  title = {Technical Review of the Da {{Vinci}} Surgical Telemanipulator},
  author = {Freschi, C. and Ferrari, V. and Melfi, F. and Ferrari, M. and Mosca, F. and Cuschieri, A.},
  year = {2013},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  volume = {9},
  number = {4},
  pages = {396--406},
  issn = {1478-596X},
  doi = {10.1002/rcs.1468},
  urldate = {2024-04-12},
  abstract = {Background The da Vinci robotic surgical telemanipulator has been utilized in several surgical specialties for varied procedures, and the users' experiences have been widely published. To date, no detailed system technical analyses have been performed. Methods A detailed review was performed of all publications and patents about the technical aspects of the da Vinci robotic system. Results Published technical literature on the da Vinci system highlight strengths and weaknesses of the robot design. While the system facilitates complex surgical operations and has a low malfunction rate, the lack of haptic (especially tactile) feedback and collisions between the robotic arms remain the major limitations of the system. Accurate, preplanned positioning of access ports is essential. Conclusion Knowledge of the technical aspects of the da Vinci robot is important for optimal use. We confirmed the excellent system functionality and ease of use for surgeons without an engineering background. Research and development of the surgical robot has been predominant in the literature. Future trends address robot miniaturization and intelligent control design. Copyright {\copyright} 2012 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2012 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {da Vinci,laparoscopy,surgical robotics},
  file = {C:\Users\benja\Zotero\storage\TH5CYI6H\rcs.html}
}

@inproceedings{friedrichInteractiveGenerationFlexible1998,
  title = {Interactive Generation of Flexible Robot Programs},
  booktitle = {{{ICRA}}},
  author = {Friedrich, H. and Holle, J. and Dillmann, R.},
  year = {1998},
  volume = {1},
  pages = {538--543},
  doi = {10.1109/ROBOT.1998.677029},
  urldate = {2019-06-30},
  abstract = {Service robots require interactive programming interfaces that allow users without programming experience to easily instruct the robots. Systems following the Programming by Demonstration (PbD) paradigm that were developed within the East years are getting closer to this goal. However, most of these systems lack the possibility for the user to supervise and alter the course of program generation after the initial demonstration was performed. I n this paper we present an approach, where the user is able t o supervise the entire program generation process and t o annotate, and edit system hypotheses. Moreover, the knowledge representation and algorithms presented enable the user to generalize the generated program b y annotating conditions and object selection criteria via a 3 0 simulation and graphical user interface. The resulting PbD-system widens the PbD approach i n robotics to the interactive generation of flexible robot programs based on demonstration and annotations.},
  isbn = {978-0-7803-4300-9},
  langid = {english}
}

@inproceedings{frostigCompilingMachineLearning2018,
  title = {Compiling Machine Learning Programs via High-Level Tracing},
  booktitle = {{{SysML}}},
  author = {Frostig, Roy and Johnson, Matthew and Leary, Chris},
  year = {2018},
  urldate = {2022-05-08}
}

@article{fuCatSQLRealWorld2023,
  title = {{{CatSQL}}: {{Towards Real World Natural Language}} to {{SQL Applications}}},
  shorttitle = {{{CatSQL}}},
  author = {Fu, Han and Liu, Chang and Wu, Bin and Li, Feifei and Tan, Jian and Sun, Jianling},
  year = {2023},
  month = feb,
  journal = {Proc. VLDB Endow.},
  volume = {16},
  number = {6},
  pages = {1534--1547},
  issn = {2150-8097},
  doi = {10.14778/3583140.3583165},
  urldate = {2024-09-04},
  abstract = {Natural language to SQL (NL2SQL) techniques provide a convenient interface to access databases, especially for non-expert users, to conduct various data analytics. Existing methods often employ either a rule-base approach or a deep learning based solution. The former is hard to generalize across different domains. Though the latter generalizes well, it often results in queries with syntactic or semantic errors, thus may be even not executable. In this work, we bridge the gap between the two and design a new framework to significantly improve both accuracy and runtime. In particular, we develop a novel CatSQL sketch, which constructs a template with slots that initially serve as placeholders, and tightly integrates with a deep learning model to fill in these slots with meaningful contents based on the database schema. Compared with the widely used sequence-to-sequence-based approaches, our sketch-based method does not need to generate keywords which are boilerplates in the template, and can achieve better accuracy and run much faster. Compared with the existing sketch-based approaches, our CatSQL sketch is more general and versatile, and can leverage the values already filled in on certain slots to derive the rest ones for improved performance. In addition, we propose the Semantics Correction technique, which is the first that leverages database domain knowledge in a deep learning based NL2SQL solution. Semantics Correction is a post-processing routine, which checks the initially generated SQL queries by applying rules to identify and correct semantic errors. This technique significantly improves the NL2SQL accuracy. We conduct extensive evaluations on both single-domain and cross-domain benchmarks and demonstrate that our approach significantly outperforms the previous ones in terms of both accuracy and throughput. In particular, on the state-of-the-art NL2SQL benchmark Spider, our CatSQL prototype outperforms the best of the previous solutions by 4 points on accuracy, while still achieving a throughput up to 63 times higher.}
}

@book{Fujii.1996,
  title = {Proceedings of the 23rd Annual Conference on {{Computer}} Graphics and Interactive Techniques - {{SIGGRAPH}} '96},
  editor = {Fujii, John},
  year = {1996},
  publisher = {ACM Press},
  address = {New York, New York, USA},
  doi = {10.1145/237170},
  isbn = {0-89791-746-4}
}

@inproceedings{fusslExplanationUserInterface2024,
  title = {An {{Explanation User Interface}} for a {{Knowledge Graph-Based XAI Approach}} to {{Process Analysis}}},
  booktitle = {Advanced {{Information Systems Engineering Workshops}}},
  author = {F{\"u}{\ss}l, Anne and Nissen, Volker and Heringklee, Stefan Horst},
  editor = {Almeida, Jo{\~a}o Paulo A. and Di Ciccio, Claudio and Kalloniatis, Christos},
  year = {2024},
  pages = {72--84},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-61003-5_7},
  abstract = {In consulting practice, effective use of AI technologies presupposes consultant's and client's ability to understand generated results. Our knowledge graph-based approach to explainable process analyses represents a hybrid AI approach that integrates symbolic approaches of structured knowledge and interactive machine learning methods for making algorithmic procedures traceable and representing analysis results in a human-readable form. In order to display identified weaknesses and suitable improvement measures of analyzed business processes in a user-understandable way and to enable human-in-the-loop interactions, an explainable, user-friendly interface is required. While much attention is paid to the computational aspects of generating explanations, there is a need for further research into the design of explanation user interfaces. A systematic literature review was conducted to derive a design catalog, which was demonstrated and evaluated by developing a suitable XUI for our knowledge graph-based explainable process analysis.},
  isbn = {978-3-031-61003-5},
  langid = {english},
  keywords = {Consulting self-service,Explanation user interface,Hybrid XAI,Knowledge graph,Process analysis},
  file = {C:\Users\benja\Zotero\storage\W6ZE546N\Fl et al. - 2024 - An Explanation User Interface for a Knowledge Graph-Based XAI Approach to Process Analysis.pdf}
}

@inproceedings{fuUsingLSTMGRU2016,
  title = {Using {{LSTM}} and {{GRU}} Neural Network Methods for Traffic Flow Prediction},
  booktitle = {2016 31st {{Youth Academic Annual Conference}} of {{Chinese Association}} of {{Automation}} ({{YAC}})},
  author = {Fu, R. and Zhang, Z. and Li, L.},
  year = {2016},
  month = nov,
  pages = {324--328},
  doi = {10.1109/YAC.2016.7804912},
  abstract = {Accurate and real-time traffic flow prediction is important in Intelligent Transportation System (ITS), especially for traffic control. Existing models such as ARMA, ARIMA are mainly linear models and cannot describe the stochastic and nonlinear nature of traffic flow. In recent years, deep-learning-based methods have been applied as novel alternatives for traffic flow prediction. However, which kind of deep neural networks is the most appropriate model for traffic flow prediction remains unsolved. In this paper, we use Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU) neural network (NN) methods to predict short-term traffic flow, and experiments demonstrate that Recurrent Neural Network (RNN) based deep learning methods such as LSTM and GRU perform better than auto regressive integrated moving average (ARIMA) model. To the best of our knowledge, this is the first time that GRU is applied to traffic flow prediction.},
  keywords = {ARIMA,ARIMA model,auto regressive integrated moving average,autoregressive moving average processes,Conferences,Decision support systems,deep-learning-based methods,Economic indicators,gated recurrent units,GRU,GRU neural network methods,Integrated circuits,intelligent transportation system,linear models,long short term memory,LSTM,LSTM neural network methods,neural nets,Predictive models,recurrent neural network,telecommunication computing,telecommunication traffic,traffic flow,traffic flow prediction}
}

@article{gabrielArtificialIntelligenceValues2020,
  title = {Artificial {{Intelligence}}, {{Values}}, and {{Alignment}}},
  author = {Gabriel, Iason},
  year = {2020},
  month = sep,
  journal = {Minds and Machines},
  volume = {30},
  number = {3},
  pages = {411--437},
  issn = {1572-8641},
  doi = {10.1007/s11023-020-09539-2},
  urldate = {2024-03-04},
  abstract = {This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify `true' moral principles for AI; rather, it is to identify fair principles for alignment that receive reflective endorsement despite widespread variation in people's moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.},
  langid = {english},
  keywords = {Artificial intelligence,Machine learning,Moral philosophy,Political theory,Value alignment}
}

@article{galceranSurveyCoveragePath2013,
  title = {A Survey on Coverage Path Planning for Robotics},
  author = {Galceran, Enric and Carreras, Marc},
  year = {2013},
  month = dec,
  journal = {Robotics and Autonomous Systems},
  volume = {61},
  number = {12},
  pages = {1258--1276},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2013.09.004},
  urldate = {2021-02-25},
  abstract = {Coverage Path Planning (CPP) is the task of determining a path that passes over all points of an area or volume of interest while avoiding obstacles. This task is integral to many robotic applications, such as vacuum cleaning robots, painter robots, autonomous underwater vehicles creating image mosaics, demining robots, lawn mowers, automated harvesters, window cleaners and inspection of complex structures, just to name a few. A considerable body of research has addressed the CPP problem. However, no updated surveys on CPP reflecting recent advances in the field have been presented in the past ten years. In this paper, we present a review of the most successful CPP methods, focusing on the achievements made in the past decade. Furthermore, we discuss reported field applications of the described CPP methods. This work aims to become a starting point for researchers who are initiating their endeavors in CPP. Likewise, this work aims to present a comprehensive review of the recent breakthroughs in the field, providing links to the most interesting and successful works.},
  langid = {english},
  keywords = {Coverage path planning,Motion planning,Path planning},
  file = {C:\Users\benja\Zotero\storage\JKDIR58Q\S092188901300167X.html}
}

@article{galDeepBayesianActive2017,
  title = {Deep {{Bayesian Active Learning}} with {{Image Data}}},
  author = {Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.02910 [cs, stat]},
  eprint = {1703.02910},
  primaryclass = {cs, stat},
  urldate = {2020-12-19},
  abstract = {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\DI5ZL2Q9\1703.html}
}

@inproceedings{galDropoutBayesianApproximation2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = jun,
  pages = {1050--1059},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2021-05-18},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian mode...},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\3PGAUA2Z\gal16.html}
}

@article{galDropoutBayesianApproximation2016a,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = oct,
  journal = {arXiv:1506.02142 [cs, stat]},
  eprint = {1506.02142},
  primaryclass = {cs, stat},
  urldate = {2020-12-19},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\BXQLKVTX\1506.html}
}

@incollection{galickiRobotTaskPlanning1999,
  title = {Robot {{Task Planning}}},
  booktitle = {Basics of {{Robotics}}: {{Theory}} and {{Components}} of {{Manipulators}} and {{Robots}}},
  author = {Galicki, Miroslaw},
  year = {1999},
  series = {{{CISM}} Courses and Lectures / {{International Centre}} for {{Mechanical Sciences}}},
  number = {402},
  pages = {319--377},
  publisher = {Springer},
  address = {Wien},
  isbn = {978-3-211-83150-2}
}

@inproceedings{Gall.2009,
  title = {Motion Capture Using Joint Skeleton Tracking and Surface Estimation},
  booktitle = {2009 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Gall, Juergen and Stoll, Carsten and {de Aguiar}, Edilson and Theobalt, Christian and Rosenhahn, Bodo and Seidel, Hans-Peter},
  year = {2009},
  pages = {1746--1753},
  publisher = {IEEE},
  doi = {10.1109/CVPR.2009.5206755},
  bookpagination = {page},
  isbn = {978-1-4244-3992-8}
}

@article{Gao.2018,
  title = {{{SurfelWarp}}: {{Efficient}} Non-Volumetric Single View Dynamic Reconstruction},
  author = {Gao, Wei and Tedrake, Russ},
  year = {2018}
}

@inproceedings{gaoGET3DGenerativeModel2022,
  title = {{{GET3D}}: {{A Generative Model}} of {{High Quality 3D Textured Shapes Learned}} from {{Images}}},
  shorttitle = {{{GET3D}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gao, Jun and Shen, Tianchang and Wang, Zian and Chen, Wenzheng and Yin, Kangxue and Li, Daiqing and Litany, Or and Gojcic, Zan and Fidler, Sanja},
  year = {2022},
  month = may,
  urldate = {2024-01-11},
  abstract = {As several industries are moving towards modeling massive 3D virtual worlds, the need for content creation tools that can scale in terms of the quantity, quality, and diversity of 3D content is becoming evident. In our work, we aim to train performant 3D generative models that synthesize textured meshes which can be directly consumed by 3D rendering engines, thus immediately usable in downstream applications. Prior works on 3D generative modeling either lack geometric details, are limited in the mesh topology they can produce, typically do not support textures, or utilize neural renderers in the synthesis process, which makes their use in common 3D software non-trivial. In this work, we introduce GET3D, a Generative model that directly generates Explicit Textured 3D meshes with complex topology, rich geometric details, and high fidelity textures. We bridge recent success in the differentiable surface modeling, differentiable rendering as well as 2D Generative Adversarial Networks to train our model from 2D image collections. GET3D is able to generate high-quality 3D textured meshes, ranging from cars, chairs, animals, motorbikes and human characters to buildings, achieving significant improvements over previous methods.},
  langid = {english}
}

@article{gaoTexttoSQLEmpoweredLarge2024,
  title = {Text-to-{{SQL Empowered}} by {{Large Language Models}}: {{A Benchmark Evaluation}}},
  shorttitle = {Text-to-{{SQL Empowered}} by {{Large Language Models}}},
  author = {Gao, Dawei and Wang, Haibin and Li, Yaliang and Sun, Xiuyu and Qian, Yichen and Ding, Bolin and Zhou, Jingren},
  year = {2024},
  month = may,
  journal = {Proc. VLDB Endow.},
  volume = {17},
  number = {5},
  pages = {1132--1145},
  issn = {2150-8097},
  doi = {10.14778/3641204.3641221},
  urldate = {2024-09-04},
  abstract = {Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborate their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6\% execution accuracy and sets a new bar.To explore the potential of open-source LLM, we investigate them in various scenarios, and further enhance their performance with supervised fine-tuning. Our explorations highlight open-source LLMs' potential in Text-to-SQL, as well as the advantages and disadvantages of the supervised fine-tuning. Additionally, towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. We hope that our work provides a deeper understanding of Text-to-SQL with LLMs, and inspires further investigations and broad applications.},
  file = {C:\Users\benja\Zotero\storage\5Q8SLHNT\Gao et al. - 2024 - Text-to-SQL Empowered by Large Language Models A Benchmark Evaluation.pdf}
}

@book{garcezNeuralSymbolicLearningSystems2002,
  title = {Neural-{{Symbolic Learning Systems}}: {{Foundations}} and {{Applications}}},
  shorttitle = {Neural-{{Symbolic Learning Systems}}},
  author = {d'Avila Garcez, Artur S. and Gabbay, Dov M. and Broda, Krysia B.},
  year = {2002},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  isbn = {978-1-85233-512-0}
}

@article{garcezNeurosymbolicAI3rd2020,
  title = {Neurosymbolic {{AI}}: {{The}} 3rd {{Wave}}},
  shorttitle = {Neurosymbolic {{AI}}},
  author = {Garcez, A. and Lamb, L.},
  year = {2020},
  journal = {ArXiv},
  abstract = {The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.}
}

@inproceedings{garcia2016real,
  title = {Real-Time Segmentation of Non-Rigid Surgical Tools Based on Deep Learning and Tracking},
  booktitle = {International Workshop on Computer-Assisted and Robotic Endoscopy},
  author = {{Garc{\'i}a-Peraza-Herrera}, Luis C and Li, Wenqi and Gruijthuijsen, Caspar and Devreker, Alain and Attilakos, George and Deprest, Jan and Poorten, Emmanuel Vander and Stoyanov, Danail and Vercauteren, Tom and Ourselin, S{\'e}bastien},
  year = {2016},
  pages = {84--95},
  organization = {Springer}
}

@inproceedings{garneloConditionalNeuralProcesses2018,
  title = {Conditional {{Neural Processes}}},
  booktitle = {{{ICML}}},
  author = {Garnelo, Marta and Rosenbaum, Dan and Maddison, Christopher and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo and Eslami, S. M. Ali},
  year = {2018},
  month = jul,
  pages = {1704--1713},
  issn = {1938-7228},
  urldate = {2020-07-12},
  abstract = {Deep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), explo...},
  chapter = {Machine Learning},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\D6W3RF5V\garnelo18a.html}
}

@article{garrettIntegratedTaskMotion2020,
  title = {Integrated {{Task}} and {{Motion Planning}}},
  author = {Garrett, Caelan Reed and Chitnis, Rohan and Holladay, Rachel and Kim, Beomjoon and Silver, Tom and Kaelbling, Leslie Pack and {Lozano-P{\'e}rez}, Tom{\'a}s},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.01083 [cs]},
  eprint = {2010.01083},
  primaryclass = {cs},
  urldate = {2021-05-30},
  abstract = {The problem of planning for a robot that operates in environments containing a large number of objects, taking actions to move itself through the world as well as to change the state of the objects, is known as task and motion planning (TAMP). TAMP problems contain elements of discrete task planning, discrete-continuous mathematical programming, and continuous motion planning, and thus cannot be effectively addressed by any of these fields directly. In this paper, we define a class of TAMP problems and survey algorithms for solving them, characterizing the solution methods in terms of their strategies for solving the continuous-space subproblems and their techniques for integrating the discrete and continuous components of the search.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\MJMB5QW8\2010.html}
}

@misc{garzaTimeGPT12024,
  title = {{{TimeGPT-1}}},
  author = {Garza, Azul and Challu, Cristian and {Mergenthaler-Canseco}, Max},
  year = {2024},
  month = may,
  number = {arXiv:2310.03589},
  eprint = {2310.03589},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.03589},
  urldate = {2024-08-07},
  abstract = {In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Applications},
  file = {C:\Users\benja\Zotero\storage\SGZ9EV3L\2310.html}
}

@inproceedings{gauntDifferentiableProgramsNeural2017,
  title = {Differentiable {{Programs}} with {{Neural Libraries}}},
  booktitle = {{{ICML}}},
  author = {Gaunt, Alexander L. and Brockschmidt, Marc and Kushman, Nate and Tarlow, Daniel},
  year = {2017},
  month = jul,
  pages = {1213--1222},
  urldate = {2019-07-16},
  abstract = {We develop a framework for combining differentiable programming languages with neural networks. Using this framework we create end-to-end trainable systems that learn to write interpretable algorit...},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\R56Y6PR8\gaunt17a.html}
}

@inproceedings{gautamMetaLearningParameterizedFirstOrder2023,
  title = {Meta-{{Learning Parameterized First-Order Optimizers Using Differentiable Convex Optimization}}},
  booktitle = {2023 62nd {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Gautam, Tanmay and Pfrommer, Samuel and Sojoudi, Somayeh},
  year = {2023},
  month = dec,
  pages = {2284--2291},
  issn = {2576-2370},
  doi = {10.1109/CDC49753.2023.10383342},
  urldate = {2024-09-07},
  abstract = {Conventional optimization methods in machine learning and controls rely heavily on first-order update rules. Selecting the right method and hyperparameters for a particular task often involves trial-and-error or practitioner intuition, motivating the field of meta-learning. We generalize a broad family of preexisting update rules by proposing a meta-learning framework in which the inner loop optimization step involves solving a differentiable convex optimization (DCO). We illustrate the theoretical appeal of this approach by showing that it enables one-step optimization of a family of linear least squares problems, given that the meta-learner has sufficient exposure to similar tasks. Various instantiations of the DCO update rule are compared to conventional optimizers on a range of illustrative experimental settings.},
  keywords = {Complex networks,Convergence,Convex functions,Machine learning,Metalearning,Optimization methods,Task analysis},
  file = {C:\Users\benja\Zotero\storage\N55FIXYT\Gautam et al. - 2023 - Meta-Learning Parameterized First-Order Optimizers Using Differentiable Convex Optimization.pdf}
}

@incollection{geifmanSelectiveClassificationDeep2017,
  title = {Selective {{Classification}} for {{Deep Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Geifman, Yonatan and {El-Yaniv}, Ran},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {4878--4887},
  publisher = {Curran Associates, Inc.},
  urldate = {2020-10-13},
  file = {C\:\\Users\\benja\\Zotero\\storage\\AD8BZTGM\\7073-selective-classification-for-deep-neural-networks.html;C\:\\Users\\benja\\Zotero\\storage\\AQDKUVPP\\7073-selective-classification-for-deep-neural-networks.html}
}

@article{geilingerADDAnalyticallyDifferentiable2020,
  title = {{{ADD}}: Analytically Differentiable Dynamics for Multi-Body Systems with Frictional Contact},
  shorttitle = {{{ADD}}},
  author = {Geilinger, Moritz and Hahn, David and Zehnder, Jonas and B{\"a}cher, Moritz and Thomaszewski, Bernhard and Coros, Stelian},
  year = {2020},
  month = nov,
  journal = {ACM Transactions on Graphics},
  volume = {39},
  number = {6},
  pages = {190:1--190:15},
  issn = {0730-0301},
  doi = {10.1145/3414685.3417766},
  urldate = {2022-03-30},
  abstract = {We present a differentiable dynamics solver that is able to handle frictional contact for rigid and deformable objects within a unified framework. Through a principled mollification of normal and tangential contact forces, our method circumvents the main difficulties inherent to the non-smooth nature of frictional contact. We combine this new contact model with fully-implicit time integration to obtain a robust and efficient dynamics solver that is analytically differentiable. In conjunction with adjoint sensitivity analysis, our formulation enables gradient-based optimization with adaptive trade-offs between simulation accuracy and smoothness of objective function landscapes. We thoroughly analyse our approach on a set of simulation examples involving rigid bodies, visco-elastic materials, and coupled multi-body systems. We furthermore showcase applications of our differentiable simulator to parameter estimation for deformable objects, motion planning for robotic manipulation, trajectory optimization for compliant walking robots, as well as efficient self-supervised learning of control policies.},
  keywords = {contact mechanics,deformable models,differentiable simulation,optimization,rigid-body dynamics,robotics}
}

@misc{geminiteamGeminiFamilyHighly2023,
  title = {Gemini: {{A Family}} of {{Highly Capable Multimodal Models}}},
  shorttitle = {Gemini},
  author = {Gemini Team and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M. and Hauth, Anja and Millican, Katie and Silver, David and Petrov, Slav and Johnson, Melvin and Antonoglou, Ioannis and Schrittwieser, Julian and Glaese, Amelia and Chen, Jilin and Pitler, Emily and Lillicrap, Timothy and Lazaridou, Angeliki and Firat, Orhan and Molloy, James and Isard, Michael and Barham, Paul R. and Hennigan, Tom and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Doherty, Ryan and Collins, Eli and Meyer, Clemens and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Tucker, George and Piqueras, Enrique and Krikun, Maxim and Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and Roelofs, Becca and White, Ana{\"i}s and Andreassen, Anders and {von Glehn}, Tamara and Yagati, Lakshman and Kazemi, Mehran and Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and Frechette, Alexandre and Smith, Charlotte and Culp, Laura and Proleev, Lev and Luan, Yi and Chen, Xi and Lottes, James and Schucher, Nathan and Lebron, Federico and Rrustemi, Alban and Clay, Natalie and Crone, Phil and Kocisky, Tomas and Zhao, Jeffrey and Perz, Bartek and Yu, Dian and Howard, Heidi and Bloniarz, Adam and Rae, Jack W. and Lu, Han and Sifre, Laurent and Maggioni, Marcello and Alcober, Fred and Garrette, Dan and Barnes, Megan and Thakoor, Shantanu and Austin, Jacob and {Barth-Maron}, Gabriel and Wong, William and Joshi, Rishabh and Chaabouni, Rahma and Fatiha, Deeni and Ahuja, Arun and Liu, Ruibo and Li, Yunxuan and Cogan, Sarah and Chen, Jeremy and Jia, Chao and Gu, Chenjie and Zhang, Qiao and Grimstad, Jordan and Hartman, Ale Jakse and Chadwick, Martin and Tomar, Gaurav Singh and Garcia, Xavier and Senter, Evan and Taropa, Emanuel and Pillai, Thanumalayan Sankaranarayana and Devlin, Jacob and Laskin, Michael and Casas, Diego de Las and Valter, Dasha and Tao, Connie and Blanco, Lorenzo and Badia, Adri{\`a} Puigdom{\`e}nech and Reitter, David and Chen, Mianna and Brennan, Jenny and Rivera, Clara and Brin, Sergey and Iqbal, Shariq and Surita, Gabriela and Labanowski, Jane and Rao, Abhi and Winkler, Stephanie and Parisotto, Emilio and Gu, Yiming and Olszewska, Kate and Zhang, Yujing and Addanki, Ravi and Miech, Antoine and Louis, Annie and Shafey, Laurent El and Teplyashin, Denis and Brown, Geoff and Catt, Elliot and Attaluri, Nithya and Balaguer, Jan and Xiang, Jackie and Wang, Pidong and Ashwood, Zoe and Briukhov, Anton and Webson, Albert and Ganapathy, Sanjay and Sanghavi, Smit and Kannan, Ajay and Chang, Ming-Wei and Stjerngren, Axel and Djolonga, Josip and Sun, Yuting and Bapna, Ankur and Aitchison, Matthew and Pejman, Pedram and Michalewski, Henryk and Yu, Tianhe and Wang, Cindy and Love, Juliette and Ahn, Junwhan and Bloxwich, Dawn and Han, Kehang and Humphreys, Peter and Sellam, Thibault and Bradbury, James and Godbole, Varun and Samangooei, Sina and Damoc, Bogdan and Kaskasoli, Alex and Arnold, S{\'e}bastien M. R. and Vasudevan, Vijay and Agrawal, Shubham and Riesa, Jason and Lepikhin, Dmitry and Tanburn, Richard and Srinivasan, Srivatsan and Lim, Hyeontaek and Hodkinson, Sarah and Shyam, Pranav and Ferret, Johan and Hand, Steven and Garg, Ankush and Paine, Tom Le and Li, Jian and Li, Yujia and Giang, Minh and Neitz, Alexander and Abbas, Zaheer and York, Sarah and Reid, Machel and Cole, Elizabeth and Chowdhery, Aakanksha and Das, Dipanjan and Rogozi{\'n}ska, Dominika and Nikolaev, Vitaly and Sprechmann, Pablo and Nado, Zachary and Zilka, Lukas and Prost, Flavien and He, Luheng and Monteiro, Marianne and Mishra, Gaurav and Welty, Chris and Newlan, Josh and Jia, Dawei and Allamanis, Miltiadis and Hu, Clara Huiyi and {de Liedekerke}, Raoul and Gilmer, Justin and Saroufim, Carl and Rijhwani, Shruti and Hou, Shaobo and Shrivastava, Disha and Baddepudi, Anirudh and Goldin, Alex and Ozturel, Adnan and Cassirer, Albin and Xu, Yunhan and Sohn, Daniel and Sachan, Devendra and Amplayo, Reinald Kim and Swanson, Craig and Petrova, Dessie and Narayan, Shashi and Guez, Arthur and Brahma, Siddhartha and Landon, Jessica and Patel, Miteyan and Zhao, Ruizhe and Villela, Kevin and Wang, Luyu and Jia, Wenhao and Rahtz, Matthew and Gim{\'e}nez, Mai and Yeung, Legg and Lin, Hanzhao and Keeling, James and Georgiev, Petko and Mincu, Diana and Wu, Boxi and Haykal, Salem and Saputro, Rachel and Vodrahalli, Kiran and Qin, James and Cankara, Zeynep and Sharma, Abhanshu and Fernando, Nick and Hawkins, Will and Neyshabur, Behnam and Kim, Solomon and Hutter, Adrian and Agrawal, Priyanka and {Castro-Ros}, Alex and van den Driessche, George and Wang, Tao and Yang, Fan and Chang, Shuo-yiin and Komarek, Paul and McIlroy, Ross and Lu{\v c}i{\'c}, Mario and Zhang, Guodong and Farhan, Wael and Sharman, Michael and Natsev, Paul and Michel, Paul and Cheng, Yong and Bansal, Yamini and Qiao, Siyuan and Cao, Kris and Shakeri, Siamak and Butterfield, Christina and Chung, Justin and Rubenstein, Paul Kishan and Agrawal, Shivani and Mensch, Arthur and Soparkar, Kedar and Lenc, Karel and Chung, Timothy and Pope, Aedan and Maggiore, Loren and Kay, Jackie and Jhakra, Priya and Wang, Shibo and Maynez, Joshua and Phuong, Mary and Tobin, Taylor and Tacchetti, Andrea and Trebacz, Maja and Robinson, Kevin and Katariya, Yash and Riedel, Sebastian and Bailey, Paige and Xiao, Kefan and Ghelani, Nimesh and Aroyo, Lora and Slone, Ambrose and Houlsby, Neil and Xiong, Xuehan and Yang, Zhen and Gribovskaya, Elena and Adler, Jonas and Wirth, Mateo and Lee, Lisa and Li, Music and Kagohara, Thais and Pavagadhi, Jay and Bridgers, Sophie and Bortsova, Anna and Ghemawat, Sanjay and Ahmed, Zafarali and Liu, Tianqi and Powell, Richard and Bolina, Vijay and Iinuma, Mariko and Zablotskaia, Polina and Besley, James and Chung, Da-Woon and Dozat, Timothy and Comanescu, Ramona and Si, Xiance and Greer, Jeremy and Su, Guolong and Polacek, Martin and Kaufman, Rapha{\"e}l Lopez and Tokumine, Simon and Hu, Hexiang and Buchatskaya, Elena and Miao, Yingjie and Elhawaty, Mohamed and Siddhant, Aditya and Tomasev, Nenad and Xing, Jinwei and Greer, Christina and Miller, Helen and Ashraf, Shereen and Roy, Aurko and Zhang, Zizhao and Ma, Ada and Filos, Angelos and Besta, Milos and Blevins, Rory and Klimenko, Ted and Yeh, Chih-Kuan and Changpinyo, Soravit and Mu, Jiaqi and Chang, Oscar and Pajarskas, Mantas and Muir, Carrie and Cohen, Vered and Lan, Charline Le and Haridasan, Krishna and Marathe, Amit and Hansen, Steven and Douglas, Sholto and Samuel, Rajkumar and Wang, Mingqiu and Austin, Sophia and Lan, Chang and Jiang, Jiepu and Chiu, Justin and Lorenzo, Jaime Alonso and Sj{\"o}sund, Lars Lowe and Cevey, S{\'e}bastien and Gleicher, Zach and Avrahami, Thi and Boral, Anudhyan and Srinivasan, Hansa and Selo, Vittorio and May, Rhys and Aisopos, Konstantinos and Hussenot, L{\'e}onard and Soares, Livio Baldini and Baumli, Kate and Chang, Michael B. and Recasens, Adri{\`a} and Caine, Ben and Pritzel, Alexander and Pavetic, Filip and Pardo, Fabio and Gergely, Anita and Frye, Justin and Ramasesh, Vinay and Horgan, Dan and Badola, Kartikeya and Kassner, Nora and Roy, Subhrajit and Dyer, Ethan and Campos, V{\'i}ctor and Tomala, Alex and Tang, Yunhao and Badawy, Dalia El and White, Elspeth and Mustafa, Basil and Lang, Oran and Jindal, Abhishek and Vikram, Sharad and Gong, Zhitao and Caelles, Sergi and Hemsley, Ross and Thornton, Gregory and Feng, Fangxiaoyu and Stokowiec, Wojciech and Zheng, Ce and Thacker, Phoebe and {\"U}nl{\"u}, {\c C}a{\u g}lar and Zhang, Zhishuai and Saleh, Mohammad and Svensson, James and Bileschi, Max and Patil, Piyush and Anand, Ankesh and Ring, Roman and Tsihlas, Katerina and Vezer, Arpi and Selvi, Marco and Shevlane, Toby and Rodriguez, Mikel and Kwiatkowski, Tom and Daruki, Samira and Rong, Keran and Dafoe, Allan and FitzGerald, Nicholas and {Gu-Lemberg}, Keren and Khan, Mina and Hendricks, Lisa Anne and Pellat, Marie and Feinberg, Vladimir and {Cobon-Kerr}, James and Sainath, Tara and Rauh, Maribeth and Hashemi, Sayed Hadi and Ives, Richard and Hasson, Yana and Li, YaGuang and Noland, Eric and Cao, Yuan and Byrd, Nathan and Hou, Le and Wang, Qingze and Sottiaux, Thibault and Paganini, Michela and Lespiau, Jean-Baptiste and Moufarek, Alexandre and Hassan, Samer and Shivakumar, Kaushik and {van Amersfoort}, Joost and Mandhane, Amol and Joshi, Pratik and Goyal, Anirudh and Tung, Matthew and Brock, Andrew and Sheahan, Hannah and Misra, Vedant and Li, Cheng and Raki{\'c}evi{\'c}, Nemanja and Dehghani, Mostafa and Liu, Fangyu and Mittal, Sid and Oh, Junhyuk and Noury, Seb and Sezener, Eren and Huot, Fantine and Lamm, Matthew and De Cao, Nicola and Chen, Charlie and Elsayed, Gamaleldin and Chi, Ed and Mahdieh, Mahdis and Tenney, Ian and Hua, Nan and Petrychenko, Ivan and Kane, Patrick and Scandinaro, Dylan and Jain, Rishub and Uesato, Jonathan and Datta, Romina and Sadovsky, Adam and Bunyan, Oskar and Rabiej, Dominik and Wu, Shimu and Zhang, John and Vasudevan, Gautam and Leurent, Edouard and Alnahlawi, Mahmoud and Georgescu, Ionut and Wei, Nan and Zheng, Ivy and Chan, Betty and Rabinovitch, Pam G. and Stanczyk, Piotr and Zhang, Ye and Steiner, David and Naskar, Subhajit and Azzam, Michael and Johnson, Matthew and Paszke, Adam and Chiu, Chung-Cheng and Elias, Jaume Sanchez and Mohiuddin, Afroz and Muhammad, Faizan and Miao, Jin and Lee, Andrew and Vieillard, Nino and Potluri, Sahitya and Park, Jane and Davoodi, Elnaz and Zhang, Jiageng and Stanway, Jeff and Garmon, Drew and Karmarkar, Abhijit and Dong, Zhe and Lee, Jong and Kumar, Aviral and Zhou, Luowei and Evens, Jonathan and Isaac, William and Chen, Zhe and Jia, Johnson and Levskaya, Anselm and Zhu, Zhenkai and Gorgolewski, Chris and Grabowski, Peter and Mao, Yu and Magni, Alberto and Yao, Kaisheng and Snaider, Javier and Casagrande, Norman and Suganthan, Paul and Palmer, Evan and Irving, Geoffrey and Loper, Edward and Faruqui, Manaal and Arkatkar, Isha and Chen, Nanxin and Shafran, Izhak and Fink, Michael and Casta{\~n}o, Alfonso and Giannoumis, Irene and Kim, Wooyeol and Rybi{\'n}ski, Miko{\l}aj and Sreevatsa, Ashwin and Prendki, Jennifer and Soergel, David and Goedeckemeyer, Adrian and Gierke, Willi and Jafari, Mohsen and Gaba, Meenu and Wiesner, Jeremy and Wright, Diana Gage and Wei, Yawen and Vashisht, Harsha and Kulizhskaya, Yana and Hoover, Jay and Le, Maigo and Li, Lu and Iwuanyanwu, Chimezie and Liu, Lu and Ramirez, Kevin and Khorlin, Andrey and Cui, Albert and LIN, Tian and Georgiev, Marin and Wu, Marcus and Aguilar, Ricardo and Pallo, Keith and Chakladar, Abhishek and Repina, Alena and Wu, Xihui and {van der Weide}, Tom and Ponnapalli, Priya and Kaplan, Caroline and Simsa, Jiri and Li, Shuangfeng and Dousse, Olivier and Yang, Fan and Piper, Jeff and Ie, Nathan and Lui, Minnie and Pasumarthi, Rama and Lintz, Nathan and Vijayakumar, Anitha and Thiet, Lam Nguyen and Andor, Daniel and Valenzuela, Pedro and Paduraru, Cosmin and Peng, Daiyi and Lee, Katherine and Zhang, Shuyuan and Greene, Somer and Nguyen, Duc Dung and Kurylowicz, Paula and Velury, Sarmishta and Krause, Sebastian and Hardin, Cassidy and Dixon, Lucas and Janzer, Lili and Choo, Kiam and Feng, Ziqiang and Zhang, Biao and Singhal, Achintya and Latkar, Tejasi and Zhang, Mingyang and Le, Quoc and Abellan, Elena Allica and Du, Dayou and McKinnon, Dan and Antropova, Natasha and Bolukbasi, Tolga and Keller, Orgad and Reid, David and Finchelstein, Daniel and Raad, Maria Abi and Crocker, Remi and Hawkins, Peter and Dadashi, Robert and Gaffney, Colin and Lall, Sid and Franko, Ken and Filonov, Egor and Bulanova, Anna and Leblond, R{\'e}mi and Yadav, Vikas and Chung, Shirley and Askham, Harry and Cobo, Luis C. and Xu, Kelvin and Fischer, Felix and Xu, Jun and Sorokin, Christina and Alberti, Chris and Lin, Chu-Cheng and Evans, Colin and Zhou, Hao and Dimitriev, Alek and Forbes, Hannah and Banarse, Dylan and Tung, Zora and Liu, Jeremiah and Omernick, Mark and Bishop, Colton and Kumar, Chintu and Sterneck, Rachel and Foley, Ryan and Jain, Rohan and Mishra, Swaroop and Xia, Jiawei and Bos, Taylor and Cideron, Geoffrey and Amid, Ehsan and Piccinno, Francesco and Wang, Xingyu and Banzal, Praseem and Gurita, Petru and Noga, Hila and Shah, Premal and Mankowitz, Daniel J. and Polozov, Alex and Kushman, Nate and Krakovna, Victoria and Brown, Sasha and Bateni, MohammadHossein and Duan, Dennis and Firoiu, Vlad and Thotakuri, Meghana and Natan, Tom and Mohananey, Anhad and Geist, Matthieu and Mudgal, Sidharth and Girgin, Sertan and Li, Hui and Ye, Jiayu and Roval, Ofir and Tojo, Reiko and Kwong, Michael and {Lee-Thorp}, James and Yew, Christopher and Yuan, Quan and Bagri, Sumit and Sinopalnikov, Danila and Ramos, Sabela and Mellor, John and Sharma, Abhishek and Severyn, Aliaksei and Lai, Jonathan and Wu, Kathy and Cheng, Heng-Tze and Miller, David and Sonnerat, Nicolas and Vnukov, Denis and Greig, Rory and Beattie, Jennifer and Caveness, Emily and Bai, Libin and Eisenschlos, Julian and Korchemniy, Alex and Tsai, Tomy and Jasarevic, Mimi and Kong, Weize and Dao, Phuong and Zheng, Zeyu and Liu, Frederick and Yang, Fan and Zhu, Rui and Geller, Mark and Teh, Tian Huey and Sanmiya, Jason and Gladchenko, Evgeny and Trdin, Nejc and Sozanschi, Andrei and Toyama, Daniel and Rosen, Evan and Tavakkol, Sasan and Xue, Linting and Elkind, Chen and Woodman, Oliver and Carpenter, John and Papamakarios, George and Kemp, Rupert and Kafle, Sushant and Grunina, Tanya and Sinha, Rishika and Talbert, Alice and Goyal, Abhimanyu and Wu, Diane and {Owusu-Afriyie}, Denese and Du, Cosmo and Thornton, Chloe and {Pont-Tuset}, Jordi and Narayana, Pradyumna and Li, Jing and Fatehi, Sabaer and Wieting, John and Ajmeri, Omar and Uria, Benigno and Zhu, Tao and Ko, Yeongil and Knight, Laura and H{\'e}liou, Am{\'e}lie and Niu, Ning and Gu, Shane and Pang, Chenxi and Tran, Dustin and Li, Yeqing and Levine, Nir and Stolovich, Ariel and Kalb, Norbert and {Santamaria-Fernandez}, Rebeca and Goenka, Sonam and Yustalim, Wenny and Strudel, Robin and Elqursh, Ali and Lakshminarayanan, Balaji and Deck, Charlie and Upadhyay, Shyam and Lee, Hyo and Dusenberry, Mike and Li, Zonglin and Wang, Xuezhi and Levin, Kyle and Hoffmann, Raphael and {Holtmann-Rice}, Dan and Bachem, Olivier and Yue, Summer and Arora, Sho and Malmi, Eric and Mirylenka, Daniil and Tan, Qijun and Koh, Christy and Yeganeh, Soheil Hassas and P{\~o}der, Siim and Zheng, Steven and Pongetti, Francesco and Tariq, Mukarram and Sun, Yanhua and Ionita, Lucian and Seyedhosseini, Mojtaba and Tafti, Pouya and Kotikalapudi, Ragha and Liu, Zhiyu and Gulati, Anmol and Liu, Jasmine and Ye, Xinyu and Chrzaszcz, Bart and Wang, Lily and Sethi, Nikhil and Li, Tianrun and Brown, Ben and Singh, Shreya and Fan, Wei and Parisi, Aaron and Stanton, Joe and Kuang, Chenkai and Koverkathu, Vinod and {Choquette-Choo}, Christopher A. and Li, Yunjie and Lu, T. J. and Ittycheriah, Abe and Shroff, Prakash and Sun, Pei and Varadarajan, Mani and Bahargam, Sanaz and Willoughby, Rob and Gaddy, David and Dasgupta, Ishita and Desjardins, Guillaume and Cornero, Marco and Robenek, Brona and Mittal, Bhavishya and Albrecht, Ben and Shenoy, Ashish and Moiseev, Fedor and Jacobsson, Henrik and Ghaffarkhah, Alireza and Rivi{\`e}re, Morgane and Walton, Alanna and Crepy, Cl{\'e}ment and Parrish, Alicia and Liu, Yuan and Zhou, Zongwei and Farabet, Clement and Radebaugh, Carey and Srinivasan, Praveen and {van der Salm}, Claudia and Fidjeland, Andreas and Scellato, Salvatore and {Latorre-Chimoto}, Eri and {Klimczak-Pluci{\'n}ska}, Hanna and Bridson, David and {de Cesare}, Dario and Hudson, Tom and Mendolicchio, Piermaria and Walker, Lexi and Morris, Alex and Penchev, Ivo and Mauger, Matthew and Guseynov, Alexey and Reid, Alison and Odoom, Seth and Loher, Lucia and Cotruta, Victor and Yenugula, Madhavi and Grewe, Dominik and Petrushkina, Anastasia and Duerig, Tom and Sanchez, Antonio and Yadlowsky, Steve and Shen, Amy and Globerson, Amir and Kurzrok, Adam and Webb, Lynette and Dua, Sahil and Li, Dong and Lahoti, Preethi and Bhupatiraju, Surya and Hurt, Dan and Qureshi, Haroon and Agarwal, Ananth and Shani, Tomer and Eyal, Matan and Khare, Anuj and Belle, Shreyas Rammohan and Wang, Lei and Tekur, Chetan and Kale, Mihir Sanjay and Wei, Jinliang and Sang, Ruoxin and Saeta, Brennan and Liechty, Tyler and Sun, Yi and Zhao, Yao and Lee, Stephan and Nayak, Pandu and Fritz, Doug and Vuyyuru, Manish Reddy and Aslanides, John and Vyas, Nidhi and Wicke, Martin and Ma, Xiao and Bilal, Taylan and Eltyshev, Evgenii and Balle, Daniel and Martin, Nina and Cate, Hardie and Manyika, James and Amiri, Keyvan and Kim, Yelin and Xiong, Xi and Kang, Kai and Luisier, Florian and Tripuraneni, Nilesh and Madras, David and Guo, Mandy and Waters, Austin and Wang, Oliver and Ainslie, Joshua and Baldridge, Jason and Zhang, Han and Pruthi, Garima and Bauer, Jakob and Yang, Feng and Mansour, Riham and Gelman, Jason and Xu, Yang and Polovets, George and Liu, Ji and Cai, Honglong and Chen, Warren and Sheng, XiangHai and Xue, Emily and Ozair, Sherjil and Yu, Adams and Angermueller, Christof and Li, Xiaowei and Wang, Weiren and Wiesinger, Julia and Koukoumidis, Emmanouil and Tian, Yuan and Iyer, Anand and Gurumurthy, Madhu and Goldenson, Mark and Shah, Parashar and Blake, M. K. and Yu, Hongkun and Urbanowicz, Anthony and Palomaki, Jennimaria and Fernando, Chrisantha and Brooks, Kevin and Durden, Ken and Mehta, Harsh and Momchev, Nikola and Rahimtoroghi, Elahe and Georgaki, Maria and Raul, Amit and Ruder, Sebastian and Redshaw, Morgan and Lee, Jinhyuk and Jalan, Komal and Li, Dinghua and Perng, Ginger and Hechtman, Blake and Schuh, Parker and Nasr, Milad and Chen, Mia and Milan, Kieran and Mikulik, Vladimir and Strohman, Trevor and Franco, Juliana and Green, Tim and Hassabis, Demis and Kavukcuoglu, Koray and Dean, Jeffrey and Vinyals, Oriol},
  year = {2023},
  month = dec,
  number = {arXiv:2312.11805},
  eprint = {2312.11805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.11805},
  urldate = {2024-01-05},
  abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\benja\Zotero\storage\XBEYMMAX\2312.html}
}

@book{gendreauHandbookMetaheuristics2010,
  title = {Handbook of {{Metaheuristics}}},
  editor = {Gendreau, Michel and Potvin, Jean-Yves},
  year = {2010},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  volume = {146},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4419-1665-5},
  urldate = {2023-06-01},
  isbn = {978-1-4419-1663-1 978-1-4419-1665-5},
  langid = {english},
  keywords = {algorithms,genetic algorithm,genetic algorithms,genetic programming,heuristics,learning,metaheuristic,operations research,optimization}
}

@misc{GenerativeModels2016,
  title = {Generative {{Models}}},
  year = {2016},
  month = jun,
  journal = {OpenAI},
  urldate = {2020-10-03},
  abstract = {This post describes four projects that share a common theme of enhancing or using generative models, a branch of unsupervised learning techniques in machine learning.},
  langid = {english}
}

@article{georgievskiHTNPlanningOverview2015,
  title = {{{HTN}} Planning: {{Overview}}, Comparison, and Beyond},
  shorttitle = {{{HTN}} Planning},
  author = {Georgievski, Ilche and Aiello, Marco},
  year = {2015},
  month = may,
  journal = {Artificial Intelligence},
  volume = {222},
  pages = {124--156},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2015.02.002},
  urldate = {2024-04-23},
  abstract = {Hierarchies are one of the most common structures used to understand and conceptualise the world. Within the field of Artificial Intelligence (AI) planning, which deals with the automation of world-relevant problems, Hierarchical Task Network (HTN) planning is the branch that represents and handles hierarchies. In particular, the requirement for rich domain knowledge to characterise the world enables HTN planning to be very useful, and also to perform well. However, the history of almost 40 years obfuscates the current understanding of HTN planning in terms of accomplishments, planning models, similarities and differences among hierarchical planners, and its current and objective image. On top of these issues, the ability of hierarchical planning to truly cope with the requirements of real-world applications has been often questioned. As a remedy, we propose a framework-based approach where we first provide a basis for defining different formal models of hierarchical planning, and define two models that comprise a large portion of HTN planners. Second, we provide a set of concepts that helps in interpreting HTN planners from the aspect of their search space. Then, we analyse and compare the planners based on a variety of properties organised in five segments, namely domain authoring, expressiveness, competence, computation and applicability. Furthermore, we select Web service composition as a real-world and current application, and classify and compare the approaches that employ HTN planning to solve the problem of service composition. Finally, we conclude with our findings and present directions for future work. In summary, we provide a novel and comprehensive viewpoint on a core AI planning technique.},
  keywords = {AI planning,Hierarchical task networks,Web service composition},
  file = {C:\Users\benja\Zotero\storage\5P25UISC\S0004370215000247.html}
}

@article{georgievskiOverviewHierarchicalTask2014,
  title = {An {{Overview}} of {{Hierarchical Task Network Planning}}},
  author = {Georgievski, Ilche and Aiello, Marco},
  year = {2014},
  month = mar,
  journal = {arXiv:1403.7426 [cs]},
  eprint = {1403.7426},
  primaryclass = {cs},
  urldate = {2019-05-20},
  abstract = {Hierarchies are the most common structure used to understand the world better. In galaxies, for instance, multiple-star systems are organised in a hierarchical system. Then, governmental and company organisations are structured using a hierarchy, while the Internet, which is used on a daily basis, has a space of domain names arranged hierarchically. Since Artificial Intelligence (AI) planning portrays information about the world and reasons to solve some of world's problems, Hierarchical Task Network (HTN) planning has been introduced almost 40 years ago to represent and deal with hierarchies. Its requirement for rich domain knowledge to characterise the world enables HTN planning to be very useful, but also to perform well. However, the history of almost 40 years obfuscates the current understanding of HTN planning in terms of accomplishments, planning models, similarities and differences among hierarchical planners, and its current and objective image. On top of these issues, attention attracts the ability of hierarchical planning to truly cope with the requirements of applications from the real world. We propose a framework-based approach to remedy this situation. First, we provide a basis for defining different formal models of hierarchical planning, and define two models that comprise a large portion of HTN planners. Second, we provide a set of concepts that helps to interpret HTN planners from the aspect of their search space. Then, we analyse and compare the planners based on a variety of properties organised in five segments, namely domain authoring, expressiveness, competence, performance and applicability. Furthermore, we select Web service composition as a real-world and current application, and classify and compare the approaches that employ HTN planning to solve the problem of service composition. Finally, we conclude with our findings and present directions for future work.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence}
}

@article{Georgopoulos1989a,
  title = {Visuomotor Coordination in Reaching and Locomotion.},
  author = {Georgopoulos, A P and Grillner, S},
  year = {1989},
  month = sep,
  journal = {Science},
  volume = {245},
  number = {4923},
  eprint = {2675307},
  eprinttype = {pubmed},
  pages = {1209--10},
  urldate = {2018-05-10},
  abstract = {Locomotion and reaching have traditionally been regarded as separate motor activities. In fact, they may be closely connected both from an evolutionary and a neurophysiological viewpoint. Reaching seems to have evolved from the neural systems responsible for the active and precise positioning of the limb during locomotion; moreover, it seems to be organized in the spinal cord. The motor cortex and its corticospinal outflow are preferentially engaged when precise positioning of the limb is needed during locomotion and are also involved during reaching and active positioning of the hand near objects of interest. All of these motor activities require visuomotor coordination, and it is this coordination that could be achieved by the motor cortex and interconnected parietal and cerebellar areas.},
  pmid = {2675307}
}

@inproceedings{gersLearningForgetContinual1999,
  title = {Learning to Forget: Continual Prediction with {{LSTM}}},
  shorttitle = {Learning to Forget},
  booktitle = {1999 {{Ninth International Conference}} on {{Artificial Neural Networks ICANN}} 99. ({{Conf}}. {{Publ}}. {{No}}. 470)},
  author = {Gers, F. A. and Schmidhuber, J. and Cummins, F.},
  year = {1999},
  month = sep,
  volume = {2},
  pages = {850-855 vol.2},
  doi = {10.1049/cp:19991218},
  abstract = {Long short-term memory (LSTM) can solve many tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams without explicitly marked sequence ends. Without resets, the internal state values may grow indefinitely and eventually cause the network to break down. Our remedy is an adaptive "forget gate" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review an illustrative benchmark problem on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve a continual version of that problem. LSTM with forget gates, however, easily solves it in an elegant way.},
  keywords = {adaptive forget gate,learning,long short-term memory,recurrent neural nets,recurrent neural networks,resource allocation}
}

@book{Gerstner,
  title = {Neuronal {{Dynamics}} : {{From Single Neurons}} to {{Networks}} and {{Models}} of {{Cognition}}},
  author = {Gerstner, {\relax Wulfram}. and Kistler, Werner M. and Naud, {\relax Richard}. and Paninski, {\relax Liam}.},
  year = {2014},
  urldate = {2018-06-21},
  abstract = {Machine generated contents note: pt. ONE FOUNDATIONS OF NEURONAL DYNAMICS -- 1. Introduction: neurons and mathematics -- 1.1. Elements of neuronal systems -- 1.2. Elements of neuronal dynamics -- 1.3. Integrate-and-fire models -- 1.4. Limitations of the leaky integrate-and-fire model -- 1.5. What can we expect from integrate-and-fire models? -- 1.6. Summary -- 2. Ion channels and the Hodgkin--Huxley model -- 2.1. Equilibrium potential -- 2.2. Hodgkin--Huxley model -- 2.3. The zoo of ion channels -- 2.4. Summary -- 3. Dendrites and synapses -- 3.1. Synapses -- 3.2. Spatial structure: the dendritic tree -- 3.3. Spatial structure: axons -- 3.4.Compartmental models -- 3.5. Summary -- 4. Dimensionality reduction and phase plane analysis -- 4.1. Threshold effects -- 4.2. Reduction to two dimensions -- 4.3. Phase plane analysis -- 4.4. Type I and type II neuron models -- 4.5. Threshold and excitability -- 4.6. Separation of time scales and reduction to one dimension -- 4.7. Summary. Contents note continued: pt. TWO GENERALIZED INTEGRATE-AND-FIRE NEURONS -- 5. Nonlinear integrate-and-fire models -- 5.1. Thresholds in a nonlinear integrate-and-fire model -- 5.2. Exponential integrate-and-fire model -- 5.3. Quadratic integrate and fire -- 5.4. Summary -- 6. Adaptation and firing patterns -- 6.1. Adaptive exponential integrate-and-fire -- 6.2. Firing patterns -- 6.3. Biophysical origin of adaptation -- 6.4. Spike Response Model (SRM) -- 6.5. Summary -- 7. Variability of spike trains and neural codes -- 7.1. Spike-train variability -- 7.2. Mean firing rate -- 7.3. Interval distribution and coefficient of variation -- 7.4. Autocorrelation function and noise spectrum -- 7.5. Renewal statistics -- 7.6. The problem of neural coding -- 7.7. Summary -- 8. Noisy input models: barrage of spike arrivals -- 8.1. Noise input -- 8.2. Stochastic spike arrival -- 8.3. Subthreshold vs. superthreshold regime -- 8.4. Diffusion limit and Fokker--Planck equation (*) -- 8.5. Summary. Contents note continued: 9. Noisy output: escape rate and soft threshold -- 9.1. Escape noise -- 9.2. Likelihood of a spike train -- 9.3. Renewal approximation of the Spike Response Model -- 9.4. From noisy inputs to escape noise -- 9.5. Summary -- 10. Estimating parameters of probabilistic neuron models -- 10.1. Parameter optimization in linear and nonlinear models -- 10.2. Statistical formulation of encoding models -- 10.3. Evaluating goodness-of-fit -- 10.4. Closed-loop stimulus design -- 10.5. Summary -- 11. Encoding and decoding with stochastic neuron models -- 11.1. Encoding models for intracellular recordings -- 11.2. Encoding models in systems neuroscience -- 11.3. Decoding -- 11.4. Summary -- pt. THREE NETWORKS OF NEURONS AND POPULATION ACTIVITY -- 12. Neuronal populations -- 12.1. Columnar organization -- 12.2. Identical neurons: a mathematical abstraction -- 12.3. Connectivity schemes -- 12.4. From microscopic to macroscopic -- 12.5. Summary. Contents note continued: 13. Continuity equation and the Fokker--Planck approach -- 13.1. Continuity equation -- 13.2. Stochastic spike arrival -- 13.3. Fokker--Planck equation -- 13.4.Networks of leaky integrate-and-fire neurons -- 13.5.Networks of nonlinear integrate-and-fire neurons -- 13.6. Neuronal adaptation and synaptic conductance -- 13.7. Summary -- 14. Quasi-renewal theory and the integral-equation approach -- 14.1. Population activity equations -- 14.2. Recurrent networks and interacting populations -- 14.3. Linear response to time-dependent input -- 14.4. Density equations vs. integral equations -- 14.5. Adaptation in population equations -- 14.6. Heterogeneity and finite size -- 14.7. Summary -- 15. Fast transients and rate models -- 15.1. How fast are population responses? -- 15.2. Fast transients vs. slow transients in models -- 15.3. Rate models -- 15.4. Summary -- pt. FOUR DYNAMICS OF COGNITION -- 16.Competing populations and decision making -- 16.1. Perceptual decision making. Contents note continued: 16.2.Competition through common inhibition -- 16.3. Dynamics of decision making -- 16.4. Alternative decision models -- 16.5. Human decisions, determinism, and free will -- 16.6. Summary -- 17. Memory and attractor dynamics -- 17.1. Associations and memory -- 17.2. Hopfield model -- 17.3. Memory networks with spiking neurons -- 17.4. Summary -- 18. Cortical field models for perception -- 18.1. Spatial continuum model -- 18.2. Input-driven regime and sensory cortex models -- 18.3. Bump attractors and spontaneous pattern formation -- 18.4. Summary -- 19. Synaptic plasticity and learning -- 19.1. Hebb rule and experiments -- 19.2. Models of Hebbian learning -- 19.3. Unsupervised learning -- 19.4. Reward-based learning -- 19.5. Summary -- 20. Outlook: dynamics in plastic networks -- 20.1. Reservoir computing -- 20.2. Oscillations: good or bad? -- 20.3. Helping patients -- 20.4. Summary.},
  isbn = {1-107-63519-5}
}

@inproceedings{ghadirzadehBayesianMetaLearningFewShot2021,
  title = {Bayesian {{Meta-Learning}} for {{Few-Shot Policy Adaptation Across Robotic Platforms}}},
  booktitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Ghadirzadeh, Ali and Chen, Xi and Poklukar, Petra and Finn, Chelsea and Bj{\"o}rkman, M{\aa}rten and Kragic, Danica},
  year = {2021},
  month = sep,
  pages = {1274--1280},
  issn = {2153-0866},
  doi = {10.1109/IROS51168.2021.9636628},
  urldate = {2024-06-15},
  abstract = {Reinforcement learning methods can achieve significant performance but require a large amount of training data collected on the same robotic platform. A policy trained with expensive data is rendered useless after making even a minor change to the robot hardware. In this paper, we address the challenging problem of adapting a policy, trained to perform a task, to a novel robotic hardware platform given only few demonstrations of robot motion trajectories on the target robot. We formulate it as a few-shot meta-learning problem where the goal is to find a meta-model that captures the common structure shared across different robotic platforms such that data-efficient adaptation can be performed. We achieve such adaptation by introducing a learning framework consisting of a probabilistic gradient-based meta-learning algorithm that models the uncertainty arising from the few-shot setting with a low-dimensional latent variable. We experimentally evaluate our framework on a simulated reaching and a real-robot picking task using 400 simulated robots generated by varying the physical parameters of an existing set of robotic platforms. Our results show that the proposed method can successfully adapt a trained policy to different robotic platforms with novel physical parameters and the superiority of our meta-learning algorithm compared to state-of-the-art methods for the introduced few-shot policy adaptation problem.},
  keywords = {Adaptation models,Hardware,Probabilistic logic,Reinforcement learning,Robot motion,Training data,Uncertainty},
  file = {C:\Users\benja\Zotero\storage\MZE7MUYK\9636628.html}
}

@book{ghallabAutomatedPlanningTheory2004,
  title = {{Automated Planning: Theory and Practice}},
  shorttitle = {{Automated Planning}},
  author = {Ghallab, Malik and Nau, Dana S. and Traverso, Paolo and Malik, Ghallab},
  year = {2004},
  month = may,
  publisher = {Morgan Kaufmann Publishers In},
  address = {Amsterdam ; Boston},
  abstract = {Automated planning technology now plays a significant role in a variety of demanding applications, ranging from controlling space vehicles and robots to playing the game of bridge. These real-world applications create new opportunities for synergy between theory and practice: observing what works well in practice leads to better theories of planning, and better theories lead to better performance of practical applications.Automated Planning mirrors this dialogue by offering a comprehensive, up-to-date resource on both the theory and practice of automated planning. The book goes well beyond classical planning, to include temporal planning, resource scheduling, planning under uncertainty, and modern techniques for plan generation, such as task decomposition, propositional satisfiability, constraint satisfaction, and model checking.The authors combine over 30 years experience in planning research and development to offer an invaluable text to researchers, professionals, and graduate students.Provides a thorough understanding of AI planning theory and practice, and how they relate to each otherCovers all the contemporary topics of planning, as well as important practical applications of planning, such as model checking and game playingPresents case studies and applications in planning engineering, space, robotics, CAD/CAM, process control, emergency operations, and gamesProvides lecture notes, examples of programming assignments, pointers to downloadable planning systems and related information online},
  isbn = {978-1-55860-856-6},
  langid = {Englisch}
}

@article{ghorbelKinematicErrorHarmonic2001,
  title = {On the {{Kinematic Error}} in {{Harmonic Drive Gears}}},
  author = {Ghorbel, F. and Gandhi, P. and Alpeter, Friedhelm},
  year = {2001},
  month = mar,
  journal = {Journal of Mechanical Design},
  volume = {123},
  number = {1},
  pages = {90--97},
  doi = {10.1115/1.1334379},
  abstract = {The results of this paper offer a new perspective in the understanding of the mechanism of kinematic error and will be valuable in the mechanical design of harmonic drive gears as well as in the dynamic modeling and precision control of harmonicDrive systems. Harmonic drive gears are widely used in space applications, robotics, and precision positioning systems because of their attractive attributes including near-zero backlash, high speed reduction ratio, compact size, and small weight. On the other hand, they possess an inherent periodic positioning error known as kinematic error responsible for transmission performance degradation. No definite understanding of the mechanism of kinematic error as well as its characterization is available in the literature. In this paper, we report analytical and experimental results on kinematic error using a dedicated research Harmonic Drive Test Apparatus. We first show that the error referred to in the literature as kinematic error actually consists of a basic component, representing ``pure'' kinematic error, colored with a second component resulting from inherent torsional flexibility in the harmonic drive gear. The latter component explains the source of variability in published kinematic error profiles. The decomposition of the kinematic error into a basic component and a flexibility related component is demonstrated experimentally as well as analytically by matching a mathematical model to experimental data. We also characterize the dependence of the kinematic error on inertial load, gear assembly, and rotational speed. The results of this paper offer a new perspective in the understanding of the mechanism of kinematic error and will be valuable in the mechanical design of harmonic drive gears as well as in the dynamic modeling and precision control of harmonic drive systems. @DOI: 10.1115/1.1334379\#}
}

@inproceedings{ghoshOctoOpenSourceGeneralist2024,
  title = {Octo: {{An Open-Source Generalist Robot Policy}}},
  shorttitle = {Octo},
  author = {Ghosh, Dibya and Walke, Homer and Pertsch, Karl and Black, Kevin and Mees, Oier and Dasari, Sudeep and Hejna, Joey and Kreiman, Tobias and Xu, Charles and Luo, Jianlan and Tan, You and Chen, Lawrence and Vuong, Quan and Xiao, Ted and Sanketi, Pannag and Sadigh, Dorsa and Finn, Chelsea and Levine, Sergey},
  year = {2024},
  month = jul,
  doi = {10.15607/RSS.2024.XX.090}
}

@inproceedings{gibbonsOptimisationDynamicGait2009,
  title = {Optimisation of {{Dynamic Gait}} for {{Small Bipedal Robots}}},
  booktitle = {Proceedings of the 4th {{Workshop}} on {{Humanoid Soccer Robots}}},
  author = {Gibbons, P. and Mason, M. and Vicente, A. and Bugmann, G. and Culverhouse, P.},
  year = {2009},
  pages = {9--14},
  publisher = {IEEE},
  address = {Paris},
  urldate = {2021-06-21},
  abstract = {parameters that effect dynamic gait is included along with a discussion of how these are implemented in a servo skeleton. The gait parameters are stride height, hip swing amplitude and step period. The paper reports on the optimisation of these parameters through a systematic exploration of the parameter space. The quality of the parameters is evaluated in terms of lateral head movement and foot clearance. This research was facilitated by the use of an online wireless programming interface that enables rapid testing of all the gait parameters. The gait quality was observed to be optimal at shorter step periods and larger stride heights.},
  isbn = {978-88-95872-03-2},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\6FTT7JW9\b5faad60ac0a5c3668dc14f24da9f0fc8b6cffd1.html}
}

@mastersthesis{giengerOptimierungWerkzeugbahnFuer2018,
  title = {{Optimierung der Werkzeugbahn f{\"u}r Oberfl{\"a}chenbearbeitungsprozesse in der Industrierobotik}},
  author = {Gienger, Lennart},
  year = {2018},
  address = {Karlsruhe},
  langid = {ngerman},
  school = {Karlsruhe Institute of Technology}
}

@book{gillaniOntologicalPhysicsbasedMotion2015,
  title = {Ontological {{Physics-based Motion Planning}} for {{Manipulation}}},
  author = {Gillani, Muhayyuddin and Akbari, Ali and Rosell, Jan},
  year = {2015},
  month = sep,
  doi = {10.1109/ETFA.2015.7301404},
  abstract = {Robotic manipulation involves actions where contacts occur between the robot and the objects. In this scope, the availability of physics-based engines allows motion planners to comprise dynamics between rigid bodies, which is necessary for planning this type of actions. However, physics-based motion planning is computationally intensive due to the high dimensionality of the state space and the need to work with a low integration step to find accurate solutions. On the other hand, manipulation actions change the environment and conditions further actions and motions. To cope with this issue, the representation of manipulation actions using ontologies enables a semantic-based inference process that alleviates the computational cost of motion planning. This paper proposes a manipulation planning framework where physics-based motion planning is enhanced with ontological knowledge representation and reasoning. The proposal has been implemented and is illustrated and validated with a simple example. Its use in grasping tasks in cluttered environments is currently under development.},
  file = {C:\Users\benja\Zotero\storage\5XHX72MZ\Gillani et al. - 2015 - Ontological Physics-based Motion Planning for Mani.pdf}
}

@article{gilpinExplainingExplanationsOverview2018,
  title = {Explaining {{Explanations}}: {{An Overview}} of {{Interpretability}} of {{Machine Learning}}},
  shorttitle = {Explaining {{Explanations}}},
  author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  year = {2018},
  month = oct,
  journal = {2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)},
  pages = {80--89},
  publisher = {IEEE},
  address = {Turin, Italy},
  doi = {10.1109/DSAA.2018.00018},
  urldate = {2024-03-03},
  abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
  isbn = {9781538650905}
}

@inproceedings{Give13,
  title = {Cloud {{Computing}} for {{Industrial Automation Systems}} : {{A Comprehensive Overview}}},
  booktitle = {2013 {{IEEE}} 18th {{Conference}} on {{Emerging Technologies Factory Automation}} ({{ETFA}})},
  author = {Givehchi, Omid and Trsek, Henning and Jasperneite, J{\"u}rgen},
  year = {2013},
  pages = {1--4}
}

@article{glimmHermiTOWL22014,
  title = {{{HermiT}}: {{An OWL}} 2 {{Reasoner}}},
  shorttitle = {{{HermiT}}},
  author = {Glimm, Birte and Horrocks, Ian and Motik, Boris and Stoilos, Giorgos and Wang, Zhe},
  year = {2014},
  month = oct,
  journal = {Journal of Automated Reasoning},
  volume = {53},
  number = {3},
  pages = {245--269},
  issn = {1573-0670},
  doi = {10.1007/s10817-014-9305-1},
  urldate = {2019-08-13},
  abstract = {This system description paper introduces the OWL 2 reasoner HermiT. The reasoner is fully compliant with the OWL 2 Direct Semantics as standardised by the World Wide Web Consortium (W3C). HermiT is based on the hypertableau calculus, and it supports a wide range of standard and novel optimisations that improve the performance of reasoning on real-world ontologies. Apart from the standard OWL 2 reasoning task of entailment checking, HermiT supports several specialised reasoning services such as class and property classification, as well as a range of features outside the OWL 2 standard such as DL-safe rules, SPARQL queries, and description graphs. We discuss the system's architecture, and we present an overview of the techniques used to support the mentioned reasoning tasks. We further compare the performance of reasoning in HermiT with that of FaCT++ and Pellet---two other popular and widely used OWL 2 reasoners.},
  langid = {english},
  keywords = {Class classification,Ontologies,Optimisations,OWL,Property classification}
}

@article{glimmHermiTOWL22014a,
  title = {{{HermiT}}: {{An OWL}} 2 {{Reasoner}}},
  shorttitle = {{{HermiT}}},
  author = {Glimm, Birte and Horrocks, Ian and Motik, Boris and Stoilos, Giorgos and Wang, Zhe},
  year = {2014},
  month = oct,
  journal = {Journal of Automated Reasoning},
  volume = {53},
  number = {3},
  pages = {245--269},
  issn = {0168-7433},
  doi = {10.1007/s10817-014-9305-1},
  urldate = {2023-01-31},
  abstract = {This system description paper introduces the OWL 2 reasoner HermiT. The reasoner is fully compliant with the OWL 2 Direct Semantics as standardised by the World Wide Web Consortium (W3C). HermiT is based on the hypertableau calculus, and it supports a wide range of standard and novel optimisations that improve the performance of reasoning on real-world ontologies. Apart from the standard OWL 2 reasoning task of entailment checking, HermiT supports several specialised reasoning services such as class and property classification, as well as a range of features outside the OWL 2 standard such as DL-safe rules, SPARQL queries, and description graphs. We discuss the system's architecture, and we present an overview of the techniques used to support the mentioned reasoning tasks. We further compare the performance of reasoning in HermiT with that of FaCT++ and Pellet--two other popular and widely used OWL 2 reasoners.},
  keywords = {Class classification,Ontologies,Optimisations,OWL,Property classification}
}

@book{gluckInteractiveTaskLearning2019,
  title = {Interactive {{Task Learning}}: {{Humans}}, {{Robots}}, and {{Agents Acquiring New Tasks}} through {{Natural Interactions}}},
  shorttitle = {Interactive {{Task Learning}}},
  author = {Gluck, Kevin A. and Laird, John E. and Lupp, Julia},
  year = {2019},
  month = sep,
  publisher = {MIT Press},
  address = {Cambridge, MA},
  abstract = {null},
  isbn = {978-0-262-03882-9},
  langid = {english}
}

@article{goldenbergCompleteGeneralizedSolution1985,
  title = {A Complete Generalized Solution to the Inverse Kinematics of Robots},
  author = {Goldenberg, A. and Benhabib, B. and Fenton, R.},
  year = {1985},
  month = mar,
  journal = {IEEE Journal on Robotics and Automation},
  volume = {1},
  number = {1},
  pages = {14--20},
  issn = {2374-8710},
  doi = {10.1109/JRA.1985.1086995},
  urldate = {2024-03-31},
  abstract = {The kinematic transformation between task space and joint configuration coordinates is nonlinear and configuration dependent. A solution to the inverse kinematics is a vector of joint configuration coordinates that corresponds to a set of task space coordinates. For a class of robots closed form solutions always exist, but constraints on joint displacements cannot be systematically incorporated in the process of obtaining a solution. An iterative solution is presented that is suitable for any class of robots having rotary or prismatic joints, with any arbitrary number of degrees of freedom, including both standard and kinematically redundant robots. The solution can be obtained subject to specified constraints and based on certain performance criteria. The solution is based on a new rapidly convergent constrained nonlinear optimization algorithm which uses a modified Newton-Raphson technique for solving a system nonlinear equations. The algorithm is illustrated using as an example a kinematically redundant robot.},
  keywords = {Closed-form solution,Constraint optimization,End effectors,Iterative methods,Manipulators,Nonlinear equations,Orbital robotics,Robot control,Robot kinematics,Service robots},
  file = {C:\Users\benja\Zotero\storage\ZDG2LJ9H\1086995.html}
}

@article{Goodale1992,
  title = {Separate Visual Pathways for Perception and Action},
  author = {Goodale, Melvyn A. and Milner, A.David},
  year = {1992},
  month = jan,
  journal = {Trends in Neurosciences},
  volume = {15},
  number = {1},
  pages = {20--25},
  publisher = {Elsevier Current Trends},
  issn = {0166-2236},
  doi = {10.1016/0166-2236(92)90344-8},
  urldate = {2018-07-04},
  abstract = {Accumulating neuropsychological, electrophysiological and behavioural evidence suggests that the neural substrates of visual perception may be quite distinct from those underlying the visual control of actions. In other words, the set of object descriptions that permit identification and recognition may be computed independently of the set of descriptions that allow an observer to shape the hand appropriately to pick up an object. We propose that the ventral stream of projections from the striate cortex to the inferotemporal cortex plays the major role in the perceptual identification of objects, while the dorsal stream projecting from the striate cortex to the posterior parietal region mediates the required sensorimotor transformations for visually guided actions directed at such objects.}
}

@misc{Goog17,
  title = {Google {{Machine Learning Services}}},
  year = {2017},
  publisher = {Alphabet Inc.},
  urldate = {2017-06-23}
}

@misc{googleBard2023,
  title = {{Bard}},
  author = {{Google}},
  year = {2023},
  urldate = {2024-01-05},
  abstract = {Hier erf{\"a}hrst du mehr {\"u}ber Bard, das KI-Tool von Google, mit dem du deine Ideen zum Leben erwecken kannst.},
  howpublished = {https://bard.google.com},
  langid = {ngerman},
  file = {C:\Users\benja\Zotero\storage\FBQ533W9\bard.google.com.html}
}

@misc{googlePythonClientLibrary2024,
  title = {Python Client Library},
  author = {{Google}},
  year = {2024},
  month = jul,
  urldate = {2024-07-25},
  howpublished = {Google LLC},
  file = {C:\Users\benja\Zotero\storage\5HMIGZ7Z\latest.html}
}

@inproceedings{gootjesApplyingFeedbackControl2017,
  title = {Applying Feedback Control to Improve {{3D}} Printing Quality},
  author = {Gootjes, D.},
  year = {2017},
  urldate = {2024-04-03},
  abstract = {Small-scale 3D-printing has become a common appearance. Today there are many companies specialized in selling and designing small-scale 3D-printers . Applications range from creating custom-made toys to creating spare parts for the 3D-printer itself as well as medical applications such as skull parts, knee replacements and hip replacements. The research and development start-up company HB{\textbar}3D uses a 6 degrees-of-freedom robotic arm combined with a plasticating single-screw extruder and heated hose to create large-scale thermoplast products. One challenge is to scale up the 3D-printing process while ensuring a high-quality product. Feedback control looks promising in increasing the dimensional accuracy of large-scale printed objects by influencing the screw speed to minimize flow disruptions. The goal is to create a simulation of a model predictive controller and show that an output flow trajectory can be tracked within 1 percent accuracy. The flow response is modeled using a commercially available computational fluid dynamics software package (ANSYS FLUENT Academic Research version 18.1). This computationally intensive model is simplified using system identification techniques; resulting in a wiener model. Finally, a simulation of using a model predictive controller to track an output flow reference signal is then performed.}
}

@article{gorostizaEnduserProgrammingSocial2011,
  title = {End-User Programming of a Social Robot by Dialog},
  author = {Gorostiza, Javi F. and Salichs, Miguel A.},
  year = {2011},
  month = dec,
  journal = {Robotics and Autonomous Systems},
  volume = {59},
  number = {12},
  pages = {1102--1114},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2011.07.009},
  urldate = {2024-08-15},
  abstract = {One of the main challenges faced by social robots is how to provide intuitive, natural and enjoyable usability for the end-user. In our ordinary environment, social robots could be important tools for education and entertainment (edutainment) in a variety of ways. This paper presents a Natural Programming System (NPS) that is geared to non-expert users. The main goal of such a system is to provide an enjoyable interactive platform for the users to build different programs within their social robot platform. The end-user can build a complex net of actions and conditions (a sequence) in a social robot via mixed-initiative dialogs and multimodal interaction. The system has been implemented and tested in Maggie, a real social robot with multiple skills, conceived as a general HRI researching platform. The robot's internal features (skills) have been implemented to be verbally accessible to the end-user, who can combine them into others that are more complex following a bottom-up model. The built sequence is internally implemented as a Sequence Function Chart (SFC), which allows parallel execution, modularity and re-use. A multimodal Dialog Manager System (DMS) takes charge of keeping the coherence of the interaction. This work is thought for bringing social robots closer to non-expert users, who can play the game of ``teaching how to do things'' with the robot.},
  keywords = {Dialog manager system,Human-robot dialogs,Instruction-based learning,Natural programming,Petri nets,Semantic grammars,Sequence function charts,Social robotics}
}

@article{goyalInductiveBiasesDeep2021,
  title = {Inductive {{Biases}} for {{Deep Learning}} of {{Higher-Level Cognition}}},
  author = {Goyal, Anirudh and Bengio, Yoshua},
  year = {2021},
  month = feb,
  journal = {arXiv:2011.15091 [cs, stat]},
  eprint = {2011.15091},
  primaryclass = {cs, stat},
  urldate = {2021-06-21},
  abstract = {A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behavior of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans' abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\5EUZVQAG\2011.html}
}

@incollection{grafRoPHaRobusteWahrnehmungsfahigkeiten2020,
  title = {{{RoPHa}} - {{Robuste Wahrnehmungsf{\"a}higkeiten}} F{\"u}r {{Roboter}} Zur {{Unterst{\"u}tzung}} {\"A}lterer {{Nutzer}} Im H{\"a}uslichen {{Umfeld}}},
  booktitle = {Autonome {{Roboter}} F{\"u}r {{Assistenz}}\-funktionen},
  author = {Graf, Birgit and Jordan, Florian and Blume, Gabriele and Martin, Ronny and {Abdel-Keream}, Mona and Mania, Patrick and Gronbach, Fabian and Emmerich, Christian and Schaller, Raphael and Katic, Darko and Alt, Benjamin},
  year = {2020},
  pages = {118--133},
  publisher = {Bundesanstalt f{\"u}r Arbeitsschutz und Arbeitsmedizin},
  address = {Dortmund},
  abstract = {Ziel des RoPHa-Projekts war es, Grundfunktionen f{\"u}r die Unterst{\"u}tzung {\"a}lterer und pfleged{\"u}rftiger Menschen bei der Handhabung typischer Alltagsobjekte zu entwickeln. Als Use Case f{\"u}r die beispielhafte Umsetzung und Integration der entwickelten Grundfunktionen wurde die mundgerechte Bereitstellung von Nahrung betrachtet. Daf{\"u}r wurden zun{\"a}chst eine systematische Anforderungsanalyse inklusive der Betrachtung von ELS-Aspekten und eine Risikobetrachtung durchgef{\"u}hrt. Danach wurde ein Interaktionskonzept basierend auf einer tabletgest{\"u}tzten GUI und Sprachausgaben entwickelt. Im Bereich der Perzeption wurden Funktionen f{\"u}r die Objekterkennung, die Lokalisierung und Klassifizierung von Speisen, die Wahrnehmung des Menschen und f{\"u}r die aufgabenspezifische Auswahl und Parametrierung von Perzeptionsfunktionen entwickelt und in ein Gesamtsystem integriert. F{\"u}r die sichere Manipulation wurden unterschiedliche elementare Handhabungsfertigkeiten, bspw. f{\"u}r das Schneiden, Bestreuen, Aufnehmen und Anreichen von Speisen entwickelt. Im Rahmen von zwei Testreihen mit Mitarbeitern aus der Pflege wurden die entwickelten Szenarien evaluiert.},
  copyright = {All rights reserved},
  keywords = {my},
  file = {C:\Users\benja\Zotero\storage\DTA2TDQH\servlet.html}
}

@article{gravesAssociativeCompressionNetworks2018,
  title = {Associative {{Compression Networks}} for {{Representation Learning}}},
  author = {Graves, Alex and Menick, Jacob and van den Oord, Aaron},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.02476 [cs, stat]},
  eprint = {1804.02476},
  primaryclass = {cs, stat},
  urldate = {2019-12-04},
  abstract = {This paper introduces Associative Compression Networks (ACNs), a new framework for variational autoencoding with neural networks. The system differs from existing variational autoencoders (VAEs) in that the prior distribution used to model each code is conditioned on a similar code from the dataset. In compression terms this equates to sequentially transmitting the dataset using an ordering determined by proximity in latent space. Since the prior need only account for local, rather than global variations in the latent space, the coding cost is greatly reduced, leading to rich, informative codes. Crucially, the codes remain informative when powerful, autoregressive decoders are used, which we argue is fundamentally difficult with normal VAEs. Experimental results on MNIST, CIFAR-10, ImageNet and CelebA show that ACNs discover high-level latent features such as object class, writing style, pose and facial expression, which can be used to cluster and classify the data, as well as to generate diverse and convincing samples. We conclude that ACNs are a promising new direction for representation learning: one that steps away from IID modelling, and towards learning a structured description of the dataset as a whole.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{gravesGeneratingSequencesRecurrent2014,
  title = {Generating {{Sequences With Recurrent Neural Networks}}},
  author = {Graves, Alex},
  year = {2014},
  month = jun,
  journal = {arXiv:1308.0850 [cs]},
  eprint = {1308.0850},
  primaryclass = {cs},
  urldate = {2019-11-22},
  abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing}
}

@article{gravesHybridComputingUsing2016,
  title = {Hybrid Computing Using a Neural Network with Dynamic External Memory},
  author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and {Grabska-Barwi{\'n}ska}, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri{\`a} Puigdom{\`e}nech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
  year = {2016},
  month = oct,
  journal = {Nature},
  volume = {538},
  number = {7626},
  pages = {471--476},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature20101},
  urldate = {2019-07-11},
  langid = {english}
}

@article{gravesNeuralTuringMachines2014,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  month = oct,
  journal = {arXiv:1410.5401 [cs]},
  eprint = {1410.5401},
  primaryclass = {cs},
  urldate = {2019-07-11},
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{gravesSpeechRecognitionDeep2013,
  title = {Speech Recognition with Deep Recurrent Neural Networks},
  booktitle = {2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Graves, A. and Mohamed, A. and Hinton, G.},
  year = {2013},
  month = may,
  pages = {6645--6649},
  doi = {10.1109/ICASSP.2013.6638947},
  abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
  keywords = {Acoustics,connectionist temporal classification,deep neural networks,deep recurrent neural networks,end-to-end training methods,long short-term memory RNN architecture,Noise,recurrent neural networks,Recurrent neural networks,sequential data,speech recognition,Speech recognition,Training,Vectors}
}

@article{grayExplicitImplicitModels2022,
  title = {Explicit versus Implicit Models: {{What}} Are Good Languages for Modeling?},
  shorttitle = {Explicit versus Implicit Models},
  author = {Gray, Jeff and Rumpe, Bernhard},
  year = {2022},
  month = jun,
  journal = {Software and Systems Modeling},
  volume = {21},
  number = {3},
  pages = {839--841},
  issn = {1619-1374},
  doi = {10.1007/s10270-022-01001-4},
  urldate = {2024-04-17},
  langid = {english}
}

@article{greenHumanRobotCollaborationLiterature2008,
  title = {Human-{{Robot Collaboration}}: {{A Literature Review}} and {{Augmented Reality Approach}} in {{Design}}},
  shorttitle = {Human-{{Robot Collaboration}}},
  author = {Green, Scott A. and Billinghurst, Mark and Chen, XiaoQi and Chase, J. Geoffrey},
  year = {2008},
  month = mar,
  journal = {International Journal of Advanced Robotic Systems},
  volume = {5},
  number = {1},
  pages = {1},
  publisher = {SAGE Publications},
  issn = {1729-8814},
  doi = {10.5772/5664},
  urldate = {2020-10-07},
  abstract = {NASA's vision for space exploration stresses the cultivation of human-robotic systems. Similar systems are also envisaged for a variety of hazardous earthbound applications such as urban search and rescue. Recent research has pointed out that to reduce human workload, costs, fatigue driven error and risk, intelligent robotic systems will need to be a significant part of mission design. However, little attention has been paid to joint human-robot teams. Making human-robot collaboration natural and efficient is crucial. In particular, grounding, situational awareness, a common frame of reference and spatial referencing are vital in effective communication and collaboration. Augmented Reality (AR), the overlaying of computer graphics onto the real worldview, can provide the necessary means for a human-robotic system to fulfill these requirements for effective collaboration. This article reviews the field of human-robot interaction and augmented reality, investigates the potential avenues for creating natural human-robot collaboration through spatial dialogue utilizing AR and proposes a holistic architectural design for human-robot collaboration.},
  langid = {english}
}

@article{gronsundAugmentingAlgorithmEmerging2020,
  title = {Augmenting the Algorithm: {{Emerging}} Human-in-the-Loop Work Configurations},
  shorttitle = {Augmenting the Algorithm},
  author = {Gr{\o}nsund, Tor and Aanestad, Margunn},
  year = {2020},
  month = jun,
  journal = {The Journal of Strategic Information Systems},
  series = {Strategic {{Perspectives}} on {{Digital Work}} and {{Organizational Transformation}}},
  volume = {29},
  number = {2},
  pages = {101614},
  issn = {0963-8687},
  doi = {10.1016/j.jsis.2020.101614},
  urldate = {2024-09-21},
  abstract = {How do configurations of humans and algorithms evolve as firms adopt artificial intelligence (AI) capabilities, and what are the implications for work and organization? We explored these questions through a two-year long case study of an organization in the international maritime trade that introduced automated algorithmic support for data analysis and prediction work. Drawing on a human--machine configuration perspective, we found that humans and the algorithm were configured and reconfigured in multiple ways over time as the organization dealt with the introduction of algorithmic analysis. In contrast to replacing human work, the emergent configurations required new roles and redistribution of extant expertise to augment and improve the accuracy of the algorithm. Our analysis suggests that the new configuration resembled a human-in-the-loop pattern, comprised of both the augmentation work of auditing (i.e. the generation of a ground truth and assessment of the algorithmic output against this) as well as the work of altering the algorithm and the data acquisition architecture. Our research points to the strategic importance of a human-in-the-loop pattern for organizational reflexivity to ensure that the performance of the algorithm meets the organization's requirements and changes in the environment.},
  keywords = {Artificial intelligence (AI),Augmentation,Automation,Digital work,Human-in-the-loop,Human-machine configurations},
  file = {C\:\\Users\\benja\\Zotero\\storage\\TWQAI88E\\Grnsund und Aanestad - 2020 - Augmenting the algorithm Emerging human-in-the-loop work configurations.pdf;C\:\\Users\\benja\\Zotero\\storage\\ZY72UPSD\\S0963868720300226.html}
}

@article{groszCollaborativePlansComplex1996,
  title = {Collaborative Plans for Complex Group Action},
  author = {Grosz, Barbara J. and Kraus, Sarit},
  year = {1996},
  month = oct,
  journal = {Artificial Intelligence},
  volume = {86},
  number = {2},
  pages = {269--357},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(95)00103-4},
  urldate = {2020-10-07},
  abstract = {The original formulation of SharedPlans by B. Grosz and C. Sidner (1990) was developed to provide a model of collaborative planning in which it was not necessary for one agent to have intentions-to toward an act of a different agent. Unlike other contemporaneous approaches (J.R. Searle, 1990), this formulation provided for two agents to coordinate their activities without introducing any notion of irreducible joint intentions. However, it only treated activities that directly decomposed into single-agent actions, did not address the need for agents to commit to their joint activity, and did not adequately deal with agents having only partial knowledge of the way in which to perform an action. This paper provides a revised and expanded version of SharedPlans that addresses these shortcomings. It also reformulates Pollack's (1990) definition of individual plans to handle cases in which a single agent has only partial knowledge; this reformulation meshes with the definition of SharedPlans. The new definitions also allow for contracting out certain actions. The formalization that results has the features required by Bratman's (1992) account of shared cooperative activity and is more general than alternative accounts (H. Levesque et al., 1990; E. Sonenberg et al., 1992).},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\CB8I4Q3J\0004370295001034.html}
}

@inproceedings{grothGoalConditionedEndtoEndVisuomotor2021,
  title = {Goal-{{Conditioned End-to-End Visuomotor Control}} for {{Versatile Skill Primitives}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Groth, Oliver and Hung, Chia-Man and Vedaldi, Andrea and Posner, Ingmar},
  year = {2021},
  month = may,
  pages = {1319--1325},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9560752},
  urldate = {2024-04-28},
  abstract = {Visuomotor control (VMC) is an effective means of achieving basic manipulation tasks such as pushing or pick- and-place from raw images. Conditioning VMC on desired goal states is a promising way of achieving versatile skill primitives. However, common conditioning schemes either rely on task-specific fine tuning - e.g. using one-shot imitation learning (IL) - or on sampling approaches using a forward model of scene dynamics i.e. model-predictive control (MPC), leaving deployability and planning horizon severely limited. In this paper we propose a conditioning scheme which avoids these pitfalls by learning the controller and its conditioning in an end-to-end manner. Our model predicts complex action sequences based directly on a dynamic image representation of the robot motion and the distance to a given target observation. In contrast to related works, this enables our approach to efficiently perform complex manipulation tasks from raw image observations without predefined control primitives or test time demonstrations. We report significant improvements in task success over representative MPC and IL baselines. We also demonstrate our model's generalisation capabilities in challenging, unseen tasks featuring visual noise, cluttered scenes and unseen object geometries.},
  keywords = {Dynamics,Geometry,Image representation,Planning,Predictive models,Robot motion,Visualization},
  file = {C:\Users\benja\Zotero\storage\L84T859F\9560752.html}
}

@misc{GrundlagenRobotikExterner2020,
  title = {Grundlagen Der {{Robotik}}: {{Externer Automatikbetrieb}}},
  year = {2020},
  month = may,
  urldate = {2020-05-12},
  howpublished = {https://www.xplore-dna.net/mod/page/view.php?id=1153},
  file = {C:\Users\benja\Zotero\storage\H88AWNMD\view.html}
}

@article{Gruninga,
  title = {Spiking {{Neural Networks}}: {{Principles}} and {{Challenges}}},
  author = {Gr{\"u}ning, Andr{\'e} and Bohte, Sander M},
  urldate = {2018-05-10},
  abstract = {Over the last decade, various spiking neural network models have been proposed, along with a similarly increasing interest in spiking models of computation in computational neuroscience. The aim of this tutorial paper is to outline some of the common ground in state-of-the-art spiking neural networks as well as open challenges.}
}

@book{GruyterHandbookArtificial2024,
  title = {The {{De Gruyter Handbook}} of {{Artificial Intelligence}}, {{Identity}} and {{Technology Studies}}},
  year = {2024},
  month = jul,
  publisher = {De Gruyter},
  doi = {10.1515/9783110721751},
  urldate = {2024-07-22},
  abstract = {The De Gruyter Handbook of Artificial Intelligence, Identity and Technology Studies examines the relationship of the social sciences to artificial intelligence, surveying the various convergences and divergences between science and technology studies on the one hand and identity transformations on the other. It provides representative coverage of all aspects of the AI revolution, from employment to education to military warfare, impacts on public policy and governance and the future of ethics. How is AI currently transforming social, economic, cultural and psychological processes? This handbook answers these questions by looking at recent developments in supercomputing, deep learning and neural networks, including such topics as AI mobile technology, social robotics, big data and digital research. It focuses especially on mechanisms of identity by defining AI as a new context for self-exploration and social relations and analyzing phenomena such as race, ethnicity and gender politics in human-machine interfaces.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  isbn = {978-3-11-072175-1},
  langid = {english},
  keywords = {Kunstliche Intelligenz,Mensch-Maschine,Soziale Robotik,Steuerung und Ethik Kunstlicher Intelligenz,Technologie}
}

@inproceedings{grzeszczukNeuroAnimatorFastNeural1998,
  title = {{{NeuroAnimator}}: Fast Neural Network Emulation and Control of Physics-Based Models},
  shorttitle = {{{NeuroAnimator}}},
  booktitle = {Proceedings of the 25th Annual Conference on {{Computer}} Graphics and Interactive Techniques},
  author = {Grzeszczuk, Radek and Terzopoulos, Demetri and Hinton, Geoffrey},
  year = {1998},
  month = jul,
  series = {{{SIGGRAPH}} '98},
  pages = {9--20},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/280814.280816},
  urldate = {2022-03-26},
  isbn = {978-0-89791-999-9},
  keywords = {backpropagation,dynamical systems,learning,motion control,neural networks,physics-based animation,simulation}
}

@article{guDeepReinforcementLearning2016,
  title = {Deep {{Reinforcement Learning}} for {{Robotic Manipulation}} with {{Asynchronous Off-Policy Updates}}},
  author = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  year = {2016},
  month = oct,
  journal = {arXiv:1610.00633 [cs]},
  eprint = {1610.00633},
  primaryclass = {cs},
  urldate = {2019-05-17},
  abstract = {Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\G34KGIUR\1610.html}
}

@article{Guennebaud.2010,
  title = {Eigen V3},
  author = {Guennebaud, Ga{\"e}l and Jacob, Beno{\^i}t and others},
  year = {2010},
  lastvisited = {2019-07-22}
}

@article{gugliermoLearningBehaviorTrees2023,
  title = {Learning {{Behavior Trees From Planning Experts Using Decision Tree}} and {{Logic Factorization}}},
  author = {Gugliermo, Simona and Schaffernicht, Erik and Koniaris, Christos and Pecora, Federico},
  year = {2023},
  month = jun,
  journal = {IEEE Robotics and Automation Letters},
  volume = {8},
  number = {6},
  pages = {3534--3541},
  issn = {2377-3766},
  doi = {10.1109/LRA.2023.3268598},
  urldate = {2024-04-25},
  abstract = {The increased popularity of Behavior Trees (BTs) in different fields of robotics requires efficient methods for learning BTs from data instead of tediously handcrafting them. Recent research in learning from demonstration reported encouraging results that this letter extends, improves and generalizes to arbitrary planning domains. We propose BT-Factor as a new method for learning expert knowledge by representing it in a BT. Execution traces of previously manually designed plans are used to generate a BT employing a combination of decision tree learning and logic factorization techniques originating from circuit design. We test BT-Factor in an industrially-relevant simulation environment from a mining scenario and compare it against a state-of-the-art BT learning method. The results show that our method generates compact BTs easy to interpret, and capable to capture accurately the relations that are implicit in the training data.},
  keywords = {Batteries,Behavior-based systems,Behavioral sciences,Circuit synthesis,Decision trees,intelligent transportation systems,learning from demonstration,Partitioning algorithms,Planning,Task analysis}
}

@article{gunjalDetectingPreventingHallucinations2024,
  title = {Detecting and {{Preventing Hallucinations}} in {{Large Vision Language Models}}},
  author = {Gunjal, Anisha and Yin, Jihan and Bas, Erhan},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {16},
  pages = {18135--18143},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i16.29771},
  urldate = {2024-06-16},
  abstract = {Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a Multimodal Hallucination Detection Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling (RS). We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41\% and 55\% respectively. We also find that our reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15\% and 57\% respectively, and has strong correlation with human evaluated accuracy scores. The dataset is available at https://github.com/hendryx-scale/mhal-detect.},
  copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {NLP: Safety and Robustness}
}

@article{gunningDARPAsExplainableArtificial2019,
  title = {{{DARPA}}'s {{Explainable Artificial Intelligence}} ({{XAI}}) {{Program}}},
  author = {Gunning, David and Aha, David},
  year = {2019},
  month = jun,
  journal = {AI Magazine},
  volume = {40},
  number = {2},
  pages = {44--58},
  issn = {2371-9621},
  doi = {10.1609/aimag.v40i2.2850},
  urldate = {2024-02-04},
  abstract = {Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA's explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems' explanations improve user understanding, user trust, and user task performance.},
  copyright = {Copyright (c) 2019 AI Magazine},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\8VLX4QGL\Gunning and Aha - 2019 - DARPAs Explainable Artificial Intelligence (XAI) .pdf}
}

@article{Guo.2016,
  title = {A Comprehensive Performance Evaluation of {{3D}} Local Feature Descriptors},
  author = {Guo, Yulan and Bennamoun, Mohammed and Sohel, Ferdous and Lu, Min and Wan, Jianwei and Kwok, Ngai Ming},
  year = {2016},
  journal = {International Journal of Computer Vision},
  volume = {116},
  number = {1},
  pages = {66--89},
  issn = {0920-5691},
  doi = {10.1007/s11263-015-0824-y},
  pagination = {page}
}

@article{Guo.2017,
  title = {Real-Time Geometry, Albedo, and Motion Reconstruction Using a Single {{RGB-D}} Camera},
  author = {Guo, Kaiwen and Xu, Feng and Yu, Tao and Liu, Xiaoyang and Dai, Qionghai and Liu, Yebin},
  year = {2017},
  journal = {ACM Transactions on Graphics},
  volume = {36},
  number = {3},
  pages = {1--13},
  issn = {07300301},
  doi = {10.1145/3083722},
  pagination = {page}
}

@inproceedings{guoCalibrationModernNeural2017,
  title = {On {{Calibration}} of {{Modern Neural Networks}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  year = {2017},
  month = jul,
  pages = {1321--1330},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2021-05-18},
  abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover...},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\ZGFP5UGS\guo17a.html}
}

@article{guoRecentTrendsTask2023,
  title = {Recent {{Trends}} in {{Task}} and {{Motion Planning}} for {{Robotics}}: {{A Survey}}},
  shorttitle = {Recent {{Trends}} in {{Task}} and {{Motion Planning}} for {{Robotics}}},
  author = {Guo, Huihui and Wu, Fan and Qin, Yunchuan and Li, Ruihui and Li, Keqin and Li, Kenli},
  year = {2023},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {13s},
  pages = {289:1--289:36},
  issn = {0360-0300},
  doi = {10.1145/3583136},
  urldate = {2024-04-25},
  abstract = {Autonomous robots are increasingly served in real-world unstructured human environments with complex long-horizon tasks, such as restaurant serving and office delivery. Task and motion planning (TAMP) is a recent research method in Artificial Intelligence Planning for these applications. TAMP integrates high-level abstract reasoning with the low-level geometric feasibility check and thus is more comprehensive than traditional task planning methods. While regular TAMP approaches are challenged by different types of uncertainties and the generalization of various applications when implemented in real-world scenarios. This article systematically reviews the most relevant approaches to TAMP and classifies them according to their features and emphasis; it categorizes the challenges and presents online TAMP and machine learning-based TAMP approaches for addressing them.},
  keywords = {learning for planning,online planning,Task and motion planning}
}

@inproceedings{guptaRelayPolicyLearning2020,
  title = {Relay {{Policy Learning}}: {{Solving Long-Horizon Tasks}} via {{Imitation}} and {{Reinforcement Learning}}},
  shorttitle = {Relay {{Policy Learning}}},
  booktitle = {Proceedings of the {{Conference}} on {{Robot Learning}}},
  author = {Gupta, Abhishek and Kumar, Vikash and Lynch, Corey and Levine, Sergey and Hausman, Karol},
  year = {2020},
  month = may,
  pages = {1025--1037},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-09-29},
  abstract = {We present relay policy learning, a method for imitation and reinforcement learning that can solve multi-stage, long-horizon robotic tasks. This general and universally-applicable, two-phase approach consists of an imitation learning stage resulting in goal-conditioned hierarchical policies that can be easily improved using fine-tuning via reinforcement learning in the subsequent phase. Our method, while not necessarily perfect at imitation learning, is very amenable to further improvement via environment interaction allowing it to scale to challenging long-horizon tasks. In particular, we simplify the long-horizon policy learning problem by using a novel data-relabeling algorithm for learning goal-conditioned hierarchical policies, where the low-level only acts for a fixed number of steps, regardless of the goal achieved. While we rely on demonstration data to bootstrap policy learning, we do not assume access to demonstrations of specific tasks. Instead, our approach can leverage unstructured and unsegmented demonstrations of semantically meaningful behaviors that are not only less burdensome to provide, but also can greatly facilitate further improvement using reinforcement learning. We demonstrate the effectiveness of our method on a number of multi-stage, long-horizon manipulation tasks in a challenging kitchen simulation environment.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\BDK962X6\Gupta et al. - 2020 - Relay Policy Learning Solving Long-Horizon Tasks via Imitation and Reinforcement Learning.pdf}
}

@article{gurdurbrooRethinkingEngineeringEducation2022,
  title = {Rethinking {{Engineering Education}} at the {{Age}} of {{Industry}} 5.0},
  author = {G{\"u}rd{\"u}r Broo, Didem and Kaynak, Okyay and Sait, Sadiq M.},
  year = {2022},
  month = jan,
  journal = {Journal of Industrial Information Integration},
  volume = {25},
  pages = {100311},
  issn = {2452-414X},
  doi = {10.1016/j.jii.2021.100311},
  urldate = {2024-01-26},
  abstract = {During the last two decades, profound technological changes have taken place around us, supported by disruptive advances, both on the software and hardware sides. An amalgamation of information, communication, and artificial intelligence is taking place, as well as the cross-fertilization of a wide range of concepts, referred to as the digital transformation. While the discussion on how to operationalize the new intelligent systems of the fourth industrial revolution, Industry 4.0, is still going on; the dominant characteristics of the fifth industrial revolution, Industry 5.0 -- going beyond producing goods and services for profit -- requires all to think and act differently. As a result of the convergence phenomenon, the boundaries between different disciplines are eroding, necessitating a thorough discussion on what engineering education should be like in the future. In this paper, after presenting a brief history of engineering education, the recent paradigm changes are discussed, which essentially stress that skills must prevail over degrees to deal with challenges posed by the trends of the fifth industrial revolution. Later, before concluding the paper four strategies are presented such as lifelong learning and transdisciplinary education (1), sustainability, resilience, and human-centric design modules (2), hands-on data fluency and management courses (3) and human-agent/machine/robot/computer interaction experiences (4).},
  keywords = {21st century skills,Digital Transformation,Engineering education,Higher education,Industry 5.0},
  file = {C:\Users\benja\Zotero\storage\4D5ELLEM\S2452414X21001059.html}
}

@inproceedings{gurumurthyMAMEModelAgnosticMetaExploration2020,
  title = {{{MAME}} : {{Model-Agnostic Meta-Exploration}}},
  shorttitle = {{{MAME}}},
  booktitle = {Proceedings of the {{Conference}} on {{Robot Learning}}},
  author = {Gurumurthy, Swaminathan and Kumar, Sumit and Sycara, Katia},
  year = {2020},
  month = may,
  pages = {910--922},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-15},
  abstract = {Meta-Reinforcement learning approaches aim to develop learning procedures that can adapt quickly to a distribution of tasks with the help of a few examples. Developing efficient exploration strategies capable of finding the most useful samples becomes critical in such settings. Existing approaches towards finding efficient exploration strategies add auxiliary objectives to promote exploration by the pre-update policy, however, this makes the adaptation using a few gradient steps difficult as the pre-update (exploration) and post-update (exploitation) policies are often quite different. Instead, we propose to explicitly model a separate exploration policy for the task distribution. Having two different policies gives more flexibility in training the exploration policy and also makes adaptation to any specific task easier. We show that using self-supervised or supervised learning objectives for adaptation allows for more efficient inner-loop updates and also demonstrate the superior performance of our model compared to prior works in this domain.},
  langid = {english}
}

@misc{guSurveyRoboticManipulation2023,
  title = {A {{Survey}} on {{Robotic Manipulation}} of {{Deformable Objects}}: {{Recent Advances}}, {{Open Challenges}} and {{New Frontiers}}},
  shorttitle = {A {{Survey}} on {{Robotic Manipulation}} of {{Deformable Objects}}},
  author = {Gu, Feida and Zhou, Yanmin and Wang, Zhipeng and Jiang, Shuo and He, Bin},
  year = {2023},
  month = dec,
  number = {arXiv:2312.10419},
  eprint = {2312.10419},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.10419},
  urldate = {2024-04-11},
  abstract = {Deformable object manipulation (DOM) for robots has a wide range of applications in various fields such as industrial, service and health care sectors. However, compared to manipulation of rigid objects, DOM poses significant challenges for robotic perception, modeling and manipulation, due to the infinite dimensionality of the state space of deformable objects (DOs) and the complexity of their dynamics. The development of computer graphics and machine learning has enabled novel techniques for DOM. These techniques, based on data-driven paradigms, can address some of the challenges that analytical approaches of DOM face. However, some existing reviews do not include all aspects of DOM, and some previous reviews do not summarize data-driven approaches adequately. In this article, we survey more than 150 relevant studies (data-driven approaches mainly) and summarize recent advances, open challenges, and new frontiers for aspects of perception, modeling and manipulation for DOs. Particularly, we summarize initial progress made by Large Language Models (LLMs) in robotic manipulation, and indicates some valuable directions for further research. We believe that integrating data-driven approaches and analytical approaches can provide viable solutions to open challenges of DOM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\TWEH4437\2312.html}
}

@patent{guVorrichtungEntgratenMit2016,
  title = {{Vorrichtung zum Entgraten mit visuellem Sensor und Kraftsensor}},
  author = {Gu, Yihua},
  year = {2016},
  month = dec,
  number = {DE102014108956B4},
  urldate = {2021-03-01},
  abstract = {Eine Entgratungsvorrichtung umfasst ein Entgratungswerkzeug zum Entfernen von Graten von einem Objekt, einen Roboter zum Bewegen des Objekts oder des Werkzeugs, einen Kraftsensor zum Erfassen der Kraft, die auf das Werkzeug wirkt, und einen visuellen Sensor, der die Position eines Gratabschnitts des Objekts erfasst. In der Entgratungsvorrichtung wird Information hinsichtlich der Formdaten des Gratabschnitts und der Lage des Werkzeugs vorab aus dreidimensionalen Daten des Objekts erhalten. Ausgehend von den Formdaten und der Lage des Werkzeugs wird ein Roboterprogramm erzeugt. Abh{\"a}ngig von einem tats{\"a}chlichen Gratabschnitt, den der visuelle Sensor erkennt, wird das Roboterprogramm nach Bedarf ersetzt. W{\"a}hrend des Entgratens wird der Roboter mit einer Kraftregelung gesteuert, in der der erfasste Wert des Kraftsensors verwendet wird.},
  assignee = {FANUC Corp},
  langid = {ngerman},
  nationality = {DE},
  keywords = {actual,burr,deburring,robot,tool}
}

@inproceedings{guzhvaMultifoldAccelerationNeural2009,
  title = {Multifold {{Acceleration}} of {{Neural Network Computations Using GPU}}},
  booktitle = {Artificial {{Neural Networks}} -- {{ICANN}} 2009},
  author = {Guzhva, Alexander and Dolenko, Sergey and Persiantsev, Igor},
  editor = {Alippi, Cesare and Polycarpou, Marios and Panayiotou, Christos and Ellinas, Georgios},
  year = {2009},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {373--380},
  publisher = {Springer Berlin Heidelberg},
  abstract = {With emergence of graphics processing units (GPU) of the latest generation, it became possible to undertake neural network based computations using GPU on serially produced video display adapters. In this study, NVIDIA CUDA technology has been used to implement standard back-propagation algorithm for training multiple perceptrons simultaneously on GPU. For the problem considered, GPU-based implementation (on NVIDIA GTX 260 GPU) has lead to a 50x speed increase compared to a highly optimized CPU-based computer program, and more than 150x compared to a commercially available CPU-based software (NeuroShell 2) (AMD Athlon 64 Dual core 6000+ processor).},
  isbn = {978-3-642-04274-4},
  langid = {english},
  keywords = {GPGPU,neural networks,NVIDIA CUDA,parallel computations,perceptron}
}

@article{haarnojaSoftActorCriticAlgorithms2018,
  title = {Soft {{Actor-Critic Algorithms}} and {{Applications}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.05905 [cs, stat]},
  eprint = {1812.05905},
  primaryclass = {cs, stat},
  urldate = {2019-05-17},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\AJPS2BTW\1812.html}
}

@article{hackenbergUsingDifferentiableProgramming2021,
  title = {Using {{Differentiable Programming}} for {{Flexible Statistical Modeling}}},
  author = {Hackenberg, Maren and Grodd, Marlon and Kreutz, Clemens and Fischer, Martina and Esins, Janina and Grabenhenrich, Linus and Karagiannidis, Christian and Binder, Harald},
  year = {2021},
  month = nov,
  journal = {The American Statistician},
  volume = {0},
  number = {0},
  pages = {1--10},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2021.2002189},
  urldate = {2022-03-26},
  abstract = {Differentiable programming has recently received much interest as a paradigm that facilitates taking gradients of computer programs. While the corresponding flexible gradient-based optimization approaches so far have been used predominantly for deep learning or enriching the latter with modeling components, we want to demonstrate that they can also be useful for statistical modeling per se, for example, for quick prototyping when classical maximum likelihood approaches are challenging or not feasible. In an application from a COVID-19 setting, we use differentiable programming to quickly build and optimize a flexible prediction model adapted to the data quality challenges at hand. Specifically, we develop a regression model, inspired by delay differential equations, that can bridge temporal gaps of observations in the central German registry of COVID-19 intensive care cases for predicting future demand. With this exemplary modeling challenge, we illustrate how differentiable programming can enable simple gradient-based optimization of the model by automatic differentiation. This allowed us to quickly prototype a model under time pressure that outperforms simpler benchmark models. We thus exemplify the potential of differentiable programming also outside deep learning applications to provide more options for flexible applied statistical modeling.},
  keywords = {Differential equations,Machine learning,Optimization,Workflow}
}

@article{haddadinFrankaEmikaRobot2022,
  title = {The {{Franka Emika Robot}}: {{A Reference Platform}} for {{Robotics Research}} and {{Education}}},
  shorttitle = {The {{Franka Emika Robot}}},
  author = {Haddadin, Sami and Parusel, Sven and Johannsmeier, Lars and Golz, Saskia and Gabl, Simon and Walch, Florian and Sabaghian, Mohamadreza and J{\"a}hne, Christoph and Hausperger, Lukas and Haddadin, Simon},
  year = {2022},
  month = jun,
  journal = {IEEE Robotics \& Automation Magazine},
  volume = {29},
  number = {2},
  pages = {46--64},
  issn = {1558-223X},
  doi = {10.1109/MRA.2021.3138382},
  urldate = {2024-09-18},
  abstract = {The importance of robots for industry, research, education, and society as a whole is steadily increasing as reflected by the number of available systems and installed robots, not only in industry but also in the public sector and households. Software-only robotics researchers usually rely on commercially available robots which, in the case of manipulators, are primarily designed for industrial purposes and are often far from their needs. This article is a hands-on tutorial on the Franka Emika robot, the first series of industrial artificial intelligence (AI)-ready tactile robot platforms. Beyond industrial use, the systems can be seamlessly expanded to fulfill the demands of research and education across all robotics and AI disciplines. To satisfy the needs of such a wide variety of fields, it provides three different interfaces: Desk, a high-level app-based user interface for easy and fast task programming; Robot Integrated Development Environment (RIDE), a command-based programming environment used to create high-performance robot skills that enables programming custom apps and integrating external sensors; and the Franka control interface (FCI), a 1-kHz low-level torque and position control interface that exploits the also-available Langrangian dynamics robot model. We take a close look at implementations with all interfaces, ranging from simple solutions, apps, and controllers to robot-learning examples illustrating how to exploit all the advantages of this platform in ongoing robotics research and education.},
  keywords = {Artificial intelligence,Education,Research and development,Robot kinematics,Robot sensing systems,Robots,Sensors,Service robots},
  file = {C\:\\Users\\benja\\Zotero\\storage\\D7BNDT8T\\Haddadin et al. - 2022 - The Franka Emika Robot A Reference Platform for Robotics Research and Education.pdf;C\:\\Users\\benja\\Zotero\\storage\\LUCRMSUC\\9721535.html}
}

@inproceedings{haDistillingHierarchicalPolicy2021,
  title = {Distilling a {{Hierarchical Policy}} for {{Planning}} and {{Control}} via {{Representation}} and {{Reinforcement Learning}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Ha, Jung-Su and Park, Young-Jin and Chae, Hyeok-Joo and Park, Soon-Seo and Choi, Han-Lim},
  year = {2021},
  month = may,
  pages = {4459--4466},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9561017},
  abstract = {We present a hierarchical planning and control framework that enables an agent to perform various tasks and adapt to a new task flexibly. Rather than learning an individual policy for each particular task, the proposed framework, DISH, distills a hierarchical policy from a set of tasks by representation and reinforcement learning. The framework is based on the idea of latent variable models that represent high-dimensional observations using low-dimensional latent variables. The resulting policy consists of two levels of hierarchy: (i) a planning module that reasons a sequence of latent intentions that would lead to an optimistic future and (ii) a feedback control policy, shared across the tasks, that executes the inferred intention. Because the planning is performed in low-dimensional latent space, the learned policy can immediately be used to solve or adapt to new tasks without additional training. We demonstrate the proposed framework can learn compact representations (3- and 1-dimensional latent states and commands for a humanoid with 197- and 36-dimensional state features and actions) while solving a small number of imitation tasks, and the resulting policy is directly applicable to other types of tasks, i.e., navigation in cluttered environments.},
  keywords = {Conferences,Data models,Humanoid robots,Navigation,Reinforcement learning,State feedback,Training},
  file = {C:\Users\benja\Zotero\storage\ZRLG2NS2\9561017.html}
}

@incollection{hageleIndustrialRobotics2016,
  title = {Industrial {{Robotics}}},
  booktitle = {Springer {{Handbook}} of {{Robotics}}},
  author = {H{\"a}gele, Martin and Nilsson, Klas and Pires, J. Roberto and Bischoff, Rainer},
  year = {2016},
  edition = {2},
  pages = {1385--1421},
  publisher = {Springer},
  address = {Berlin}
}

@article{haideggerAutonomySurgicalRobots2019,
  title = {Autonomy for {{Surgical Robots}}: {{Concepts}} and {{Paradigms}}},
  shorttitle = {Autonomy for {{Surgical Robots}}},
  author = {Haidegger, Tam{\'a}s},
  year = {2019},
  month = may,
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  volume = {1},
  number = {2},
  pages = {65--76},
  issn = {2576-3202},
  doi = {10.1109/TMRB.2019.2913282},
  urldate = {2024-04-12},
  abstract = {Robot-assisted and computer-integrated surgery provides innovative, minimally invasive solutions to heal complex injuries and diseases. The dominant portion of these surgical interventions has been performed with master-slave teleoperation systems, which are not capable of autonomous task execution or cognitive decision making. Much of the most advanced technologies foundered on the drawing boards or at the research labs for a long time, partially due to the fact that the surgical domain is resistant to the introduction of new hazards via the increased complexity of novel solutions. It has been seen with similar heavily regulated areas that internationally accepted standards can facilitate the adoption of new technologies in a safe manner. This paper reviews the existing autonomous capabilities of surgical robots, and investigates the major barriers of development presented by the lack of autonomy benchmarks and standards. The emerging safety standard environment is presented, as a key enabling factor to the commercialization of autonomous surgical robots. A practical scale is introduced to assess the level of autonomy of current and future surgical robots. Regarding the forthcoming robotic platforms, it is crucial to improve the transparency of the regulatory environment, streamline the standardization framework, and increase the social acceptance.},
  keywords = {Computer-integrated surgery,degree of autonomy,level of autonomy,Medical robotics,Robot kinematics,Robot sensing systems,robot standardization,robot-assisted minimally invasive surgery,Safety,Service robots,Surgery}
}

@inproceedings{haidExplainingAlgorithmicDecisions2023,
  title = {Explaining {{Algorithmic Decisions}}: {{Design Guidelines}} for {{Explanations}} in {{User Interfaces}}},
  shorttitle = {Explaining Algorithmic Decisions},
  booktitle = {Human {{Factors}} in {{Software}} and {{Systems Engineering}}},
  author = {Haid, Charlotte and Lang, Alicia and Fottner, Johannes},
  year = {2023},
  volume = {94},
  publisher = {AHFE Open Acces},
  issn = {27710718},
  doi = {10.54941/ahfe1003764},
  urldate = {2024-02-04},
  abstract = {Artificial Intelligence (AI)-based decision support is becoming a growing issue in manufacturing and logistics. Users of AI-based systems have the claim to understand  the decisions made by the systems. In addition, users like workers or managers, but also works councils in companies, demand transparency in the use of AI. Given this background, AI research faces the challenge of making the decisions of algorithmic systems explainable. Algorithms, especially in the field of AI, but also classical algorithms do not provide an explanation for their decision. To generate such explanations, new algorithms have been designed to explain the decisions of the other algorithms post hoc. This subfield is called explainable artificial intelligence (XAI). Methods like local interpretable model-agnostic explanations (LIME), shapley additive explanations (SHAP) or layer-wise relevance propagation (LRP) can be applied. LIME is an algorithm that can explain the predictions of any classifier by learning an interpretable model around the prediction locally. In the case of image recognition, for example, a LIME algorithm can highlight the image areas based on which the algorithm arrived at its decision. They even show that the algorithm can also come to a result based on the image caption. SHAP, a game theoretic approach that can be applied to the output of any machine learning model, connects optimal credit allocation with local explanations. It uses Shapley values as in game theory for the allocation. In the research of XAI, explanatory user interfaces and user interactions have hardly been studied. One of the most crucial factors to make a model understandable through explanations is the involvement of users in XAI. Human-computer interaction skills are needed in addition to technical expertise. According to Miller and Molnar, good explanations should be designed contrastively to explain why event A happened instead of another event B, rather than just emphasizing why event A occurred. In addition, it is important that explanations are limited to only one or two causes and are thus formulated selectively. In literature, four guidelines to be respected for explanations are formulated: use a natural language, use various methods to explain, adapt to mental models of users and be responsive, so a user can ask follow-up questions. The explanations are often very mathematical and a deep knowledge of details is needed to understand the explanations. In this paper, we present design guidelines to help make explanations of algorithms understandable and user-friendly. We use the example of AI-based algorithmic scheduling in logistics and show the importance of a comprehensive user interface in explaining decisions. In our use case, AI-based shift scheduling in logistics, where workers are assigned to workplaces based on their preferences, we designed a user interface to support transparency as well as explainability of the underlying algorithm and then evaluated it with various users and two different user interfaces. We show excerpts from the user interface and our explanations for the users and give recommendations for the creation of explanations in user interfaces.},
  isbn = {978-1-958651-70-4},
  langid = {english}
}

@inproceedings{haiduActionRecognitionInterpretation2016,
  title = {Action Recognition and Interpretation from Virtual Demonstrations},
  booktitle = {2016 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Haidu, Andrei and Beetz, Michael},
  year = {2016},
  month = oct,
  pages = {2833--2838},
  issn = {2153-0866},
  doi = {10.1109/IROS.2016.7759439},
  abstract = {To properly perform tasks based on abstract instructions, autonomous robots need refined reasoning skills in order to bridge the gap between the ambiguous descriptions and the comprehensive information needed to execute the implied actions. In this article, we present an automated knowledge acquisition system from human executed tasks in virtual environments, and extend the knowledge processing system KNOWROB[1] to be capable to reason on the acquired data. We have set up two scenarios in a physics based simulator: creating a pancake, and garnishing a pizza dough. Users where asked to execute these tasks using the provided tools and ingredients. Using a data processing module we then collect the low-level data and the relevant abstract events from the performed episodes. The recorded data is then made available in a format that robots can understand, by using a symbolic layer to interconnect the two data types in a seamless way.},
  keywords = {action interpretation,action recognition,ambiguous descriptions,automated knowledge acquisition system,autonomous robots,Cognition,comprehensive information,Dairy products,data processing module,Databases,human executed tasks,intelligent robots,knowledge acquisition,Knowledge based systems,knowledge processing system,KNOWROB,low-level data collect,manipulators,mobile robots,pancake,Physics,physics based simulator,pizza dough garnishing,reasoning skills,Robots,symbolic layer,virtual demonstrations,Virtual environments},
  file = {C:\Users\benja\Zotero\storage\DU4SJ2C6\7759439.html}
}

@inproceedings{haiduAutomatedAcquisitionStructured2021,
  title = {Automated Acquisition of Structured, Semantic Models of Manipulation Activities from Human {{VR}} Demonstration},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Haidu, Andrei and Beetz, Michael},
  year = {2021},
  month = may,
  pages = {9460--9466},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9562016},
  abstract = {In this paper we present a system capable of collecting and annotating, human performed, robot understandable, everyday activities from virtual environments. The human movements are mapped in the simulated world using off-the-shelf virtual reality devices with full body, and eye tracking capabilities. All the interactions in the virtual world are physically simulated, thus movements and their effects are closely relatable to the real world. During the activity execution, a subsymbolic data logger is recording the environment and the human gaze on a per-frame basis, enabling offline scene reproduction and replays. Coupled with the physics engine, online monitors (symbolic data loggers) are parsing (using various grammars) and recording events, actions, and their effects in the simulated world.},
  keywords = {Gaze tracking,Grammar,Performance evaluation,Robots,Semantics,Solid modeling,Virtual environments},
  file = {C:\Users\benja\Zotero\storage\6UCRNVYA\9562016.html}
}

@inproceedings{haiduKnowRobSIMGameEngineEnabled2018,
  title = {{{KnowRobSIM}} --- {{Game Engine-Enabled Knowledge Processing Towards Cognition-Enabled Robot Control}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Haidu, A. and Be{\ss}ler, D. and Bozcuo{\u g}lu, A. K. and Beetz, M.},
  year = {2018},
  month = oct,
  pages = {4491--4498},
  doi = {10.1109/IROS.2018.8593935},
  abstract = {AI knowledge representation and reasoning methods consider actions to be blackboxes that abstract away from how they are executed. This abstract view does not suffice for the decision making capabilities required by robotic agents that are to accomplish manipulation tasks. Such robots have to reason about how to pour without spilling, where to grasp a pot, how to open different containers, and so on. To enable such reasoning it is necessary to consider how objects are perceived, how motions can be executed and parameterized, and how motion parameterization affects the physical effects of actions. To this end, we propose to complement and extend symbolic reasoning methods with KnowRobSIM, an additional reasoning infrastructure based on modern game engine technology, including the subsymbolic world modeling through data structures, action simulation based on physics engine, and world scene rendering. We demonstrate how KnowRobSIM can perform powerful reasoning, prediction, and learning tasks that are required for informed decision making in object manipulation.},
  keywords = {action simulation,AI knowledge representation,Cognition,cognition-enabled robot control,cognitive systems,computer games,control engineering computing,data structures,Data structures,decision making,decision making capabilities,Engines,Force,game engine-enabled knowledge processing,Games,inference mechanisms,knowledge representation,KnowRobSIM,manipulation tasks,manipulators,modern game engine technology,motion control,motion parameterization,object manipulation,physics engine,reasoning methods,rendering (computer graphics),robotic agents,Robots,symbolic reasoning methods,world scene rendering}
}

@article{hamiltonQuaternionsNewSystem1844,
  title = {On Quaternions; or on a New System of Imaginaries in Algebra},
  author = {Hamilton, Sir William Rowan},
  year = {1844},
  month = jul,
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {25},
  number = {163},
  pages = {10--13},
  issn = {1941-5966},
  doi = {10.1080/14786444408644923},
  urldate = {2019-07-20}
}

@article{hammondAutomaticFailureRecovery2019,
  title = {Automatic {{Failure Recovery}} for {{End-User Programs}} on {{Service Mobile Robots}}},
  author = {Hammond, Jenna Claire and Biswas, Joydeep and Guha, Arjun},
  year = {2019},
  month = sep,
  journal = {arXiv:1909.02778 [cs]},
  eprint = {1909.02778},
  primaryclass = {cs},
  urldate = {2021-12-20},
  abstract = {For service mobile robots to be most effective, it must be possible for non-experts and even end-users to program them to do new tasks. Regardless of the programming method (e.g., by demonstration or traditional programming), robot task programs are challenging to write, because they rely on multiple actions to succeed, including human-robot interactions. Unfortunately, interactions are prone to fail, because a human may perform the wrong action (e.g., if the robot's request is not clear). Moreover, when the robot cannot directly observe the human action, it may not detect the failure until several steps after it occurs. Therefore, writing fault-tolerant robot tasks is beyond the ability of non-experts. This paper presents a principled approach to detect and recover from a broad class of failures that occur in end-user programs on service mobile robots. We present a two-tiered Robot Task Programming Language (RTPL): 1) an expert roboticist uses a specification language to write a probabilistic model of the robot's actions and interactions, and 2) a non-expert then writes an ordinary sequential program for a particular task. The RTPL runtime system executes the task program sequentially, while using the probabilistic model to build a Bayesian network that tracks possible, unobserved failures. If an error is observed, RTPL uses Bayesian inference to find the likely root cause of the error, and then attempts to re-execute a portion of the program for recovery. Our empirical results show that RTPL 1) allows complex tasks to be written concisely, 2) correctly identifies the root cause of failure, and 3) allows multiple tasks to recover from a variety of errors, without task-specific error-recovery code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\D76Q2HWA\1909.html}
}

@article{Hana.2018,
  title = {A Comprehensive Review of {{3D}} Point Cloud Descriptors},
  author = {Hana, Xian-Feng and Jin, Jesse S. and Xie, Juan and Wang, Ming-Jie and Jiang, Wei},
  year = {2018},
  abstract = {The introduction of inexpensive 3D data acquisition devices has promisingly facilitated the wide availability and popularity of 3D point cloud, which attracts more attention on the effective extraction of novel 3D point cloud descriptors for accurate and efficient of 3D computer vision tasks. However, how to de- velop discriminative and robust feature descriptors from various point clouds remains a challenging task. This paper comprehensively investigates the exist- ing approaches for extracting 3D point cloud descriptors which are categorized into three major classes: local-based descriptor, global-based descriptor and hybrid-based descriptor. Furthermore, experiments are carried out to present a thorough evaluation of performance of several state-of-the-art 3D point cloud descriptors used widely in practice in terms of descriptiveness, robustness and efficiency.}
}

@article{haneyRoboticassistedLaparoscopicBowel2023,
  title = {Robotic-Assisted versus Laparoscopic Bowel Anastomoses: Randomized Crossover in Vivo Experimental Study},
  shorttitle = {Robotic-Assisted versus Laparoscopic Bowel Anastomoses},
  author = {Haney, Cael{\'a}n Max and Kowalewski, Karl-Friedrich and Schmidt, Mona Wanda and Lang, Franziska and Bintintan, Vasile and Fan, Carolyn and Wehrtmann, Fabian and {Studier-Fischer}, Alexander and Felinska, Eleni Amelia and {M{\"u}ller-Stich}, Beat Peter and Nickel, Felix},
  year = {2023},
  month = aug,
  journal = {Surgical Endoscopy},
  volume = {37},
  number = {8},
  pages = {5894--5901},
  issn = {1432-2218},
  doi = {10.1007/s00464-023-10044-7},
  abstract = {BACKGROUND: Initial learning curves are potentially shorter in robotic-assisted surgery (RAS) than in conventional laparoscopic surgery (LS). There is little evidence to support this claim. Furthermore, there is limited evidence how skills from LS transfer to RAS. METHODS: A randomized controlled, assessor blinded crossover study to compare how RAS na{\"i}ve surgeons (n\,=\,40) performed linear-stapled side-to-side bowel anastomoses in an in vivo porcine model with LS and RAS. Technique was rated using the validated anastomosis objective structured assessment of skills (A-OSATS) score and the conventional OSATS score. Skill transfer from LS to RAS was measured by comparing the RAS performance of LS novices and LS experienced surgeons. Mental and physical workload was measured with the NASA-task load index (NASA-Tlx) and the Borg-scale. OUTCOMES: In the overall cohort, there were no differences between RAS and LS for surgical performance (A-OSATS, time, OSATS). Surgeons that were na{\"i}ve in both LS and RAS had significantly higher A-OSATS scores in RAS (Mean (Standard deviation (SD)): LS: 48.0\,{\textpm}\,12.1; RAS: 52.0\,{\textpm}\,7.5); p\,=\,0.044) mainly deriving from better bowel positioning (LS: 8.7\,{\textpm}\,1.4; RAS: 9.3\,{\textpm}\,1.0; p\,=\,0.045) and closure of enterotomy (LS: 12.8\,{\textpm}\,5.5; RAS: 15.6\,{\textpm}\,4.7; p\,=\,0.010). There was no statistically significant difference in how LS novices and LS experienced surgeons performed in RAS [Mean (SD): novices: 48.9\,{\textpm}\,9.0; experienced surgeons: 55.9\,{\textpm}\,11.0; p\,=\,0.540]. Mental and physical demand was significantly higher after LS. CONCLUSION: The initial performance was improved for RAS versus LS for linear stapled bowel anastomosis, whereas workload was higher for LS. There was limited transfer of skills from LS to RAS.},
  langid = {english},
  pmcid = {PMC10338398},
  pmid = {37072638},
  keywords = {Anastomosis Surgical,Animals,Clinical Competence,Cross-Over Studies,Digestive System Surgical Procedures,Humans,Laparoscopy,Robotic Surgical Procedures,Surgeons,Swine}
}

@article{hanockaPoint2MeshSelfpriorDeformable2020,
  title = {{{Point2Mesh}}: A Self-Prior for Deformable Meshes},
  shorttitle = {{{Point2Mesh}}},
  author = {Hanocka, Rana and Metzer, Gal and Giryes, Raja and {Cohen-Or}, Daniel},
  year = {2020},
  month = aug,
  journal = {ACM Transactions on Graphics},
  volume = {39},
  number = {4},
  pages = {126:126:1--126:126:12},
  issn = {0730-0301},
  doi = {10.1145/3386569.3392415},
  urldate = {2024-01-11},
  abstract = {In this paper, we introduce Point2Mesh, a technique for reconstructing a surface mesh from an input point cloud. Instead of explicitly specifying a prior that encodes the expected shape properties, the prior is defined automatically using the input point cloud, which we refer to as a self-prior. The self-prior encapsulates reoccurring geometric repetitions from a single shape within the weights of a deep neural network. We optimize the network weights to deform an initial mesh to shrink-wrap a single input point cloud. This explicitly considers the entire reconstructed shape, since shared local kernels are calculated to fit the overall object. The convolutional kernels are optimized globally across the entire shape, which inherently encourages local-scale geometric self-similarity across the shape surface. We show that shrink-wrapping a point cloud with a self-prior converges to a desirable solution; compared to a prescribed smoothness prior, which often becomes trapped in undesirable local minima. While the performance of traditional reconstruction approaches degrades in non-ideal conditions that are often present in real world scanning, i.e., unoriented normals, noise and missing (low density) parts, Point2Mesh is robust to non-ideal conditions. We demonstrate the performance of Point2Mesh on a large variety of shapes with varying complexity.},
  keywords = {geometric deep learning,shape analysis,surface reconstruction}
}

@article{Hansch.2014,
  title = {Comparison of {{3D}} Interest Point Detectors and Descriptors for Point Cloud Fusion},
  author = {H{\"a}nsch, R. and Weber, T. and Hellwich, O.},
  year = {2014},
  journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume = {II-3},
  pages = {57--64},
  doi = {10.5194/isprsannals-II-3-57-2014},
  pagination = {page}
}

@inproceedings{hansenVisuotactileRLLearningMultimodal2022,
  title = {Visuotactile-{{RL}}: {{Learning Multimodal Manipulation Policies}} with {{Deep Reinforcement Learning}}},
  shorttitle = {Visuotactile-{{RL}}},
  booktitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Hansen, Johanna and Hogan, Francois and Rivkin, Dmitriy and Meger, David and Jenkin, Michael and Dudek, Gregory},
  year = {2022},
  month = may,
  pages = {8298--8304},
  doi = {10.1109/ICRA46639.2022.9812019},
  urldate = {2024-04-28},
  abstract = {Manipulating objects with dexterity requires timely feedback that simultaneously leverages the senses of vision and touch. In this paper, we focus on the problem setting where both visual and tactile sensors provide pixel-level feedback for Visuotactile reinforcement learning agents. We investigate the challenges associated with multimodal learning and propose several improvements to existing RL methods; including tactile gating, tactile data augmentation, and visual degradation. When compared with visual-only and tactile-only baselines, our Visuotactile-RL agents showcase (1) significant improvements in contact-rich tasks; (2) improved robustness to visual changes (lighting/camera view) in the workspace; and (3) resilience to physical changes in the task environment (weight/friction of objects).},
  keywords = {Optical feedback,Optical sensors,Perturbation methods,Reinforcement learning,Robustness,Tactile sensors,Visualization},
  file = {C:\Users\benja\Zotero\storage\MELKRUDL\9812019.html}
}

@article{Haouchine.2012,
  title = {Physics-Based Augmented Reality for {{3D}} Deformable Object},
  author = {Haouchine, Nazim and Dequidt, J{\'e}r{\'e}mie and Kerrien, Erwan and Berger, Marie-Odile and Cotin, St{\'e}phane},
  year = {2012},
  publisher = {The Eurographics Association},
  doi = {10.2312/PE/vriphys/vriphys12/031-038},
  abstract = {This paper introduces an original method to perform augmented or mixed reality on deformable objects. Compared to state-of-the-art techniques, our method is able to track deformations of volumetric objects and not only surfacic objects. A flexible framework that relies on the combination of a 3D motion estimation and a physicsbased deformable model used as a regularization and interpolation step allows to perform a non-rigid and robust registration. Results are exposed, based on computer-generated datasets and video sequences of real environments in order to assess the relevance of our approach.}
}

@inproceedings{haouchine2016segmentation,
  title = {Segmentation and Labelling of Intra-Operative Laparoscopic Images Using Structure from Point Cloud},
  booktitle = {2016 {{IEEE}} 13th International Symposium on Biomedical Imaging ({{ISBI}})},
  author = {Haouchine, Nazim and Cotin, Stephane},
  year = {2016},
  pages = {115--118},
  organization = {IEEE}
}

@article{harmelenBoxologyDesignPatterns2019,
  title = {A {{Boxology}} of {{Design Patterns}} for {{Hybrid Learning}} and {{Reasoning Systems}}},
  author = {van Harmelen, Frank and ten Teije, Annette},
  year = {2019},
  month = jan,
  journal = {Journal of Web Engineering},
  volume = {18},
  number = {1},
  pages = {97--124},
  publisher = {River Publishers},
  issn = {1540-9589, 1544-5976},
  doi = {10.13052/jwe1540-9589.18133},
  urldate = {2021-05-18},
  abstract = {A Boxology of Design Patterns for Hybrid Learning and Reasoning Systems},
  file = {C:\Users\benja\Zotero\storage\QSJQBGFW\journal_read_html_article.html}
}

@inproceedings{Harris.1988,
  title = {A Combined Corner and Edge Detector},
  booktitle = {Procedings of the Alvey Vision Conference 1988},
  author = {Harris, C. and Stephens, M.},
  editor = {Taylor, C. J.},
  year = {1988},
  pages = {23.1--23.6},
  publisher = {Alvey Vision Club},
  doi = {10.5244/C.2.23},
  bookpagination = {page}
}

@book{Hartley.2015,
  title = {Multiple View Geometry in Computer Vision},
  author = {Hartley, Richard and Zisserman, Andrew},
  year = {2015},
  edition = {2. ed., 13.pr},
  publisher = {Cambridge Univ. Press},
  address = {Cambridge},
  isbn = {0-521-54051-8}
}

@article{hartNasaTaskLoadIndex2006,
  title = {Nasa-{{Task Load Index}} ({{NASA-TLX}}); 20 {{Years Later}}},
  author = {Hart, Sandra G.},
  year = {2006},
  month = oct,
  journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  volume = {50},
  number = {9},
  pages = {904--908},
  publisher = {SAGE Publications Inc},
  issn = {1071-1813},
  doi = {10.1177/154193120605000909},
  urldate = {2024-03-22},
  abstract = {NASA-TLX is a multi-dimensional scale designed to obtain workload estimates from one or more operators while they are performing a task or immediately afterwards. The years of research that preceded subscale selection and the weighted averaging approach resulted in a tool that has proven to be reasonably easy to use and reliably sensitive to experimentally important manipulations over the past 20 years. Its use has spread far beyond its original application (aviation), focus (crew complement), and language (English). This survey of 550 studies in which NASA-TLX was used or reviewed was undertaken to provide a resource for a new generation of users. The goal was to summarize the environments in which it has been applied, the types of activities the raters performed, other variables that were measured that did (or did not) covary, methodological issues, and lessons learned},
  langid = {english}
}

@inproceedings{hasan2019u,
  title = {U-{{NetPlus}}: {{A}} Modified Encoder-Decoder {{U-Net}} Architecture for Semantic and Instance Segmentation of Surgical Instruments from Laparoscopic Images},
  booktitle = {2019 41st Annual International Conference of the {{IEEE}} Engineering in Medicine and Biology Society ({{EMBC}})},
  author = {Hasan, SM Kamrul and Linte, Cristian A},
  year = {2019},
  pages = {7205--7211},
  organization = {IEEE}
}

@phdthesis{HassanAzal.2016,
  title = {Full {{3D}} Reconstruction of Dynamic Non-Rigid Scenes: {{Acquisition}} and Enhancement},
  author = {{Hassan Azal}},
  year = {2016},
  address = {Luxembourg},
  school = {Faculty of Sciences, Technology and Communication / Universit{\'e} du Luxembourg}
}

@article{hasselgrenNeuralTemporalAdaptive2020,
  title = {Neural {{Temporal Adaptive Sampling}} and {{Denoising}}},
  author = {Hasselgren, J. and Munkberg, J. and Salvi, M. and Patney, A. and Lefohn, A.},
  year = {2020},
  month = may,
  journal = {Computer Graphics Forum},
  volume = {39},
  number = {2},
  pages = {147--155},
  issn = {0167-7055, 1467-8659},
  doi = {10.1111/cgf.13919},
  urldate = {2020-12-19},
  abstract = {Despite recent advances in Monte Carlo path tracing at interactive rates, denoised image sequences generated with few samples per-pixel often yield temporally unstable results and loss of high-frequency details. We present a novel adaptive rendering method that increases temporal stability and image fidelity of low sample count path tracing by distributing samples via spatio-temporal joint optimization of sampling and denoising. Adding temporal optimization to the sample predictor enables it to learn spatio-temporal sampling strategies such as placing more samples in disoccluded regions, tracking specular highlights, etc; adding temporal feedback to the denoiser boosts the effective input sample count and increases temporal stability. The temporal approach also allows us to remove the initial uniform sampling step typically present in adaptive sampling algorithms. The sample predictor and denoiser are deep neural networks that we co-train end-to-end over multiple consecutive frames. Our approach is scalable, allowing trade-off between quality and performance, and runs at near real-time rates while achieving significantly better image quality and temporal stability than previous methods.},
  langid = {english}
}

@article{hasterokPAISEProcessModel2022,
  title = {{{PAISE}}{\textregistered} -- Process Model for {{AI}} Systems Engineering},
  author = {Hasterok, Constanze and Stompe, Janina},
  year = {2022},
  month = sep,
  journal = {at - Automatisierungstechnik},
  volume = {70},
  number = {9},
  pages = {777--786},
  publisher = {De Gruyter (O)},
  issn = {2196-677X},
  doi = {10.1515/auto-2022-0020},
  urldate = {2024-02-01},
  abstract = {The application of artificial-intelligence-(AI)-based methods within the context of complex systems poses new challenges within the product life cycle. The process model for AI systems engineering, PAISE {\textregistered} , addresses these challenges by combining approaches from the disciplines of systems engineering, software development and data science. The general approach builds on a component-wise development of the overall system including an AI component. This allows domain specific development processes to be parallelized. At the same time, component dependencies are tested within interdisciplinary checkpoints, thus resulting in a refinement of component specifications.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {AI systems engineering,artificial intelligence,machine learning,process model,systems engineering}
}

@inproceedings{hatamizadehDeepActiveLesion2019,
  title = {Deep {{Active Lesion Segmentation}}},
  booktitle = {Machine {{Learning}} in {{Medical Imaging}}},
  author = {Hatamizadeh, Ali and Hoogi, Assaf and Sengupta, Debleena and Lu, Wuyue and Wilcox, Brian and Rubin, Daniel and Terzopoulos, Demetri},
  editor = {Suk, Heung-Il and Liu, Mingxia and Yan, Pingkun and Lian, Chunfeng},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {98--105},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-32692-0_12},
  abstract = {Lesion segmentation is an important problem in computer-assisted diagnosis that remains challenging due to the prevalence of low contrast, irregular boundaries that are unamenable to shape priors. We introduce Deep Active Lesion Segmentation (DALS), a fully automated segmentation framework that leverages the powerful nonlinear feature extraction abilities of fully Convolutional Neural Networks (CNNs) and the precise boundary delineation abilities of Active Contour Models (ACMs). Our DALS framework benefits from an improved level-set ACM formulation with a per-pixel-parameterized energy functional and a novel multiscale encoder-decoder CNN that learns an initialization probability map along with parameter maps for the ACM. We evaluate our lesion segmentation model on a new Multiorgan Lesion Segmentation (MLS) dataset that contains images of various organs, including brain, liver, and lung, across different imaging modalities---MR and CT. Our results demonstrate favorable performance compared to competing methods, especially for small training datasets.},
  isbn = {978-3-030-32692-0},
  langid = {english},
  keywords = {Active contour model,Deep learning,Lesion segmentation,Level sets}
}

@article{haugManipulatorKinematicsDynamics2021,
  title = {Manipulator {{Kinematics}} and {{Dynamics}} on {{Differentiable Manifolds}}: {{Part I Kinematics}}},
  shorttitle = {Manipulator {{Kinematics}} and {{Dynamics}} on {{Differentiable Manifolds}}},
  author = {Haug, Edward J.},
  year = {2021},
  month = nov,
  journal = {Journal of Computational and Nonlinear Dynamics},
  volume = {17},
  number = {021002},
  issn = {1555-1415},
  doi = {10.1115/1.4052652},
  urldate = {2024-03-31},
  abstract = {Using basic tools of Euclidian space differential geometry, maximal singularity free components of the regular manipulator configuration space are defined, with conditions that establish the space as a differentiable manifold. This structure shows that the conventional categorization of manipulators as either serial or parallel is incomplete and that three distinct categories of manipulator must be accounted for; (1) serial manipulators in which inputs globally determine outputs, (2) explicit parallel manipulators in which outputs globally determine inputs, and (3) compound manipulators in which there is no global input or output mapping. Results of differential geometry are used to show that configuration space differentiable manifolds in each category are partitioned into maximal, disjoint, path-connected components in which the manipulator is singularity free and may be effectively controlled. This extends local analytical properties of manipulators that are used for analysis and control to global validity on maximal components of regular manipulator configuration space, providing explicit criteria for avoidance of singular behavior. Model manipulators in each of the three categories are analyzed to illustrate application of the differentiable manifold structure, using only multivariable calculus and linear algebra. Computational methods for forward and inverse kinematics and construction of ordinary differential equations of manipulator dynamics on differentiable manifolds are presented in Part II of this paper, in support of manipulator control.},
  file = {C:\Users\benja\Zotero\storage\V7M6RCZK\Manipulator-Kinematics-and-Dynamics-on.html}
}

@inproceedings{hayesAutonomouslyConstructingHierarchical2016,
  title = {Autonomously Constructing Hierarchical Task Networks for Planning and Human-Robot Collaboration},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Hayes, Bradley and Scassellati, Brian},
  year = {2016},
  month = may,
  pages = {5469--5476},
  doi = {10.1109/ICRA.2016.7487760},
  urldate = {2024-09-29},
  abstract = {Collaboration between humans and robots requires solutions to an array of challenging problems, including multi-agent planning, state estimation, and goal inference. There already exist feasible solutions for many of these challenges, but they depend upon having rich task models. In this work we detail a novel type of Hierarchical Task Network we call a Clique/Chain HTN (CC-HTN), alongside an algorithm for autonomously constructing them from topological properties derived from graphical task representations. As the presented method relies on the structure of the task itself, our work imposes no particular type of symbolic insight into motor primitives or environmental representation, making it applicable to a wide variety of use cases critical to human-robot interaction. We present evaluations within a multi-resolution goal inference task and a transfer learning application showing the utility of our approach.},
  keywords = {Cognition,Collaboration,Markov processes,Planning,Robots,State estimation,Training},
  file = {C:\Users\benja\Zotero\storage\6CN7SDEG\7487760.html}
}

@article{hebbalBayesianOptimizationUsing2019,
  title = {Bayesian {{Optimization}} Using {{Deep Gaussian Processes}}},
  author = {Hebbal, Ali and Brevault, Loic and Balesdent, Mathieu and Talbi, El-Ghazali and Melab, Nouredine},
  year = {2019},
  month = may,
  journal = {arXiv:1905.03350 [cs, stat]},
  eprint = {1905.03350},
  primaryclass = {cs, stat},
  urldate = {2021-06-21},
  abstract = {Bayesian Optimization using Gaussian Processes is a popular approach to deal with the optimization of expensive black-box functions. However, because of the a priori on the stationarity of the covariance matrix of classic Gaussian Processes, this method may not be adapted for non-stationary functions involved in the optimization problem. To overcome this issue, a new Bayesian Optimization approach is proposed. It is based on Deep Gaussian Processes as surrogate models instead of classic Gaussian Processes. This modeling technique increases the power of representation to capture the non-stationarity by simply considering a functional composition of stationary Gaussian Processes, providing a multiple layer structure. This paper proposes a new algorithm for Global Optimization by coupling Deep Gaussian Processes and Bayesian Optimization. The specificities of this optimization method are discussed and highlighted with academic test cases. The performance of the proposed algorithm is assessed on analytical test cases and an aerospace design optimization problem and compared to the state-of-the-art stationary and non-stationary Bayesian Optimization approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Engineering Finance and Science,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\FLWLA3YQ\1905.html}
}

@inproceedings{heControlBatchSize2019,
  title = {Control {{Batch Size}} and {{Learning Rate}} to {{Generalize Well}}: {{Theoretical}} and {{Empirical Evidence}}},
  shorttitle = {Control {{Batch Size}} and {{Learning Rate}} to {{Generalize Well}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {He, Fengxiang and Liu, Tongliang and Tao, Dacheng},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-28}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  urldate = {2024-05-01},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8{\texttimes} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization},
  file = {C:\Users\benja\Zotero\storage\C7F9WA32\7780459.html}
}

@article{heessEmergenceLocomotionBehaviours2017,
  title = {Emergence of {{Locomotion Behaviours}} in {{Rich Environments}}},
  author = {Heess, Nicolas and TB, Dhruva and Sriram, Srinivasan and Lemmon, Jay and Merel, Josh and Wayne, Greg and Tassa, Yuval and Erez, Tom and Wang, Ziyu and Eslami, S. M. Ali and Riedmiller, Martin and Silver, David},
  year = {2017},
  month = jul,
  journal = {arXiv:1707.02286 [cs]},
  eprint = {1707.02286},
  primaryclass = {cs},
  urldate = {2019-07-14},
  abstract = {The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Specifically, we train agents in diverse environmental contexts, and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion -- behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed in this video.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence}
}

@inproceedings{hegerRobustRoboticAssembly2010,
  title = {Robust Robotic Assembly through Contingencies, Plan Repair and Re-Planning},
  booktitle = {2010 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Heger, F. W. and Singh, S.},
  year = {2010},
  month = may,
  pages = {3825--3830},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2010.5509274},
  abstract = {Enabling mobile robots to assemble large structures in constrained environments requires planning systems that are both capable of dealing with high complexity and can provide robust execution in the face of run-time failures. We achieve execution robustness through exception handling capabilities that are seamlessly integrated throughout the planning system. Having these recovery mechanisms in place allows us to leverage their capabilities to compensate for problems introduced by approximations made during planning. Turning an apparent problem into an opportunity, we are able to plan complex assembly tasks and execute them robustly without the computational cost associated with more sophisticated planners and apply some of the savings toward recovering from unforeseen run-time errors. We show results where simple planning strategies paired with exception-handling are able to achieve the same outcomes (and in less time) as more elaborate methods would.},
  keywords = {Assembly systems,Manipulators,mobile robots,Mobile robots,Orbital robotics,path planning,plan repair,planning systems,recovery mechanisms,Robot kinematics,Robot sensing systems,robotic assembly,Robotic assembly,Robotics and automation,robust robotic assembly,Robustness,run-time errors,run-time failures,Runtime environment},
  file = {C:\Users\benja\Zotero\storage\RHT2U8HT\5509274.html}
}

@inproceedings{heidenDiSECtDifferentiableSimulation2021,
  title = {{{DiSECt}}: {{A Differentiable Simulation Engine}} for {{Autonomous Robotic Cutting}}},
  shorttitle = {{{DiSECt}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVII}}},
  author = {Heiden, Eric and Macklin, Miles and Narang, Yashraj S. and Fox, Dieter and Garg, Animesh and Ramos, Fabio},
  year = {2021},
  month = jul,
  volume = {17},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-7-8},
  file = {C:\Users\benja\Zotero\storage\ZJ2S3U88\p067.html}
}

@inproceedings{heimannIndustrialRobotProgramming2020,
  title = {Industrial {{Robot Programming Methods}}: {{A Scoping Review}}},
  shorttitle = {Industrial {{Robot Programming Methods}}},
  booktitle = {2020 25th {{IEEE International Conference}} on {{Emerging Technologies}} and {{Factory Automation}} ({{ETFA}})},
  author = {Heimann, Oliver and Guhl, Jan},
  year = {2020},
  month = sep,
  volume = {1},
  pages = {696--703},
  issn = {1946-0759},
  doi = {10.1109/ETFA46521.2020.9211997},
  abstract = {Easy-to-use programming methods for robotic manufacturing systems are a key element in enabling a wider audience to benefit from increasing automation. This paper provides an overview of current programming methods for industrial mating operations. More specifically, the review focuses on programming methods for welding and assembly tasks. While welding is mainly concerned with the exact definition of the free space trajectories, assembly tasks revolve around the contact with the environment. Thus, the two tasks at hand not only represent two of the most common robotic applications, but also two distinct groups of operations that place very different demands on the programming interface.},
  keywords = {Conferences,Industrial Robots,Manufacturing automation,Programming Methods,Review,Robot programming,Service robots,Task analysis,Trajectory,Welding},
  file = {C:\Users\benja\Zotero\storage\6VU87IXX\9211997.html}
}

@inproceedings{heimbergerAssessingAIReadinessProduction2023,
  title = {Assessing {{AI-Readiness}} in {{Production}}---{{A Conceptual Approach}}},
  booktitle = {Intelligent and {{Transformative Production}} in {{Pandemic Times}}},
  author = {Heimberger, Heidi and Horvat, Djerdj and Schultmann, Frank},
  editor = {Huang, Chin-Yin and Dekkers, Rob and Chiu, Shun Fung and Popescu, Daniela and Quezada, Luis},
  year = {2023},
  pages = {249--257},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-18641-7_24},
  abstract = {Due to its high potential to perform many tasks faster, more accurately and in greater detail than humans artificial intelligence (AI) has been attracting growing attention across industries. In manufacturing, AI, in combination with digital sensors, networks and software-based automation, defines a new industrialization age. The integration of AI into production processes promises to boost the productivity, efficiency, as well as the automation of processes. However, AI adoption in manufacturing is currently still in its early stage and lacks practical experiences. This raises the question, to which extent manufacturing companies are ready to implement AI. While approaches to assessing the maturity in terms of the digitalization or Industry 4.0 (I4.0) of manufacturing companies are well established and discussed in the literature, approaches that specifically address AI in manufacturing are still lacking. To address this gap, we present an approach to analyze and monitor the readiness of manufacturing firms for working with AI technologies. In accordance with the existing assessment concepts of digitalization and I4.0, our approach examines different areas of digital technologies on the product and production level of manufacturing firms. Moreover, it incorporates the key foundation for AI---security and data---into a conceptual model. We generally assume that companies need to achieve a certain level of digital readiness in three key dimensions in order to be ready for implementing AI-based technologies. We operationalize these dimensions through a variety of product- and production-specific as well as data- and safety-related indicators. In order to illustrate the implementation of our concept in practical terms, we present the results of the readiness assessment of two German manufacturing companies.},
  isbn = {978-3-031-18641-7},
  langid = {english},
  keywords = {Artificial intelligence,Manufacturing,Readiness},
  file = {C:\Users\benja\Zotero\storage\TTUS6N3B\Heimberger et al. - 2023 - Assessing AI-Readiness in ProductionA Conceptual Approach.pdf}
}

@article{heimbergerExploringFactorsDriving2024,
  title = {Exploring the Factors Driving {{AI}} Adoption in Production: A Systematic Literature Review and Future Research Agenda},
  shorttitle = {Exploring the Factors Driving {{AI}} Adoption in Production},
  author = {Heimberger, Heidi and Horvat, Djerdj and Schultmann, Frank},
  year = {2024},
  month = aug,
  journal = {Information Technology and Management},
  issn = {1573-7667},
  doi = {10.1007/s10799-024-00436-z},
  urldate = {2024-09-10},
  abstract = {Our paper analyzes the current state of research on artificial intelligence (AI) adoption from a production perspective. We represent a holistic view on the topic which is necessary to get a first understanding of AI in a production-context and to build a comprehensive view on the different dimensions as well as factors influencing its adoption. We review the scientific literature published between 2010 and May~2024 to analyze the current state of research on AI in production. Following a systematic approach to select relevant studies, our literature review is based on a sample of articles that contribute to production-specific AI adoption. Our results reveal that the topic has been emerging within the last years and that AI adoption research in production is to date still in an early stage. We are able to systematize and explain 35 factors with a significant role for AI adoption in production and classify the results in a framework. Based on the factor analysis, we establish a future research agenda that serves as a basis for future research and addresses open questions. Our paper provides an overview of the current state of the research on the adoption of AI in a production-specific context, which forms a basis for further studies as well as a starting point for a better understanding of the implementation of AI in practice.},
  langid = {english},
  keywords = {Adoption factors,AI adoption,Artificial intelligence,Artificial Intelligence,Production,Systematic literature review,Technology adoption},
  file = {C:\Users\benja\Zotero\storage\H9RFVWLD\Heimberger et al. - 2024 - Exploring the factors driving AI adoption in production a systematic literature review and future r.pdf}
}

@inproceedings{heMaskRCNN2017,
  title = {Mask {{R-CNN}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
  year = {2017},
  pages = {2961--2969},
  urldate = {2023-03-06}
}

@article{henaffModelBasedPlanningDiscrete2018,
  title = {Model-{{Based Planning}} with {{Discrete}} and {{Continuous Actions}}},
  author = {Henaff, Mikael and Whitney, William F. and LeCun, Yann},
  year = {2018},
  month = apr,
  journal = {arXiv:1705.07177 [cs]},
  eprint = {1705.07177},
  primaryclass = {cs},
  urldate = {2020-03-17},
  abstract = {Action planning using learned and differentiable forward models of the world is a general approach which has a number of desirable properties, including improved sample complexity over modelfree RL methods, reuse of learned models across different tasks, and the ability to perform efficient gradient-based optimization in continuous action spaces. However, this approach does not apply straightforwardly when the action space is discrete. In this work, we show that it is in fact possible to effectively perform planning via backprop in discrete action spaces, using a simple paramaterization of the actions vectors on the simplex combined with input noise when training the forward model. Our experiments show that this approach can match or outperform model-free RL and discrete planning methods on gridworld navigation tasks in terms of performance and/or planning time while using limited environment interactions, and can additionally be used to perform model-based control in a challenging new task where the action space combines discrete and continuous actions. We furthermore propose a policy distillation approach which yields a fast policy network which can be used at inference time, removing the need for an iterative planning procedure.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence}
}

@inproceedings{Herda.2000,
  title = {Skeleton-Based Motion Capture for Robust Reconstruction of Human Motion},
  booktitle = {Proceedings Computer Animation 2000},
  author = {Herda, L. and Fua, P. and Plankers, R. and Boulic, R. and Thalmann, D.},
  year = {2000},
  pages = {77--83},
  publisher = {IEEE Comput. Soc},
  doi = {10.1109/CA.2000.889046},
  bookpagination = {page},
  isbn = {0-7695-0683-6}
}

@inproceedings{heRethinkingImageNetPreTraining2019,
  title = {Rethinking {{ImageNet Pre-Training}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {He, Kaiming and Girshick, Ross and Dollar, Piotr},
  year = {2019},
  month = oct,
  pages = {4917--4926},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2019.00502},
  urldate = {2024-06-14},
  abstract = {We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine-tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when: (i) using only 10\% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data-a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of `pretraining and fine-tuning' in computer vision.},
  keywords = {Computer vision,Data models,Detectors,Object detection,Schedules,Task analysis,Training},
  file = {C:\Users\benja\Zotero\storage\U44TPX6I\9010930.html}
}

@phdthesis{Hermann.2018,
  title = {Reaktive Bewegungsplanung Auf {{3D-Sensordaten}} Mittels {{GPU-basierter}} Kollisionserkennung: {{Untersuchung}} von Hochgradig Parallelen Algorithmen F{\"u}r Mobile Serviceroboter},
  author = {Hermann, Andreas},
  year = {2018},
  address = {Karlsruhe},
  school = {KIT-Fakult{\"a}t f{\"u}r Informatik / Karlsruher Institut f{\"u}r Technologie}
}

@inproceedings{hermannHardwareSoftwareArchitecture2013,
  title = {Hardware and Software Architecture of the Bimanual Mobile Manipulation Robot {{HoLLiE}} and Its Actuated Upper Body},
  booktitle = {2013 {{IEEE}}/{{ASME International Conference}} on {{Advanced Intelligent Mechatronics}}},
  author = {Hermann, A. and Sun, J. and Xue, Z. and Ruehl, S. W. and Oberlaender, J. and Roennau, A. and Zoellner, J. M. and Dillmann, R.},
  year = {2013},
  month = jul,
  pages = {286--292},
  issn = {2159-6255},
  doi = {10.1109/AIM.2013.6584106},
  abstract = {We present our recent work on the soft- and hardware design of the bimanual mobile manipulation platform HoLLiE that is equipped with an actuated upper body. The goal was to develop a robust but extensible robot with a non-intimidating abstract anthropomatic appearance based on a combination of industrial robotic components and intelligent mechatronics. With a range of different sensors and a highly articulated body HoLLiE can handle everyday objects, interact with humans in multiple ways and therefore be employed in various service robotic scenarios. We demonstrate the usability of our concept by quantifying the workspace and its stability and also briefly describe the software components.},
  keywords = {Collision avoidance,Hardware,Joints,Mobile communication,Service robots,Springs},
  file = {C:\Users\benja\Zotero\storage\VEFUULLJ\6584106.html}
}

@article{hernandez-lobatoProbabilisticBackpropagationScalable2015,
  title = {Probabilistic {{Backpropagation}} for {{Scalable Learning}} of {{Bayesian Neural Networks}}},
  author = {{Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and Adams, Ryan P.},
  year = {2015},
  month = jul,
  journal = {arXiv:1502.05336 [stat]},
  eprint = {1502.05336},
  primaryclass = {stat},
  urldate = {2021-05-04},
  abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\VL3FCXB2\1502.html}
}

@misc{hernandezDifferentiableProgrammingGeneralization2022,
  title = {Differentiable Programming: {{Generalization}}, Characterization and Limitations of Deep Learning},
  shorttitle = {Differentiable Programming},
  author = {Hern{\'a}ndez, Adri{\'a}n and Millerioux, Gilles and Amig{\'o}, Jos{\'e} M.},
  year = {2022},
  month = may,
  number = {arXiv:2205.06898},
  eprint = {2205.06898},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.06898},
  urldate = {2024-06-25},
  abstract = {In the past years, deep learning models have been successfully applied in several cognitive tasks. Originally inspired by neuroscience, these models are specific examples of differentiable programs. In this paper we define and motivate differentiable programming, as well as specify some program characteristics that allow us to incorporate the structure of the problem in a differentiable program. We analyze different types of differentiable programs, from more general to more specific, and evaluate, for a specific problem with a graph dataset, its structure and knowledge with several differentiable programs using those characteristics. Finally, we discuss some inherent limitations of deep learning and differentiable programs, which are key challenges in advancing artificial intelligence, and then analyze possible solutions},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\ALN29FI8\2205.html}
}

@inproceedings{hessFormalVerificationManeuver2014,
  title = {Formal Verification of Maneuver Automata for Parameterized Motion Primitives},
  booktitle = {2014 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {He{\ss}, Daniel and Althoff, Matthias and Sattel, Thomas},
  year = {2014},
  month = sep,
  pages = {1474--1481},
  issn = {2153-0866},
  doi = {10.1109/IROS.2014.6942751},
  urldate = {2024-04-16},
  abstract = {An increasing amount of robotic systems is developed for safety-critical scenarios, such as automated cars operating in public road traffic or robots collaborating with humans in flexible manufacturing systems. For this reason, it is important to provide methods that formally verify the safety of robotic systems. This is challenging since robots operate in continuous action spaces in partially unknown environments so that there exists no finite set of scenarios that can be verified before deployment. Verifying the safety during the operation based on the current perception of the environment is often infeasible due to the computational demand of formal verification methods. In this work, we compute sets of behaviors for parameterized motion primitives using reachability analysis, which is used to build a maneuver automaton that connects motion primitives in a safe way. Thus, the computationally expensive task of building a maneuver automaton is performed offline. The proposed analysis method provides the whole set of possible behaviors so that it can be verified whether forbidden state-space regions are avoided during the operation of the robot, to e.g. avoid colliding with obstacles. The method is applied to continuous sets of parameterized motion primitives, making it possible to verify infinitely many motions within the parameter space, which to the best knowledge of the authors has not been published before. The approach is demonstrated for collision avoidance of road vehicles.},
  keywords = {Automata,Collision avoidance,Reachability analysis,Trajectory,Vehicle dynamics,Vehicles},
  file = {C:\Users\benja\Zotero\storage\9QDWYBY4\6942751.html}
}

@article{Hestenes.1952,
  title = {Methods of Conjugate Gradients for Solving Linear Systems},
  author = {Hestenes, M. R. and Stiefel, E.},
  year = {1952},
  journal = {Journal of Research of the National Bureau of Standards},
  volume = {49},
  number = {6},
  pages = {409},
  issn = {0091-0635},
  doi = {10.6028/jres.049.044},
  pagination = {page}
}

@book{Heyden.2006,
  title = {Computer Vision - {{ECCV}} 2002: 7th European Conference on Computer Vision Copenhagen, Denmark, May 28-31, 2002 Proceedings, Part {{IV}}},
  author = {Heyden, Anders and Johansen, Peter and Nielsen, Mads and Sparr, Gunnar},
  year = {2006},
  series = {Lecture Notes in Computer Science},
  volume = {2353},
  publisher = {{Springer-Verlag Berlin Heidelberg / European Conference on Computer Vision and ECCV 2002}},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-47979-1},
  isbn = {3-540-43748-7}
}

@article{higySpeechRecognitionICub2018,
  title = {Speech {{Recognition}} for the {{iCub Platform}}},
  author = {Higy, Bertrand and Mereta, Alessio and Metta, Giorgio and Badino, Leonardo},
  year = {2018},
  month = feb,
  journal = {Frontiers in Robotics and AI},
  volume = {5},
  pages = {10},
  issn = {2296-9144},
  doi = {10.3389/frobt.2018.00010},
  urldate = {2024-08-15},
  abstract = {This paper describes open source software (available at https://github.com/robotology/natural-speech) to build automatic speech recognition (ASR) systems and run them within the YARP platform. The toolkit is designed (i) to allow non-ASR experts to easily create their own ASR system and run it on iCub and (ii) to build deep learning-based models specifically addressing the main challenges an ASR system faces in the context of verbal human--iCub interactions. The toolkit mostly consists of Python, C++ code and shell scripts integrated in YARP. As additional contribution, a second codebase (written in Matlab) is provided for more expert ASR users who want to experiment with bio-inspired and developmental learning-inspired ASR systems. Specifically, we provide code for two distinct kinds of speech recognition: ``articulatory'' and ``unsupervised'' speech recognition. The first is largely inspired by influential neurobiological theories of speech perception which assume speech perception to be mediated by brain motor cortex activities. Our articulatory systems have been shown to outperform strong deep learning-based baselines. The second type of recognition systems, the ``unsupervised'' systems, do not use any supervised information (contrary to most ASR systems, including our articulatory systems). To some extent, they mimic an infant who has to discover the basic speech units of a language by herself. In addition, we provide resources consisting of pre-trained deep learning models for ASR, and a 2.5-h speech dataset of spoken commands, the VoCub dataset, which can be used to adapt an ASR system to the typical acoustic environments in which iCub operates.},
  pmcid = {PMC7805979},
  pmid = {33500897}
}

@article{himesAcceleratingBayesianInference2020,
  title = {Accelerating {{Bayesian Inference}} via {{Neural Networks}}: {{Application}} to {{Exoplanet Retrievals}}},
  shorttitle = {Accelerating {{Bayesian Inference}} via {{Neural Networks}}},
  author = {Himes, M. D. and Harrington, J. and Cobb, A. D. and Baydin, A. G. and Soboczenski, F. and O'Beirne, M. D. and Zorzan, S. and Wright, D. and Scheffer, Z. and {Domagal-Goldman}, S. and Arney, G.},
  year = {2020},
  month = oct,
  journal = {AAS Division of Planetary Science meeting},
  volume = {52},
  pages = {207.07},
  urldate = {2024-09-29},
  abstract = {Exoplanet atmospheric retrieval pairs a radiative transfer (RT) model with a Bayesian framework to infer the atmospheric properties of an exoplanet. This technique's runtime is typically dominated by the RT calculations, which can take seconds per model evaluation depending on model complexity (e.g., spectral resolution, opacity sources, clouds). When executing many retrievals (e.g., to plan for observations or future instruments/telescopes), the computational cost can become significant. Neural networks (NNs) can offer a fast, accurate approximation to a complex process, such as RT, after training on a sufficient data set. By using an NN surrogate model in place of an RT code, retrieval compute costs can be reduced by orders of magnitude. We demonstrate our technique on observations of HD 189733 b. The Bhattacharyya coefficients between the 1D marginalized posteriors of the NN-accelerated retrieval and the Bayesian Atmospheric Radiative Transfer (BART) code are {$>$}0.9647, with a mean of 0.9905. We find a reduction in forward model compute time by a factor of {\textasciitilde}20 per parallel chain when using an Intel i7-4770 central processing unit (CPU). By utilizing an Nvidia Titan Xp graphics processing unit, forward model compute time is reduced by a factor of {\textasciitilde}275 per chain compared to that CPU. Our open-source implementation can be readily applied to other problems beyond exoplanet retrieval. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research. This research was supported by the NASA Fellowship Activity under NASA Grant 80NSSC20K0682 and NASA Exoplanets Research Program grant NNX17AB62G.},
  annotation = {ADS Bibcode: 2020DPS....5220707H}
}

@article{Hint06,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  year = {2006},
  volume = {18},
  pages = {1527--1554}
}

@incollection{Hinterstoisser.2013,
  title = {Model Based Training, Detection and Pose Estimation of Texture-Less {{3D}} Objects in Heavily Cluttered Scenes},
  booktitle = {Computer Vision -- {{ACCV}} 2012},
  author = {Hinterstoisser, Stefan and Lepetit, Vincent and Ilic, Slobodan and Holzer, Stefan and Bradski, Gary and Konolige, Kurt and Navab, Nassir},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Lee, Kyoung Mu and Matsushita, Yasuyuki and Rehg, James M. and Hu, Zhanyi},
  year = {2013},
  series = {Lecture Notes in Computer Science},
  volume = {7724},
  pages = {548--562},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-37331-2_42},
  bookpagination = {page},
  isbn = {978-3-642-37330-5}
}

@book{hintonLearningDistributedRepresentations1989,
  title = {Learning Distributed Representations of Concepts},
  author = {Hinton, Geoffrey E.},
  year = {1989},
  series = {Parallel Distributed Processing:  {{Implications}} for Psychology and Neurobiology},
  pages = {61},
  publisher = {Clarendon Press/Oxford University Press},
  address = {New York, NY, US},
  abstract = {role specific units / choosing the role-specific representations / giving the network the freedom to choose representations  a network that learns distributed representations / microinferences and scientific laws / generalization  the back-propagation learning procedure / the learning parameters used for the family tree simulation (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  isbn = {978-0-19-852178-5},
  keywords = {Artificial Intelligence,Cognitive Processes,Concept Formation,Models},
  file = {C:\Users\benja\Zotero\storage\D7KXTDAG\1990-97441-002.html}
}

@article{Hinz2018,
  title = {Speeding up the {{Hyperparameter Optimization}} of {{Deep Convolutional Neural Networks}}},
  author = {Hinz, Tobias and {Navarro-Guerrero}, Nicol{\'a}s and Magg, Sven and Wermter, Stefan},
  year = {2018},
  month = jun,
  journal = {International Journal of Computational Intelligence and Applications},
  volume = {17},
  number = {02},
  pages = {1850008},
  publisher = {World Scientific Publishing Company},
  issn = {1469-0268},
  doi = {10.1142/S1469026818500086},
  urldate = {2019-02-17},
  abstract = {Most learning algorithms require the practitioner to manually set the values of many hyperparameters before the learning process can begin. However, with modern algorithms, the evaluation of a given hyperparameter setting can take a considerable amount of time and the search space is often very high-dimensional. We suggest using a lower-dimensional representation of the original data to quickly identify promising areas in the hyperparameter space. This information can then be used to initialize the optimization algorithm for the original, higher-dimensional data. We compare this approach with the standard procedure of optimizing the hyperparameters only on the original input. We perform experiments with various state-of-the-art hyperparameter optimization algorithms such as random search, the tree of parzen estimators (TPEs), sequential model-based algorithm configuration (SMAC), and a genetic algorithm (GA). Our experiments indicate that it is possible to speed up the optimization process by using lower-...},
  keywords = {Bayesian optimization,convolutional neural networks,genetic algorithm,hyperparameter importance,Hyperparameter optimization}
}

@techreport{hirzleSteuerungstechnischeStandardsAls2008,
  title = {{Steuerungstechnische Standards als Fundament f{\"u}r die Leittechnik}},
  author = {Hirzle, Anton and Alonso Garcia, Alexander and Burkhardt, Andreas},
  year = {2008},
  month = jan,
  institution = {DIV Deutscher Industrieverlag GmbH},
  urldate = {2024-09-28},
  abstract = {Aufgrund der steigenden Anforderungen an Funktionalit{\"a}t, Leistungsf{\"a}higkeit und Flexibilit{\"a}t von Fertigungseinrichtungen nimmt die Komplexit{\"a}t in der Automatisierungstechnik stetig zu. Die Daimler AG begegnet dieser Entwicklung mit einer konsequenten Standardisierung der Automatisierungs- und Steuerungstechnik f{\"u}r Montagewerke. Die Standardisierungsstrategie besteht hierbei aus drei, aufeinander aufbauenden Umsetzungsstufen. In Stufe 1 liegt der Schwerpunkt auf der gezielten Entwicklung und [{\dots}]},
  langid = {ngerman},
  file = {C:\Users\benja\Zotero\storage\CVLUXILR\steuerungstechnische-standards-als-fundament-fuer-die-leittechnik.html}
}

@article{hitzlerNeuroSymbolicApproachesArtificial2022,
  title = {Neuro-{{Symbolic Approaches}} in {{Artificial Intelligence}}},
  author = {Hitzler, Pascal and Eberhart, Aaron and Ebrahimi, Monireh and Sarker, Md Kamruzzaman and Zhou, Lu},
  year = {2022},
  month = mar,
  journal = {National Science Review},
  doi = {10.1093/nsr/nwac035},
  abstract = {Neuro-Symbolic Artificial Intelligence refers to a field of research and applications that combines machine learning methods based on artificial neural networks, such as deep learning, with symbolic approaches to computing and Artificial Intelligence (AI), as can be found for example in the AI subfield of Knowledge Representation and Reasoning. Neuro-Symbolic AI has a long history, however it remained a rather niche topic until recently, when landmark advances in machine learning -- prompted by deep learning -- caused a significant rise in interest and research activity in combining neural and symbolic methods. In this overview, we provide a rough guide to key research directions, and literature pointers for anybody interested in learning more about the field.}
}

@book{hitzlerNeuroSymbolicArtificialIntelligence2022,
  title = {{Neuro-Symbolic Artificial Intelligence: The State of the Art}},
  shorttitle = {{Neuro-Symbolic Artificial Intelligence}},
  author = {Hitzler and P and Sarker and M.K},
  year = {2022},
  month = jan,
  publisher = {IOS Press},
  address = {Washington},
  isbn = {978-1-64368-244-0},
  langid = {Englisch}
}

@article{Hoch01,
  title = {Gradient {{Flow}} in {{Recurrent Nets}}: {{The Difficulty}} of {{Learning Long-Term Dependencies}}},
  author = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, J{\"u}rgen},
  year = {2001},
  publisher = {A field guide to dynamical recurrent neural networks. IEEE Press}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Comput.},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2019-07-23},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}

@article{hoebertKnowledgedrivenFrameworkIndustrial2023,
  title = {Knowledge-Driven Framework for Industrial Robotic Systems},
  author = {Hoebert, Timon and Lepuschitz, Wilfried and Vincze, Markus and Merdan, Munir},
  year = {2023},
  month = feb,
  journal = {Journal of Intelligent Manufacturing},
  volume = {34},
  number = {2},
  pages = {771--788},
  issn = {1572-8145},
  doi = {10.1007/s10845-021-01826-8},
  urldate = {2024-09-17},
  abstract = {Due to their advantages, there is an increase of applying robotic systems for small batch production as well as for complex manufacturing processes. However, programming and configuring robots is time and resource consuming while being also accompanied by high costs that are especially challenging for small- and medium-sized enterprises. The current way of programming industrial robots by using teach-in control devices and/or using vendor-specific programming languages is in general a complex activity that requires extensive knowledge in the robotics domain. It is therefore important to offer new practical methods for the programming of industrial robots that provide flexibility and versatility in order to achieve feasible robotics solutions for small lot size productions. This paper focuses on the development of a knowledge-driven framework, which should overcome the limitations of state-of-the-art robotics solutions and enhance the agility and autonomy of industrial robotics systems using ontologies as a knowledge-source. The framework includes reasoning and perception abilities as well as the ability to generate plans, select appropriate actions, and finally execute these actions. In this context, a challenge is the fusion of vision system information with the decision-making component, which can use this information for generating the assembly tasks and executable programs. The introduced product model in the form of an ontology enables that the framework can semantically link perception data to product models to consequently derive handling operations and required tools. Besides, the framework enables an easier adaption of robot-based production systems for individualized production, which requires swift configuration and efficient planning. The presented approach is demonstrated in a laboratory environment with an industrial pilot test case. Our application shows the potential to reduce the efforts needed to program robots in an automated production environment. In this context, the benefits as well as shortcomings of the approach are also discussed in the paper.},
  langid = {english},
  keywords = {Artificial Intelligence,Automated planning,Industrial robot,Ontology,Perception},
  file = {C:\Users\benja\Zotero\storage\P4G8RC3U\Hoebert et al. - 2023 - Knowledge-driven framework for industrial robotic systems.pdf}
}

@inproceedings{hoferCoupledLearningAction2016,
  title = {Coupled Learning of Action Parameters and Forward Models for Manipulation},
  booktitle = {{{IROS}}},
  author = {Hofer, Sebastian and Brock, Oliver},
  year = {2016},
  month = oct,
  pages = {3893--3899},
  publisher = {IEEE},
  address = {Daejeon, South Korea},
  doi = {10.1109/IROS.2016.7759573},
  urldate = {2019-05-17},
  abstract = {The effectiveness of robot interaction depends on the robot's ability to perform task-relevant actions and on the degree to which it is able to predict the outcomes of these actions. In this paper we argue that the two learning problems -- learning actions and learning forward models -- must be tightly coupled for each of them to be successful. We present an approach that is able to learn a set of continuous action parameters and relational forward models from the robot's own experience. We formalize our approach as simultaneously clustering experiences in a continuous and a relational representation. Our experiments in a simulated manipulation experiment show that this form of coupled subsymbolic and symbolic learning is required for the robot to acquire taskrelevant action capabilities.},
  isbn = {978-1-5090-3762-9},
  langid = {english}
}

@article{hoffmanMeasuresExplainableAI2023,
  title = {Measures for {{Explainable AI}}: {{Explanation Goodness}}, {{User Satisfaction}}, {{Mental Models}}, {{Curiosity}}, {{Trust}}, and {{Human-AI Performance}}},
  shorttitle = {Measures for Explainable {{AI}}},
  author = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
  year = {2023},
  month = feb,
  journal = {Frontiers in Computer Science},
  volume = {5},
  publisher = {Frontiers},
  issn = {2624-9898},
  doi = {10.3389/fcomp.2023.1096257},
  urldate = {2024-03-25},
  abstract = {If a user is presented an AI system that portends to explain how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explainable AI system (XAI) is any good? This entails some key concepts of measurement. We present specific methods for enabling developers and researchers to: (1) Assess the a priori goodness of explanations, (2) Assess users' satisfaction with explanations, (3) Assess user's understanding of an AI system, (4) Assess user's curiosity and need for explanations, (5) Assess whether the user's trust and reliance on the AI are appropriate, and finally, (6) Evaluate how the human-XAI work system performs. The methods we present derives from our integration of extensive research literatures and our own psychometric evaluations.},
  langid = {english},
  keywords = {curiosity,explanation goodness,Mental Models,performance,Trust}
}

@article{Hoffmann2012,
  title = {The Implications of Embodiment for Behavior and Cognition: Animal and Robotic Case Studies},
  author = {Hoffmann, Matej and Pfeifer, Rolf},
  year = {2012},
  journal = {arXiv preprint arXiv:1202.0440},
  eprint = {1202.0440},
  archiveprefix = {arXiv}
}

@incollection{hoffmannEverythingYouAlways2011,
  title = {Everything {{You Always Wanted}} to {{Know}} about {{Planning}}: ({{But Were Afraid}} to {{Ask}})},
  shorttitle = {Everything {{You Always Wanted}} to {{Know}} about {{Planning}}},
  booktitle = {{{KI}} 2011: {{Advances}} in {{Artificial Intelligence}}},
  author = {Hoffmann, J{\"o}rg},
  editor = {Bach, Joscha and Edelkamp, Stefan},
  year = {2011},
  volume = {7006},
  pages = {1--13},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-24455-1_1},
  urldate = {2021-05-30},
  abstract = {Domain-independent planning is one of the long-standing sub-areas of Artificial Intelligence (AI), aiming at approaching human problem-solving flexibility. The area has long had an affinity towards playful illustrative examples, imprinting it on the mind of many a student as an area concerned with the rearrangement of blocks, and with the order in which to put on socks and shoes (not to mention the disposal of bombs in toilets). Working on the assumption that this ``student'' is you -- the readers in earlier stages of their careers -- I herein aim to answer three questions that you surely desired to ask back then already: What is it good for? Does it work? Is it interesting to do research in? Answering the latter two questions in the affirmative (of course!), I outline some of the major developments of the last decade, revolutionizing the ability of planning to scale up, and the understanding of the enabling technology. Answering the first question, I point out that modern planning proves to be quite useful for solving practical problems - including, perhaps, yours.},
  isbn = {978-3-642-24454-4 978-3-642-24455-1},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\JTSN3LHI\Hoffmann - 2011 - Everything You Always Wanted to Know about Plannin.pdf}
}

@misc{hoffmannTrainingComputeOptimalLarge2022,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  year = {2022},
  month = mar,
  number = {arXiv:2203.15556},
  eprint = {2203.15556},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.15556},
  urldate = {2024-01-25},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\ZLU58CSB\2203.html}
}

@article{hoggLearningHierarchicalTask2016,
  title = {Learning {{Hierarchical Task Models}} from {{Input Traces}}},
  author = {Hogg, Chad and {Mu{\~n}oz-Avila}, H{\'e}ctor and Kuter, Ugur},
  year = {2016},
  journal = {Computational Intelligence},
  volume = {32},
  number = {1},
  pages = {3--48},
  issn = {1467-8640},
  doi = {10.1111/coin.12044},
  urldate = {2024-04-24},
  abstract = {We describe HTN-MAKER, an algorithm for learning hierarchical planning knowledge in the form of task-reduction methods for hierarchical task networks (HTNs). HTN-MAKER takes as input a set of planning states from a classical planning domain and plans that are applicable to those states, as well as a set of semantically annotated tasks to be accomplished. The algorithm analyzes this semantic information to determine which portion of the input plans accomplishes a particular task and constructs task-reduction methods based on those analyses. We present theoretical results showing that HTN-MAKER is sound and complete. Our experiments in five well-known planning domains confirm the theoretical results and demonstrate convergence toward a set of HTN methods that can be used to solve any problem expressible as a classical planning problem in that domain, relative to a set of goal types for which tasks have been defined. In three of the five domains, HTN planning with the learned methods scales much better than a modern classical planner.},
  copyright = {{\copyright} 2014 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {automated planning,HTN planning,machine learning},
  file = {C:\Users\benja\Zotero\storage\A3PICJNS\coin.html}
}

@inproceedings{holenaNeuralNetworksSurrogate2010,
  title = {Neural {{Networks}} as {{Surrogate Models}} for {{Measurements}} in {{Optimization Algorithms}}},
  booktitle = {Analytical and {{Stochastic Modeling Techniques}} and {{Applications}}},
  author = {Hole{\v n}a, Martin and Linke, David and Rodemerck, Uwe and Bajer, Luk{\'a}{\v s}},
  editor = {{Al-Begain}, Khalid and Fiems, Dieter and Knottenbelt, William J.},
  year = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {351--366},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-13568-2_25},
  abstract = {The paper deals with surrogate modelling, a modern approach to the optimization of objective functions evaluated via measurements. The approach leads to a substantial decrease of time and costs of evaluation of the objective function, a property that is particularly attractive in evolutionary optimization. The paper recalls common strategies for using surrogate models in evolutionary optimization, and proposes two extensions to those strategies -- extension to boosted surrogate models and extension to using a set of models. These are currently being implemented, in connection with surrogate modelling based on feed-forward neural networks, in a software tool for problem-tailored evolutionary optimization of catalytic materials. The paper presents results of experimentally testing already implemented parts and comparing boosted surrogate models with models without boosting, which clearly confirms the usefulness of both proposed extensions.},
  isbn = {978-3-642-13568-2},
  langid = {english},
  keywords = {boosting,evolutionary optimization,Functions evaluated via measurements,neural networks,surrogate modelling}
}

@misc{honnibalSpaCyIndustrialstrengthNatural,
  title = {{{spaCy}}: {{Industrial-strength Natural Language Processing}} ({{NLP}}) in {{Python}}},
  author = {Honnibal, Matthew and Montani, Ines},
  urldate = {2023-08-16},
  howpublished = {https://github.com/explosion/spaCy},
  file = {C:\Users\benja\Zotero\storage\PJGQJTPD\spaCy.html}
}

@misc{honnibalSpacyIndustrialstrengthNatural2024,
  title = {Spacy: {{Industrial-strength Natural Language Processing}} ({{NLP}}) in {{Python}}},
  shorttitle = {Spacy},
  author = {Honnibal, Matthew and Montani, Ines},
  year = {2024},
  month = jul,
  urldate = {2024-07-25},
  copyright = {OSI Approved :: MIT License},
  keywords = {Scientific/Engineering},
  file = {C:\Users\benja\Zotero\storage\DQDRZPRB\spacy.html}
}

@article{hoosProgrammingOptimization2012,
  title = {Programming by Optimization},
  author = {Hoos, Holger H.},
  year = {2012},
  month = feb,
  journal = {Communications of the ACM},
  volume = {55},
  number = {2},
  pages = {70--80},
  issn = {0001-0782},
  doi = {10.1145/2076450.2076469},
  urldate = {2023-06-01},
  abstract = {Avoid premature commitment, seek design alternatives, and automatically generate performance-optimized software.}
}

@article{Hopf82,
  title = {Neural {{Networks}} and {{Physical Systems}} with {{Emergent Collective Computational Abilities}}},
  author = {Hopfield, John J},
  year = {1982},
  volume = {79},
  number = {8},
  pages = {2554--2558}
}

@article{hopfieldNeuralNetworksPhysical1982,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities},
  author = {Hopfield, J. J.},
  year = {1982},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {79},
  number = {8},
  pages = {2554--2558},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.79.8.2554},
  urldate = {2019-07-21},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  langid = {english},
  pmid = {6953413}
}

@article{hoRetinaGANObjectawareApproach2020,
  title = {{{RetinaGAN}}: {{An Object-aware Approach}} to {{Sim-to-Real Transfer}}},
  shorttitle = {{{RetinaGAN}}},
  author = {Ho, Daniel and Rao, Kanishka and Xu, Zhuo and Jang, Eric and Khansari, Mohi and Bai, Yunfei},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.03148 [cs]},
  eprint = {2011.03148},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {The success of deep reinforcement learning (RL) and imitation learning (IL) in vision-based robotic manipulation typically hinges on the expense of large scale data collection. With simulation, data to train a policy can be collected efficiently at scale, but the visual gap between sim and real makes deployment in the real world difficult. We introduce RetinaGAN, a generative adversarial network (GAN) approach to adapt simulated images to realistic ones with object-detection consistency. RetinaGAN is trained in an unsupervised manner without task loss dependencies, and preserves general object structure and texture in adapted images. We evaluate our method on three real world tasks: grasping, pushing, and door opening. RetinaGAN improves upon the performance of prior sim-to-real methods for RL-based object instance grasping and continues to be effective even in the limited data regime. When applied to a pushing task in a similar visual domain, RetinaGAN demonstrates transfer with no additional real data requirements. We also show our method bridges the visual gap for a novel door opening task using imitation learning in a new visual domain. Visit the project website at https://retinagan.github.io/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics,I.2.9},
  file = {C:\Users\benja\Zotero\storage\5GEC792R\2011.html}
}

@article{Horn.1988,
  title = {Closed-Form Solution of Absolute Orientation Using Orthonormal Matrices},
  author = {Horn, Berthold K. P. and Hilden, Hugh M. and Negahdaripour, Shahriar},
  year = {1988},
  journal = {Journal of the Optical Society of America A},
  volume = {5},
  number = {7},
  pages = {1127},
  issn = {1084-7529},
  doi = {10.1364/JOSAA.5.001127},
  pagination = {page}
}

@article{Horn91,
  title = {Approximation {{Capabilities}} of {{Multilayer Feedforward Networks}}},
  author = {Hornik, Kurt},
  year = {1991},
  volume = {4},
  number = {2},
  pages = {251--257}
}

@article{horneNeuralNetworksRobotics1990,
  title = {Neural Networks in Robotics: {{A}} Survey},
  shorttitle = {Neural Networks in Robotics},
  author = {Horne, Bill and Jamshidi, M. and Vadiee, Nader},
  year = {1990},
  month = mar,
  journal = {Journal of Intelligent and Robotic Systems},
  volume = {3},
  number = {1},
  pages = {51--66},
  issn = {1573-0409},
  doi = {10.1007/BF00368972},
  urldate = {2024-04-27},
  abstract = {The purpose of this paper is to provide an overview of the research being done in neural network approaches to robotics, outline the strengths and weaknesses of current approaches, and predict future trends in this area.},
  langid = {english},
  keywords = {Neural networks,nonlinear control,robotics}
}

@article{hornikApproximationCapabilitiesMultilayer1991,
  title = {Approximation Capabilities of Multilayer Feedforward Networks},
  author = {Hornik, Kurt},
  year = {1991},
  journal = {Neural Networks},
  volume = {4},
  number = {2},
  pages = {251--257},
  issn = {08936080},
  doi = {10.1016/0893-6080(91)90009-T},
  urldate = {2019-07-10},
  abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to V(p.) performance criteria, for arbitrary finite input environment measures Jl., provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
  langid = {english}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  urldate = {2024-04-27},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  file = {C:\Users\benja\Zotero\storage\3T2CRDDQ\0893608089900208.html}
}

@inproceedings{horridgeManchesterOWLSyntax2006,
  title = {The {{Manchester OWL Syntax}}},
  booktitle = {{{OWLED}}},
  author = {Horridge, Matthew and Drummond, Nick and Goodwin, John and Rector, Alan L. and Stevens, Robert and Wang, Hai},
  year = {2006},
  abstract = {This paper describes a new syntax that can be used to write OWL ontologies, and fragments of OWL ontologies for presentation and editing purposes. The syntax, which is known as the Manchester OWL Syntax, was developed in response to a demand from a wide range of users, who do not have a Description Logic background, for a``less logician like'' syntax. The Manchester OWL Syntax is derived from the OWL Abstract Syntax, but is less verbose and minimises the use of brackets. This means that it is quick and easy to read and write. The important features of the syntax are discussed, and a reference implementation of a Java based parser is described.},
  keywords = {Abstract syntax,Description logic,Java,Ontology (information science),Reference implementation,Web Ontology Language}
}

@inproceedings{horridgeOWLAPIJava2009,
  title = {The {{OWL API}}: {{A Java API}} for {{Working}} with {{OWL}} 2 {{Ontologies}}},
  shorttitle = {The {{OWL API}}},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{OWL}}: {{Experiences}} and {{Directions}} - {{Volume}} 529},
  author = {Horridge, Matthew and Bechhofer, Sean},
  year = {2009},
  series = {{{OWLED}}'09},
  pages = {49--58},
  publisher = {CEUR-WS.org},
  address = {Aachen, Germany, Germany},
  urldate = {2019-08-13},
  abstract = {This paper presents the OWL API a high level Application Programming Interface (API) for working with OWL 2 ontologies. The API is closely aligned with the OWL 2 structural specification. It supports parsing and rendering in the syntaxes defined in the W3C specification, namely, the Functional Syntax, RDF/XML, OWL/XML and the Manchester OWL Syntax. Finally, the reference implementation of the API, which is written in Java, includes validators for the various OWL 2 profiles - OWL 2 QL, OWL 2 EL and OWL 2 RL.}
}

@inproceedings{horvatAIReadinessIntegrated2023,
  title = {{{AI Readiness}}: {{An Integrated Socio-technical Framework}}},
  shorttitle = {{{AI Readiness}}},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Production Research}} -- {{Americas}}},
  author = {Horvat, Djerdj and Heimberger, Heidi},
  editor = {Deschamps, Fernando and {Pinheiro de Lima}, Edson and {Gouv{\^e}a da Costa}, S{\'e}rgio E. and G. Trentin, Marcelo},
  year = {2023},
  pages = {548--557},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-36121-0_69},
  abstract = {Despite its high potential for increasing efficiency, productivity and automation, the implementation of Artificial Intelligence (AI) technologies in production is still at a very early stage and lacks practical experience, particularly in the small and medium sized enterprise segment. One of the main reasons for that is the lack of knowledge on how ready the companies are for adopting and using AI effectively. While approaches to assessing readiness and even maturity in terms of digitalization or Industry 4.0 are well established and discussed in the literature and used actively in companies' practice, approaches that address AI are still in their infancy. Addressing this gap, this paper presents a qualitative approach to an in-depth analysis and monitoring of the readiness of manufacturing firms for working with AI. Drawing on the literature on technology adoption on the one hand, and the implementation of AI on the other, we develop an integrative framework covering different socio-technical dimensions. For their operationalization, we use a large range of technical, organizational and environmental categories. In order to illustrate the implementation of our approach in practical terms, we present the results of assessing the AI readiness of a German manufacturing company.},
  isbn = {978-3-031-36121-0},
  langid = {english},
  keywords = {AI,Artificial Intelligence,manufacturing,readiness},
  file = {C:\Users\benja\Zotero\storage\T2QHIWL7\Horvat und Heimberger - 2023 - AI Readiness An Integrated Socio-technical Framework.pdf}
}

@article{hoskinsIterativeInversionNeural1992,
  title = {Iterative Inversion of Neural Networks and Its Application to Adaptive Control},
  author = {Hoskins, D. A. and Hwang, J. N. and Vagners, J.},
  year = {1992},
  month = mar,
  journal = {IEEE Transactions on Neural Networks},
  volume = {3},
  number = {2},
  pages = {292--301},
  issn = {1045-9227},
  doi = {10.1109/72.125870},
  abstract = {An iterative constrained inversion technique is used to find the control inputs to the plant. That is, rather than training a controller network and placing this network directly in the feedback or feedforward paths, the forward model of the plant is learned, and iterative inversion is performed on line to generate control commands. The control approach allows the controllers to respond online to changes in the plant dynamics. This approach also attempts to avoid the difficulty of analysis introduced by most current neural network controllers, which place the highly nonlinear neural network directly in the feedback path. A neural network-based model reference adaptive controller is also proposed for systems having significant dynamics between the control inputs and the observed (or desired) outputs and is demonstrated on a simple linear control system. These results are interpreted in terms of the need for a dither signal for on-line identification of dynamic systems.{$<>$}},
  keywords = {Adaptive control,Aerodynamics,computerised control,Control system synthesis,Control systems,dither signal,dynamic systems,Feedback control,forward model,iterative constrained inversion technique,linear control system,linear systems,Lyapunov method,model reference adaptive control systems,neural nets,neural network-based model reference adaptive controller,neural networks,Neural networks,Neurofeedback,on-line identification,Stability,Training data},
  file = {C:\Users\benja\Zotero\storage\KSM7WC75\125870.html}
}

@article{hospedalesMetaLearningNeuralNetworks2021,
  title = {Meta-{{Learning}} in {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}} in {{Neural Networks}}},
  author = {Hospedales, Timothy M and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos J.},
  year = {2021},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3079209},
  abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where a given task is solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many of the conventional challenges of deep learning, including data and computation bottlenecks, as well as the fundamental issue of generalization. In this survey we describe the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning, multi-task learning, and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning including few-shot learning, reinforcement learning and architecture search. Finally, we discuss outstanding challenges and promising areas for future research.},
  keywords = {Deep learning,Few-Shot Learning,Learning-to-Learn,Machine learning algorithms,Meta-Learning,Neural Architecture Search,Neural networks,Optimization,Predictive models,Task analysis,Training,Transfer Learning}
}

@inproceedings{howellCALIPSODifferentiableSolver2023,
  title = {{{CALIPSO}}: {{A Differentiable Solver}} for~{{Trajectory Optimization}} with~{{Conic}} and~{{Complementarity Constraints}}},
  shorttitle = {{{CALIPSO}}},
  booktitle = {Robotics {{Research}}},
  author = {Howell, Taylor A. and Tracy, Kevin and Le Cleac'h, Simon and Manchester, Zachary},
  editor = {Billard, Aude and Asfour, Tamim and Khatib, Oussama},
  year = {2023},
  pages = {504--521},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-25555-7_34},
  abstract = {We present a new solver for non-convex trajectory optimization problems that is specialized for robotics applications. CALIPSO, or the Conic Augmented Lagrangian Interior-Point SOlver, combines several strategies for constrained numerical optimization to natively handle second-order cones and complementarity constraints. It reliably solves challenging motion planning problems that include contact-implicit formulations of impacts and Coulomb friction, thrust limits subject to conic constraints, and state-triggered constraints where general-purpose nonlinear programming solvers like SNOPT and Ipopt fail to converge. Additionally, CALIPSO supports efficient differentiation of solutions with respect to problem data, enabling bi-level optimization applications like auto-tuning of feedback policies. Reliable convergence of the solver is demonstrated on a range of problems from manipulation, locomotion, and aerospace domains. An open-source implementation of this solver is available.},
  isbn = {978-3-031-25555-7},
  langid = {english},
  keywords = {Contact dynamics,Robotics,Trajectory optimization},
  file = {C:\Users\benja\Zotero\storage\3DHYFT3E\Howell et al. - 2023 - CALIPSO A Differentiable Solver forTrajectory Optimization withConic andComplementarity Constrai.pdf}
}

@article{howellTrajectoryOptimizationOptimizationBased2022,
  title = {Trajectory {{Optimization}} with {{Optimization-Based Dynamics}}},
  author = {Howell, Taylor A. and Le Cleac'h, Simon and Singh, Sumeet and Florence, Pete and Manchester, Zachary and Sindhwani, Vikas},
  year = {2022},
  month = jul,
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {3},
  pages = {6750--6757},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3152696},
  urldate = {2024-09-06},
  abstract = {We present a framework for bi-level trajectory optimization in which a system's dynamics are encoded as the solution to a constrained optimization problem and smooth gradients of this lower-level problem are passed to an upper-level trajectory optimizer. This optimization-based dynamics representation enables constraint handling, additional variables, and non-smooth behavior to be abstracted away from the upper-level optimizer, and allows classical unconstrained optimizers to synthesize trajectories for more complex systems. We provide a path-following method for efficient evaluation of constrained dynamics and utilize the implicit-function theorem to compute smooth gradients of this representation. We demonstrate the framework by modeling systems from locomotion, aerospace, and manipulation domains including: acrobot with joint limits, cart-pole subject to Coulomb friction, Raibert hopper, rocket landing with thrust limits, and planar-push task with optimization-based dynamics and then optimize trajectories using iterative LQR.},
  keywords = {Aerodynamics,Dynamics,Friction,Heuristic algorithms,motion and path planning,Optimization,optimization and optimal control,Planning,Task analysis,Trajectory optimization},
  file = {C\:\\Users\\benja\\Zotero\\storage\\7VK625L8\\Howell et al. - 2022 - Trajectory Optimization with Optimization-Based Dynamics.pdf;C\:\\Users\\benja\\Zotero\\storage\\ZMTDV8NY\\9718153.html}
}

@article{hsuWhatsLeftConcept2023,
  title = {What's {{Left}}? {{Concept Grounding}} with {{Logic-Enhanced Foundation Models}}},
  shorttitle = {What's {{Left}}?},
  author = {Hsu, Joy and Mao, Jiayuan and Tenenbaum, Josh and Wu, Jiajun},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {38798--38814},
  urldate = {2024-08-16},
  langid = {english}
}

@book{Huang.2007,
  title = {Two- and Three-Dimensional Methods for Inspection and Metrology v},
  editor = {Huang, Peisen S.},
  year = {2007},
  series = {{{SPIE}} Proceedings},
  publisher = {SPIE}
}

@inproceedings{huangCode3SystemEndtoEnd2017,
  title = {Code3: {{A System}} for {{End-to-End Programming}} of {{Mobile Manipulator Robots}} for {{Novices}} and {{Experts}}},
  shorttitle = {Code3},
  booktitle = {Proceedings of the 2017 {{ACM}}/{{IEEE International Conference}} on {{Human-Robot Interaction}}},
  author = {Huang, Justin and Cakmak, Maya},
  year = {2017},
  month = mar,
  series = {{{HRI}} '17},
  pages = {453--462},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2909824.3020215},
  urldate = {2024-08-15},
  abstract = {This paper introduces Code3, a system for user-friendly, rapid programming of mobile manipulator robots. The system is designed to let non-roboticists and roboticists alike program end-to-end manipulation tasks. To accomplish this, Code3 provides three integrated components for perception, manipulation, and high-level programming. The perception component helps users define a library of object and scene parts that the robot can later detect. The manipulation component lets users define actions for manipulating objects or scene parts through programming by demonstration. Finally, the high-level programming component provides a drag-and-drop interface with which users can program the logic and control flow to accomplish a task using their previously specified perception and manipulation capabilities. We present findings from an observational user study with non-roboticist programmers (N=10) that demonstrate their ability to quickly learn Code3 and program a PR2 robot to do manipulation tasks. We also demonstrate how the system is expressive enough for an expert to rapidly program highly complex manipulation tasks like playing tic-tac-toe and reconfiguring an object to be graspable.},
  isbn = {978-1-4503-4336-7}
}

@article{huangKernelizedMovementPrimitives2019,
  title = {Kernelized Movement Primitives},
  author = {Huang, Yanlong and Rozo, Leonel and Silv{\'e}rio, Jo{\~a}o and Caldwell, Darwin G},
  year = {2019},
  month = jun,
  journal = {The International Journal of Robotics Research},
  volume = {38},
  number = {7},
  pages = {833--852},
  publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  doi = {10.1177/0278364919846363},
  urldate = {2024-04-17},
  abstract = {Imitation learning has been studied widely as a convenient way to transfer human skills to robots. This learning approach is aimed at extracting relevant motion patterns from human demonstrations and subsequently applying these patterns to different situations. Despite the many advancements that have been achieved, solutions for coping with unpredicted situations (e.g., obstacles and external perturbations) and high-dimensional inputs are still largely absent. In this paper, we propose a novel kernelized movement primitive (KMP), which allows the robot to adapt the learned motor skills and fulfill a variety of additional constraints arising over the course of a task. Specifically, KMP is capable of learning trajectories associated with high-dimensional inputs owing to the kernel treatment, which in turn renders a model with fewer open parameters in contrast to methods that rely on basis functions. Moreover, we extend our approach by exploiting local trajectory representations in different coordinate systems that describe the task at hand, endowing KMP with reliable extrapolation capabilities in broader domains. We apply KMP to the learning of time-driven trajectories as a special case, where a compact parametric representation describing a trajectory and its first-order derivative is utilized. In order to verify the effectiveness of our method, several examples of trajectory modulations and extrapolations associated with time inputs, as well as trajectory adaptations with high-dimensional inputs are provided.},
  langid = {english}
}

@article{huangNeuralTaskGraphs2018,
  title = {Neural {{Task Graphs}}: {{Generalizing}} to {{Unseen Tasks}} from a {{Single Video Demonstration}}},
  shorttitle = {Neural {{Task Graphs}}},
  author = {Huang, De-An and Nair, Suraj and Xu, Danfei and Zhu, Yuke and Garg, Animesh and {Fei-Fei}, Li and Savarese, Silvio and Niebles, Juan Carlos},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.03480 [cs]},
  eprint = {1807.03480},
  primaryclass = {cs},
  urldate = {2019-06-30},
  abstract = {Our goal is to generate a policy to complete an unseen task given just a single video demonstration of the task in a given domain. We hypothesize that to successfully generalize to unseen complex tasks from a single video demonstration, it is necessary to explicitly incorporate the compositional structure of the tasks into the model. To this end, we propose Neural Task Graph (NTG) Networks, which use conjugate task graph as the intermediate representation to modularize both the video demonstration and the derived policy. We empirically show NTG achieves inter-task generalization on two complex tasks: Block Stacking in BulletPhysics and Object Collection in AI2-THOR. NTG improves data efficiency with visual input as well as achieve strong generalization without the need for dense hierarchical supervision. We further show that similar performance trends hold when applied to real-world data. We show that NTG can effectively predict task structure on the JIGSAWS surgical dataset and generalize to unseen tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics}
}

@inproceedings{huangNeuralTaskGraphs2019,
  title = {Neural {{Task Graphs}}: {{Generalizing}} to {{Unseen Tasks From}} a {{Single Video Demonstration}}},
  shorttitle = {Neural {{Task Graphs}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, D. and Nair, S. and Xu, D. and Zhu, Y. and Garg, A. and {Fei-Fei}, L. and Savarese, S. and Niebles, J. C.},
  year = {2019},
  month = jun,
  pages = {8557--8566},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.00876},
  abstract = {Our goal is to generate a policy to complete an unseen task given just a single video demonstration of the task in a given domain. We hypothesize that to successfully generalize to unseen complex tasks from a single video demonstration, it is necessary to explicitly incorporate the compositional structure of the tasks into the model. To this end, we propose Neural Task Graph (NTG) Networks, which use conjugate task graph as the intermediate representation to modularize both the video demonstration and the derived policy. We empirically show NTG achieves inter-task generalization on two complex tasks: Block Stacking in BulletPhysics and Object Collection in AI2-THOR. NTG improves data efficiency with visual input as well as achieve strong generalization without the need for dense hierarchical supervision. We further show that similar performance trends hold when applied to real-world data. We show that NTG can effectively predict task structure on the JIGSAWS surgical dataset and generalize to unseen tasks.},
  keywords = {conjugate task graph,graph theory,inter-task generalization,intermediate representation,JIGSAWS surgical dataset,learning (artificial intelligence),network theory (graphs),neural task graph networks,NTG,Robotics + Driving,single video demonstration,task structure,unseen complex tasks,video signal processing,Visual Reasoning},
  file = {C\:\\Users\\benja\\Zotero\\storage\\29FTP558\\8953382.html;C\:\\Users\\benja\\Zotero\\storage\\EZ7AT2AL\\8953382.html}
}

@inproceedings{huangSkillBasedProgrammingSystem2018,
  title = {A {{Skill-Based Programming System}} for {{Robotic Furniture Assembly}}},
  booktitle = {2018 {{IEEE}} 16th {{International Conference}} on {{Industrial Informatics}} ({{INDIN}})},
  author = {Huang, Pei-Chi and Hsieh, Yi-Hsuan and Mok, Aloysius K.},
  year = {2018},
  month = jul,
  pages = {355--361},
  issn = {2378-363X},
  doi = {10.1109/INDIN.2018.8472030},
  abstract = {Ready-to-assemble furniture is a popular trend for today's furniture companies such as IKEA due to its relative lower price and easier delivery to customers than assembled furniture. However, assembling furniture from an instruction manual by customers themselves is a tedious task. With the advance in robotics in recent years, having a robot to perform the furniture assembly is a viable idea but the cost of robotic furniture assembly is a barrier, as the overhead of using artificial intelligence techniques such as deep learning can be prohibitive. This paper presents a robotic system with a library of assembly skills that are acquired by machine learning and can be reused for different furniture sets. By applying these skills, the robot can be programmed to automatically perform the assembly task. We describe how to design a robotic task programming system that supports composition of skills and how to specify a complex assembly task to be completed by the robot with its skill set.},
  keywords = {artificial intelligence,assembled furniture,assembling,assembling furniture,assembly skills,Cameras,complex assembly task,furniture companies,furniture industry,Grippers,intelligent robots,learning (artificial intelligence),ready-to-assemble furniture,robot programming,Robot sensing systems,Robotic assembly,robotic furniture assembly,robotic system,robotic task programming system,skill-based programming system,Target recognition,Task analysis},
  file = {C:\Users\benja\Zotero\storage\8KPRE3FF\8472030.html}
}

@misc{huangSurveyHallucinationLarge2023,
  title = {A {{Survey}} on {{Hallucination}} in {{Large Language Models}}: {{Principles}}, {{Taxonomy}}, {{Challenges}}, and {{Open Questions}}},
  shorttitle = {A {{Survey}} on {{Hallucination}} in {{Large Language Models}}},
  author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
  year = {2023},
  month = nov,
  number = {arXiv:2311.05232},
  eprint = {2311.05232},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.05232},
  urldate = {2024-03-21},
  abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\benja\Zotero\storage\AAPHECYF\2311.html}
}

@article{huangSurveySafetyTrustworthiness2020,
  title = {A Survey of Safety and Trustworthiness of Deep Neural Networks: {{Verification}}, Testing, Adversarial Attack and Defence, and Interpretability},
  shorttitle = {A Survey of Safety and Trustworthiness of Deep Neural Networks},
  author = {Huang, Xiaowei and Kroening, Daniel and Ruan, Wenjie and Sharp, James and Sun, Youcheng and Thamo, Emese and Wu, Min and Yi, Xinping},
  year = {2020},
  month = aug,
  journal = {Computer Science Review},
  volume = {37},
  pages = {100270},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2020.100270},
  urldate = {2022-04-30},
  abstract = {In the past few years, significant progress has been made on deep neural networks (DNNs) in achieving human-level performance on several long-standing tasks. With the broader deployment of DNNs on various applications, the concerns over their safety and trustworthiness have been raised in public, especially after the widely reported fatal incidents involving self-driving cars. Research to address these concerns is particularly active, with a significant number of papers released in the past few years. This survey paper conducts a review of the current research effort into making DNNs safe and trustworthy, by focusing on four aspects: verification, testing, adversarial attack and defence, and interpretability. In total, we survey 202 papers, most of which were published after 2017.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\YMXRSMLB\S1574013719302527.html}
}

@inproceedings{huangVisualLanguageMaps2023,
  title = {Visual {{Language Maps}} for {{Robot Navigation}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Huang, Chenguang and Mees, Oier and Zeng, Andy and Burgard, Wolfram},
  year = {2023},
  month = may,
  pages = {10608--10615},
  doi = {10.1109/ICRA48891.2023.10160969},
  urldate = {2024-06-16},
  abstract = {Grounding language to the visual observations of a navigating agent can be performed using off-the-shelf visual-language models pretrained on Internet-scale data (e.g., image captions). While this is useful for matching images to natural language descriptions of object goals, it remains disjoint from the process of mapping the environment, so that it lacks the spatial precision of classic geometric maps. To address this problem, we propose VLMaps, a spatial map representation that directly fuses pretrained visual-language features with a 3D reconstruction of the physical world. VLMaps can be autonomously built from video feed on robots using standard exploration approaches and enables natural language indexing of the map without additional labeled data. Specifically, when combined with large language models (LLMs), VLMaps can be used to (i) translate natural language commands into a sequence of open-vocabulary navigation goals (which, beyond prior work, can be spatial by construction, e.g., ``in between the sofa and the TV'' or ``three meters to the right of the chair'') directly localized in the map, and (ii) can be shared among multiple robots with different embodiments to generate new obstacle maps on-the-fly (by using a list of obstacle categories). Extensive experiments carried out in simulated and real-world environments show that VLMaps enable navigation according to more complex language instructions than existing methods. Videos are available at https://vlmaps.github.io.},
  keywords = {Grounding,Meters,Natural languages,Navigation,Three-dimensional displays,TV,Visualization}
}

@inproceedings{hubenSparseAutoencodersFind2023,
  title = {Sparse {{Autoencoders Find Highly Interpretable Features}} in {{Language Models}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Huben, Robert and Cunningham, Hoagy and Smith, Logan Riggs and Ewart, Aidan and Sharkey, Lee},
  year = {2023},
  month = oct,
  urldate = {2024-05-02},
  abstract = {One of the roadblocks to a better understanding of neural networks' internals is {\textbackslash}textit\{polysemanticity\}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is {\textbackslash}textit\{superposition\}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task {\textbackslash}citep\{wang2022interpretability\} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
  langid = {english}
}

@inproceedings{huChainQueenRealTimeDifferentiable2019,
  title = {{{ChainQueen}}: {{A Real-Time Differentiable Physical Simulator}} for {{Soft Robotics}}},
  shorttitle = {{{ChainQueen}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Hu, Yuanming and Liu, Jiancheng and Spielberg, Andrew and Tenenbaum, Joshua B. and Freeman, William T. and Wu, Jiajun and Rus, Daniela and Matusik, Wojciech},
  year = {2019},
  month = may,
  pages = {6265--6271},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2019.8794333},
  urldate = {2024-05-24},
  abstract = {Physical simulators have been widely used in robot planning and control. Among them, differentiable simulators are particularly favored, as they can be incorporated into gradient-based optimization algorithms that are efficient in solving inverse problems such as optimal control and motion planning. Therefore, rigid body simulators and recently their differentiable variants are studied extensively. Simulating deformable objects is, however, more challenging compared to rigid body dynamics. The underlying physical laws of deformable objects are more complex, and the resulting systems have orders of magnitude more degrees of freedom and there-fore they are significantly more computationally expensive to simulate. Computing gradients with respect to physical design or controller parameters is typically even more computationally challenging. In this paper, we propose a real-time, differentiable hybrid Lagrangian-Eulerian physical simulator for deformable objects, ChainQueen, based on the Moving Least Squares Material Point Method (MLS-MPM). MLS-MPM can simulate deformable objects with collisions and can be seamlessly incorporated into soft robotic systems. We demonstrate that our simulator achieves high precision in both forward simulation and backward gradient computation. We have successfully employed it in a diverse set of inference, control and co-design tasks for soft robotics.},
  keywords = {Computational modeling,Graphics processing units,Inverse problems,Planning,Soft robotics,Three-dimensional displays}
}

@inproceedings{huDeepLearningAssistedHighResolution2020,
  title = {Deep-{{Learning Assisted High-Resolution Binocular Stereo Depth Reconstruction}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Hu, Yaoyu and Zhen, Weikun and Scherer, Sebastian},
  year = {2020},
  month = may,
  pages = {8637--8643},
  issn = {2577-087X},
  doi = {10.1109/ICRA40945.2020.9196655},
  urldate = {2024-04-12},
  abstract = {This work presents dense stereo reconstruction using high-resolution images for infrastructure inspections. The state-of-the-art stereo reconstruction methods, both learning and non-learning ones, consume too much computational resource on high-resolution data. Recent learning-based methods achieve top ranks on most benchmarks. However, they suffer from the generalization issue due to lack of task-specific training data. We propose to use a less resource demanding non-learning method, guided by a learning-based model, to handle high-resolution images and achieve accurate stereo reconstruction. The deep-learning model produces an initial disparity prediction with uncertainty for each pixel of the down-sampled stereo image pair. The uncertainty serves as a self-measurement of its generalization ability and the perpixel searching range around the initially predicted disparity. The downstream process performs a modified version of the Semi-Global Block Matching method with the up-sampled perpixel searching range. The proposed deep-learning assisted method is evaluated on the Middlebury dataset and high-resolution stereo images collected by our customized binocular stereo camera. The combination of learning and non-learning methods achieves better performance on 12 out of 15 cases of the Middlebury dataset. In our infrastructure inspection experiments, the average 3D reconstruction error is less than 0.004m.},
  keywords = {Computational modeling,Image reconstruction,Image resolution,Predictive models,Proposals,Three-dimensional displays,Uncertainty},
  file = {C:\Users\benja\Zotero\storage\I57FUFB6\9196655.html}
}

@inproceedings{huDiffTaichiDifferentiableProgramming2019,
  title = {{{DiffTaichi}}: {{Differentiable Programming}} for {{Physical Simulation}}},
  shorttitle = {{{DiffTaichi}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Hu, Yuanming and Anderson, Luke and Li, Tzu-Mao and Sun, Qi and Carr, Nathan and {Ragan-Kelley}, Jonathan and Durand, Fredo},
  year = {2019},
  month = sep,
  urldate = {2020-10-18},
  abstract = {We study the problem of learning and optimizing through physical simulations via differentiable programming, using our proposed DiffSim programming language and compiler.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\M4344FDC\forum.html}
}

@misc{huGeneralPurposeRobotsFoundation2023,
  title = {Toward {{General-Purpose Robots}} via {{Foundation Models}}: {{A Survey}} and {{Meta-Analysis}}},
  shorttitle = {Toward {{General-Purpose Robots}} via {{Foundation Models}}},
  author = {Hu, Yafei and Xie, Quanting and Jain, Vidhi and Francis, Jonathan and Patrikar, Jay and Keetha, Nikhil and Kim, Seungchan and Xie, Yaqi and Zhang, Tianyi and Zhao, Shibo and Chong, Yu Quan and Wang, Chen and Sycara, Katia and {Johnson-Roberson}, Matthew and Batra, Dhruv and Wang, Xiaolong and Scherer, Sebastian and Kira, Zsolt and Xia, Fei and Bisk, Yonatan},
  year = {2023},
  month = dec,
  number = {arXiv:2312.08782},
  eprint = {2312.08782},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.08782},
  urldate = {2024-04-29},
  abstract = {Building general-purpose robots that can operate seamlessly, in any environment, with any object, and utilizing various skills to complete diverse tasks has been a long-standing goal in Artificial Intelligence. Unfortunately, however, most existing robotic systems have been constrained - having been designed for specific tasks, trained on specific datasets, and deployed within specific environments. These systems usually require extensively-labeled data, rely on task-specific models, have numerous generalization issues when deployed in real-world scenarios, and struggle to remain robust to distribution shifts. Motivated by the impressive open-set performance and content generation capabilities of web-scale, large-capacity pre-trained models (i.e., foundation models) in research fields such as Natural Language Processing (NLP) and Computer Vision (CV), we devote this survey to exploring (i) how these existing foundation models from NLP and CV can be applied to the field of robotics, and also exploring (ii) what a robotics-specific foundation model would look like. We begin by providing an overview of what constitutes a conventional robotic system and the fundamental barriers to making it universally applicable. Next, we establish a taxonomy to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics. Finally, we discuss key challenges and promising future directions in using foundation models for enabling general-purpose robotic systems. We encourage readers to view our living GitHub repository of resources, including papers reviewed in this survey as well as related projects and repositories for developing foundation models for robotics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\FNWCP9IC\2312.html}
}

@article{Hui.2017,
  title = {Visual Tracking of Deformation and Classification of Non-Rigid Objects with Robot Hand Probing},
  author = {Hui, Fei and Payeur, Pierre and Cretu, Ana-Maria},
  year = {2017},
  journal = {Robotics},
  volume = {6},
  number = {1},
  pages = {5},
  doi = {10.3390/robotics6010005},
  pagination = {page}
}

@article{huLearningDemonstrationsMultiLevel2022,
  title = {Learning {{From Demonstrations Via Multi-Level}} and {{Multi-Attention Domain-Adaptive Meta-Learning}}},
  author = {Hu, Ziye and Gan, Zhongxue and Li, Wei and Guo, Weikun and Gao, Xiang and Zhu, Jiwei},
  year = {2022},
  month = oct,
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {4},
  pages = {11910--11917},
  issn = {2377-3766},
  doi = {10.1109/LRA.2022.3207558},
  urldate = {2024-06-15},
  abstract = {Despite significant advances in few-shot classification, object detection, or speech recognition in recent years, training an effective robot to adapt to previously unseen environments in a small data regime is still a long-lasting problem for learning from demonstrations (LfD). A promising solution is meta-learning. However, we notice that simply constructing a model with a more complicated and deeper network via previous meta-learning methods does not perform well as we expected. One possible reason is that the shallow features are gradually lost as the network deepens, while these shallow features play an essential role in the adaptation process of meta-learning. Thus, we present a novel yet effective Multi-Level and Multi-Attention Domain-Adaptive Domain-Adaptive Meta-Learning (MLMA-DAML) framework, which meta-learns multiple visual features via different attention heads to update the model policy. Once the model is updated, our MLMA-DAML predicts robot actions (e.g., positions of end-effectors) via fully connected layers (FCL). As we notice that directly converting visual signals to robot actions via FCL following prior methods is not robust to perform robot manipulation tasks, we further extend our MLMA-DAML to MLMA-DAML++. The proposed MLMA-DAML++ learns an effective representation of manipulation tasks via an extra goal prediction network with convolutional layers (CL) to predict more reliable robot actions (represented by feature pixels/grids). Extensive experiments on a UR5 robot arm demonstrate that our proposed methods significantly outperform current related state-of-the-art methods in different real-world placing settings.},
  keywords = {Adaptation models,Deep learning methods,Feature extraction,Head,learning from demonstr- ation,meta-learning,Microstrip,Robots,Task analysis,Visualization},
  file = {C:\Users\benja\Zotero\storage\NKRSNPQT\9894675.html}
}

@inproceedings{huraultCertifiedLogicBasedExplainable2023,
  title = {Certified {{Logic-Based Explainable AI}} -- {{The Case}} of~{{Monotonic Classifiers}}},
  booktitle = {Tests and {{Proofs}}},
  author = {Hurault, Aur{\'e}lie and {Marques-Silva}, Joao},
  editor = {Prevosto, Virgile and Seceleanu, Cristina},
  year = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {51--67},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-38828-6_4},
  abstract = {The continued advances in artificial intelligence (AI), including those in machine learning (ML), raise concerns regarding their deployment in high-risk and safety-critical domains. Motivated by these concerns, there have been calls for the verification of systems of AI, including their explanation. Nevertheless, tools for the verification of systems of AI are complex, and so error-prone. This paper describes one initial effort towards the certification of logic-based explainability algorithms, focusing on monotonic classifiers. Concretely, the paper starts by using the proof assistant Coq to prove the correctness of recently proposed algorithms for explaining monotonic classifiers. Then, the paper proves that the algorithms devised for monotonic classifiers can be applied to the larger family of stable classifiers. Finally, confidence code, extracted from the proofs of correctness, is used for computing explanations that are guaranteed to be correct. The experimental results included in the paper show the scalability of the proposed approach for certifying explanations.},
  isbn = {978-3-031-38828-6},
  langid = {english},
  keywords = {Certification,Formal Explainability}
}

@article{husmannModelPredictiveForce2019,
  title = {Model {{Predictive Force Control}} in {{Grinding}} Based on a {{Lightweight Robot}}},
  author = {Husmann, S. and Stemmler, S. and H{\"a}hnel, S. and Vogelgesang, S. and Abel, D. and Bergs, T.},
  year = {2019},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {9th {{IFAC Conference}} on {{Manufacturing Modelling}}, {{Management}} and {{Control MIM}} 2019},
  volume = {52},
  number = {13},
  pages = {1779--1784},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2019.11.459},
  urldate = {2021-02-25},
  abstract = {The extensive effort in manual labour for mould manufacturing is the main reason for outsourcing manufacturing resources to low-wage countries. Furthermore, the international mould and die sector faces a lack of skilled workers due to the unattractive working conditions, for example the stressing, monotonous work and a partly hazardous working atmosphere (e.g. nickel dust). Especially, the finishing process (e.g. grinding, lapping, polishing) of freeform surfaces is still less automated in metal processing. Therefore, many efforts are initiated in order to fully automate the finishing process, but the today's approaches are usually limited on prototypal implemented and cost-intensive machine solutions. This work introduces a finishing control approach for freeform surfaces using a lightweight robot. The robot is used as an actuator as well as a sensor for closed-loop force control. The optimal force trajectory is determined by a CAD-model which describes the geometry of the workpiece. The noisy measured force signal is filtered by a Kalman filter which also estimates the system state for the model predictive controller (MPC). Finally, a simple position controller controls the position of the tool center point (TCP) and the MPC controls the force which occur on the tool. The results show that the force can be controlled with a higher accuracy than with a conventional impedance control, which is commercially provided.},
  langid = {english},
  keywords = {die making,finishing,flexible manufacturing systems,Grinding,model-based control,mould}
}

@article{husseinImitationLearningSurvey2017,
  title = {Imitation {{Learning}}: {{A Survey}} of {{Learning Methods}}},
  shorttitle = {Imitation {{Learning}}},
  author = {Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina},
  year = {2017},
  month = apr,
  journal = {ACM Computing Surveys},
  volume = {50},
  number = {2},
  pages = {21:1--21:35},
  issn = {0360-0300},
  doi = {10.1145/3054912},
  urldate = {2021-09-16},
  abstract = {Imitation learning techniques aim to mimic human behavior in a given task. An agent (a learning machine) is trained to perform a task from demonstrations by learning a mapping between observations and actions. The idea of teaching by imitation has been around for many years; however, the field is gaining attention recently due to advances in computing and sensing as well as rising demand for intelligent applications. The paradigm of learning by imitation is gaining popularity because it facilitates teaching complex tasks with minimal expert knowledge of the tasks. Generic imitation learning methods could potentially reduce the problem of teaching a task to that of providing demonstrations, without the need for explicit programming or designing reward functions specific to the task. Modern sensors are able to collect and transmit high volumes of data rapidly, and processors with high computational power allow fast processing that maps the sensory data to actions in a timely manner. This opens the door for many potential AI applications that require real-time perception and reaction such as humanoid robots, self-driving vehicles, human computer interaction, and computer games, to name a few. However, specialized algorithms are needed to effectively and robustly learn models as learning by imitation poses its own set of challenges. In this article, we survey imitation learning methods and present design options in different steps of the learning process. We introduce a background and motivation for the field as well as highlight challenges specific to the imitation problem. Methods for designing and evaluating imitation learning tasks are categorized and reviewed. Special attention is given to learning methods in robotics and games as these domains are the most popular in the literature and provide a wide array of problems and methodologies. We extensively discuss combining imitation learning approaches using different sources and methods, as well as incorporating other motion learning methods to enhance imitation. We also discuss the potential impact on industry, present major applications, and highlight current and future research directions.},
  keywords = {deep learning,feature representations,Imitation learning,intelligent agents,learning from demonstrations,learning from experience,reinforcement learning,robotics,self-improvement}
}

@book{Hutchison.2010,
  title = {Computer Vision -- {{ECCV}} 2010},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
  year = {2010},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-15558-1},
  isbn = {978-3-642-15557-4}
}

@book{Hutchison.2013,
  title = {Computer Vision -- {{ACCV}} 2012},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Lee, Kyoung Mu and Matsushita, Yasuyuki and Rehg, James M. and Hu, Zhanyi},
  year = {2013},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-37331-2},
  isbn = {978-3-642-37330-5}
}

@article{huynhMetrics3DRotations2009,
  title = {Metrics for {{3D Rotations}}: {{Comparison}} and {{Analysis}}},
  shorttitle = {Metrics for {{3D Rotations}}},
  author = {Huynh, Du Q.},
  year = {2009},
  month = oct,
  journal = {Journal of Mathematical Imaging and Vision},
  volume = {35},
  number = {2},
  pages = {155--164},
  issn = {1573-7683},
  doi = {10.1007/s10851-009-0161-2},
  urldate = {2019-08-23},
  abstract = {3D rotations arise in many computer vision, computer graphics, and robotics problems and evaluation of the distance between two 3D rotations is often an essential task. This paper presents a detailed analysis of six functions for measuring distance between 3D rotations that have been proposed in the literature. Based on the well-developed theory behind 3D rotations, we demonstrate that five of them are bi-invariant metrics on SO(3) but that only four of them are boundedly equivalent to each other. We conclude that it is both spatially and computationally more efficient to use quaternions for 3D rotations. Lastly, by treating the two rotations as a true and an estimated rotation matrix, we illustrate the geometry associated with iso-error measures.},
  langid = {english},
  keywords = {3D rotations,Distance functions,Lie algebra,Matrix Lie group,Quaternions}
}

@article{hwangSolvingInverseProblems,
  title = {Solving {{Inverse Problems}} by {{Bayesian Iterative Inversion}} of {{Neural Networks}}},
  author = {Hwang, Jenq-Neng},
  pages = {16},
  abstract = {Neural Networks have long been applied to inverse problems solving. The literature documents a development from the use of neural networks as explicit inverses, to Neural Network Iterative Inversion (NNII), and nally to Bayesian Neural Network Iterative Inversion (BNNII), which adds a Bayesian superstructure to NNII. Inverse problems have been often considered ill-posed, i.e., the statement of the problem does not thoroughly constrain the solution space. BNNII takes advantage of this lack of information by adding additional informative constraints to the problem solution using Bayesian methodology. Inverse problems in remote sensing applications a ord opportunities for inclusion of prior probabilities, noise distributions, ground truth information, smoothness constraints, and other informative constraints within a Bayesian probabilistic framework. This paper reviews some of these techniques and shows how ground truth information and/or information regarding the particular parameter contour under reconstruction, and information regarding the underlying physical process, can be e ectively added to the problem solution, and thus provides a signi cant performance improvement.},
  langid = {english}
}

@article{hymanAnastomoticLeaksIntestinal2007,
  title = {Anastomotic {{Leaks After Intestinal Anastomosis}}: {{It}}'s {{Later Than You Think}}},
  shorttitle = {Anastomotic {{Leaks After Intestinal Anastomosis}}},
  author = {Hyman, Neil and Manchester, Thomas L. and Osler, Turner and Burns, Betsy and Cataldo, Peter A.},
  year = {2007},
  month = feb,
  journal = {Annals of Surgery},
  volume = {245},
  number = {2},
  pages = {254},
  issn = {0003-4932},
  doi = {10.1097/01.sla.0000225083.27182.85},
  urldate = {2024-04-12},
  abstract = {Purpose:~           Anastomotic leaks are among the most dreaded complications after colorectal surgery. However, problems with definitions and the retrospective nature of previous analyses have been major limitations. We sought to use a prospective database to define the true incidence and presentation of anastomotic leakage after intestinal anastomosis.           Methods:~           A prospective database of two colorectal surgeons was reviewed over a 10-year period (1995--2004). The incidence of leak by surgical site, timing of diagnosis, method of detection, and treatment was noted. Complications were entered prospectively by a nurse practitioner directly involved in patient care. Standardized criteria for diagnosis were used. A logistic regression model was used to discriminate statistical variation.           Results:~           A total of 1223 patients underwent resection and anastomosis during the study period. Mean age was 59.1 years. Leaks occurred in 33 patients (2.7\%). Diagnosis was made a mean of 12.7 days postoperatively, including four beyond 30 days (12.1\%). There was no difference in leak rate by surgeon (3.6\% vs. 2.2\%; P = 0.08). The leak rate was similar by surgical site except for a markedly increased leak rate with ileorectal anastomosis (P = 0.001). Twelve leaks were diagnosed clinically versus 21 radiographically. Contrast enema correctly identified only 4 of 10 leaks, whereas CT correctly identified 17 of 19. A total of 14 of 33 (42\%) patients had their leak diagnosed only after readmission. Fifteen patients required fecal diversion, whereas 18 could be managed nonoperatively.           Conclusions:~           Anastomotic leaks are frequently diagnosed late in the postoperative period and often after initial hospital discharge, highlighting the importance of prospective data entry and adequate follow-up. CT scan is the preferred diagnostic modality when imaging is required. More than half of leaks can be managed without fecal diversion.},
  langid = {american},
  file = {C:\Users\benja\Zotero\storage\DANDLY5A\anastomotic_leaks_after_intestinal_anastomosis_.14.html}
}

@article{ibarzHowTrainYour2021,
  title = {How to {{Train Your Robot}} with {{Deep Reinforcement Learning}}; {{Lessons We}}'ve {{Learned}}},
  author = {Ibarz, Julian and Tan, Jie and Finn, Chelsea and Kalakrishnan, Mrinal and Pastor, Peter and Levine, Sergey},
  year = {2021},
  month = apr,
  journal = {The International Journal of Robotics Research},
  volume = {40},
  number = {4-5},
  eprint = {2102.02915},
  pages = {698--721},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364920987859},
  urldate = {2021-09-20},
  abstract = {Deep reinforcement learning (RL) has emerged as a promising approach for autonomously acquiring complex behaviors from low level sensor observations. Although a large portion of deep RL research has focused on applications in video games and simulated control, which does not connect with the constraints of learning in real environments, deep RL has also demonstrated promise in enabling physical robots to learn complex skills in the real world. At the same time,real world robotics provides an appealing domain for evaluating such algorithms, as it connects directly to how humans learn; as an embodied agent in the real world. Learning to perceive and move in the real world presents numerous challenges, some of which are easier to address than others, and some of which are often not considered in RL research that focuses only on simulated domains. In this review article, we present a number of case studies involving robotic deep RL. Building off of these case studies, we discuss commonly perceived challenges in deep RL and how they have been addressed in these works. We also provide an overview of other outstanding challenges, many of which are unique to the real-world robotics setting and are not often the focus of mainstream RL research. Our goal is to provide a resource both for roboticists and machine learning researchers who are interested in furthering the progress of deep RL in the real world.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\HI5TV2WY\2102.html}
}

@book{IEEERSJInternationalConferenceonIntelligentRobotsandSystems.2014,
  title = {2014 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems: {{Chicago}}, {{IL}}, Sept. 14-18, 2014},
  year = {2014},
  publisher = {{IEEE / IEEE/RSJ International Conference on Intelligent Robots and Systems and IROS}},
  address = {Piscataway, NJ},
  isbn = {978-1-4799-6933-3}
}

@misc{ieeestandardsassociationISOIECIEEE2023,
  type = {Standard},
  title = {{{ISO}}/{{IEC}}/{{IEEE International Standard}} - {{Systems}} and Software Engineering--{{System}} Life Cycle Processes},
  author = {{IEEE Standards Association}},
  year = {2023},
  month = may,
  number = {15288-2023},
  address = {Piscataway, NJ},
  urldate = {2024-09-17},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\TIFRZBNV\10424.html}
}

@article{IFR.2018,
  title = {Robots Double Worldwide by 2020: 3 Million Industrial Robots Used by 2020},
  author = {{IFR}},
  year = {2018},
  publisher = {International Federation of Robotics},
  address = {Frankfurt},
  lastvisited = {2019-06-17}
}

@article{IFR.2019,
  title = {Roboterdichte: {{USA}} {\"U}bertrifft China Um Mehr Als Das Doppelte},
  author = {{IFR}},
  year = {2019},
  publisher = {International Federation of Robotics},
  address = {Frankfurt},
  lastvisited = {2019-06-17}
}

@article{ijspeertDynamicalMovementPrimitives2013,
  title = {Dynamical {{Movement Primitives}}: {{Learning Attractor Models}} for {{Motor Behaviors}}},
  shorttitle = {Dynamical {{Movement Primitives}}},
  author = {Ijspeert, A. J. and Nakanishi, J. and Hoffmann, H. and Pastor, P. and Schaal, S.},
  year = {2013},
  month = feb,
  journal = {Neural Computation},
  volume = {25},
  number = {2},
  pages = {328--373},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00393},
  abstract = {Nonlinear dynamical systems have been used in many disciplines to model complex behaviors, including biological motor control, robotics, perception, economics, traffic prediction, and neuroscience. While often the unexpected emergent behavior of nonlinear systems is the focus of investigations, it is of equal importance to create goal-directed behavior (e.g., stable locomotion from a system of coupled oscillators under perceptual guidance). Modeling goal-directed behavior with nonlinear systems is, however, rather difficult due to the parameter sensitivity of these systems, their complex phase transitions in response to subtle parameter changes, and the difficulty of analyzing and predicting their long-term behavior; intuition and time-consuming parameter tuning play a major role. This letter presents and reviews dynamical movement primitives, a line of research for modeling attractor behaviors of autonomous nonlinear dynamical systems with the help of statistical learning techniques. The essence of our approach is to start with a simple dynamical system, such as a set of linear differential equations, and transform those into a weakly nonlinear system with prescribed attractor dynamics by means of a learnable autonomous forcing term. Both point attractors and limit cycle attractors of almost arbitrary complexity can be generated. We explain the design principle of our approach and evaluate its properties in several example applications in motor control and robotics.},
  file = {C\:\\Users\\benja\\Zotero\\storage\\QIS28SDY\\Ijspeert et al. - 2013 - Dynamical Movement Primitives Learning Attractor .pdf;C\:\\Users\\benja\\Zotero\\storage\\M8DVKPE5\\6797340.html}
}

@inproceedings{ijspeertLearningRhythmicMovements2002,
  title = {Learning Rhythmic Movements by Demonstration Using Nonlinear Oscillators},
  booktitle = {{{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Ijspeert, Auke Jan and Nakanishi, Jun and Schaal, Stefan},
  year = {2002},
  month = sep,
  volume = {1},
  pages = {958--963},
  doi = {10.1109/IRDS.2002.1041514},
  urldate = {2024-03-13},
  keywords = {Humans,Limit-cycles,Microwave integrated circuits,Motor drives,Nonlinear dynamical systems,Oscillators,Robots,Shape control,Stability,Statistical learning}
}

@book{Inaba.2016,
  title = {Robotics Research},
  editor = {Inaba, Masayuki and Corke, Peter},
  year = {2016},
  series = {Springer Tracts in Advanced Robotics},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-28872-7},
  isbn = {978-3-319-28870-3}
}

@techreport{Indu13,
  title = {Recommendations for {{Implementing}} the {{Strategic Initiative INDUSTRIE}} 4.0},
  author = {Kagermann, Henning and Wahlster, Wolfgang and Helbig, Johannes},
  year = {2013},
  institution = {{National Academy of Science and Engineering}}
}

@misc{Indu17,
  title = {The {{Industrial Internet Consortium}}: {{A Global Not-For-Profit Partnership}} of {{Industry}}, {{Government}} and {{Academia}}},
  year = {2017},
  publisher = {Industrial Internet Consortium},
  urldate = {2017-06-23}
}

@article{innesDifferentiableProgrammingSystem2019,
  title = {A {{Differentiable Programming System}} to {{Bridge Machine Learning}} and {{Scientific Computing}}},
  author = {Innes, Mike and Edelman, Alan and Fischer, Keno and Rackauckas, Chris and Saba, Elliot and Shah, Viral B. and Tebbutt, Will},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.07587 [cs]},
  eprint = {1907.07587},
  primaryclass = {cs},
  urldate = {2020-10-18},
  abstract = {Scientific computing is increasingly incorporating the advancements in machine learning and the ability to work with large amounts of data. At the same time, machine learning models are becoming increasingly sophisticated and exhibit many features often seen in scientific computing, stressing the capabilities of machine learning frameworks. Just as the disciplines of scientific computing and machine learning have shared common underlying infrastructure in the form of numerical linear algebra, we now have the opportunity to further share new computational infrastructure, and thus ideas, in the form of Differentiable Programming. We describe Zygote, a Differentiable Programming system that is able to take gradients of general program structures. We implement this system in the Julia programming language. Our system supports almost all language constructs (control flow, recursion, mutation, etc.) and compiles high-performance code without requiring any user intervention or refactoring to stage computations. This enables an expressive programming model for deep learning, but more importantly, it enables us to incorporate a large ecosystem of libraries in our models in a straightforward way. We discuss our approach to automatic differentiation, including its support for advanced techniques such as mixed-mode, complex and checkpointed differentiation, and present several examples of differentiating programs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {C:\Users\benja\Zotero\storage\7RIWMFIE\1907.html}
}

@article{innesFashionableModellingFlux2018,
  title = {Fashionable {{Modelling}} with {{Flux}}},
  author = {Innes, Mike and Saba, Elliot and Fischer, Keno and Gandhi, Dhairya and Rudilosso, Marco Concetto and Joy, Neethu Mariya and Karmali, Tejan and Pal, Avik and Shah, Viral B.},
  year = {2018},
  journal = {ArXiv},
  abstract = {A framework named Flux is presented that shows how further refinement of the core ideas of machine learning, built upon the foundation of the Julia programming language, can yield an environment that is simple, easily modifiable, and performant. Machine learning as a discipline has seen an incredible surge of interest in recent years due in large part to a perfect storm of new theory, superior tooling, renewed interest in its capabilities. We present in this paper a framework named Flux that shows how further refinement of the core ideas of machine learning, built upon the foundation of the Julia programming language, can yield an environment that is simple, easily modifiable, and performant. We detail the fundamental principles of Flux as a framework for differentiable programming, give examples of models that are implemented within Flux to display many of the language and framework-level features that contribute to its ease of use and high productivity, display internal compiler techniques used to enable the acceleration and performance that lies at the heart of Flux, and finally give an overview of the larger ecosystem that Flux fits inside of.}
}

@incollection{Innmann.2016,
  title = {{{VolumeDeform}}: {{Real-time}} Volumetric Non-Rigid Reconstruction},
  booktitle = {Computer Vision -- {{ECCV}} 2016},
  author = {Innmann, Matthias and Zollh{\"o}fer, Michael and Nie{\ss}ner, Matthias and Theobalt, Christian and Stamminger, Marc},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  series = {Lecture Notes in Computer Science},
  volume = {9912},
  pages = {362--379},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-46484-8_22},
  abstract = {We present a novel approach for the reconstruction of dynamic geometric shapes using a single hand-held consumer-grade RGB-D sensor at real-time rates. Our method does not require a pre-defined shape template to start with and builds up the scene model from scratch during the scanning process. Geometry and motion are parameterized in a unified manner by a volumetric representation that encodes a distance field of the surface geometry as well as the non-rigid space deformation. Motion tracking is based on a set of extracted sparse color features in combination with a dense depth-based constraint formulation. This enables accurate tracking and drastically reduces drift inherent to standard model-to-depth alignment. We cast finding the optimal deformation of space as a non-linear regularized variational optimization problem by enforcing local smoothness and proximity to the input constraints. The problem is tackled in real-time at the camera's capture rate using a data-parallel flip-flop optimization strategy. Our results demonstrate robust tracking even for fast motion and scenes that lack geometric features.},
  bookpagination = {page},
  isbn = {978-3-319-46483-1}
}

@article{IntegrierteEntwicklungSPS,
  title = {{Integrierte Entwicklung von SPS und Roboterprogrammen}},
  pages = {2},
  langid = {ngerman}
}

@article{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = feb,
  journal = {arXiv:1502.03167 [cs]},
  eprint = {1502.03167},
  primaryclass = {cs},
  urldate = {2019-08-11},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{ionescuProgrammingCobotsVoice2021,
  title = {Programming Cobots by Voice: {{A}} Human-Centered, Web-Based Approach},
  shorttitle = {Programming Cobots by Voice},
  author = {Ionescu, Tudor B. and Schlund, Sebastian},
  year = {2021},
  month = jan,
  journal = {Procedia CIRP},
  series = {8th {{CIRP Conference}} of {{Assembly Technology}} and {{Systems}}},
  volume = {97},
  pages = {123--129},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2020.05.213},
  urldate = {2024-08-15},
  abstract = {We present a new voice-based programming approach and software framework for collaborative robots (cobots) based on the Web Speech API, which is now supported by most modern browsers. The framework targets human programmable interfaces (HPIs), which can be used by people with little or no programming experience. The framework follows a meta-programming approach by enabling users to program cobots by voice in addition to using a mouse, tablet or keyboard. Upon a voice instruction (such as move, pick, release, etc.), the framework automates the manual tasks required to manipulate the vendor-provided HPI. The main advantages of this approach are simplified, guided programming, which only requires the knowledge of 5--10 voice instructions; increased programming speed by up to 46\% compared to the manual approach; and the possibility of sharing programs as videos. The open-source framework was evaluated using two application scenarios.},
  keywords = {Assembly,Cobots,Collaborative robots,GUI automation,Robot programming,Speech API,Speech-based programming,Voice-based programming}
}

@inproceedings{irieLSTMGRUHighway2016,
  title = {{{LSTM}}, {{GRU}}, {{Highway}} and a {{Bit}} of {{Attention}}: {{An Empirical Overview}} for {{Language Modeling}} in {{Speech Recognition}}},
  shorttitle = {{{LSTM}}, {{GRU}}, {{Highway}} and a {{Bit}} of {{Attention}}},
  booktitle = {Interspeech 2016},
  author = {Irie, Kazuki and T{\"u}ske, Zolt{\'a}n and Alkhouli, Tamer and Schl{\"u}ter, Ralf and Ney, Hermann},
  year = {2016},
  month = sep,
  pages = {3519--3523},
  doi = {10.21437/Interspeech.2016-491},
  urldate = {2019-08-06},
  abstract = {Popularized by the long short-term memory (LSTM), multiplicative gates have become a standard means to design artificial neural networks with intentionally organized information flow. Notable examples of such architectures include gated recurrent units (GRU) and highway networks. In this work, we first focus on the evaluation of each of the classical gated architectures for language modeling for large vocabulary speech recognition. Namely, we evaluate the highway network, lateral network, LSTM and GRU. Furthermore, the motivation underlying the highway network also applies to LSTM and GRU. An extension specific to the LSTM has been recently proposed with an additional highway connection between the memory cells of adjacent LSTM layers. In contrast, we investigate an approach which can be used with both LSTM and GRU: a highway network in which the LSTM or GRU is used as the transformation function. We found that the highway connections enable both standalone feedforward and recurrent neural language models to benefit better from the deep structure and provide a slight improvement of recognition accuracy after interpolation with count models. To complete the overview, we include our initial investigations on the use of the attention mechanism for learning word triggers.},
  langid = {english}
}

@patent{itzekVerfahrenUndVorrichtung2014,
  title = {Verfahren Und {{Vorrichtung}} Zum {{Polieren}} Eines {{Bauteils}}},
  author = {Itzek, Wolfgang and Marqua{\ss}, Wolfgang and Piontke, Steffen and Wolters, Christian},
  year = {2014},
  month = jul,
  number = {DE102013021389A1},
  urldate = {2021-03-01},
  abstract = {Die Erfindung betrifft ein Verfahren zum Polieren eines Bauteils (12), bei welchem ein Polierger{\"a}t (16) an einem einen Kraft- und/oder Momentensensor umfassenden Leichtbauroboter (14) angebracht und das Polierger{\"a}t (14) mittels des Leichbauroboters (14) mit einer vorgegebenen Kraft in Querrichtung (y) und Hochrichtung (z) an das Bauteil (12) angelegt sowie anschlie{\ss}end in einer Bahnrichtung (x) entlang des Bauteils (12) bewegt wird. Des Weiteren betrifft die Erfindung eine Vorrichtung (10) zum Polieren eines Bauteils (12).},
  assignee = {Daimler AG},
  nationality = {DE},
  keywords = {component,lightweight robot,polishing,robot,transverse}
}

@misc{IXONCloud2020,
  title = {{{IXON Cloud}}},
  year = {2020},
  month = may,
  journal = {IXON},
  urldate = {2020-05-15},
  abstract = {IXON delivers an all-in-one Industrial IoT solution for remote service to machines. PLC Remote Access (VPN), Web HMI, Data Logging, Dashboards, Alarms and more. Discover IXON Cloud.},
  howpublished = {https://www.ixon.cloud/},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\3DVK6I8L\www.ixon.cloud.html}
}

@article{Izadi.2011,
  title = {{{KinectFusion}}},
  author = {Izadi, Shahram and Davison, Andrew and Fitzgibbon, Andrew and Kim, David and Hilliges, Otmar and Molyneaux, David and Newcombe, Richard and Kohli, Pushmeet and Shotton, Jamie and Hodges, Steve and Freeman, Dustin},
  year = {2011},
  journal = {Proc. ISMAR},
  pages = {559},
  doi = {10.1145/2047196.2047270},
  pagination = {page}
}

@article{Jacobson.2018,
  title = {Libigl: {{A}} Simple {{C}}++ Geometry Processing Library},
  author = {Jacobson, Alec and Panozzo, Daniele and others},
  year = {2018},
  lastvisited = {2019-07-22}
}

@article{jacobssonModuleBasedSkillOntology2015,
  title = {A {{Module-Based Skill Ontology}} for {{Industrial Robots}}},
  author = {Jacobsson, Ludwig},
  year = {2015},
  journal = {undefined},
  urldate = {2021-11-19},
  abstract = {A skill representation model is developed by studying, normalizing, extending and modularizing the current skill concepts implemented in a modularized ontology structure and implemented in the LTH knowledge base. With industrial robots ready to take the next step in mastering manufacturing  tasks new approaches to reduce the programing effort are needed. This  is achieved by introducing skills as robot \&quot;know-how\&quot; and using them as a  higher abstraction level of robot instructions during programming. The skills  are reusable items providing motion control and rich declarative descriptions  of complex robot capabilities. Storing the skills requires an adequate knowledge  representation model that enables reuse and reasoning on skills and simplifies  knowledge management. In this thesis we develop a skill representation  model and implement it in the LTH knowledge base. It was developed by  studying, normalizing, extending and modularizing the current skill concepts  in the LTH knowledge base. The developed model is effectively a class hierarchy  of the skill concepts implemented in a modularized ontology structure.  The resulting model clarifies the intrinsic concepts of a skill and presents a  module structure that enables the future development and reuse of skills in  general.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\2SRHW8RI\df64bdf36cf7512899d12719175ae07d9487cfdd.html}
}

@phdthesis{Jaek08,
  title = {Aus {{Beobachtung}} Des {{Menschen}} Gelernte Probabilistische {{Objektmanipulation}} Durch {{Serviceroboter}}},
  author = {J{\"a}kel, Rainer},
  year = {2008},
  school = {Karlsruhe Institute of Technology}
}

@phdthesis{Jaek13,
  title = {Learning of Generalized Manipulation Strategies in Service Robotics},
  author = {J{\"a}kel, Rainer},
  year = {2013},
  school = {Karlsruhe Institute of Technology}
}

@patent{jaekelVerfahrenUndSystem2016,
  title = {{Verfahren und System zur Programmierung eines Roboters}},
  author = {J{\"a}kel, Rainer and Dirschl, Gerhard},
  year = {2016},
  month = may,
  number = {EP3013537A1},
  urldate = {2020-05-12},
  assignee = {Artiminds Robotics GmbH},
  langid = {ngerman},
  keywords = {condition,conditions,execution,movement,robot},
  annotation = {Inventors: \_:n5189}
}

@article{jaferiWhyAnastomosesLeak2021,
  title = {Why {{Do Anastomoses Leak}}?},
  author = {Jaferi, Mehraneh D. and Nfonsam, Valentine and Shogan, Benjamin and Hyman, Neil},
  year = {2021},
  month = oct,
  journal = {Journal of Gastrointestinal Surgery},
  volume = {25},
  number = {10},
  pages = {2728--2731},
  issn = {1873-4626},
  doi = {10.1007/s11605-021-05103-0},
  urldate = {2024-04-12},
  langid = {english},
  keywords = {Anastomoses Leak}
}

@book{Jahne.2012,
  title = {Digitale Bildverarbeitung},
  author = {J{\"a}hne, Bernd},
  year = {2012},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-04952-1},
  isbn = {978-3-642-04951-4}
}

@article{jaiswalUnderstandingSkillsGap2024,
  title = {Understanding the Skills Gap between Higher Education and Industry in the {{UK}} in Artificial Intelligence Sector},
  author = {Jaiswal, Khushi and Kuzminykh, Ievgeniia and Modgil, Sanjay},
  year = {2024},
  month = sep,
  journal = {Industry and Higher Education},
  pages = {09504222241280441},
  publisher = {SAGE Publications Ltd},
  issn = {0950-4222},
  doi = {10.1177/09504222241280441},
  urldate = {2024-09-09},
  abstract = {As Artificial Intelligence (AI) changes how businesses work, there's a growing need for people who can work in this sector. This paper investigates how well universities in United Kingdom offering courses in AI, prepare students for jobs in the real world. To gain insight into the differences between university curricula and industry demands we review the contents of taught courses and job advertisement portals. By using custom data scraping tools to gather information from job advertisements and university curricula, and frequency and Naive Bayes classifier analysis, this study will show exactly what skills industry is looking for. In this study we identified 12 skill categories that were used for mapping. The study showed that the university curriculum in the AI domain is well balanced in most technical skills, including Programming and Machine learning subjects, but have a gap in Data Science and Maths and Statistics sk{\textbackslash}ill categories.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\IF543U66\Jaiswal et al. - 2024 - Understanding the skills gap between higher education and industry in the UK in artificial intellige.pdf}
}

@incollection{jakelLayeredProgrammingDemonstration2012,
  title = {Layered {{Programming}} by {{Demonstration}} and {{Planning}} for {{Autonomous Robot Manipulation}}},
  booktitle = {Advanced {{Bimanual Manipulation}}: {{Results}} from the {{DEXMART Project}}},
  author = {J{\"a}kel, Rainer and R{\"u}hl, Steffen W. and {Schmidt-Rohr}, Sven R. and L{\"o}sch, Martin and Xue, Zhixing and Dillmann, R{\"u}diger},
  editor = {Siciliano, Bruno},
  year = {2012},
  series = {Springer {{Tracts}} in {{Advanced Robotics}}},
  pages = {1--57},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-29041-1_1},
  urldate = {2020-09-18},
  abstract = {We propose a layered system for autonomous planning of complex service robot environment manipulation challenges. Motion planning, logic-based planning and probabilistic mission planning are integrated into a single system and planning models are generated using Programming by [human] Demonstration (PbD). The strength of planning models arises from the flexibility they give the robot in dealing with changing scenes and highly varying sequences of events. This comes at the cost of complex planning model representations and generation, however. Manually engineering very general descriptions covering a large sets of challenges is infeasible as is learning them exclusively by robot self-exploration. Thus, we present PbD for planning models together with generation of parameters from analysis of geometric scene properties to tackle that difficulty. Experimental results show the applicability of these techniques on natural learning and autonomous execution of complex robot manipulation challenges.},
  isbn = {978-3-642-29041-1},
  langid = {english},
  keywords = {Gaussian Mixture Model,Humanoid Robot,Intelligent Robot,Manipulation Task,Service Robot}
}

@phdthesis{jakelLearningGeneralizedManipulation2013,
  title = {Learning of {{Generalized Manipulation Strategies}} in {{Service Robotics}}},
  author = {J{\"a}kel, Rainer},
  year = {2013},
  address = {Karlsruhe},
  urldate = {2019-06-30},
  school = {Karlsruhe Institute of Technology}
}

@inproceedings{jakelRepresentationConstrainedPlanning2010,
  title = {Representation and Constrained Planning of Manipulation Strategies in the Context of {{Programming}} by {{Demonstration}}},
  booktitle = {{{ICRA}}},
  author = {J{\"a}kel, R. and {Schmidt-Rohr}, S. R. and L{\"o}sch, M. and Dillmann, R.},
  year = {2010},
  month = may,
  pages = {162--169},
  doi = {10.1109/ROBOT.2010.5509959},
  abstract = {In Programming by Demonstration, a flexible representation of manipulation motions is necessary to learn and generalize from human demonstrations. In contrast to subsymbolic representations of trajectories, e.g. based on a Gaussian Mixture Model, a partially symbolic representation of manipulation strategies based on a temporal satisfaction problem with domain constraints is developed. By using constrained motion planning and a geometric constraint representation, generalization to different robot systems and new environments is achieved. In order to plan learned manipulation strategies the RRT-based algorithm by Stilman et al. is extended to consider, that multiple sets of constraints are possible during the extension of the search tree.},
  keywords = {Anthropomorphism,Automatic programming,constrained motion planning,constrained planning,constraint handling,domain constraints,Fingers,Gaussian mixture model,Gaussian processes,geometric constraint representation,human demonstrations,Humans,learned manipulation strategy,manipulation motions,manipulators,Motion planning,Orbital robotics,path planning,programming by demonstration,robot programming,Robot programming,Robot sensing systems,robot systems,Robotics and automation,RRT-based algorithm,search tree,Strategic planning,subsymbolic representations,temporal satisfaction problem,tree searching},
  file = {C:\Users\benja\Zotero\storage\8H9RCCHA\5509959.html}
}

@inproceedings{jangBCZZeroShotTask2021,
  title = {{{BC-Z}}: {{Zero-Shot Task Generalization}} with {{Robotic Imitation Learning}}},
  shorttitle = {{{BC-Z}}},
  booktitle = {5th {{Annual Conference}} on {{Robot Learning}}},
  author = {Jang, Eric and Irpan, Alex and Khansari, Mohi and Kappler, Daniel and Ebert, Frederik and Lynch, Corey and Levine, Sergey and Finn, Chelsea},
  year = {2021},
  month = jun,
  urldate = {2021-11-10},
  abstract = {In this paper, we study the problem of enabling a vision-based robotic manipulation system to generalize to novel tasks, a long-standing challenge in robot learning. We approach the challenge from...},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\FC3BBRXQ\forum.html}
}

@inproceedings{jangEndtoEndLearningSemantic2017,
  title = {End-to-{{End Learning}} of {{Semantic Grasping}}},
  booktitle = {{{CoRL}}},
  author = {Jang, Eric and Vijayanarasimhan, Sudheendra and Pastor, Peter and Ibarz, Julian and Levine, Sergey},
  year = {2017},
  month = oct,
  pages = {119--132},
  issn = {1938-7228},
  urldate = {2020-06-30},
  abstract = {We consider the task of semantic robotic grasping, in which a robot picks up an object of a user-specified class using only monocular images. Inspired by the two-stream hypothesis of visual reasoni...},
  chapter = {Machine Learning},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\8IDVMCMT\jang17a.html}
}

@inproceedings{jangEndtoEndLearningSemantic2017a,
  title = {End-to-{{End Learning}} of {{Semantic Grasping}}},
  booktitle = {{{CoRL}}},
  author = {Jang, Eric and Vijayanarasimhan, Sudheendra and Pastor, Peter and Ibarz, Julian and Levine, Sergey},
  year = {2017},
  month = oct,
  pages = {119--132},
  issn = {1938-7228},
  urldate = {2020-06-30},
  abstract = {We consider the task of semantic robotic grasping, in which a robot picks up an object of a user-specified class using only monocular images. Inspired by the two-stream hypothesis of visual reasoni...},
  chapter = {Machine Learning},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\SGD64XTE\jang17a.html}
}

@incollection{janiakExtendedJacobianInverse2008,
  title = {Extended {{Jacobian Inverse Kinematics}} and {{Approximation}} of {{Distributions}}},
  booktitle = {Advances in {{Robot Kinematics}}: {{Analysis}} and {{Design}}},
  author = {Janiak, Mariusz and Tcho{\'n}, Krzysztof},
  editor = {Lenar{\v c}i{\v c}, Jadran and Wenger, Philippe},
  year = {2008},
  pages = {138--146},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-1-4020-8600-7_15},
  urldate = {2024-03-31},
  abstract = {The paper addresses the synthesis problem of repeatable Jacobian inverse kinematics algorithms for robotic manipulators. For the kinematics of redundancy 1 this synthesis is accomplished by defining an extended Jacobian inverse that in certain sense approximates the Jacobian pseudo-inverse. The approximation problem is formulated in differential geometric terms, and solved using the existing results on the approximation of a prescribed codistribution by an integrable codistribution. As an illustration, extended Jacobian inverses are derived for the normal form kinematics of a stationary manipulator and a mobile robot.},
  isbn = {978-1-4020-8600-7},
  langid = {english},
  keywords = {approximation,distribution,extended Jacobian inverse,Jacobian pseudo-inverse,redundancy,robot kinematics}
}

@article{jasimPositionIdentificationForceGuided2014,
  title = {Position {{Identification}} in {{Force-Guided Robotic Peg-in-Hole Assembly Tasks}}},
  author = {Jasim, Ibrahim F. and Plapper, Peter W. and Voos, Holger},
  year = {2014},
  month = jan,
  journal = {Procedia CIRP},
  series = {5th {{CATS}} 2014 - {{CIRP Conference}} on {{Assembly Technologies}} and {{Systems}}},
  volume = {23},
  pages = {217--222},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2014.10.077},
  urldate = {2021-08-06},
  abstract = {Position uncertainty is inevitable in many force-guided robotic assembly tasks. Such uncertainty can cause a significant delay, extra energy expenditure, and may even results in detriments to the mated parts or the robot itself. This article suggests a strategy for identifying the accurate hole position in force-guided robotic peg-in-hole assembly tasks through employing only the captured wrench (the Cartesian forces and torques) signals of the manipulated. In the framework of using the Contact-State (CS) modeling for such robotic tasks, the identification of the hole position is realized through detecting the CS that corresponds for the phase of the peg-on-hole, that is the phase in which the peg is located precisely on the hole. Expectation Maximization-based Gaussian Mixtures Model (EM-GMM) CS modeling scheme is employed in detecting the CS corresponding for the peg-on-hole phase. Only the wrench signals are used in modeling and detecting the phases of the assembly process. The considered peg-in-hole assembly process starts from free space and as soon as the peg touches the environment with missing the hole, a spiral search path is followed that would survey the whole environment surface. When the CS of the peg-on-hole is detected, the hole position is identified. Experiments are conducted on a KUKA Lightweight Robot (LWR) doing typical peg-in-hole assembly tasks. Multiple hole positions are considered and excellent performance of the proposed identification strategy is shown.},
  langid = {english},
  keywords = {Force-guided robots,peg-in-hole assembly,position uncertainty,robotic assembly},
  file = {C:\Users\benja\Zotero\storage\NZEV2TLE\S2212827114011342.html}
}

@phdthesis{Jaspers.2012,
  type = {Diplomarbeit},
  title = {Kombination von Bild- Und Tiefeninformationen F{\"u}r Keypointbasierte Objekterkennung},
  author = {Jaspers, Hanno},
  year = {2012},
  address = {Dortmund},
  school = {Institut f{\"u}r Roboterforschung / Technische Universit{\"a}t Dortmund}
}

@inproceedings{jatavallabhulaBayesianObjectModels2023,
  title = {Bayesian {{Object Models}} for {{Robotic Interaction}} with {{Differentiable Probabilistic Programming}}},
  booktitle = {Proceedings of {{The}} 6th {{Conference}} on {{Robot Learning}}},
  author = {Jatavallabhula, Krishna Murthy and Macklin, Miles and Fox, Dieter and Garg, Animesh and Ramos, Fabio},
  year = {2023},
  month = mar,
  pages = {1563--1574},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-08-24},
  abstract = {A hallmark of human intelligence is the ability to build rich mental models of previously unseen objects from very few interactions. To achieve true, continuous autonomy, robots too must possess this ability. Importantly, to integrate with the probabilistic robotics software stack, such models must encapsulate the uncertainty (resulting from noisy dynamics and observation models) in a prescriptive manner. We present Bayesian Object Models (BOMs): generative (probabilistic) models that encode both the structural and kinodynamic attributes of an object. BOMs are implemented in the form of a differentiable probabilistic program that models latent scene structure, object dynamics, and observation models. This allows for efficient and automated Bayesian inference -- samples (object trajectories) drawn from the BOM are compared with a small set of real-world observations and used to compute a likelihood function. Our model comprises a differentiable tree structure sampler and a differentiable physics engine, enabling gradient computation through this likelihood function. This enables gradient-based Bayesian inference to efficiently update the distributional parameters of our model. BOMs outperform several recent approaches, including differentiable physics-based, gradient-free, and neural inference schemes. Further information at: https://bayesianobjects.github.io/},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\W2FWYM9X\Jatavallabhula et al. - 2023 - Bayesian Object Models for Robotic Interaction wit.pdf}
}

@inproceedings{Javdani.2011,
  title = {Modeling and Perception of Deformable One-Dimensional Objects},
  booktitle = {{{3D}} Is Here: {{Point}} Cloud Library ({{PCL}})},
  author = {Javdani, Shervin and Tandon, Sameep and Tang, Jie and O'Brien, James F. and Abbeel, Pieter},
  editor = {Rusu, Radu Bogdan and Cousins, Steve},
  year = {2011},
  pages = {1607--1614},
  publisher = {IEEE},
  bookpagination = {page},
  isbn = {978-1-61284-386-5}
}

@misc{jax2018github,
  title = {{{JAX}}: Composable Transformations of {{Python}}+{{NumPy}} Programs},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and {Wanderman-Milne}, Skye and Zhang, Qiao},
  year = {2018}
}

@article{Jeannerod1995a,
  title = {Grasping Objects: The Cortical Mechanisms of Visuomotor Transformation.},
  author = {Jeannerod, M and Arbib, M A and Rizzolatti, G and Sakata, H},
  year = {1995},
  month = jul,
  journal = {Trends in neurosciences},
  volume = {18},
  number = {7},
  eprint = {7571012},
  eprinttype = {pubmed},
  pages = {314--20},
  issn = {0166-2236},
  urldate = {2018-05-10},
  abstract = {Grasping requires coding of the object's intrinsic properties (size and shape), and the transformation of these properties into a pattern of distal (finger and wrist) movements. Computational models address this behavior through the interaction of perceptual and motor schemas. In monkeys, the transformation of an object's intrinsic properties into specific grips takes place in a circuit that is formed by the inferior parietal lobule and the inferior premotor area (area F5). Neurons in both these areas code size, shape and orientation of objects, and specific types of grip that are necessary to grasp them. Grasping movements are coded more globally in the inferior parietal lobule, whereas they are more segmented in area F5. In humans, neuropsychological studies of patients with lesions to the parietal lobule confirm that primitive shape characteristics of an object for grasping are analyzed in the parietal lobe, and also demonstrate that this 'pragmatic' analysis of objects is separated from the 'semantic' analysis performed in the temporal lobe.},
  pmid = {7571012}
}

@misc{JedesZehnteFahrzeug,
  title = {{Jedes zehnte Fahrzeug f{\"a}hrt bis 2030 autonom}},
  journal = {Statista},
  urldate = {2022-01-10},
  abstract = {Marktvolumen f{\"u}r autonomes Fahren soll bis 2030 auf 13,7 Milliarden US-Dollar wachsen},
  howpublished = {https://de.statista.com/presse/p/autonomes\_fahren\_2020//},
  langid = {ngerman},
  file = {C:\Users\benja\Zotero\storage\CM57IEV5\autonomes_fahren_2020.html}
}

@misc{jensenIndustrialPolicyAdvanced2023,
  title = {Industrial {{Policy}} for {{Advanced AI}}: {{Compute Pricing}} and the {{Safety Tax}}},
  shorttitle = {Industrial {{Policy}} for {{Advanced AI}}},
  author = {Jensen, Mckay and {Emery-Xu}, Nicholas and Trager, Robert},
  year = {2023},
  month = feb,
  number = {arXiv:2302.11436},
  eprint = {2302.11436},
  primaryclass = {econ, q-fin},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.11436},
  urldate = {2024-03-04},
  abstract = {Using a model in which agents compete to develop a potentially dangerous new technology (AI), we study how changes in the pricing of factors of production (computational resources) affect agents' strategies, particularly their spending on safety meant to reduce the danger from the new technology. In the model, agents split spending between safety and performance, with safety determining the probability of a ``disaster" outcome, and performance determining the agents' competitiveness relative to their peers. For given parameterizations, we determine the theoretically optimal spending strategies by numerically computing Nash equilibria. Using this approach we find that (1) in symmetric scenarios, compute price increases are safety-promoting if and only if the production of performance scales faster than the production of safety; (2) the probability of a disaster can be made arbitrarily low by providing a sufficiently large subsidy to a single agent; (3) when agents differ in productivity, providing a subsidy to the more productive agent is often better for aggregate safety than providing the same subsidy to other agent(s) (with some qualifications, which we discuss); (4) when one agent is much more safety-conscious, in the sense of believing that safety is more difficult to achieve, relative to his competitors, subsidizing that agent is typically better for aggregate safety than subsidizing its competitors; however, subsidizing an agent that is only somewhat more safety-conscious often decreases safety. Thus, although subsidizing a much more safety-conscious, or productive, agent often improves safety as intuition suggests, subsidizing a somewhat more safety-conscious or productive agent can often be harmful.},
  archiveprefix = {arXiv},
  keywords = {Economics - General Economics},
  file = {C:\Users\benja\Zotero\storage\ULQFMAFX\2302.html}
}

@article{jensenInversionFeedforwardNeural1999,
  title = {Inversion of Feedforward Neural Networks: Algorithms and Applications},
  shorttitle = {Inversion of Feedforward Neural Networks},
  author = {Jensen, C.A. and Reed, R.D. and Marks, R.J. and {El-Sharkawi}, M.A. and {Jae-Byung Jung} and Miyamoto, R.T. and Anderson, G.M. and Eggen, C.J.},
  year = {1999},
  month = sep,
  journal = {Proceedings of the IEEE},
  volume = {87},
  number = {9},
  pages = {1536--1549},
  issn = {00189219},
  doi = {10.1109/5.784232},
  urldate = {2019-05-17},
  langid = {english}
}

@article{jhaExploringDeepLearning2021,
  title = {Exploring {{Deep Learning Methods}} for {{Real-Time Surgical Instrument Segmentation}} in {{Laparoscopy}}},
  author = {Jha, Debesh and Ali, Sharib and Tomar, Nikhil Kumar and Riegler, Michael A. and Johansen, Dag and Johansen, Havard D. and Halvorsen, Pal},
  year = {2021},
  month = jul,
  journal = {2021 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI)},
  pages = {1--4},
  publisher = {IEEE},
  address = {Athens, Greece},
  doi = {10.1109/BHI50953.2021.9508610},
  urldate = {2024-04-12},
  abstract = {Minimally Invasive Surgery (MIS) is a surgical intervention used to examine the organs inside the abdomen and has been widely used due to its effectiveness over open surgery. Due to the hardware improvements such as high definition cameras, this procedure has significantly improved and new software methods have open potential for computer-assisted procedures. However, there exists challenges and requirement to improve detection and tracking of the position of the instruments during these surgical procedures. To this end, we evaluate and compare some popular deep learning methods that can potentially be explored for the automated segmentation of surgical instruments in laparoscopy, an important step towards tool tracking. Our experimental results demonstrate that the Dual decoder attention network (DDANet) produces a superior result compared to other recent deep learning methods. DDANet produces a dice coefficient of 0.8739 and mean intersection over union of 0.8183 for the Robust Medical Instrument Segmentation (ROBUST-MIS) Challenge 2019 dataset, at a real-time speed of 101.36 frames per second which is critical for such procedures.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {9781665403580}
}

@article{ji3DConvolutionalNeural2013,
  title = {{{3D Convolutional Neural Networks}} for {{Human Action Recognition}}},
  author = {Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
  year = {2013},
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {35},
  number = {1},
  pages = {221--231},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2012.59},
  urldate = {2024-07-19},
  abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.},
  keywords = {3D convolution,action recognition,Computational modeling,Computer architecture,convolutional neural networks,Deep learning,Feature extraction,Kernel,model combination,Solid modeling,Three dimensional displays,Videos}
}

@article{jiangMultirobotSystemAutonomous2023,
  title = {A Multirobot System for Autonomous Deployment and Recovery of a Blade Crawler for Operations and Maintenance of Offshore Wind Turbine Blades},
  author = {Jiang, Zhengyi and Jovan, Ferdian and Moradi, Peiman and Richardson, Tom and Bernardini, Sara and Watson, Simon and Weightman, Andrew and Hine, Duncan},
  year = {2023},
  journal = {Journal of Field Robotics},
  volume = {40},
  number = {1},
  pages = {73--93},
  issn = {1556-4967},
  doi = {10.1002/rob.22117},
  urldate = {2024-07-27},
  abstract = {Offshore wind farms will play a vital role in the global ambition of net zero energy generation. Future offshore wind farms will be larger and further from the coast, meaning that traditional human-based operations and maintenance approaches will become infeasible due to safety, cost, and skills shortages. The use of remotely operated or autonomous robotic assistants to undertake these activities provides an attractive alternative solution. This paper presents an autonomous multirobot system which is able to transport, deploy and retrieve a wind turbine blade inspection robot using an unmanned aerial vehicle (UAV). The proposed solution is a fully autonomous system including a robot deployment interface for deployment, a mechatronic link-hook module (LHM) for retrieval, both installed on the underside of a UAV, a mechatronic on-load attaching module installed on the robotic payload and an intelligent global mission planner. The LHM is integrated with a 2-DOF hinge that can operate either passively or actively to reduce the swing motion of a slung load by approximately 30\%. The mechatronic modules can be coupled and decoupled by special maneuvers of the UAV, and the intelligent global mission planner coordinates the operations of the UAV and the mechatronic modules for synchronous and seamless actions. For navigation in the vicinity of wind turbine blades, a visual-based localization merged with the location knowledge from Global Navigation Satellite System has been developed. A proof-of-concept system was field tested on a full-size decommissioned wind-turbine blade. The results show that the experimental system is able to deploy and retrieve a robotic payload onto and from a wind turbine blade safely and robustly without the need for human intervention. The vicinity localization and navigation system have shown an accuracy of 0.65 and 0.44 m in the horizontal and vertical directions, respectively. Furthermore, this study shows the feasibility of systems toward autonomous inspection and maintenance of offshore windfarms.},
  copyright = {{\copyright} 2022 The Authors. Journal of Field Robotics published by Wiley Periodicals LLC.},
  langid = {english},
  keywords = {field robotics,multirobot cooperation,systems design,UAVs}
}

@misc{jiangRoboticPerceptionTransparent2023,
  title = {Robotic {{Perception}} of {{Transparent Objects}}: {{A Review}}},
  shorttitle = {Robotic {{Perception}} of {{Transparent Objects}}},
  author = {Jiang, Jiaqi and Cao, Guanqun and Deng, Jiankang and Do, Thanh-Toan and Luo, Shan},
  year = {2023},
  month = oct,
  number = {arXiv:2304.00157},
  eprint = {2304.00157},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.00157},
  urldate = {2024-03-21},
  abstract = {Transparent object perception is a rapidly developing research problem in artificial intelligence. The ability to perceive transparent objects enables robots to achieve higher levels of autonomy, unlocking new applications in various industries such as healthcare, services and manufacturing. Despite numerous datasets and perception methods being proposed in recent years, there is still a lack of in-depth understanding of these methods and the challenges in this field. To address this gap, this article provides a comprehensive survey of the platforms and recent advances for robotic perception of transparent objects. We highlight the main challenges and propose future directions of various transparent object perception tasks, i.e., segmentation, reconstruction, and pose estimation. We also discuss the limitations of existing datasets in diversity and complexity, and the benefits of employing multi-modal sensors, such as RGB-D cameras, thermal cameras, and polarised imaging, for transparent object perception. Furthermore, we identify perception challenges in complex and dynamic environments, as well as for objects with changeable geometries. Finally, we provide an interactive online platform to navigate each reference: {\textbackslash}url\{https://sites.google.com/view/transperception\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\I2ST4T7B\2304.html}
}

@inproceedings{jiangTaskMotionPlanningReinforcement2019,
  title = {Task-{{Motion Planning}} with {{Reinforcement Learning}} for {{Adaptable Mobile Service Robots}}},
  booktitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Jiang, Yuqian and Yang, Fangkai and Zhang, Shiqi and Stone, Peter},
  year = {2019},
  month = nov,
  pages = {7529--7534},
  issn = {2153-0866},
  doi = {10.1109/IROS40897.2019.8967680},
  abstract = {Task-motion planning (TMP) addresses the problem of efficiently generating executable and low-cost task plans in a discrete space such that the (initially unknown) action costs are determined by motion plans in a corresponding continuous space. A task-motion plan for a mobile service robot that behaves in a highly dynamic domain can be sensitive to domain uncertainty and changes, leading to suboptimal behaviors or execution failures. In this paper, we propose a novel framework, TMP-RL, which is an integration of TMP and reinforcement learning (RL), to solve the problem of robust TMP in dynamic and uncertain domains. The robot first generates a low-cost, feasible task-motion plan by iteratively planning in the discrete space and updating relevant action costs evaluated by the motion planner in continuous space. During execution, the robot learns via model-free RL to further improve its task-motion plans. RL enables adaptability to the current domain, but can be costly with regards to experience; using TMP, which does not rely on experience, can jump-start the learning process before executing in the real world. TMP-RL is evaluated in a mobile service robot domain where the robot navigates in an office area, showing significantly improved adaptability to unseen domain dynamics over TMP and task planning (TP)-RL methods.},
  keywords = {adaptable mobile service robots,feasible task-motion plan,iterative methods,learning (artificial intelligence),low-cost task plans,mobile robots,mobile service robot domain,path planning,reinforcement learning,service robots,task planning-RL methods,task-motion planning,TMP-RL},
  file = {C:\Users\benja\Zotero\storage\TVDN3Y6U\8967680.html}
}

@article{jiangTaskPlanningRobotics2019,
  title = {Task Planning in Robotics: An Empirical Comparison of {{PDDL-}} and {{ASP-based}} Systems},
  shorttitle = {Task Planning in Robotics},
  author = {Jiang, Yu-qian and Zhang, Shi-qi and Khandelwal, Piyush and Stone, Peter},
  year = {2019},
  month = mar,
  journal = {Frontiers of Information Technology \& Electronic Engineering},
  volume = {20},
  number = {3},
  pages = {363--373},
  issn = {2095-9230},
  doi = {10.1631/FITEE.1800514},
  urldate = {2021-05-29},
  abstract = {Robots need task planning algorithms to sequence actions toward accomplishing goals that are impossible through individual actions. Off-the-shelf task planners can be used by intelligent robotics practitioners to solve a variety of planning problems. However, many different planners exist, each with different strengths and weaknesses, and there are no general rules for which planner would be best to apply to a given problem. In this study, we empirically compare the performance of state-of-the-art planners that use either the planning domain description language (PDDL) or answer set programming (ASP) as the underlying action language. PDDL is designed for task planning, and PDDL-based planners are widely used for a variety of planning problems. ASP is designed for knowledge-intensive reasoning, but can also be used to solve task planning problems. Given domain encodings that are as similar as possible, we find that PDDL-based planners perform better on problems with longer solutions, and ASP-based planners are better on tasks with a large number of objects or tasks in which complex reasoning is required to reason about action preconditions and effects. The resulting analysis can inform selection among general-purpose planning systems for particular robot task planning domains.},
  langid = {english}
}

@article{Jimenez.2012,
  title = {Survey on Model-Based Manipulation Planning of Deformable Objects},
  author = {Jim{\'e}nez, P.},
  year = {2012},
  journal = {Robotics and Computer-Integrated Manufacturing},
  volume = {28},
  number = {2},
  pages = {154--163},
  issn = {07365845},
  doi = {10.1016/j.rcim.2011.08.002},
  pagination = {page}
}

@inproceedings{jinPontryaginDifferentiableProgramming2020,
  title = {Pontryagin {{Differentiable Programming}}: {{An End-to-End Learning}} and {{Control Framework}}},
  shorttitle = {Pontryagin {{Differentiable Programming}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jin, Wanxin and Wang, Zhaoran and Yang, Zhuoran and Mou, Shaoshuai},
  year = {2020},
  volume = {33},
  pages = {7979--7992},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-06-25},
  abstract = {This paper develops a Pontryagin differentiable programming (PDP) methodology, which establishes a unified framework to solve a broad class of learning and control tasks. The PDP  distinguishes from existing methods by two novel techniques: first, we  differentiate through Pontryagin's Maximum Principle,  and this allows  to obtain the analytical derivative of a  trajectory with respect to tunable parameters within an optimal control system,  enabling end-to-end learning of   dynamics, policies, or/and control objective functions; and second, we propose an auxiliary control system in the backward pass of the PDP framework, and  the output of this auxiliary control system is the analytical derivative of the original system's trajectory with respect to the  parameters, which can be iteratively solved using standard control tools. We investigate three learning modes of the PDP: inverse reinforcement learning,  system identification, and  control/planning. We demonstrate the capability of the PDP in each learning mode on different high-dimensional systems, including multilink robot arm,  6-DoF maneuvering UAV, and 6-DoF rocket powered landing.}
}

@inproceedings{jinSafePontryaginDifferentiable2024,
  title = {Safe Pontryagin Differentiable Programming},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Jin, Wanxin and Mou, Shaoshuai and Pappas, George J.},
  year = {2024},
  month = jun,
  series = {{{NIPS}} '21},
  pages = {16034--16050},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2024-09-06},
  abstract = {We propose a Safe Pontryagin Differentiable Programming (Safe PDP) methodology, which establishes a theoretical and algorithmic framework to solve a broad class of safety-critical learning and control tasks---problems that require the guarantee of safety constraint satisfaction at any stage of the learning and control progress. In the spirit of interior-point methods, Safe PDP handles different types of system constraints on states and inputs by incorporating them into the cost or loss through barrier functions. We prove three fundamentals of the proposed Safe PDP: first, both the solution and its gradient in the backward pass can be approximated by solving their more efficient unconstrained counterparts; second, the approximation for both the solution and its gradient can be controlled for arbitrary accuracy by a barrier parameter; and third, importantly, all intermediate results throughout the approximation and optimization strictly respect the constraints, thus guaranteeing safety throughout the entire learning and control process. We demonstrate the capabilities of Safe PDP in solving various safety-critical tasks, including safe policy optimization, safe motion planning, and learning MPCs from demonstrations, on different challenging systems such as 6-DoF maneuvering quadrotor and 6-DoF rocket powered landing.},
  isbn = {978-1-71384-539-3}
}

@article{johnsCoarsetoFineImitationLearning2021,
  title = {Coarse-to-{{Fine Imitation Learning}}: {{Robot Manipulation}} from a {{Single Demonstration}}},
  shorttitle = {Coarse-to-{{Fine Imitation Learning}}},
  author = {Johns, Edward},
  year = {2021},
  month = may,
  journal = {arXiv:2105.06411 [cs]},
  eprint = {2105.06411},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {We introduce a simple new method for visual imitation learning, which allows a novel robot manipulation task to be learned from a single human demonstration, without requiring any prior knowledge of the object being interacted with. Our method models imitation learning as a state estimation problem, with the state defined as the end-effector's pose at the point where object interaction begins, as observed from the demonstration. By modelling a manipulation task as a coarse, approach trajectory followed by a fine, interaction trajectory, this state estimator can be trained in a self-supervised manner, by automatically moving the end-effector's camera around the object. At test time, the end-effector is moved to the estimated state through a linear path, at which point the demonstration's end-effector velocities are simply repeated, enabling convenient acquisition of a complex interaction trajectory without actually needing to explicitly learn a policy. Real-world experiments on 8 everyday tasks show that our method can learn a diverse range of skills from just a single human demonstration, whilst also yielding a stable and interpretable controller.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\V9PIZ9NF\2105.html}
}

@article{Johnson.1999,
  title = {Using Spin Images for Efficient Object Recognition in Cluttered {{3D}} Scenes},
  author = {Johnson, A. E. and Hebert, M.},
  year = {1999},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {21},
  number = {5},
  pages = {433--449},
  issn = {01628828},
  doi = {10.1109/34.765655},
  pagination = {page}
}

@inproceedings{jokinenNeedGroundingLLMbased2024,
  title = {The {{Need}} for {{Grounding}} in {{LLM-based Dialogue Systems}}},
  booktitle = {Proceedings of the {{Workshop}}: {{Bridging Neurons}} and {{Symbols}} for {{Natural Language Processing}} and {{Knowledge Graphs Reasoning}} ({{NeusymBridge}}) @ {{LREC-COLING-2024}}},
  author = {Jokinen, Kristiina},
  editor = {Dong, Tiansi and Hinrichs, Erhard and Han, Zhen and Liu, Kang and Song, Yangqiu and Cao, Yixin and Hempelmann, Christian F. and Sifa, Rafet},
  year = {2024},
  month = may,
  pages = {45--52},
  publisher = {{ELRA and ICCL}},
  address = {Torino, Italia},
  urldate = {2024-08-16},
  abstract = {Grounding is a pertinent part of the design of LLM-based dialogue systems. Although research on grounding has a long tradition, the paradigm shift caused by LLMs has brought the concept onto the foreground, in particular in the context of cognitive robotics. To avoid generation of irrelevant or false information, the system needs to ground its utterances into real-world events, and to avoid the statistical parrot effect, the system needs to construct shared understanding of the dialogue context and of the partner's intents. Grounding and construction of the shared context enables cooperation between the participants, and thus supports trustworthy interaction. This paper discusses grounding using neural LLM technology. It aims to bridge neural and symbolic computing on the cognitive architecture level, so as to contribute to a better understanding of how conversational reasoning and collaboration can be linked to LLM implementations to support trustworthy and flexible interaction.}
}

@inproceedings{jonschkowskiDifferentiableParticleFilters2018,
  title = {Differentiable {{Particle Filters}}: {{End-to-End Learning}} with {{Algorithmic Priors}}},
  shorttitle = {Differentiable {{Particle Filters}}},
  booktitle = {Robotics: {{Science}} and {{Systems XIV}}},
  author = {Jonschkowski, Rico and Rastogi, Divyam and Brock, Oliver},
  year = {2018},
  month = jun,
  volume = {14},
  urldate = {2024-04-29},
  isbn = {978-0-9923747-4-7}
}

@inproceedings{jordanAuslegungAssistenzfunktionenFur2019,
  title = {Auslegung von {{Assistenzfunktionen}} F{\"u}r Die Robotische {{Unterst{\"u}tzung}} {\"A}lterer {{Menschen}} Am {{Esstisch}} Unter {{Ber{\"u}cksichtigung}} Ethischer Und Sicherheitstechnischer {{Randbedingungen}}},
  booktitle = {Zukunft Der {{Pflege}}},
  author = {Jordan, Florian and Graf, Birgit and Bormann, Richard and Worch, Jan-Hendrik and {Abdel-Keream}, Mona and Neumann, Michael and Mania, Patrick and Beetz, Michael and Emmerich, Christian and Schaller, Raphael and Suppa, Michael and Katic, Darko and Aumann, Florian and Blume, Gabi and Martin, Ronny},
  year = {2019},
  pages = {57--58},
  publisher = {Pflegepraxiszentrum},
  address = {Berlin}
}

@misc{joublinCoPALCorrectivePlanning2023,
  title = {{{CoPAL}}: {{Corrective Planning}} of {{Robot Actions}} with {{Large Language Models}}},
  shorttitle = {{{CoPAL}}},
  author = {Joublin, Frank and Ceravola, Antonello and Smirnov, Pavel and Ocker, Felix and Deigmoeller, Joerg and Belardinelli, Anna and Wang, Chao and Hasler, Stephan and Tanneberg, Daniel and Gienger, Michael},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07263},
  eprint = {2310.07263},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.07263},
  urldate = {2024-04-30},
  abstract = {In the pursuit of fully autonomous robotic systems capable of taking over tasks traditionally performed by humans, the complexity of open-world environments poses a considerable challenge. Addressing this imperative, this study contributes to the field of Large Language Models (LLMs) applied to task and motion planning for robots. We propose a system architecture that orchestrates a seamless interplay between multiple cognitive levels, encompassing reasoning, planning, and motion generation. At its core lies a novel replanning strategy that handles physically grounded, logical, and semantic errors in the generated plans. We demonstrate the efficacy of the proposed feedback architecture, particularly its impact on executability, correctness, and time complexity via empirical evaluation in the context of a simulation and two intricate real-world scenarios: blocks world, barman and pizza preparation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\95P4TFBV\2310.html}
}

@inproceedings{joulinInferringAlgorithmicPatterns2015,
  title = {Inferring {{Algorithmic Patterns}} with {{Stack-Augmented Recurrent Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Joulin, Armand and Mikolov, Tomas},
  year = {2015},
  month = dec,
  volume = {1},
  pages = {190--198},
  publisher = {MIT Press},
  address = {Montreal, Canada},
  abstract = {Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.},
  langid = {english}
}

@article{julianNeverStopLearning2020,
  title = {Never {{Stop Learning}}: {{The Effectiveness}} of {{Fine-Tuning}} in {{Robotic Reinforcement Learning}}},
  shorttitle = {Never {{Stop Learning}}},
  author = {Julian, Ryan and Swanson, Benjamin and Sukhatme, Gaurav S. and Levine, Sergey and Finn, Chelsea and Hausman, Karol},
  year = {2020},
  month = jul,
  journal = {arXiv:2004.10190 [cs, stat]},
  eprint = {2004.10190},
  primaryclass = {cs, stat},
  urldate = {2021-06-21},
  abstract = {One of the great promises of robot learning systems is that they will be able to learn from their mistakes and continuously adapt to ever-changing environments. Despite this potential, most of the robot learning systems today are deployed as a fixed policy and they are not being adapted after their deployment. Can we efficiently adapt previously learned behaviors to new environments, objects and percepts in the real world? In this paper, we present a method and empirical evidence towards a robot learning framework that facilitates continuous adaption. In particular, we demonstrate how to adapt vision-based robotic manipulation policies to new variations by fine-tuning via off-policy reinforcement learning, including changes in background, object shape and appearance, lighting conditions, and robot morphology. Further, this adaptation uses less than 0.2\% of the data necessary to learn the task from scratch. We find that our approach of adapting pre-trained policies leads to substantial performance gains over the course of fine-tuning, and that pre-training via RL is essential: training from scratch or adapting from supervised ImageNet features are both unsuccessful with such small amounts of data. We also find that these positive results hold in a limited continual learning setting, in which we repeatedly fine-tune a single lineage of policies using data from a succession of new tasks. Our empirical conclusions are consistently supported by experiments on simulated manipulation tasks, and by 52 unique fine-tuning experiments on a real robotic grasping system pre-trained on 580,000 grasps.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\EI5VGCDA\2004.html}
}

@inproceedings{Julier1997,
  title = {New {{Extension}} of the {{Kalman Filter}} to {{Nonlinear Systems}}},
  author = {Julier, Simon J. and Uhlmann, Jeffrey K.},
  year = {1997},
  volume = {3068},
  pages = {3012--3068}
}

@inproceedings{Julier1997a,
  title = {A {{Non-Divergent Estimation Algorithm}} in the {{Presence}} of {{Unknown Correlations}}},
  booktitle = {Proceedings of the 1997 {{American Control Conference}}},
  author = {Julier, S.J. and Uhlmann, J.K.},
  year = {1997},
  pages = {2369-2373 vol.4},
  publisher = {IEEE},
  doi = {10.1109/ACC.1997.609105},
  urldate = {2018-05-16},
  isbn = {0-7803-3832-4}
}

@incollection{Julier2008,
  title = {General {{Decentralized Data Fusion}} with {{Covariance Intersection}}},
  booktitle = {Handbook of {{Multisensor Data Fusion}}},
  author = {Julier, Simon and Uhlmann, Jeffrey K.},
  editor = {Liggins, Martin and Hall, David and Llinas, James},
  year = {2008},
  edition = {Second},
  pages = {319--343},
  publisher = {CRC Press},
  isbn = {978-1-4200-5308-1}
}

@inproceedings{Julier2009,
  title = {Estimating and {{Exploiting}} the {{Degree}} of {{Independent Information}} in {{Distributed Data Fusion}}},
  booktitle = {Fusion 2009: {{The}} 12th {{International Conference}} on {{Information Fusion}}},
  author = {Julier, Simon J.},
  year = {2009},
  publisher = {[IEEE]},
  address = {Seattle},
  urldate = {2018-05-15},
  isbn = {978-0-9824438-0-4}
}

@article{jumperHighlyAccurateProtein2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  year = {2021},
  month = aug,
  journal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  urldate = {2024-04-27},
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1--4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence---the structure prediction component of the `protein folding problem'8---has been an important open research problem for more than 50~years9. Despite recent progress10--14, existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational biophysics,Machine learning,Protein structure predictions,Structural biology}
}

@misc{junShapEGeneratingConditional2023,
  title = {Shap-{{E}}: {{Generating Conditional 3D Implicit Functions}}},
  shorttitle = {Shap-{{E}}},
  author = {Jun, Heewoo and Nichol, Alex},
  year = {2023},
  month = may,
  number = {arXiv:2305.02463},
  eprint = {2305.02463},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.02463},
  urldate = {2024-01-08},
  abstract = {We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. We release model weights, inference code, and samples at https://github.com/openai/shap-e.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\6A2LIZUW\2305.html}
}

@incollection{kadkhodamohammadi2019feature,
  title = {Feature Aggregation Decoder for Segmenting Laparoscopic Scenes},
  booktitle = {{{OR}} 2.0 Context-Aware Operating Theaters and Machine Learning in Clinical Neuroimaging},
  author = {Kadkhodamohammadi, Abdolrahim and Luengo, Imanol and Barbarisi, Santiago and Taleb, Hinde and Flouty, Evangello and Stoyanov, Danail},
  year = {2019},
  pages = {3--11},
  publisher = {Springer}
}

@inproceedings{kaelblingHierarchicalTaskMotion2011,
  title = {Hierarchical Task and Motion Planning in the Now},
  booktitle = {2011 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Kaelbling, Leslie Pack and {Lozano-Perez}, Tomas},
  year = {2011},
  month = may,
  pages = {1470--1477},
  publisher = {IEEE},
  address = {Shanghai, China},
  doi = {10.1109/ICRA.2011.5980391},
  urldate = {2020-07-11},
  abstract = {In this paper we outline an approach to the integration of task planning and motion planning that has the following key properties: It is aggressively hierarchical. It makes choices and commits to them in a top-down fashion in an attempt to limit the length of plans that need to be constructed, and thereby exponentially decrease the amount of search required. Importantly, our approach also limits the need to project the effect of actions into the far future. It operates on detailed, continuous geometric representations and partial symbolic descriptions. It does not require a complete symbolic representation of the input geometry or of the geometric effect of the task-level operations.},
  isbn = {978-1-61284-386-5},
  langid = {english}
}

@article{kaelblingIntegratedTaskMotion2013,
  title = {Integrated Task and Motion Planning in Belief Space},
  author = {Kaelbling, Leslie Pack and {Lozano-P{\'e}rez}, Tom{\'a}s},
  year = {2013},
  month = aug,
  journal = {The International Journal of Robotics Research},
  volume = {32},
  number = {9-10},
  pages = {1194--1227},
  publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  doi = {10.1177/0278364913484072},
  urldate = {2020-07-11},
  abstract = {We describe an integrated strategy for planning, perception, state estimation and action in complex mobile manipulation domains based on planning in the belief space of probability distributions over states using hierarchical goal regression (pre-image back-chaining). We develop a vocabulary of logical expressions that describe sets of belief states, which are goals and subgoals in the planning process. We show that a relatively small set of symbolic operators can give rise to task-oriented perception in support of the manipulation goals. An implementation of this method is demonstrated in simulation and on a real PR2 robot, showing robust, flexible solution of mobile manipulation problems with multiple objects and substantial uncertainty.},
  langid = {english}
}

@inproceedings{kaelblingLearningAchieveGoals1993,
  title = {Learning to {{Achieve Goals}}},
  booktitle = {International {{Joint Conference}} on {{Artificial Intelligence}}},
  author = {Kaelbling, L.},
  year = {1993},
  urldate = {2024-04-29},
  abstract = {Temporal di(cid:11)erence methods solve the temporal credit assignment problem for reinforcement learning. An important subproblem of general reinforcement learning is learning to achieve dynamic goals. Although existing temporal di(cid:11)erence methods, such as Q learning, can be applied to this problem, they do not take advantage of its special structure. This paper presents the DG-learning algorithm, which learns e(cid:14)ciently to achieve dynamically changing goals and exhibits good knowledge transfer between goals. In addition, this paper shows how traditional relaxation techniques can be applied to the problem. Finally, experimental results are given that demonstrate the su-periority of DG learning over Q learning in a moderately large, synthetic, non-deterministic domain.}
}

@inproceedings{Kala11,
  title = {{{STOMP}}: {{Stochastic Trajectory Optimization}} for {{Motion Planning}}},
  booktitle = {2011 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Kalakrishnan, Mrinal and Chitta, Sachin and Theodorou, Evangelos and Pastor, Peter and Schaal, Stefan},
  year = {2011},
  month = may,
  pages = {4569--4574}
}

@inproceedings{kalburgiApplicationCoveragePath2020,
  title = {Application of {{Coverage Path Planning Algorithm}} for {{Milling Operations}}},
  booktitle = {Intelligent {{Systems}}, {{Technologies}} and {{Applications}}},
  author = {Kalburgi, Sagar and Nair, Vishnu G. and Guruprasad, K. R.},
  editor = {Thampi, Sabu M. and Trajkovic, Ljiljana and Mitra, Sushmita and Nagabhushan, P. and {El-Alfy}, El-Sayed M. and Bojkovic, Zoran and Mishra, Deepak},
  year = {2020},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {213--220},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-15-3914-5_16},
  abstract = {In this paper, we present an algorithm for automatic tool path generation for milling operations, where, the `cutter' needs to pass through all the region that is required to be removed, without any gaps. We demonstrate the possibility of using mobile robot coverage path planning (CPP) algorithms for such applications. In the place of the robot footprint size that is used in a mobile robot CPP algorithm, here we use the size (diameter) of the tool as basis for the tool path generation. Here, we use a spanning tree-based competitive and truly complete robot coverage path planning algorithm, which is based on the approximate cellular decomposition. The proposed algorithm is first tested in V-Rep simulation environment with an arbitrary work piece and then real-time experiments were carried out on a CNC machine to demonstrate the proposed algorithm.},
  isbn = {9789811539145},
  langid = {english},
  keywords = {CNC machine tool path,Coverage path planning,Milling,ST-CTC algorithm}
}

@article{Kalman1960,
  title = {A {{New Approach}} to {{Linear Filtering}} and {{Prediction Problems}}},
  author = {Kalman, Rudolph Emil},
  year = {1960},
  journal = {Transactions of the ASME - Journal of Basic Engineering},
  volume = {82},
  number = {Series D},
  pages = {35--45}
}

@article{kamelDeepConvolutionalNeural2019,
  title = {Deep {{Convolutional Neural Networks}} for {{Human Action Recognition Using Depth Maps}} and {{Postures}}},
  author = {Kamel, Aouaidjia and Sheng, Bin and Yang, Po and Li, Ping and Shen, Ruimin and Feng, David Dagan},
  year = {2019},
  month = sep,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume = {49},
  number = {9},
  pages = {1806--1819},
  issn = {2168-2232},
  doi = {10.1109/TSMC.2018.2850149},
  urldate = {2024-07-19},
  abstract = {In this paper, we present a method (Action-Fusion) for human action recognition from depth maps and posture data using convolutional neural networks (CNNs). Two input descriptors are used for action representation. The first input is a depth motion image that accumulates consecutive depth maps of a human action, whilst the second input is a proposed moving joints descriptor which represents the motion of body joints over time. In order to maximize feature extraction for accurate action classification, three CNN channels are trained with different inputs. The first channel is trained with depth motion images (DMIs), the second channel is trained with both DMIs and moving joint descriptors together, and the third channel is trained with moving joint descriptors only. The action predictions generated from the three CNN channels are fused together for the final action classification. We propose several fusion score operations to maximize the score of the right action. The experiments show that the results of fusing the output of three channels are better than using one channel or fusing two channels only. Our proposed method was evaluated on three public datasets: 1) Microsoft action 3-D dataset (MSRAction3D); 2) University of Texas at Dallas-multimodal human action dataset; and 3) multimodal action dataset (MAD) dataset. The testing results indicate that the proposed approach outperforms most of existing state-of-the-art methods, such as histogram of oriented 4-D normals and Actionlet on MSRAction3D. Although MAD dataset contains a high number of actions (35 actions) compared to existing action RGB-D datasets, this paper surpasses a state-of-the-art method on the dataset by 6.84\%.},
  keywords = {Action recognition,Cameras,Convolutional neural networks,convolutional neural networks (CNNs),depth motion image (DMI),Feature extraction,Image color analysis,Image recognition,Information technology,moving joints descriptor (MJD),Robot sensing systems}
}

@article{kamisawaPancreaticCancer2016,
  title = {Pancreatic Cancer},
  author = {Kamisawa, Terumi and Wood, Laura D. and Itoi, Takao and Takaori, Kyoichi},
  year = {2016},
  month = jul,
  journal = {The Lancet},
  volume = {388},
  number = {10039},
  pages = {73--85},
  publisher = {Elsevier},
  issn = {0140-6736, 1474-547X},
  doi = {10.1016/S0140-6736(16)00141-0},
  urldate = {2024-04-12},
  langid = {english}
}

@article{Kanopoulos.1988,
  title = {Design of an Image Edge Detection Filter Using the {{Sobel}} Operator},
  author = {Kanopoulos, N. and Vasanthavada, N. and Baker, R. L.},
  year = {1988},
  journal = {IEEE Journal of Solid-State Circuits},
  volume = {23},
  number = {2},
  pages = {358--367},
  issn = {00189200},
  doi = {10.1109/4.996},
  pagination = {page}
}

@article{kantRecentAdvancesNeural2018,
  title = {Recent {{Advances}} in {{Neural Program Synthesis}}},
  author = {Kant, Neel},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.02353 [cs]},
  eprint = {1802.02353},
  primaryclass = {cs},
  urldate = {2019-07-10},
  abstract = {In recent years, deep learning has made tremendous progress in a number of fields that were previously out of reach for artificial intelligence. The successes in these problems has led researchers to consider the possibilities for intelligent systems to tackle a problem that humans have only recently themselves considered: program synthesis. This challenge is unlike others such as object recognition and speech translation, since its abstract nature and demand for rigor make it difficult even for human minds to attempt. While it is still far from being solved or even competitive with most existing methods, neural program synthesis is a rapidly growing discipline which holds great promise if completely realized. In this paper, we start with exploring the problem statement and challenges of program synthesis. Then, we examine the fascinating evolution of program induction models, along with how they have succeeded, failed and been reimagined since. Finally, we conclude with a contrastive look at program synthesis and future research recommendations for the field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Programming Languages}
}

@misc{kaplanScalingLawsNeural2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  number = {arXiv:2001.08361},
  eprint = {2001.08361},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.08361},
  urldate = {2024-01-25},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\HAZRXMY8\2001.html}
}

@inproceedings{karkusDifferentiableAlgorithmNetworks2019,
  title = {Differentiable {{Algorithm Networks}} for {{Composable Robot Learning}}},
  booktitle = {Robotics: {{Science}} and {{Systems}}},
  author = {Karkus, Peter and Ma, Xiao and Hsu, David and Kaelbling, L. and Lee, Wee Sun and {Lozano-Perez}, Tomas},
  year = {2019},
  doi = {10.15607/RSS.2019.XV.039},
  abstract = {This paper introduces the Differentiable Algorithm Network (DAN), a composable architecture for robot learning systems that combines the strengths of model-driven modular system design and data-driven end-to-end learning. This paper introduces the Differentiable Algorithm Network (DAN), a composable architecture for robot learning systems. A DAN is composed of neural network modules, each encoding a differentiable robot algorithm and an associated model; and it is trained end-to-end from data. DAN combines the strengths of model-driven modular system design and data-driven end-to-end learning. The algorithms and models act as structural assumptions to reduce the data requirements for learning; end-to-end learning allows the modules to adapt to one another and compensate for imperfect models and algorithms, in order to achieve the best overall system performance. We illustrate the DAN methodology through a case study on a simulated robot system, which learns to navigate in complex 3-D environments with only local visual observations and an image of a partially correct 2-D floor map.}
}

@misc{karpathyUnreasonableEffectivenessRecurrent2015,
  title = {The {{Unreasonable Effectiveness}} of {{Recurrent Neural Networks}}},
  author = {Karpathy, Andrej},
  year = {2015},
  urldate = {2019-07-10},
  howpublished = {http://karpathy.github.io/2015/05/21/rnn-effectiveness/},
  file = {C:\Users\benja\Zotero\storage\D2LEBJ87\rnn-effectiveness.html}
}

@phdthesis{Kartmann.2015,
  type = {Bachelorarbeit},
  title = {Heuristische Optimierung von Eingelerten Griffen Einer Humanoiden Roboterhand},
  author = {Kartmann, Rainer},
  year = {2015},
  address = {Karlsruhe},
  school = {Fakult{\"a}t f{\"u}r Informatik, Institut f{\"u}r Anthropromatik und Robotik / Karlsruher Institut f{\"u}r Technologie}
}

@article{Kass.1988,
  title = {Snakes: {{Active}} Contour Models},
  author = {Kass, Michael and Witkin, Andrew and Terzopoulos, Demetri},
  year = {1988},
  journal = {International Journal of Computer Vision},
  volume = {1},
  number = {4},
  pages = {321--331},
  issn = {0920-5691},
  doi = {10.1007/BF00133570},
  pagination = {page}
}

@article{kassahunSurgicalRoboticsEnhanced2016,
  title = {Surgical Robotics beyond Enhanced Dexterity Instrumentation: A Survey of Machine Learning Techniques and Their Role in Intelligent and Autonomous Surgical Actions},
  shorttitle = {Surgical Robotics beyond Enhanced Dexterity Instrumentation},
  author = {Kassahun, Yohannes and Yu, Bingbin and Tibebu, Abraham Temesgen and Stoyanov, Danail and Giannarou, Stamatia and Metzen, Jan Hendrik and Vander Poorten, Emmanuel},
  year = {2016},
  month = apr,
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  volume = {11},
  number = {4},
  pages = {553--568},
  issn = {1861-6429},
  doi = {10.1007/s11548-015-1305-z},
  urldate = {2024-04-12},
  abstract = {Advances in technology and computing play an increasingly important role in the evolution of modern surgical techniques and paradigms. This article reviews the current role of machine learning (ML) techniques in the context of surgery with a focus on surgical robotics (SR). Also, we provide a perspective on the future possibilities for enhancing the effectiveness of procedures by integrating ML in the operating room.},
  langid = {english},
  keywords = {Learning to perceive,Skill analysis,Skill learning,Surgical robotics}
}

@patent{kassowProgrammableRobotUser2013,
  title = {Programmable Robot and User Interface},
  author = {Kassow, Kristian and {\O}stergaard, Esben Hallundb{\ae}k and St{\o}y, Kasper},
  year = {2013},
  month = dec,
  number = {US8614559B2},
  urldate = {2020-09-15},
  assignee = {Universal Robots ApS},
  langid = {english},
  nationality = {US},
  keywords = {housing,joint,output,relative,robot},
  annotation = {Inventors: \_:n6262}
}

@article{katicLapOntoSPMOntologyLaparoscopic2015,
  title = {{{LapOntoSPM}}: An Ontology for Laparoscopic Surgeries and Its Application to Surgical Phase Recognition},
  shorttitle = {{{LapOntoSPM}}},
  author = {Kati{\'c}, Darko and Julliard, Chantal and Wekerle, Anna-Laura and Kenngott, Hannes and {M{\"u}ller-Stich}, Beat Peter and Dillmann, R{\"u}diger and Speidel, Stefanie and Jannin, Pierre and Gibaud, Bernard},
  year = {2015},
  month = sep,
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  volume = {10},
  number = {9},
  pages = {1427--1434},
  issn = {1861-6429},
  doi = {10.1007/s11548-015-1222-1},
  abstract = {PURPOSE: The rise of intraoperative information threatens to outpace our abilities to process it. Context-aware systems, filtering information to automatically adapt to the current needs of the surgeon, are necessary to fully profit from computerized surgery. To attain context awareness, representation of medical knowledge is crucial. However, most existing systems do not represent knowledge in a reusable way, hindering also reuse of data. Our purpose is therefore to make our computational models of medical knowledge sharable, extensible and interoperational with established knowledge representations in the form of the LapOntoSPM ontology. To show its usefulness, we apply it to situation interpretation, i.e., the recognition of surgical phases based on surgical activities. METHODS: Considering best practices in ontology engineering and building on our ontology for laparoscopy, we formalized the workflow of laparoscopic adrenalectomies, cholecystectomies and pancreatic resections in the framework of OntoSPM, a new standard for surgical process models. Furthermore, we provide a rule-based situation interpretation algorithm based on SQWRL to recognize surgical phases using the ontology. RESULTS: The system was evaluated on ground-truth data from 19 manually annotated surgeries. The aim was to show that the phase recognition capabilities are equal to a specialized solution. The recognition rates of the new system were equal to the specialized one. However, the time needed to interpret a situation rose from 0.5 to 1.8 s on average which is still viable for practical application. CONCLUSION: We successfully integrated medical knowledge for laparoscopic surgeries into OntoSPM, facilitating knowledge and data sharing. This is especially important for reproducibility of results and unbiased comparison of recognition algorithms. The associated recognition algorithm was adapted to the new representation without any loss of classification power. The work is an important step to standardized knowledge and data representation in the field on context awareness and thus toward unified benchmark data sets.},
  langid = {english},
  pmid = {26062794},
  keywords = {Adrenalectomy,Algorithms,Cholecystectomy,Computer Simulation,Equipment Design,Humans,Image Processing Computer-Assisted,Intraoperative Period,Laparoscopy,Models Anatomic,Pancreas,Reproducibility of Results,Surgery Computer-Assisted,Workflow}
}

@misc{katicMEDICAHealthIT2021,
  type = {Panel {{Discussion}}},
  title = {{{MEDICA Health IT Forum}}: {{Smart Robotics}} - {{Tech Talk}}},
  author = {Katic, Darko},
  year = {2021},
  month = nov,
  address = {D{\"u}sseldorf}
}

@misc{katicSensorbasedRobotProjects2021,
  type = {Invited Talk},
  title = {Sensor-Based {{Robot Projects}}: {{Innovative Opportunities}} and {{Solutions}} for {{Demanding Robotic Applications}}},
  author = {Katic, Darko},
  year = {2021},
  month = sep,
  address = {Dresden}
}

@misc{kauppila3DConcretePrinting2023,
  type = {Article},
  title = {{{3D Concrete Printing}} -- {{The Ultimate Guide}}},
  author = {Kauppila, Ile},
  year = {2023},
  month = aug,
  journal = {All3DP Pro},
  urldate = {2024-04-02},
  abstract = {3D printed concrete is transforming construction sites worldwide as the faster, cheaper, and more sustainable alternative. Learn what it's all about.},
  howpublished = {https://all3dp.com/1/3d-concrete-printing-guide/},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\HYDM23D2\3d-concrete-printing-guide.html}
}

@inproceedings{kaushikFastOnlineAdaptation2020,
  title = {Fast {{Online Adaptation}} in {{Robotics}} through {{Meta-Learning Embeddings}} of {{Simulated Priors}}},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Kaushik, Rituraj and Anne, Timoth{\'e}e and Mouret, Jean-Baptiste},
  year = {2020},
  month = oct,
  pages = {5269--5276},
  issn = {2153-0866},
  doi = {10.1109/IROS45743.2020.9341462},
  urldate = {2024-06-15},
  abstract = {Meta-learning algorithms can accelerate the model-based reinforcement learning (MBRL) algorithms by finding an initial set of parameters for the dynamical model such that the model can be trained to match the actual dynamics of the system with only a few data-points. However, in the real world, a robot might encounter any situation starting from motor failures to finding itself in a rocky terrain where the dynamics of the robot can be significantly different from one another. In this paper, first, we show that when meta-training situations (the prior situations) have such diverse dynamics, using a single set of meta-trained parameters as a starting point still requires a large number of observations from the real system to learn a useful model of the dynamics. Second, we propose an algorithm called FAMLE that mitigates this limitation by meta-training several initial starting points (i.e., initial parameters) for training the model and allows robots to select the most suitable starting point to adapt the model to the current situation with only a few gradient steps. We compare FAMLE to MBRL, MBRL with a meta-trained model with MAML, and model-free policy search algorithm PPO for various simulated and real robotic tasks, and show that FAMLE allows robots to adapt to novel damages in significantly fewer time-steps than the baselines.},
  keywords = {Acceleration,Adaptation models,Heuristic algorithms,Reinforcement learning,Robots,Task analysis,Training}
}

@article{kavrakiProbabilisticRoadmapsPath1996,
  title = {Probabilistic Roadmaps for Path Planning in High-Dimensional Configuration Spaces},
  author = {Kavraki, L.E. and Svestka, P. and Latombe, J.-C. and Overmars, M.H.},
  year = {1996},
  month = aug,
  journal = {IEEE Transactions on Robotics and Automation},
  volume = {12},
  number = {4},
  pages = {566--580},
  issn = {2374-958X},
  doi = {10.1109/70.508439},
  urldate = {2024-04-04},
  abstract = {A new motion planning method for robots in static workspaces is presented. This method proceeds in two phases: a learning phase and a query phase. In the learning phase, a probabilistic roadmap is constructed and stored as a graph whose nodes correspond to collision-free configurations and whose edges correspond to feasible paths between these configurations. These paths are computed using a simple and fast local planner. In the query phase, any given start and goal configurations of the robot are connected to two nodes of the roadmap; the roadmap is then searched for a path joining these two nodes. The method is general and easy to implement. It can be applied to virtually any type of holonomic robot. It requires selecting certain parameters (e.g., the duration of the learning phase) whose values depend on the scene, that is the robot and its workspace. But these values turn out to be relatively easy to choose, Increased efficiency can also be achieved by tailoring some components of the method (e.g., the local planner) to the considered robots. In this paper the method is applied to planar articulated robots with many degrees of freedom. Experimental results show that path planning can be done in a fraction of a second on a contemporary workstation (/spl ap/150 MIPS), after learning for relatively short periods of time (a few dozen seconds).},
  keywords = {Computer science,Joining processes,Laboratories,Layout,Motion planning,Orbital robotics,Path planning,Robots,Workstations},
  file = {C:\Users\benja\Zotero\storage\QJQJJL4L\508439.html}
}

@article{kawamuraImplementationCognitiveControl2008,
  title = {Implementation of {{Cognitive Control}} for a {{Humanoid Robot}}},
  author = {Kawamura, Kazuhiko and Gordon, Stephen and Ratanaswasd, Palis and Erdemir, Erdem and Hall, Joseph},
  year = {2008},
  month = dec,
  journal = {International Journal of Humanoid Robotics},
  volume = {5},
  pages = {547--586},
  doi = {10.1142/S0219843608001558},
  abstract = {Engineers have long employed control systems utilizing models and feedback loops to control real-world systems. Limitations of model-based control led to a generation of intelligent control techniques such as adaptive and fuzzy control. The human brain, on the other hand, is known to process a variety of inputs in parallel, and shift between different levels of cognitive activities while ignoring distractions to focus on the task in hand. This process, known as cognitive control in psychology, is unique to humans and a handful of animals. We are interested in implementing such cognitive control functionalities for our humanoid robot ISAC. This paper outlines the features of multiagent-based cognitive architecture for a humanoid robot and the progress made toward the realization of cognitive control functionalities using attention, working memory and internal rehearsal. Several experiments have been conducted to show that the implementation of an integrated cognitive robot architecture is feasible.}
}

@inproceedings{kazhdanPoissonSurfaceReconstruction2006,
  title = {Poisson Surface Reconstruction},
  booktitle = {Proceedings of the Fourth {{Eurographics}} Symposium on {{Geometry}} Processing},
  author = {Kazhdan, Michael and Bolitho, Matthew and Hoppe, Hugues},
  year = {2006},
  month = jun,
  series = {{{SGP}} '06},
  pages = {61--70},
  publisher = {Eurographics Association},
  address = {Goslar, Germany},
  urldate = {2020-08-25},
  abstract = {We show that surface reconstruction from oriented points can be cast as a spatial Poisson problem. This Poisson formulation considers all the points at once, without resorting to heuristic spatial partitioning or blending, and is therefore highly resilient to data noise. Unlike radial basis function schemes, our Poisson approach allows a hierarchy of locally supported basis functions, and therefore the solution reduces to a well conditioned sparse linear system. We describe a spatially adaptive multiscale algorithm whose time and space complexities are proportional to the size of the reconstructed model. Experimenting with publicly available scan data, we demonstrate reconstruction of surfaces with greater detail than previously achievable.},
  isbn = {978-3-905673-36-4}
}

@inproceedings{kazhoyanPlanTransformationsRealWorld2020,
  title = {Towards {{Plan Transformations}} for {{Real-World Mobile Fetch}} and {{Place}}},
  booktitle = {{{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Kazhoyan, Gayane and Niedzwiecki, Arthur and Beetz, Michael},
  year = {2020},
  doi = {10.1109/ICRA40945.2020.9197446},
  file = {C:\Users\benja\Zotero\storage\WMIYEEJ7\bibtexbrowser.html}
}

@inproceedings{kazhoyanProgrammingRoboticAgents2017,
  title = {Programming Robotic Agents with Action Descriptions},
  booktitle = {2017 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Kazhoyan, Gayane and Beetz, Michael},
  year = {2017},
  month = sep,
  pages = {103--108},
  issn = {2153-0866},
  doi = {10.1109/IROS.2017.8202144},
  abstract = {This paper tackles the problem of generalizing robot control programs over multiple objects, tasks and environments, based on the concept of action descriptions. These are abstract, general, semantic descriptions of an action that are augmented during execution with subsymbolic parameters specific to the context at hand. The parameters are inferred through reasoning rules, which extract the context from the action description and the belief state of the robot. The proposed system scales well with increasing number of reasoning rules required to support the knowledge-intensive manipulation tasks. The architecture combines the high-level robot control program with the reasoning engine in a modular way, thus improving the scalability of the system. The approach is validated in the context of setting a table with a PR2 robot.},
  keywords = {Cognition,Containers,Dairy products,Robot control,Semantics},
  file = {C:\Users\benja\Zotero\storage\L6EYQ8XZ\8202144.html}
}

@article{kazhoyanRobotHouseholdMarathon2020,
  title = {The {{Robot Household Marathon Experiment}}},
  author = {Kazhoyan, Gayane and Stelter, Simon and Kenfack, Franklin Kenghagho and Koralewski, Sebastian and Beetz, Michael},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.09792 [cs]},
  eprint = {2011.09792},
  primaryclass = {cs},
  urldate = {2022-03-10},
  abstract = {In this paper, we present an experiment, designed to investigate and evaluate the scalability and the robustness aspects of mobile manipulation. The experiment involves performing variations of mobile pick and place actions and opening/closing environment containers in a human household. The robot is expected to act completely autonomously for extended periods of time. We discuss the scientific challenges raised by the experiment as well as present our robotic system that can address these challenges and successfully perform all the tasks of the experiment. We present empirical results and the lessons learned as well as discuss where we hit limitations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\benja\\Zotero\\storage\\TIVRN8P3\\Kazhoyan et al. - 2020 - The Robot Household Marathon Experiment.pdf;C\:\\Users\\benja\\Zotero\\storage\\KAMZXUW3\\2011.html}
}

@book{keetIntroductionOntologyEngineering,
  title = {An {{Introduction}} to {{Ontology Engineering}}},
  author = {Keet, C. Maria},
  urldate = {2021-04-18},
  file = {C:\Users\benja\Zotero\storage\D4KJVFYW\OEbook.pdf}
}

@inproceedings{kehoeAutonomousMultilateralDebridement2014,
  title = {Autonomous Multilateral Debridement with the {{Raven}} Surgical Robot},
  booktitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Kehoe, Ben and Kahn, Gregory and Mahler, Jeffrey and Kim, Jonathan and Lee, Alex and Lee, Anna and Nakagawa, Keisuke and Patil, Sachin and Boyd, W. Douglas and Abbeel, Pieter and Goldberg, Ken},
  year = {2014},
  month = may,
  pages = {1432--1439},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2014.6907040},
  urldate = {2024-04-12},
  abstract = {Autonomous robot execution of surgical sub-tasks has the potential to reduce surgeon fatigue and facilitate supervised tele-surgery. This paper considers the sub-task of surgical debridement: removing dead or damaged tissue fragments to allow the remaining healthy tissue to heal. We present an autonomous multilateral surgical debridement system using the Raven, an open-architecture surgical robot with two cable-driven 7 DOF arms. Our system combines stereo vision for 3D perception with trajopt, an optimization-based motion planner, and model predictive control (MPC). Laboratory experiments involving sensing, grasping, and removal of 120 fragments suggest that an autonomous surgical robot can achieve robustness comparable to human performance. Our robot system demonstrated the advantage of multilateral systems, as the autonomous execution was 1.5{\texttimes} faster with two arms than with one; however, it was two to three times slower than a human. Execution speed could be improved with better state estimation that would allow more travel between MPC steps and fewer MPC replanning cycles. The three primary contributions of this paper are: (1) introducing debridement as a sub-task of interest for surgical robotics, (2) demonstrating the first reliable autonomous robot performance of a surgical sub-task using the Raven, and (3) reporting experiments that highlight the importance of accurate state estimation for future research. Further information including code, photos, and video is available at: http://rll.berkeley.edu/raven.},
  keywords = {Cameras,Grippers,Medical robotics,Planning,Robot sensing systems,Surgery},
  file = {C:\Users\benja\Zotero\storage\7H5ZNE2E\6907040.html}
}

@inproceedings{kenghaghokenfackRobotVQASceneGraphDeepLearningbased2020,
  title = {{{RobotVQA}} --- {{A Scene-Graph-}} and {{Deep-Learning-based Visual Question Answering System}} for {{Robot Manipulation}}},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Kenghagho Kenfack, Franklin and Siddiky, Feroz A. and {Balint-Benczedi}, Ferenc and Beetz, Michael},
  year = {2020},
  month = oct,
  pages = {9667--9674},
  issn = {2153-0866},
  doi = {10.1109/IROS45743.2020.9341186},
  urldate = {2024-07-18},
  abstract = {Visual robot perception has been challenging to successful robot manipulation in noisy, cluttered and dynamic environments. While some perception systems fail to provide an adequate semantics of the scene, others fail to present appropriate learning models and training data. Another major issue encountered in some robot perception systems is their inability to promptly respond to robot control programs whose realtimeness is crucial.This paper proposes an architecture to robot vision for manipulation tasks that addresses the three issues mentioned above. The architecture encompasses a generator of training datasets and a learnable scene describer, coined as RobotVQA for Robot Visual Question Answering. The architecture leverages the power of deep learning to predict and photo-realistic virtual worlds to train. RobotVQA takes as input a robot scene's RGB or RGBD image, detects all relevant objects in it, then describes in realtime each object in terms of category, color, material, shape, openability, 6D-pose and segmentation mask. Moreover, RobotVQA computes the qualitative spatial relations among those objects. We refer to such a scene description in this paper as scene graph or semantic graph of the scene. In RobotVQA, prediction and training take place in a unified manner. Finally, we demonstrate how RobotVQA is suitable for robot control systems that interpret perception as a question answering process.},
  keywords = {Computer architecture,Knowledge discovery,Robot control,Robots,Semantics,Training,Visualization},
  file = {C:\Users\benja\Zotero\storage\KMVV9DLW\9341186.html}
}

@article{kerleyBatchSizeGo2023,
  title = {Batch Size: Go Big or Go Home? {{Counterintuitive}} Improvement in Medical Autoencoders with Smaller Batch Size},
  shorttitle = {Batch Size},
  author = {Kerley, Cailey I. and Cai, Leon Y. and Tang, Yucheng and {Beason-Held}, Lori L. and Resnick, Susan M. and Cutting, Laurie E. and Landman, Bennett A.},
  year = {2023},
  month = feb,
  journal = {Proceedings of SPIE--the International Society for Optical Engineering},
  volume = {12464},
  pages = {124640H},
  issn = {0277-786X},
  doi = {10.1117/12.2653643},
  urldate = {2024-03-28},
  abstract = {Batch size is a key hyperparameter in training deep learning models. Conventional wisdom suggests larger batches produce improved model performance. Here we present evidence to the contrary, particularly when using autoencoders to derive meaningful latent spaces from data with spatially global similarities and local differences, such as electronic health records (EHR) and medical imaging. We investigate batch size effects in both EHR data from the Baltimore Longitudinal Study of Aging and medical imaging data from the multimodal brain tumor segmentation (BraTS) challenge. We train fully connected and convolutional autoencoders to compress the EHR and imaging input spaces, respectively, into 32-dimensional latent spaces via reconstruction losses for various batch sizes between 1 and 100. Under the same hyperparameter configurations, smaller batches improve loss performance for both datasets. Additionally, latent spaces derived by autoencoders with smaller batches capture more biologically meaningful information. Qualitatively, we visualize 2-dimensional projections of the latent spaces and find that with smaller batches the EHR network better separates the sex of the individuals, and the imaging network better captures the right-left laterality of tumors. Quantitatively, the analogous sex classification and laterality regressions using the latent spaces demonstrate statistically significant improvements in performance at smaller batch sizes. Finally, we find improved individual variation locally in visualizations of representative data reconstructions at lower batch sizes. Taken together, these results suggest that smaller batch sizes should be considered when designing autoencoders to extract meaningful latent spaces among EHR and medical imaging data driven by global similarities and local variation.},
  pmcid = {PMC10353832},
  pmid = {37465095}
}

@article{khadirOntologyLearningGrand2021,
  title = {Ontology Learning: {{Grand}} Tour and Challenges},
  shorttitle = {Ontology Learning},
  author = {Khadir, Ahlem Ch{\'e}rifa and Aliane, Hassina and Guessoum, Ahmed},
  year = {2021},
  month = feb,
  journal = {Computer Science Review},
  volume = {39},
  pages = {100339},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2020.100339},
  urldate = {2024-07-21},
  abstract = {Ontologies are at the core of the semantic web. As knowledge bases, they are very useful resources for many artificial intelligence applications. Ontology learning, as a research area, proposes techniques to automate several tasks of the ontology construction process to simplify the tedious work of manually building ontologies. In this paper we present the state of the art of this field. Different classes of approaches are covered (linguistic, statistical, and machine learning), including some recent ones (deep-learning-based approaches). In addition, some relevant solutions (frameworks), which offer strategies and built-in methods for ontology learning, are presented. A descriptive summary is made to point out the capabilities of the different contributions based on criteria that have to do with the produced ontology components and the degree of automation. We also highlight the challenge of evaluating ontologies to make them reliable, since it is not a trivial task in this field; it actually represents a research area on its own. Finally, we identify some unresolved issues and open questions.},
  keywords = {Deep learning,Linguistic and statistical approaches,Machine learning,Ontologies,Ontology learning},
  file = {C:\Users\benja\Zotero\storage\QYDNK54K\S1574013720304391.html}
}

@article{khalickmohammadPolishingUnevenSurfaces2017,
  title = {Polishing of Uneven Surfaces Using Industrial Robots Based on Neural Network and Genetic Algorithm},
  author = {Khalick Mohammad, Abd El and Hong, Jie and Wang, Danwei},
  year = {2017},
  month = oct,
  journal = {The International Journal of Advanced Manufacturing Technology},
  volume = {93},
  number = {1},
  pages = {1463--1471},
  issn = {1433-3015},
  doi = {10.1007/s00170-017-0524-6},
  urldate = {2021-02-25},
  abstract = {In conventional polishing processes, the polishing parameters are constant along the surface. Hence, if the desired material to be removed from the surface is not equally distributed, an over-polishing may occur for the areas with small material removal and under-polishing for the areas with large material removal. Consequently, the quality of the processed surface may not meet the manufacture requirements. In this paper, the authors proposed a polishing algorithm to deal with this problem using neural network (NNW) and genetic algorithm (GA). The NNW is used to predict the polishing performance parameters corresponding to a certain polishing parameters. In addition, the GA is employed to optimize the polishing parameters according to an objective function that includes the desired material removal and surface roughness improvement using the output from the trained NNW model. The effectiveness of the proposed algorithm is verified through experiments of polishing uneven surface.},
  langid = {english}
}

@book{kholiefDetectionSteelSurface2017,
  title = {Detection of {{Steel Surface Defect Based}} on {{Machine Learning Using Deep Auto-encoder Network}}},
  author = {Kholief, Ehab and Darwish, Samy and Fors, M},
  year = {2017},
  month = apr,
  abstract = {The non-contact inspection of surface defects has become more and more important in the manufacturing industrial systems because of the great demands on the quality of high surface finishes. The machine learning achieved impressive recognition rates in image classification tasks. In order to exploit those capabilities, this paper represents a detection and classification of surface defects on hot rolled steel strip by means of differently captured digital intensity images of that process samples. The feed-forward artificial neural networks and deep auto-encoder network as classifiers are trained for detecting six popular classes of steel defects, i.e., crazing, patches, pitted surface, inclusion, rolled-in scale, and scratches. A comparative study between the two classifiers is done with respect to the cross validation and the confusion matrix. The results demonstrate excellent defect detection outcome with very low false rates.}
}

@inproceedings{kienleMuTTMultimodalTrajectory2024,
  title = {{{MuTT}}: {{A Multimodal Trajectory Transformer}} for {{Robot Skills}}},
  shorttitle = {{{MuTT}}},
  booktitle = {{{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Kienle, Claudius and Alt, Benjamin and Celik, Onur and Becker, Philipp and Katic, Darko and J{\"a}kel, Rainer and Neumann, Gerhard},
  year = {2024},
  month = aug,
  eprint = {2407.15660},
  primaryclass = {cs},
  publisher = {IEEE},
  address = {Abu Dhabi},
  doi = {10.48550/arXiv.2407.15660},
  urldate = {2024-09-19},
  abstract = {High-level robot skills represent an increasingly popular paradigm in robot programming. However, configuring the skills' parameters for a specific task remains a manual and time-consuming endeavor. Existing approaches for learning or optimizing these parameters often require numerous real-world executions or do not work in dynamic environments. To address these challenges, we propose MuTT, a novel encoder-decoder transformer architecture designed to predict environment-aware executions of robot skills by integrating vision, trajectory, and robot skill parameters. Notably, we pioneer the fusion of vision and trajectory, introducing a novel trajectory projection. Furthermore, we illustrate MuTT's efficacy as a predictor when combined with a model-based robot skill optimizer. This approach facilitates the optimization of robot skill parameters for the current environment, without the need for real-world executions during optimization. Designed for compatibility with any representation of robot skills, MuTT demonstrates its versatility across three comprehensive experiments, showcasing superior performance across two different skill representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\benja\\Zotero\\storage\\5R53CYV4\\Kienle et al. - 2024 - MuTT A Multimodal Trajectory Transformer for Robot Skills.pdf;C\:\\Users\\benja\\Zotero\\storage\\N4J4XCHV\\2407.html}
}

@misc{kienleQueryCADGroundedQuestion2024,
  title = {{{QueryCAD}}: {{Grounded Question Answering}} for {{CAD Models}}},
  shorttitle = {{{QueryCAD}}},
  author = {Kienle, Claudius and Alt, Benjamin and Katic, Darko and J{\"a}kel, Rainer},
  year = {2024},
  month = sep,
  number = {arXiv:2409.08704},
  eprint = {2409.08704},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.08704},
  urldate = {2024-09-27},
  abstract = {CAD models are widely used in industry and are essential for robotic automation processes. However, these models are rarely considered in novel AI-based approaches, such as the automatic synthesis of robot programs, as there are no readily available methods that would allow CAD models to be incorporated for the analysis, interpretation, or extraction of information. To address these limitations, we propose QueryCAD, the first system designed for CAD question answering, enabling the extraction of precise information from CAD models using natural language queries. QueryCAD incorporates SegCAD, an open-vocabulary instance segmentation model we developed to identify and select specific parts of the CAD model based on part descriptions. We further propose a CAD question answering benchmark to evaluate QueryCAD and establish a foundation for future research. Lastly, we integrate QueryCAD within an automatic robot program synthesis framework, validating its ability to enhance deep-learning solutions for robotics by enabling them to process CAD models (https://claudius-kienle.github.com/querycad).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics,my},
  file = {C\:\\Users\\benja\\Zotero\\storage\\I2YH4CEC\\Kienle et al. - 2024 - QueryCAD Grounded Question Answering for CAD Models.pdf;C\:\\Users\\benja\\Zotero\\storage\\5EC2KGEA\\2409.html}
}

@phdthesis{Kiesser.2017,
  type = {Masterarbeit},
  title = {Simulation Deformierbarer {{Objekte}} Zur {{Analyse}} Und {{Optimierung}} Des {{Verlaufs}} von {{Kabeln}} in Der Automatisierten {{Fertigung}}},
  author = {Kiesser, Sven},
  year = {2017},
  address = {Karlsruhe},
  school = {Fakult{\"a}t f{\"u}r Informatik, Institut f{\"u}r Anthropromatik und Robotik / Karlsruher Institut f{\"u}r Technologie}
}

@article{kilbyCyberKnifeRoboticRadiosurgery2010,
  title = {The {{CyberKnife Robotic Radiosurgery System}} in 2010},
  author = {Kilby, W. and Dooley, J. R. and Kuduvalli, G. and Sayeh, S. and Maurer, C. R.},
  year = {2010},
  month = oct,
  journal = {Technology in Cancer Research \& Treatment},
  volume = {9},
  number = {5},
  pages = {433--452},
  issn = {1533-0338},
  doi = {10.1177/153303461000900502},
  abstract = {This review provides a complete technical description of the CyberKnife VSI System, the latest addition to the CyberKnife product family, which was released in September 2009. This review updates the previous technical reviews of the original system version published in the late 1990s. Technical developments over the last decade have impacted virtually every aspect of the CyberKnife System. These developments have increased the geometric accuracy of the system and have enhanced the dosimetric accuracy and quality of treatment, with advanced inverse treatment planning algorithms, rapid Monte Carlo dose calculation, and post-processing tools that allow trade-offs between treatment efficiency and dosimetric quality to be explored. This review provides a system overview with detailed descriptions of key subsystems. A detailed review of studies of geometric accuracy is also included, reporting a wide range of experiments involving phantom tests and patient data. Finally, the relationship between technical developments and the greatly increased range of clinical applications they have allowed is reviewed briefly.},
  langid = {english},
  pmid = {20815415},
  keywords = {Algorithms,Humans,Radiation Dosage,Radiosurgery,Robotics,Software}
}

@inproceedings{kilianCaptureEvaluationAirborne1996,
  title = {Capture and {{Evaluation}} of {{Airborne Laser Scanner Data}}},
  author = {Kilian, J. and Haala, N. and Englich, M.},
  year = {1996},
  urldate = {2023-03-02},
  abstract = {The development of laser sensors for the direct measurement of the terrain surface resulted in airborne systems allowing an area covering 3D data capture which are already in commercial use. By the integration of the laser scanner with sensors for the absolute orientation of the laser scanner at the time of measurement, like the NAVSTAR Global Positioning System (GPS) for the positioning task and an Inertial System (INS) for the orientation task, a powerful sensor system for the direct acquisition of 3D terrain data from an aircraft is available. Using scanning laser sensors as the main component of the laser sensor system, the points on the the terrain surface can be measured dense and well-distributed. The main purpose of the data evaluation is to derive an appropriate representation of the sensed (terrain) surfaces. This evaluation of the measured data consists of several single steps. Within this paper the di erent steps are described and results of the data evaluation are presented.}
}

@book{Kim.,
  title = {Procedings of the British Machine Vision Conference 2017},
  editor = {Kim, Tae-Kyun and Zafeiriou, Stefanos and Brostow, Gabriel and Mikolajczyk, Krystian},
  year = {2017},
  publisher = {British Machine Vision Association},
  doi = {10.5244/C.31},
  isbn = {1-901725-60-X}
}

@article{kimAttentiveNeuralProcesses2019,
  title = {Attentive {{Neural Processes}}},
  author = {Kim, Hyunjik and Mnih, Andriy and Schwarz, Jonathan and Garnelo, Marta and Eslami, Ali and Rosenbaum, Dan and Vinyals, Oriol and Teh, Yee Whye},
  year = {2019},
  month = jul,
  journal = {arXiv:1901.05761 [cs, stat]},
  eprint = {1901.05761},
  primaryclass = {cs, stat},
  urldate = {2021-01-17},
  abstract = {Neural Processes (NPs) (Garnelo et al 2018a;b) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\WTXPRBTA\1901.html}
}

@misc{kimOpenVLAOpenSourceVisionLanguageAction2024,
  title = {{{OpenVLA}}: {{An Open-Source Vision-Language-Action Model}}},
  shorttitle = {{{OpenVLA}}},
  author = {Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and Vuong, Quan and Kollar, Thomas and Burchfiel, Benjamin and Tedrake, Russ and Sadigh, Dorsa and Levine, Sergey and Liang, Percy and Finn, Chelsea},
  year = {2024},
  month = jun,
  number = {arXiv:2406.09246},
  eprint = {2406.09246},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.09246},
  urldate = {2024-06-15},
  abstract = {Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5\% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4\%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\7T7YCIJ2\2406.html}
}

@article{kimUnderstandingHumanIntention2017,
  title = {Understanding Human Intention by Connecting Perception and Action Learning in Artificial Agents},
  author = {Kim, Sangwook and Yu, Zhibin and Lee, Minho},
  year = {2017},
  month = aug,
  journal = {Neural Networks},
  series = {Advances in {{Cognitive Engineering Using Neural Networks}}},
  volume = {92},
  pages = {29--38},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2017.01.009},
  urldate = {2024-07-19},
  abstract = {To develop an advanced human--robot interaction system, it is important to first understand how human beings learn to perceive, think, and act in an ever-changing world. In this paper, we propose an intention understanding system that uses an Object Augmented-Supervised Multiple Timescale Recurrent Neural Network (OA-SMTRNN) and demonstrate the effects of perception--action connected learning in an artificial agent, which is inspired by psychological and neurological phenomena in humans. We believe that action and perception are not isolated processes in human mental development, and argue that these psychological and neurological interactions can be replicated in a human--machine scenario. The proposed OA-SMTRNN consists of perception and action modules and their connection, which are constructed of supervised multiple timescale recurrent neural networks and the deep auto-encoder, respectively, and connects their perception and action for understanding human intention. Our experimental results show the effects of perception--action connected learning, and demonstrate that robots can understand human intention with OA-SMTRNN through perception--action connected learning.},
  keywords = {Affordance,Cognitive agent,Human-robot interaction,Intention understanding,Object-Augmented Multiple Timescale Recurrent Neural Network,Perception-action connected learning},
  file = {C:\Users\benja\Zotero\storage\92B6U4YD\S0893608017300096.html}
}

@inproceedings{kimViLTVisionandLanguageTransformer2021,
  title = {{{ViLT}}: {{Vision-and-Language Transformer Without Convolution}} or {{Region Supervision}}},
  shorttitle = {{{ViLT}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  year = {2021},
  month = jul,
  pages = {5583--5594},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-01-05},
  abstract = {Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.},
  langid = {english}
}

@article{kindermannInversionNeuralNetworks1990,
  title = {Inversion of Neural Networks by Gradient Descent},
  author = {Kindermann, J and Linden, A},
  year = {1990},
  month = aug,
  journal = {Parallel Computing},
  volume = {14},
  number = {3},
  pages = {277--286},
  issn = {01678191},
  doi = {10.1016/0167-8191(90)90081-J},
  urldate = {2019-05-17},
  abstract = {Inversion answers the question of which input patterns to a trained multilayer neural network approximate a given output target. This method is a tool for visualization of the information processing capability of a network stored in its weights. This knowledge about the network enables one to make informed decisions on the improvement of the training task and the choice of training sets.},
  langid = {english}
}

@article{Kingma2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  month = dec,
  eprint = {1412.6980},
  urldate = {2019-02-19},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv}
}

@inproceedings{kingmaAdamMethodStochastic2015,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  booktitle = {3rd {{International Conference}} for {{Learning Representations}}},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2015},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  address = {San Diego},
  doi = {10.48550/arXiv.1412.6980},
  urldate = {2023-07-16},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\36J4QBCE\1412.html}
}

@book{kingmaIntroductionVariationalAutoencoders2019,
  title = {{An Introduction to Variational Autoencoders}},
  author = {Kingma, Diederik P. and Max, Welling},
  year = {2019},
  month = nov,
  series = {{Foundations and Trends in Machine Learning}},
  publisher = {Now Publishers Inc},
  address = {Boston Delft},
  abstract = {In this monograph, the authors present an introduction to the framework of variational autoencoders (VAEs) that provides a principled method for jointly learning deep latent-variable models and corresponding inference models using stochastic gradient descent. The framework has a wide array of applications from generative modeling, semi-supervised learning to representation learning.The authors expand earlier work and provide the reader with the fine detail on the important topics giving deep insight into the subject for the expert and student alike. Written in a survey-like nature the text serves as a review for those wishing to quickly deepen their knowledge of the topic.An Introduction to Variational Autoencoders provides a quick summary for the of a topic that has become an important tool in modern-day deep learning techniques.},
  isbn = {978-1-68083-622-6},
  langid = {Englisch}
}

@inproceedings{kirillovPanopticFeaturePyramid2019,
  title = {Panoptic {{Feature Pyramid Networks}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kirillov, Alexander and Girshick, Ross and He, Kaiming and Dollar, Piotr},
  year = {2019},
  pages = {6399--6408},
  urldate = {2023-02-27}
}

@inproceedings{kirillovPanopticSegmentation2019,
  title = {Panoptic {{Segmentation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollar, Piotr},
  year = {2019},
  pages = {9404--9413},
  urldate = {2021-10-25}
}

@inproceedings{kirillovSegmentAnything2023,
  title = {Segment {{Anything}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollar, Piotr and Girshick, Ross},
  year = {2023},
  pages = {4015--4026},
  urldate = {2024-08-27},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\4TGGCIMI\Kirillov et al. - 2023 - Segment Anything.pdf}
}

@article{kirkpatrickOvercomingCatastrophicForgetting2017,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and {Grabska-Barwinska}, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  year = {2017},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {13},
  pages = {3521--3526},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1611835114},
  urldate = {2024-06-01},
  abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.}
}

@incollection{kirshImplicitExplicitRepresentation2006,
  title = {Implicit and {{Explicit Representation}}},
  booktitle = {Encyclopedia of {{Cognitive Science}}},
  author = {Kirsh, David},
  year = {2006},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/0470018860.s00166},
  urldate = {2024-04-16},
  abstract = {The degree to which information is encoded explicitly in a representation is related to the computational cost of recovering or using the information. Knowledge that is implicit in a system need not be represented at all, even implicitly, if the cost of recovering it is prohibitive.},
  copyright = {Copyright {\copyright} 2006 John Wiley \& Sons, Ltd. All rights reserved.},
  isbn = {978-0-470-01886-6},
  langid = {english},
  keywords = {computation,explicit,implicit knowledge,mechanistic explanation,representation},
  file = {C:\Users\benja\Zotero\storage\K8SRJGWK\0470018860.html}
}

@incollection{kirshWhenInformationExplicitly1990,
  title = {When Is {{Information Explicitly Represented}}?},
  booktitle = {Information, {{Language}} and {{Cognition}}},
  author = {Kirsh, David},
  editor = {Hanson, Philip P.},
  year = {1990},
  publisher = {University of British Columbia Press},
  urldate = {2024-04-15}
}

@inproceedings{kittmannLetMeIntroduce2015,
  title = {Let Me {{Introduce Myself}}: {{I}} Am {{Care-O-bot}} 4, a {{Gentleman Robot}}},
  shorttitle = {Let Me {{Introduce Myself}}},
  booktitle = {Mensch Und {{Computer}} 2015. {{Proceedings}}},
  author = {Kittmann, Ralf and Fr{\"o}hlich, Tim and Sch{\"a}fer, Johannes and Reiser, Ulrich},
  year = {2015},
  pages = {223--232},
  isbn = {978-3-11-044393-6},
  file = {C:\Users\benja\Zotero\storage\GG48KV5Y\N-421345.html}
}

@inproceedings{klambauerSelfNormalizingNeuralNetworks2017,
  title = {Self-{{Normalizing Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-27},
  abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. For FNNs we considered (i) ReLU networks without normalization, (ii) batch normalization, (iii) layer normalization, (iv) weight normalization, (v) highway networks, (vi) residual networks. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep.}
}

@incollection{kleeGraphBasedTaskLibraries2015,
  title = {Graph-{{Based Task Libraries}} for {{Robots}}: {{Generalization}} and {{Autocompletion}}},
  shorttitle = {Graph-{{Based Task Libraries}} for {{Robots}}},
  booktitle = {{{AI}}*{{IA}} 2015 {{Advances}} in {{Artificial Intelligence}}},
  author = {Klee, Steven D. and Gemignani, Guglielmo and Nardi, Daniele and Veloso, Manuela},
  editor = {Gavanelli, Marco and Lamma, Evelina and Riguzzi, Fabrizio},
  year = {2015},
  volume = {9336},
  pages = {397--409},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-24309-2_30},
  urldate = {2019-06-30},
  abstract = {In this paper, we consider an autonomous robot that persists over time performing tasks and the problem of providing one additional task to the robot's task library. We present an approach to generalize tasks, represented as parameterized graphs with sequences, conditionals, and looping constructs of sensing and actuation primitives. Our approach performs graph-structure task generalization, while maintaining task executability and parameter value distributions. We present an algorithm that, given the initial steps of a new task, proposes an autocompletion based on a recognized past similar task. Our generalization and autocompletion contributions are effective on different real robots. We show concrete examples of the robot primitives and task graphs, as well as results, with Baxter. In experiments with multiple tasks, we show a significant reduction in the number of new task steps to be provided.},
  isbn = {978-3-319-24308-5 978-3-319-24309-2},
  langid = {english}
}

@book{kleinRobotikGesundheitswirtschaftEinsatzfelder2018,
  title = {Robotik in Der {{Gesundheitswirtschaft}} : {{Einsatzfelder}} Und {{Potenziale}}},
  shorttitle = {Robotik in Der {{Gesundheitswirtschaft}}},
  author = {Klein, Barbara and Graf, Birgit and Schl{\"o}mer, Inga and R{\"o}hricht, Karin and Baumgarten, Simon},
  year = {2018},
  month = feb,
  publisher = {medhochzwei Verlag GmbH},
  isbn = {978-3-86216-388-5}
}

@inproceedings{klimekHierarchicalReinforcementLearning2017,
  title = {Hierarchical {{Reinforcement Learning}} with {{Parameters}}},
  booktitle = {Proceedings of the 1st {{Annual Conference}} on {{Robot Learning}}},
  author = {Klimek, Maciej and Michalewski, Henryk and Mi{\textbackslash}lo{\'s}, Piotr},
  year = {2017},
  month = oct,
  pages = {301--313},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-03-08},
  abstract = {In this work we introduce and evaluate a model of Hierarchical Reinforcement Learning with Parameters. In the first stage we train agents to execute relatively simple actions like reaching or gripping. In the second stage we train a hierarchical manager to compose these actions to solve more complicated tasks.  The manager may pass parameters to agents thus controlling details of undertaken actions.  The hierarchical approach with parameters can be used with any optimization algorithm.   In this work we adapt to our setting methods described in [1]. We show that their theoretical foundation, including monotonicity of improvements, still holds. We experimentally compare the hierarchical reinforcement learning with the standard, non-hierarchical approach and conclude that the hierarchical learning with parameters is a viable way to improve final results and stability of learning.},
  langid = {english}
}

@incollection{kluyMenschRoboterKollaborationKMUPotenziale2022,
  title = {{Mensch-Roboter-Kollaboration in KMU -- Potenziale identifizieren, analysieren und realisieren}},
  booktitle = {{Digitalisierung der Arbeitswelt im Mittelstand 1: Ergebnisse und Best Practice des BMBF-Forschungsschwerpunkts "Zukunft der Arbeit: Mittelstand -- innovativ und sozial"}},
  author = {Kluy, Lina and K{\"o}lmel, Lena and Alt, Benjamin and Baumgartner, Marco and Deml, Barbara and Hornung, Luisa and Katic, Darko and Kinkel, Steffen and Kopp, Tobias and Lorenz, Maureen and Nicolai, Philip and Riedel, Norman and Sch{\"a}fer, Arndt and Wurll, Christian},
  editor = {Nitsch, Verena and Brandl, Christopher and H{\"a}u{\ss}ling, Roger and Lemm, Jacqueline and Gries, Thomas and Schmenk, Bernhard},
  year = {2022},
  pages = {55--97},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-64803-2_3},
  urldate = {2022-08-01},
  abstract = {Die Einf{\"u}hrung von kollaborativen Robotern (Cobots) verspricht eine aus arbeitswissenschaftlicher Sichtweise sinnvolle Zusammenarbeit zwischen Mensch und Roboter. Gerade kleinen und mittleren Unternehmen (KMU) gelingt es allerdings h{\"a}ufig nicht oder nur teilweise, die damit verbundenen Potenziale zu erschlie{\ss}en. Das vorliegende Kapitel beschreibt zum einen die Identifizierung, Analyse und Realisierung von Cobot-Potenzialen im Projekt ProBot (,,Proaktive Diagnose und Gestaltung des CoBot-Einsatzes in kleinen und mittleren Unternehmen``). Zum anderen wird die Entwicklung und der Aufbau einer interaktiven Einf{\"u}hrungsunterst{\"u}tzung zur niederschwelligen Planung und Umsetzung von Mensch-Cobot-ArbeitssystemenMensch-Cobot-Arbeitssystem (MCA) (MCA) skizziert, wobei einige der enthaltenen Methoden vertieft vorgestellt werden. Dar{\"u}ber hinaus werden anhand realer Fallbeispiele von KMU die M{\"o}glichkeiten und Grenzen von Mensch-Roboter-Kollaborationen (MRK) anschaulich dargestellt. Die interaktive Einf{\"u}hrungsunterst{\"u}tzung (KMU Cobot Coach) wird interessierten KMU unter folgender Adresse zur freien Verf{\"u}gung online bereitgestellt: https://www.kmu-cobot-coach.de/.},
  copyright = {All rights reserved},
  isbn = {978-3-662-64803-2},
  langid = {ngerman},
  keywords = {my}
}

@misc{knollSoftwareDevelopmentWorkflow,
  title = {Software {{Development Workflow}} in {{Robotics}}},
  author = {Knoll, Alois and Barner, Simon and Geisinger, Michael and Rickert, Markus},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\2MU6CFDT\Knoll et al. - Software Development Workflow in Robotics.pdf}
}

@article{knoopAbstractTaskKnowledge2008,
  title = {From {{Abstract Task Knowledge}} to {{Executable Robot Programs}}},
  author = {Knoop, Steffen and Pardowitz, Michael and Dillmann, R{\"u}diger},
  year = {2008},
  month = aug,
  journal = {Journal of Intelligent and Robotic Systems},
  volume = {52},
  number = {3-4},
  pages = {343--362},
  issn = {0921-0296, 1573-0409},
  doi = {10.1007/s10846-008-9221-x},
  urldate = {2019-06-30},
  abstract = {Robots that are capable of learning new tasks from humans need the ability to transform gathered abstract task knowledge into their own representation and dimensionality. New task knowledge that has been collected e.g. with Programming by Demonstration approaches by observing a human does not a-priori contain any robot-specific knowledge and actions, and is defined in the workspace of the human demonstrator. This article presents a new approach for mapping abstract humancentered task knowledge to a robot execution system based on the target system properties. Therefore the required background knowledge about the target system is examined and defined explicitly.},
  langid = {english}
}

@article{knuthLiterateProgramming1984,
  title = {Literate {{Programming}}},
  author = {Knuth, D. E.},
  year = {1984},
  month = jan,
  journal = {The Computer Journal},
  volume = {27},
  number = {2},
  pages = {97--111},
  issn = {0010-4620},
  doi = {10.1093/comjnl/27.2.97},
  urldate = {2024-03-07},
  abstract = {The author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. This paper presents WEB by example, and discusses why the new system appears to be an improvement over previous ones.},
  file = {C:\Users\benja\Zotero\storage\KU9WT5FH\343244.html}
}

@article{koberReinforcementLearningRobotics2013,
  title = {Reinforcement Learning in Robotics: {{A}} Survey},
  shorttitle = {Reinforcement Learning in Robotics},
  author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
  year = {2013},
  month = sep,
  journal = {The International Journal of Robotics Research},
  volume = {32},
  number = {11},
  pages = {1238--1274},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364913495721},
  urldate = {2019-07-14},
  abstract = {Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.},
  langid = {english}
}

@inproceedings{koenemannRealtimeImitationHuman2014,
  title = {Real-Time Imitation of Human Whole-Body Motions by Humanoids},
  booktitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Koenemann, J. and Burget, F. and Bennewitz, M.},
  year = {2014},
  month = may,
  pages = {2806--2812},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2014.6907261},
  abstract = {In this paper, we present a system that enables humanoid robots to imitate complex whole-body motions of humans in real time. In our approach, we use a compact human model and consider the positions of the end-effectors as well as the center of mass as the most important aspects to imitate. Our system actively balances the center of mass over the support polygon to avoid falls of the robot, which would occur when using direct imitation. For every point in time, our approach generates a statically stable pose. Hereby, we do not constrain the configurations to be in double support. Instead, we allow for changes of the support mode according to the motions to imitate. To achieve safe imitation, we use retargeting of the robot's feet if necessary and find statically stable configurations by inverse kinematics. We present experiments using human data captured with an Xsens MVN motion capture system. The results show that a Nao humanoid is able to reliably imitate complex whole-body motions in real time, which also include extended periods of time in single support mode, in which the robot has to balance on one foot.},
  keywords = {end effectors,end-effectors,Foot,human whole-body motions,humanoid robots,inverse kinematics,Joints,Kinematics,manipulator kinematics,motion control,Nao humanoid,real-time imitation,Real-time systems,Robots,Stability analysis,Trajectory,Xsens MVN motion capture system}
}

@inproceedings{koertDemonstrationBasedTrajectory2016,
  title = {Demonstration Based Trajectory Optimization for Generalizable Robot Motions},
  booktitle = {2016 {{IEEE-RAS}} 16th {{International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  author = {Koert, Dorothea and Maeda, Guilherme and Lioutikov, Rudolf and Neumann, Gerhard and Peters, Jan},
  year = {2016},
  month = nov,
  pages = {515--522},
  issn = {2164-0580},
  doi = {10.1109/HUMANOIDS.2016.7803324},
  urldate = {2024-04-09},
  abstract = {Learning motions from human demonstrations provides an intuitive way for non-expert users to teach tasks to robots. In particular, intelligent robotic co-workers should not only mimic human demonstrations but should also be able to adapt them to varying application scenarios. As such, robots must have the ability to generalize motions to different workspaces, e.g. to avoid obstacles not present during original demonstrations. Towards this goal our work proposes a unified method to (1) generalize robot motions to different workspaces, using a novel formulation of trajectory optimization that explicitly incorporates human demonstrations, and (2) to locally adapt and reuse the optimized solution in the form of a distribution of trajectories. This optimized distribution can be used, online, to quickly satisfy via-points and goals of a specific task. We validate the method using a 7 degrees of freedom (DoF) lightweight arm that grasps and places a ball into different boxes while avoiding obstacles that were not present during the original human demonstrations.},
  keywords = {Collision avoidance,Probabilistic logic,Robot motion,Trajectory optimization},
  file = {C:\Users\benja\Zotero\storage\A3DAWCBH\7803324.html}
}

@article{kojimaLargeLanguageModels2022,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang (Shane) and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {22199--22213},
  urldate = {2024-06-16},
  langid = {english}
}

@article{kokTrustRobotsChallenges2020,
  title = {Trust in {{Robots}}: {{Challenges}} and {{Opportunities}}},
  shorttitle = {Trust in {{Robots}}},
  author = {Kok, Bing Cai and Soh, Harold},
  year = {2020},
  month = dec,
  journal = {Current Robotics Reports},
  volume = {1},
  number = {4},
  pages = {297--309},
  issn = {2662-4087},
  doi = {10.1007/s43154-020-00029-y},
  urldate = {2022-05-05},
  abstract = {To assess the state-of-the-art in research on trust in robots and to examine if recent methodological advances can aid in the development of trustworthy robots.},
  langid = {english},
  keywords = {Formal methods,Human-robot interaction,Measurement,Probabilistic models,Trust}
}

@inproceedings{kolaricLocalPolicyOptimization2020,
  title = {Local {{Policy Optimization}} for {{Trajectory-Centric Reinforcement Learning}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Kolaric, Patrik and Jha, Devesh K. and Raghunathan, Arvind U. and Lewis, Frank L. and Benosman, Mouhacine and Romeres, Diego and Nikovski, Daniel},
  year = {2020},
  month = may,
  pages = {5094--5100},
  issn = {2577-087X},
  doi = {10.1109/ICRA40945.2020.9197058},
  urldate = {2024-09-07},
  abstract = {The goal of this paper is to present a method for simultaneous trajectory and local stabilizing policy optimization to generate local policies for trajectory-centric model-based reinforcement learning (MBRL). This is motivated by the fact that global policy optimization for non-linear systems could be a very challenging problem both algorithmically and numerically. However, a lot of robotic manipulation tasks are trajectory-centric, and thus do not require a global model or policy. Due to inaccuracies in the learned model estimates, an open-loop trajectory optimization process mostly results in very poor performance when used on the real system. Motivated by these problems, we try to formulate the problem of trajectory optimization and local policy synthesis as a single optimization problem. It is then solved simultaneously as an instance of nonlinear programming. We provide some results for analysis as well as achieved performance of the proposed technique under some simplifying assumptions.},
  keywords = {Learning (artificial intelligence),Robots,Robustness,Trajectory optimization,Uncertainty},
  file = {C\:\\Users\\benja\\Zotero\\storage\\SKX6HC7I\\Kolaric et al. - 2020 - Local Policy Optimization for Trajectory-Centric Reinforcement Learning.pdf;C\:\\Users\\benja\\Zotero\\storage\\85Y9FW2Q\\9197058.html}
}

@article{kolbingerArtificialIntelligenceContextaware2023,
  title = {Artificial {{Intelligence}} for Context-Aware Surgical Guidance in Complex Robot-Assisted Oncological Procedures: {{An}} Exploratory Feasibility Study},
  shorttitle = {Artificial {{Intelligence}} for Context-Aware Surgical Guidance in Complex Robot-Assisted Oncological Procedures},
  author = {Kolbinger, Fiona R. and Bodenstedt, Sebastian and Carstens, Matthias and Leger, Stefan and Krell, Stefanie and Rinner, Franziska M. and Nielen, Thomas P. and Kirchberg, Johanna and Fritzmann, Johannes and Weitz, J{\"u}rgen and Distler, Marius and Speidel, Stefanie},
  year = {2023},
  month = jul,
  journal = {European Journal of Surgical Oncology: The Journal of the European Society of Surgical Oncology and the British Association of Surgical Oncology},
  pages = {106996},
  issn = {1532-2157},
  doi = {10.1016/j.ejso.2023.106996},
  abstract = {INTRODUCTION: Complex oncological procedures pose various surgical challenges including dissection in distinct tissue planes and preservation of vulnerable anatomical structures throughout different surgical phases. In rectal surgery, violation of dissection planes increases the risk of local recurrence and autonomous nerve damage resulting in incontinence and sexual dysfunction. This work explores the feasibility of phase recognition and target structure segmentation in robot-assisted rectal resection (RARR) using machine learning. MATERIALS AND METHODS: A total of 57 RARR were recorded and subsets of these were annotated with respect to surgical phases and exact locations of target structures (anatomical structures, tissue types, static structures, and dissection areas). For surgical phase recognition, three machine learning models were trained: LSTM, MSTCN, and Trans-SVNet. Based on pixel-wise annotations of target structures in 9037 images, individual segmentation models based on DeepLabv3 were trained. Model performance was evaluated using F1 score, Intersection-over-Union (IoU), accuracy, precision, recall, and specificity. RESULTS: The best results for phase recognition were achieved with the MSTCN model (F1 score: 0.82~{\textpm}~0.01, accuracy: 0.84~{\textpm}~0.03). Mean IoUs for target structure segmentation ranged from 0.14~{\textpm}~0.22 to 0.80~{\textpm}~0.14 for organs and tissue types and from 0.11~{\textpm}~0.11 to 0.44~{\textpm}~0.30 for dissection areas. Image quality, distorting factors (i.e. blood, smoke), and technical challenges (i.e. lack of depth perception) considerably impacted segmentation performance. CONCLUSION: Machine learning-based phase recognition and segmentation of selected target structures are feasible in RARR. In the future, such functionalities could be integrated into a context-aware surgical guidance system for rectal surgery.},
  langid = {english},
  pmid = {37591704},
  keywords = {Dissection plane,Mesorectal fascia,Rectal resection,Robot-assisted surgery,Surgical data science,Surgical oncology}
}

@inproceedings{kolesnikovBigTransferBiT2020,
  title = {Big {{Transfer}} ({{BiT}}): {{General Visual Representation Learning}}},
  shorttitle = {Big {{Transfer}} ({{BiT}})},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2020: 16th {{European Conference}}, {{Glasgow}}, {{UK}}, {{August}} 23--28, 2020, {{Proceedings}}, {{Part V}}},
  author = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
  year = {2020},
  month = aug,
  pages = {491--507},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-030-58558-7_29},
  urldate = {2024-06-14},
  abstract = {Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes---from 1 example per class to 1M total examples. BiT achieves 87.5\% top-1 accuracy on ILSVRC-2012, 99.4\% on CIFAR-10, and 76.3\% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8\% on ILSVRC-2012 with 10 examples per class, and 97.0\% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.},
  isbn = {978-3-030-58557-0}
}

@article{kongHumanActionRecognition2022,
  title = {Human {{Action Recognition}} and {{Prediction}}: {{A Survey}}},
  shorttitle = {Human {{Action Recognition}} and {{Prediction}}},
  author = {Kong, Yu and Fu, Yun},
  year = {2022},
  month = may,
  journal = {International Journal of Computer Vision},
  volume = {130},
  number = {5},
  pages = {1366--1401},
  issn = {1573-1405},
  doi = {10.1007/s11263-022-01594-9},
  urldate = {2023-03-10},
  abstract = {Derived from rapid advances in computer vision and machine learning, video analysis tasks have been moving from inferring the present state to predicting the future state. Vision-based action recognition and prediction from videos are such tasks, where action recognition is to infer human actions (present state) based upon complete action executions, and action prediction to predict human actions (future state) based upon incomplete action executions. These two tasks have become particularly prevalent topics recently because of their explosively emerging real-world applications, such as visual surveillance, autonomous driving vehicle, entertainment, and video retrieval, etc. Many attempts have been devoted in the last a few decades in order to build a robust and effective framework for action recognition and prediction. In this paper, we survey the complete state-of-the-art techniques in action recognition and prediction. Existing models, popular algorithms, technical difficulties, popular action databases, evaluation protocols, and promising future directions are also provided with systematic discussions.},
  langid = {english},
  keywords = {Action prediction,Action recognition,Survey,Video data}
}

@article{konidarisRobotLearningDemonstration2012,
  title = {Robot Learning from Demonstration by Constructing Skill Trees},
  author = {Konidaris, George and Kuindersma, Scott and Grupen, Roderic and Barto, Andrew},
  year = {2012},
  month = mar,
  journal = {The International Journal of Robotics Research},
  volume = {31},
  number = {3},
  pages = {360--375},
  issn = {0278-3649},
  doi = {10.1177/0278364911428653},
  urldate = {2022-03-10},
  abstract = {We describe CST, an online algorithm for constructing skill trees from demonstration trajectories. CST segments a demonstration trajectory into a chain of component skills, where each skill has a goal and is assigned a suitable abstraction from an abstraction library. These properties permit skills to be improved efficiently using a policy learning algorithm. Chains from multiple demonstration trajectories are merged into a skill tree. We show that CST can be used to acquire skills from human demonstration in a dynamic continuous domain, and from both expert demonstration and learned control sequences on the uBot-5 mobile manipulator.},
  langid = {english},
  keywords = {changepoint detection,hierarchical reinforcement learning,Learning from demonstration,motion primitives,motion segmentation},
  file = {C:\Users\benja\Zotero\storage\SZSLPZK8\Konidaris et al. - 2012 - Robot learning from demonstration by constructing .pdf}
}

@incollection{Konolige.2016,
  title = {Range Sensing},
  booktitle = {Springer Handbook of Robotics},
  author = {Konolige, Kurt and N{\"u}chter, Andreas},
  editor = {Siciliano, Bruno and Khatib, Oussama},
  year = {2016},
  volume = {18},
  pages = {783--810},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-32552-1_31},
  bookpagination = {page},
  isbn = {978-3-319-32550-7}
}

@article{Koos13,
  title = {The {{Transferability Approach}}: {{Crossing}} the {{Reality Gap}} in {{Evolutionary Robotics}}},
  author = {Koos, Sylvain and Mouret, Jean-Baptiste and Doncieux, St{\'e}phane},
  year = {2013},
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {17},
  number = {1},
  pages = {122--145}
}

@article{koralewskiSelfSpecializationGeneralRobot2019,
  title = {Self-{{Specialization}} of {{General Robot Plans Based}} on {{Experience}}},
  author = {Koralewski, Sebastian and Kazhoyan, Gayane and Beetz, M.},
  year = {2019},
  journal = {IEEE Robotics and Automation Letters},
  doi = {10.1109/LRA.2019.2928771},
  abstract = {For robots to work outside of laboratory settings, their plans should be applicable to a variety of environments, objects, task contexts, and hardware platforms. This requires general-purpose methods that are, at this moment, not sufficiently performant for real-world applications. We propose an approach to specialize such general plans through running them for specific tasks and thereby learning appropriate specializations from experience. We present a system architecture, which collects data during plan execution for making up supervised learning problems and utilizes learned models for specializing the plans in a closed loop. We demonstrate our approach by letting a PR2 robot specialize its general fetch and place plan, whereby learned results are automatically installed into the plan. We show that the specialized plan performs better than the original plan in a statistically significant sense.}
}

@inproceedings{korberTheoreticalConsiderationsDevelopment2019,
  title = {Theoretical {{Considerations}} and {{Development}} of a {{Questionnaire}} to {{Measure Trust}} in {{Automation}}},
  booktitle = {Proceedings of the 20th {{Congress}} of the {{International Ergonomics Association}} ({{IEA}} 2018)},
  author = {K{\"o}rber, Moritz},
  editor = {Bagnara, Sebastiano and Tartaglia, Riccardo and Albolino, Sara and Alexander, Thomas and Fujita, Yushi},
  year = {2019},
  pages = {13--30},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-96074-6_2},
  abstract = {The increasing number of interactions with automated systems has sparked the interest of researchers in trust in automation because it predicts not only whether but also how an operator interacts with an automation. In this work, a theoretical model of trust in automation is established and the development and evaluation of a corresponding questionnaire (Trust in Automation, TiA) are described.},
  isbn = {978-3-319-96074-6},
  langid = {english},
  keywords = {Automated driving,Questionnaire,Trust in automation}
}

@inproceedings{kornblithBetterImageNetModels2019,
  title = {Do {{Better ImageNet Models Transfer Better}}?},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
  year = {2019},
  pages = {2661--2671},
  urldate = {2024-06-14}
}

@incollection{kortenkampRoboticSystemsArchitectures2016,
  title = {Robotic {{Systems Architectures}} and {{Programming}}},
  booktitle = {Springer {{Handbook}} of {{Robotics}}},
  author = {Kortenkamp, David and Simmons, Reid and Brugali, Davide},
  editor = {Siciliano, Bruno and Khatib, Oussama},
  year = {2016},
  series = {Springer {{Handbooks}}},
  pages = {283--306},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-32552-1_12},
  urldate = {2024-03-08},
  abstract = {Robot software systems tend to be complex. This complexity is due, in large part, to the need to control diverse sensors and actuators in real time, in the face of significant uncertainty and noise. Robot systems must work to achieve tasks while monitoring for, and reacting to, unexpected situations. Doing all this concurrently and asynchronously adds immensely to system complexity.},
  isbn = {978-3-319-32552-1},
  langid = {english},
  keywords = {Architectural Style,Common Object Request Broker Architecture,Interprocess Communication,Robot System,Unify Modeling Language}
}

@mastersthesis{kosinskiFlexibleAnomaliedetektionBasierend2018,
  title = {Flexible {{Anomaliedetektion}} Basierend Auf {{Videodaten}} F{\"u}r Fortlaufende {{Prozesse}} in Der {{Robotik}}},
  author = {Kosinski, Daniel},
  year = {2018},
  address = {Karlsruhe, Germany},
  school = {Karlsruhe Institute of Technology},
  file = {C:\Users\benja\Zotero\storage\JDLZZZXE\DanielKosinski-AnomalieDetektion.pdf}
}

@article{kotMethodRobotManipulator2021,
  title = {Method for {{Robot Manipulator Joint Wear Reduction}} by {{Finding}} the {{Optimal Robot Placement}} in a {{Robotic Cell}}},
  author = {Kot, Tom{\'a}{\v s} and Bobovsk{\'y}, Zdenko and Vysock{\'y}, Ale{\v s} and Krys, V{\'a}clav and {\v S}afa{\v r}{\'i}k, Jakub and Ru{\v z}arovsk{\'y}, Roman},
  year = {2021},
  month = jan,
  journal = {Applied Sciences},
  volume = {11},
  number = {12},
  pages = {5398},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app11125398},
  urldate = {2022-08-02},
  abstract = {We describe a method for robotic cell optimization by changing the placement of the robot manipulator within the cell in applications with a fixed end-point trajectory. The goal is to reduce the overall robot joint wear and to prevent uneven joint wear when one or several joints are stressed more than the other joints. Joint wear is approximated by calculating the integral of the mechanical work of each joint during the whole trajectory, which depends on the joint angular velocity and torque. The method relies on using a dynamic simulation for the evaluation of the torques and velocities in robot joints for individual robot positions. Verification of the method was performed using CoppeliaSim and a laboratory robotic cell with the collaborative robot UR3. The results confirmed that, with proper robot base placement, the overall wear of the joints of a robotic arm could be reduced from 22\% to 53\% depending on the trajectory.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {manipulator,optimization,robot,robotic cell,robotized workplace,wear}
}

@article{kotseruba40YearsCognitive2020,
  title = {40 Years of Cognitive Architectures: Core Cognitive Abilities and Practical Applications},
  shorttitle = {40 Years of Cognitive Architectures},
  author = {Kotseruba, Iuliia and Tsotsos, John K.},
  year = {2020},
  month = jan,
  journal = {Artificial Intelligence Review},
  volume = {53},
  number = {1},
  pages = {17--94},
  issn = {1573-7462},
  doi = {10.1007/s10462-018-9646-y},
  urldate = {2024-01-24},
  abstract = {In this paper we present a broad overview of the last 40~years of research on cognitive architectures. To date, the number of existing architectures has reached several hundred, but most of the existing surveys do not reflect this growth and instead focus on a handful of well-established architectures. In this survey we aim to provide a more inclusive and high-level overview of the research on cognitive architectures. Our final set of 84 architectures includes 49 that are still actively developed, and borrow from a diverse set of disciplines, spanning areas from psychoanalysis to neuroscience. To keep the length of this paper within reasonable limits we discuss only the core cognitive abilities, such as perception, attention mechanisms, action selection, memory, learning, reasoning and metareasoning. In order to assess the breadth of practical applications of cognitive architectures we present information on over 900 practical projects implemented using the cognitive architectures in our list. We use various visualization techniques to highlight the overall trends in the development of the field. In addition to summarizing the current state-of-the-art in the cognitive architecture research, this survey describes a variety of methods and ideas that have been tried and their relative success in modeling human cognitive abilities, as well as which aspects of cognitive behavior need more research with respect to their mechanistic counterparts and thus can further inform how cognitive science might progress.},
  langid = {english},
  keywords = {Attention,Cognitive abilities,Cognitive architectures,Perception,Practical applications,Survey}
}

@article{koverolaGeneralAttitudesRobots2022,
  title = {General {{Attitudes Towards Robots Scale}} ({{GAToRS}}): {{A New Instrument}} for {{Social Surveys}}},
  shorttitle = {General {{Attitudes Towards Robots Scale}} ({{GAToRS}})},
  author = {Koverola, Mika and Kunnari, Anton and Sundvall, Jukka and Laakasuo, Michael},
  year = {2022},
  month = sep,
  journal = {International Journal of Social Robotics},
  volume = {14},
  number = {7},
  pages = {1559--1581},
  issn = {1875-4805},
  doi = {10.1007/s12369-022-00880-3},
  urldate = {2024-03-25},
  abstract = {Psychometric scales are useful tools in understanding people's attitudes towards different aspects of life. As societies develop and new technologies arise, new validated scales are needed. Robots and artificial intelligences of various kinds are about to occupy just about every niche in human society. Several tools to measure fears and anxieties about robots do exist, but there is a definite lack of tools to measure hopes and expectations for these new technologies. Here, we create and validate a novel multi-dimensional scale which measures people's attitudes towards robots, giving equal weight to positive and negative attitudes. Our scale differentiates (a) comfort and enjoyment around robots, (b) unease and anxiety around robots, (c) rational hopes about robots in general (at societal level) and (d) rational worries about robots in general (at societal level). The scale was developed by extracting items from previous scales, crowdsourcing new items, testing through 3 scale iterations by exploratory factor analysis (Ns 135, 801 and 609) and validated in its final form of the scale by confirmatory factor analysis (N: 477). We hope our scale will be a useful instrument for social scientists who wish to study human-technology relations with a validated scale in efficient and generalizable ways.},
  langid = {english},
  keywords = {General attitudes,Multi-dimensional measurement,Robot,Robotics,Self-reporting measure}
}

@article{kramerAbstractionKeyComputing2007,
  title = {Is Abstraction the Key to Computing?},
  author = {Kramer, Jeff},
  year = {2007},
  month = apr,
  journal = {Communications of the ACM},
  volume = {50},
  number = {4},
  pages = {36--42},
  issn = {0001-0782},
  doi = {10.1145/1232743.1232745},
  urldate = {2024-09-20},
  abstract = {Why is it that some software engineers and computer scientists are able to produce clear, elegant designs and programs, while others cannot? Is it possible to improve these skills through education and training? Critical to these questions is the notion of abstraction.},
  file = {C:\Users\benja\Zotero\storage\TJ9VJKQH\Kramer - 2007 - Is abstraction the key to computing.pdf}
}

@article{kratzerPredictionHumanFullBody2020,
  title = {Prediction of {{Human Full-Body Movements}} with {{Motion Optimization}} and {{Recurrent Neural Networks}}},
  author = {Kratzer, Philipp and Toussaint, Marc and Mainprice, Jim},
  year = {2020},
  month = mar,
  journal = {arXiv:1910.01843 [cs]},
  eprint = {1910.01843},
  primaryclass = {cs},
  urldate = {2020-07-16},
  abstract = {Human movement prediction is difficult as humans naturally exhibit complex behaviors that can change drastically from one environment to the next. In order to alleviate this issue, we propose a prediction framework that decouples short-term prediction, linked to internal body dynamics, and long-term prediction, linked to the environment and task constraints. In this work we investigate encoding short-term dynamics in a recurrent neural network, while we account for environmental constraints, such as obstacle avoidance, using gradient-based trajectory optimization. Experiments on real motion data demonstrate that our framework improves the prediction with respect to state-of-the-art motion prediction methods, as it accounts to beforehand unseen environmental structures. Moreover we demonstrate on an example, how this framework can be used to plan robot trajectories that are optimized to coordinate with a human partner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\NZVD588R\1910.html}
}

@article{kristiadiBeingBayesianEven2020,
  title = {Being {{Bayesian}}, {{Even Just}} a {{Bit}}, {{Fixes Overconfidence}} in {{ReLU Networks}}},
  author = {Kristiadi, Agustinus and Hein, Matthias and Hennig, Philipp},
  year = {2020},
  month = jul,
  journal = {arXiv:2002.10118 [cs, stat]},
  eprint = {2002.10118},
  primaryclass = {cs, stat},
  urldate = {2021-05-04},
  abstract = {The point estimates of ReLU classification networks---arguably the most widely used neural network architecture---have been shown to yield arbitrarily high confidence far away from the training data. This architecture, in conjunction with a maximum a posteriori estimation scheme, is thus not calibrated nor robust. Approximate Bayesian inference has been empirically demonstrated to improve predictive uncertainty in neural networks, although the theoretical analysis of such Bayesian approximations is limited. We theoretically analyze approximate Gaussian distributions on the weights of ReLU networks and show that they fix the overconfidence problem. Furthermore, we show that even a simplistic, thus cheap, Bayesian approximation, also fixes these issues. This indicates that a sufficient condition for a calibrated uncertainty on a ReLU network is "to be a bit Bayesian". These theoretical results validate the usage of last-layer Bayesian approximation and motivate a range of a fidelity-cost trade-off. We further validate these findings empirically via various standard experiments using common deep ReLU networks and Laplace approximations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\R3PNAI7B\2002.html}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  month = dec,
  volume = {1},
  pages = {1097--1105},
  address = {Lake Tahoe, Nevada, USA},
  urldate = {2019-07-10},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english}
}

@article{kroemerReviewRobotLearning2021,
  title = {A {{Review}} of {{Robot Learning}} for {{Manipulation}}: {{Challenges}}, {{Representations}}, and {{Algorithms}}},
  shorttitle = {A {{Review}} of {{Robot Learning}} for {{Manipulation}}},
  author = {Kroemer, Oliver and Niekum, Scott and Konidaris, George},
  year = {2021},
  journal = {Journal of Machine Learning Research},
  volume = {22},
  eprint = {1907.03146},
  pages = {1--82},
  urldate = {2020-04-19},
  abstract = {A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen substantial growth in research on the problem of robot manipulation, which aims to exploit the increasing availability of affordable robot arms and grippers to create robots capable of directly interacting with the world to achieve their goals. Learning will be central to such autonomous systems, as the real world contains too much variation for a robot to expect to have an accurate model of its environment, the objects in it, or the skills required to manipulate them, in advance. We aim to survey a representative subset of that research which uses machine learning for manipulation. We describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework and highlight the many remaining research opportunities and challenges.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics}
}

@inproceedings{krogerSimpleRobustVisual2012,
  title = {Simple and Robust Visual Servo Control of Robot Arms Using an On-Line Trajectory Generator},
  booktitle = {{{ICRA}}},
  author = {Kroger, Torsten and Padial, Jose},
  year = {2012},
  month = may,
  pages = {4862--4869},
  doi = {10.1109/ICRA.2012.6225175},
  urldate = {2019-10-10},
  abstract = {Common visual servoing methods use image features to define a signal error in the feedback loops of robot motion controllers. This paper suggests a new visual servo control scheme that uses an on-line trajectory generator as an intermediate layer between image processing algorithms and robot motion controllers. The motion generation algorithm is capable of computing an entire trajectory from an arbitrary initial state of motion within one servo control cycle (typically one millisecond or less). This algorithm is fed with desired pose and velocity signals that are generated by an image processing algorithm. The advantages of this new architecture are: (a) jerklimited and continuous motions are guaranteed independently of image processing signals, (b) kinematic motion constraints as well as physical and/or artificial workspace limits can be directly considered, and (c) the system can instantaneously and safely react to sensor failures (e.g., if cameras are covered or image processing fails). Real-world experimental results using a seven-joint robot arm are presented to underline the relevance for the field of robust sensor-guided robot motion control.},
  isbn = {978-1-4673-1405-3 978-1-4673-1403-9 978-1-4673-1578-4 978-1-4673-1404-6},
  langid = {english}
}

@inproceedings{krohlingGaussianSwarmNovel2004,
  title = {Gaussian Swarm: A Novel Particle Swarm Optimization Algorithm},
  shorttitle = {Gaussian Swarm},
  booktitle = {{{IEEE Conference}} on {{Cybernetics}} and {{Intelligent Systems}}, 2004.},
  author = {Krohling, R.A.},
  year = {2004},
  month = dec,
  volume = {1},
  pages = {372-376 vol.1},
  doi = {10.1109/ICCIS.2004.1460443},
  abstract = {In this paper, a novel particle swarm optimization algorithm based on the Gaussian probability distribution is proposed. The standard particle swarm optimization (PSO) algorithm has some parameters that need to be specified before using the algorithm, e.g., the accelerating constants c/sub 1/ and c/sub 2/, the inertia weight w, the maximum velocity V/sub max/, and the number of particles of the swarm. The purpose of this work is the development of an algorithm based on the Gaussian distribution, which improves the convergence ability of PSO without the necessity of tuning these parameters. The only parameter to be specified by the user is the number of particles. The Gaussian PSO algorithm was tested on a suite of well-known benchmark functions and the results were compared with the results of the standard PSO algorithm. The simulation results shows that the Gaussian swarm outperforms the standard one.},
  keywords = {Acceleration,Convergence,Equations,Evolutionary computation,Gaussian distribution,Particle swarm optimization,Particle tracking,Probability distribution,Random number generation,Testing},
  file = {C:\Users\benja\Zotero\storage\Q5SBIAPS\1460443.html}
}

@inproceedings{krollSystemModelingBased2014,
  title = {System Modeling Based on Machine Learning for Anomaly Detection and Predictive Maintenance in Industrial Plants},
  booktitle = {Proceedings of the 2014 {{IEEE Emerging Technology}} and {{Factory Automation}} ({{ETFA}})},
  author = {Kroll, B. and Schaffranek, D. and Schriegel, S. and Niggemann, O.},
  year = {2014},
  month = sep,
  pages = {1--7},
  issn = {1946-0759},
  doi = {10.1109/ETFA.2014.7005202},
  abstract = {Electricity, water or air are some Industrial energy carriers which are struggling under the prices of primary energy carriers. The European Union for example used more 20.000.000 GWh electricity in 2011 based on the IEA Report [1]. Cyber Physical Production Systems (CPPS) are able to reduce this amount, but they also help to increase the efficiency of machines above expectations which results in a more cost efficient production. Especially in the field of improving industrial plants, one of the challenges is the implementation of anomaly detection systems. For example as wear-level detection, which improves maintenance cycles and thus leads to a better energy usage. This paper presents an approach that uses timed hybrid automata of the machines normal behavior for a predictive maintenance of industrial plants. This hybrid model reduces discrete and continuous signals (e.g. energy data) to individual states, which refer to either the present condition of the machines. This allows an effective anomaly detection by implementing a combined data acquisition and anomaly detection approach, and the outlook for other applications, such as a predictive maintenance planning. Finally, this methodology is verified by three different industrial applications.},
  keywords = {Data acquisition,Data models,Industrial plants,Learning automata,Servers,Synchronization},
  file = {C:\Users\benja\Zotero\storage\DCJRYN6G\7005202.html}
}

@incollection{krotIntuitiveMethodsIndustrial2019,
  title = {Intuitive {{Methods}} of {{Industrial Robot Programming}} in {{Advanced Manufacturing Systems}}},
  booktitle = {Intelligent {{Systems}} in {{Production Engineering}} and {{Maintenance}}},
  author = {Krot, Kamil and Kutia, Vitalii},
  editor = {Burduk, Anna and Chlebus, Edward and Nowakowski, Tomasz and Tubis, Agnieszka},
  year = {2019},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {205--214},
  publisher = {Springer International Publishing},
  address = {Cham},
  abstract = {In this article a brief review of the modern industrial robot programming methods is given. It is noted that there are a lot of research conducted to improve robot programming process, make it shorter, easier, cost-effective and user friendly. These goals can be achieved by implementing of new advanced achievements of the IT sphere into industrial robotics. Industrial robot programing by demonstration alongside with the use of virtual and augmented reality is one of the most promising technologies that can significantly reduce the integration costs and time for industrial robot integration into a production process.},
  isbn = {978-3-319-97490-3},
  langid = {english},
  keywords = {Augmented reality,Human-robot interaction,Industrial robots,Intuitive robot programming,Manufacturing systems}
}

@misc{KRQUANTEC2020,
  title = {{{KR QUANTEC}}},
  year = {2020},
  month = may,
  journal = {KUKA AG},
  urldate = {2020-05-12},
  abstract = {All-purpose robot series with the largest payload and range in the high payload class for any application from automotive, to foundry, to medical.},
  howpublished = {https://www.kuka.com/en-de/products/robot-systems/industrial-robots/kr-quantec},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\FR35E5JP\kr-quantec.html}
}

@inproceedings{krugRepresentingMovementPrimitives2013,
  title = {Representing Movement Primitives as Implicit Dynamical Systems Learned from Multiple Demonstrations},
  booktitle = {2013 16th {{International Conference}} on {{Advanced Robotics}} ({{ICAR}})},
  author = {Krug, Robert and Dimitrovz, Dimitar},
  year = {2013},
  month = nov,
  pages = {1--8},
  doi = {10.1109/ICAR.2013.6766505},
  urldate = {2024-04-22},
  abstract = {This work deals with the problem of parameter estimation of dynamical systems intended to model demonstrated motion profiles for a system of interest. The regression problem is formulated as a constrained nonlinear least squares problem. We present an approach that extends the concept of dynamical movement primitives to account for multiple demonstrations of a motion. We maintain an implicit dynamical system that resembles the demonstrated trajectories in a locally optimal way. This is achieved by solving a quadratic program (that encodes our notion of resemblance) at each sampling time step. Our method guarantees predictable state evolution even in regions of the state space not covered by the demonstrations.},
  keywords = {Dynamics,Encoding,Optimization,Parameter estimation,Robots,Trajectory,Vectors},
  file = {C:\Users\benja\Zotero\storage\II665S5Q\6766505.html}
}

@inproceedings{kuhnerClosedLoopRobotTask2018,
  title = {Closed-{{Loop Robot Task Planning Based}} on {{Referring Expressions}}},
  booktitle = {{{IROS}}},
  author = {Kuhner, D. and Aldinger, J. and Burget, F. and Gobelbecker, M. and Burgard, W. and Nebel, B.},
  year = {2018},
  month = oct,
  pages = {876--881},
  publisher = {IEEE},
  address = {Madrid},
  doi = {10.1109/IROS.2018.8593371},
  urldate = {2019-06-16},
  abstract = {Increasing the accessibility of autonomous robots also for inexperienced users requires user-friendly and highlevel control opportunities of robotic systems. While automated planning is able to decompose a complex task into a sequence of steps which reaches an intended goal, it is difficult to formulate such a goal without knowing the internals of the planning system and the exact capabilities of the robot. This becomes even more important in dynamic environments in which manipulable objects are subject to change. In this paper, we present an adaptive control interface which allows users to specify goals based on an internal world model by incrementally building referring expressions to the objects in the world. We consider fetch-and-carry tasks and automatically deduce potential high-level goals from the world model to make them available to the user. Based on its perceptions our system can react to changes in the environment by adapting the goal formulation within the domain-independent planning system.},
  isbn = {978-1-5386-8094-0},
  langid = {english}
}

@misc{kukaagKUKAWorkVisual,
  title = {{{KUKA WorkVisual}}},
  author = {{KUKA AG}},
  journal = {KUKA AG},
  urldate = {2021-03-24},
  abstract = {Greater efficiency through KUKA.WorkVisual software: find out all you need to know about our engineering suite here!},
  howpublished = {https://www.kuka.com/en-de/products/robot-systems/software/system-software/kuka\_systemsoftware/kuka\_work-visual},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\28TICH9Y\kuka_work-visual.html}
}

@misc{KUKAConnect2020,
  title = {{{KUKA Connect}}},
  year = {2020},
  month = may,
  urldate = {2020-05-15},
  howpublished = {https://connect.kuka.com/de/},
  file = {C:\Users\benja\Zotero\storage\XU3BJF4R\de.html}
}

@misc{KUKARemoteService,
  title = {{KUKA Remote Service}},
  journal = {KUKA AG},
  urldate = {2023-04-11},
  abstract = {Sichere Fernwartung f{\"u}r Ihre Roboter und Anlagen. Egal wann und wo, unsere zertifizierten KUKA Service-Techniker sind zur Stelle.},
  howpublished = {https://www.kuka.com/de-de/services/service-f\%c3\%bcr-roboter-und-maschinen/remote-service},
  langid = {ngerman}
}

@article{kulakActiveLearningBayesian2021,
  title = {Active {{Learning}} of {{Bayesian Probabilistic Movement Primitives}}},
  author = {Kulak, Thibaut and Girgin, Hakan and Odobez, Jean-Marc and Calinon, Sylvain},
  year = {2021},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {2},
  pages = {2163--2170},
  issn = {2377-3766},
  doi = {10.1109/LRA.2021.3060414},
  abstract = {Learning from Demonstration permits non-expert users to easily and intuitively reprogram robots. Among approaches embracing this paradigm, probabilistic movement primitives (ProMPs) are a well-established and widely used method to learn trajectory distributions. However, providing or requesting useful demonstrations is not easy, as quantifying what constitutes a good demonstration in terms of generalization capabilities is not trivial. In this letter, we propose an active learning method for contextual ProMPs for addressing this problem. More specifically, we learn the trajectory distributions using a Bayesian Gaussian mixture model (BGMM) and then leverage the notion of epistemic uncertainties to iteratively choose new context query points for demonstrations. We show that this approach reduces the required number of human demonstrations. We demonstrate the effectiveness of the approach on a pouring task, both in simulation and on a real 7-DoF Franka Emika robot.},
  keywords = {Bayes methods,Gaussian mixture model,Imitation learning,incremental learning,learning from demonstration,Learning systems,Robots,Task analysis,Trajectory,Uncertainty},
  annotation = {recommend},
  file = {C:\Users\benja\Zotero\storage\H5ZBZ5IF\9357895.html}
}

@inproceedings{kulakFourierMovementPrimitives2020,
  title = {Fourier Movement Primitives: An Approach for Learning Rhythmic Robot Skills from Demonstrations},
  shorttitle = {Fourier Movement Primitives},
  booktitle = {Robotics: {{Science}} and {{Systems XVI}}},
  author = {Kulak, Thibaut and Silverio, Joao and Calinon, Sylvain},
  year = {2020},
  month = jul,
  volume = {16},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-6-1},
  file = {C:\Users\benja\Zotero\storage\7L6BUZBN\p056.html}
}

@inproceedings{kulkarniHierarchicalDeepReinforcement2016,
  title = {Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation},
  shorttitle = {Hierarchical Deep Reinforcement Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kulkarni, Tejas D. and Narasimhan, Karthik R. and Saeedi, Ardavan and Tenenbaum, Joshua B.},
  year = {2016},
  month = dec,
  pages = {3682--3690},
  address = {Barcelona, Spain},
  urldate = {2020-07-01},
  abstract = {Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. One of the key difficulties is insufficient exploration, resulting in an agent being unable to learn robust policies. Intrinsically motivated agents can explore new behavior for their own sake rather than to directly solve external goals. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical action-value functions, operating at different temporal scales, with goal-driven intrinsically motivated deep reinforcement learning. A top-level q-value function learns a policy over intrinsic goals, while a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse and delayed feedback: (1) a complex discrete stochastic decision process with stochastic transitions, and (2) the classic ATARI game -'Montezuma's Revenge'.},
  isbn = {978-1-5108-3881-9}
}

@inproceedings{kulkEvaluationWalkOptimisation2011,
  title = {Evaluation of Walk Optimisation Techniques for the {{NAO}} Robot},
  booktitle = {2011 11th {{IEEE-RAS International Conference}} on {{Humanoid Robots}}},
  author = {Kulk, Jason and Welsh, James S.},
  year = {2011},
  month = oct,
  pages = {306--311},
  issn = {2164-0580},
  doi = {10.1109/Humanoids.2011.6100827},
  abstract = {Locomotion performance is a critical component of any humanoid robot application. The procedure of optimising a walk engine has a high cost in both resources and time. The selection of the most appropriate optimisation algorithm, fitness function, and parameter space to maximise the benefit-cost ratio can dramatically improve the performance of the optimisation process. In this paper, we compare different meta-optimised optimisation algorithms, different fitness functions, and two different parameter spaces, in a physics-based simulation. The purpose of the comparison is to select the most appropriate combination to be used in hardware. The combination that yields the greatest increase in walk performance given a fixed expenditure is considered as the best, and is implemented in hardware. We found that Policy Gradient Reinforcement Learning with a fitness function based on the efficiency and a parameter space expanded to include the joint stiffnesses not only performed the best, in terms of improving the walk speed and efficiency, but also in terms of selecting gaits that were more stable. This combination was then applied to the physical NAO, improving the default walk's speed by 57\% and its efficiency by 30\%.},
  keywords = {Engines,Hardware,Humanoid robots,Joints,Legged locomotion,Optimization},
  file = {C:\Users\benja\Zotero\storage\6AJ7W59T\6100827.html}
}

@inproceedings{kumarPracticeMakesPerfect2024,
  title = {Practice {{Makes Perfect}}: {{Planning}} to {{Learning Skill Parameter Policies}}},
  shorttitle = {Practice {{Makes Perfect}}},
  booktitle = {Robotics: {{Science}} and {{Systems}} 2024},
  author = {Kumar, Nishanth and Silver, Tom and McClinton, Willie and Zhao, Linfeng and Proulx, Stephen and {Lozano-P{\'e}rez}, Tom{\'a}s and Kaelbling, Leslie and Barry, Jennifer},
  year = {2024},
  month = jul,
  address = {Delft, Netherlands},
  doi = {10.15607/RSS.2024.XX.040}
}

@inproceedings{kuntzFastAnytimeMotion2017,
  title = {Fast {{Anytime Motion Planning}} in {{Point Clouds}} by {{Interleaving Sampling}} and {{Interior Point Optimization}}},
  booktitle = {{{ISRR}}},
  author = {Kuntz, A. and Bowen, Chris and Alterovitz, Ron},
  year = {2017},
  doi = {10.1007/978-3-030-28619-4_63},
  abstract = {Robotic manipulators operating in unstructured environments such as homes and offices need to plan their motions quickly while relying on real-world sensors, which typically produce point clouds. To enable intuitive, interactive, and reactive user interfaces, the motion plan computation should provide high-quality solutions quickly and in an anytime manner, meaning the algorithm progressively improves its solution and can be interrupted at any time and return a valid solution. To address these challenges, we combine two paradigms: (1) asymptotically-optimal sampling-based motion planning, which is effective at providing anytime solutions but can struggle to quickly converge to high quality solutions in high dimensional configuration spaces, and (2) optimization, which locally refines paths quickly. We propose the use of interior point optimization for its ability to perform in an anytime manner that guarantees obstacle avoidance in each iteration, and we provide a novel lazy formulation that efficiently operates directly on point cloud data. Our method iteratively alternates between anytime sampling-based motion planning and anytime, lazy interior point optimization to compute high quality motion plans quickly, converging to a globally optimal solution.}
}

@inproceedings{kunzeAcquiringTaskModels2013,
  title = {Acquiring Task Models for Imitation Learning through Games with a Purpose},
  booktitle = {2013 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Kunze, Lars and Haidu, Andrei and Beetz, Michael},
  year = {2013},
  month = nov,
  pages = {102--107},
  issn = {2153-0866},
  doi = {10.1109/IROS.2013.6696339},
  abstract = {Teaching robots everyday tasks like making pancakes by instructions requires interfaces that can be intuitively operated by non-experts. By performing novel manipulation tasks in a virtual environment using a data glove task-related information of the demonstrated actions can directly be accessed and extracted from the simulator. We translate low-level data structures of these simulations into meaningful first-order representations whereby we are able to select data segments and analyze them at an abstract level. Hence, the proposed system is a powerful tool for acquiring examples of manipulation actions and for analyzing them whereby robots can be informed how to perform a task.},
  keywords = {Containers,Damping,Data gloves,Games,Liquids,Robots,Viscosity},
  file = {C:\Users\benja\Zotero\storage\WNFZ5HPT\6696339.html}
}

@article{kurachNeuralRandomAccessMachines2015,
  title = {Neural {{Random-Access Machines}}},
  author = {Kurach, Karol and Andrychowicz, Marcin and Sutskever, Ilya},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.06392 [cs]},
  eprint = {1511.06392},
  primaryclass = {cs},
  urldate = {2019-07-11},
  abstract = {In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{kurupWhatCanCognitive2012,
  title = {What Can Cognitive Architectures Do for Robotics?},
  author = {Kurup, Unmesh and Lebiere, Christian},
  year = {2012},
  month = oct,
  journal = {Biologically Inspired Cognitive Architectures},
  volume = {2},
  pages = {88--99},
  issn = {2212-683X},
  doi = {10.1016/j.bica.2012.07.004},
  urldate = {2023-02-21},
  abstract = {Research in robotic systems has traditionally been identified with approaches that are characterized by the use of carefully crafted representations and processes to find optimal solutions. The use of such representations and processes, which we refer to as the algorithmic approach, is uniquely suited for problems requiring strong models, i.e., tasks and domains that are well defined, and/or involve close interaction with the environment. These problems have historically been the focus of robotics research because they exercise perceptual, motor and manipulation capabilities that form the basic foundational abilities required for every robotic agent. Recent work (for example ROS and Tekkotsu) on the abstraction and encapsulation of perception and motor functionality has standardized the above mentioned foundational abilities and allowed researchers to study problems in less clearly defined and open-ended domains: problems that have previously been considered the province of AI and Cognitive Science. In this paper, we argue that the study of these problems (examples of which include multi-agent interaction, instruction following and reasoning in complex domains) referred to under the rubric of Cognitive Robotics is best achieved via the use of cognitive architectures -- unified computational frameworks developed specifically for general problem solving and human cognitive modeling. We lay out the relevant architectural concepts and principles and illustrate them using nine cognitive architectures that are under active development -- Soar, ACT-R, CLARION, GMU-BICA, Polyscheme, Co-JACK, ADAPT, ACT-R/E, and SS-RICS.},
  langid = {english},
  keywords = {Cognitive architecture principles,Cognitive Robotics,Requirements for cognitive robotics},
  file = {C:\Users\benja\Zotero\storage\2NPIRCVX\S2212683X12000333.html}
}

@inproceedings{kurutachLearningPlannableRepresentations2018,
  title = {Learning Plannable Representations with Causal {{InfoGAN}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kurutach, Thanard and Tamar, Aviv and Yang, Ge and Russell, Stuart and Abbeel, Pieter},
  year = {2018},
  month = dec,
  pages = {8747--8758},
  address = {Montr{\'e}al, Canada},
  urldate = {2020-07-12},
  abstract = {In recent years, deep generative models have been shown to 'imagine' convincing high-dimensional observations such as images, audio, and even video, learning directly from raw data. In this work, we ask how to imagine goal-directed visual plans -- a plausible sequence of observations that transition a dynamical system from its current configuration to a desired goal state, which can later be used as a reference trajectory for control. We focus on systems with high-dimensional observations, such as images, and propose an approach that naturally combines representation learning and planning. Our framework learns a generative model of sequential observations, where the generative process is induced by a transition in a low-dimensional planning model, and an additional noise. By maximizing the mutual information between the generated observations and the transition in the planning model, we obtain a low-dimensional representation that best explains the causal nature of the data. We structure the planning model to be compatible with efficient planning algorithms, and we propose several such models based on either discrete or continuous states. Finally, to generate a visual plan, we project the current and goal observations onto their respective states in the planning model, plan a trajectory, and then use the generative model to transform the trajectory to a sequence of observations. We demonstrate our method on imagining plausible visual plans of rope manipulation.}
}

@inproceedings{kwiatkowskaSafetyVerificationDeep2019,
  title = {Safety Verification for Deep Neural Networks with Provable Guarantees (Invited Paper)},
  booktitle = {{{CONCUR}} 2019},
  author = {Kwiatkowska, Marta Z.},
  year = {2019},
  pages = {1:1--1:5},
  address = {Amsterdam, The Netherlands},
  doi = {10.4230/LIPIcs.CONCUR.2019.1}
}

@article{laiDeepSCMEfficientConvolutional2022,
  title = {{{DeepSCM}}: {{An}} Efficient Convolutional Neural Network Surrogate Model for the Screening of Therapeutic Antibody Viscosity},
  shorttitle = {{{DeepSCM}}},
  author = {Lai, Pin-Kuang},
  year = {2022},
  month = jan,
  journal = {Computational and Structural Biotechnology Journal},
  volume = {20},
  pages = {2143--2152},
  issn = {2001-0370},
  doi = {10.1016/j.csbj.2022.04.035},
  urldate = {2024-05-23},
  abstract = {Predicting high concentration antibody viscosity is essential for developing subcutaneous administration. Computer simulations provide promising tools to reach this aim. One such model is the spatial charge map (SCM) proposed by Agrawal and coworkers (mAbs. 2015, 8(1):43--48). SCM applies molecular dynamics simulations to calculate a score for the screening of antibody viscosity at high concentrations. However, molecular dynamics simulations are computationally costly and require structural information, a significant application bottleneck. In this work, high throughput computing was performed to calculate the SCM scores for 6596 nonredundant antibody variable regions. A convolutional neural network surrogate model, DeepSCM, requiring only sequence information, was then developed based on this dataset. The linear correlation coefficient of the DeepSCM and SCM scores achieved 0.9 on the test set (N~=~1320). The DeepSCM model was applied to screen the viscosity of 38 therapeutic antibodies that SCM correctly classified and resulted in only one misclassification. The DeepSCM model will facilitate high concentration antibody viscosity screening. The code and parameters are freely available at https://github.com/Lailabcode/DeepSCM.},
  keywords = {Antibody viscosity,Convolutional neural network,Deep learning,Developability,Molecular dynamics simulations,Spatial charge map},
  file = {C:\Users\benja\Zotero\storage\WRWS4F4X\S2001037022001520.html}
}

@inproceedings{lairdCognitiveRoboticsUsing2012,
  title = {Cognitive {{Robotics Using}} the {{Soar Cognitive Architecture}}},
  booktitle = {{{CogRob}}@{{AAAI}}},
  author = {Laird, J. and Kinkade, Keegan R. and Mohan, Shiwali and Xu, J.},
  year = {2012},
  urldate = {2024-09-18},
  abstract = {Our long-term goal is to develop autonomous robotic systems that have the cognitive abilities of humans, including communication, coordination, adapting to novel situations, and learning through experience. Our approach rests on the integration of the Soar cognitive architecture with both virtual and physical robotic systems. Soar has been used to develop a wide variety of knowledge-rich agents for complex virtual environments, including distributed training environments and interactive computer games. For development and testing in robotic virtual environments, Soar interfaces to a variety of robotic simulators and a simple mobile robot. We have recently made significant extensions to Soar that add new memories and new non-symbolic reasoning to Soar's original symbolic processing, which improves Soar abilities for control of robots. These extensions include mental imagery, episodic and semantic memory, reinforcement learning, and continuous model learning. This paper presents research in mobile robotics, relational and continuous model learning, and learning by situated, interactive instruction.}
}

@article{lairdInteractiveTaskLearning2017,
  title = {Interactive {{Task Learning}}},
  author = {Laird, John E. and Gluck, Kevin and Anderson, John and Forbus, Kenneth D. and Jenkins, Odest Chadwicke and Lebiere, Christian and Salvucci, Dario and Scheutz, Matthias and Thomaz, Andrea and Trafton, Greg and Wray, Robert E. and Mohan, Shiwali and Kirk, James R.},
  year = {2017},
  journal = {IEEE Intelligent Systems},
  volume = {32},
  number = {4},
  pages = {6--21},
  issn = {1941-1294},
  doi = {10.1109/MIS.2017.3121552},
  urldate = {2024-01-10},
  abstract = {This article presents a new research area called interactive task learning (ITL), in which an agent actively tries to learn not just how to perform a task better but the actual definition of a task through natural interaction with a human instructor while attempting to perform the task. The authors provide an analysis of desiderata for ITL systems, a review of related work, and a discussion of possible application areas for ITL systems.},
  file = {C:\Users\benja\Zotero\storage\U2D35JDH\8012335.html}
}

@article{lairdSOARArchitectureGeneral1987,
  title = {{{SOAR}}: An Architecture for General Intelligence},
  shorttitle = {{{SOAR}}},
  author = {Laird, John E. and Newell, Allen and Rosenbloom, Paul S.},
  year = {1987},
  month = sep,
  journal = {Artificial Intelligence},
  volume = {33},
  number = {1},
  pages = {1--64},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(87)90050-6},
  urldate = {2022-06-27}
}

@book{lairdSoarCognitiveArchitecture2012,
  title = {The {{Soar Cognitive Architecture}}},
  author = {Laird, John E.},
  year = {2012},
  month = apr,
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  abstract = {The definitive presentation of Soar, one AI's most enduring architectures, offering comprehensive descriptions of fundamental aspects and new components.},
  isbn = {978-0-262-12296-2},
  langid = {english}
}

@article{lakeCompositionalGeneralizationMeta,
  title = {Compositional Generalization through Meta Sequence-to-Sequence Learning},
  author = {Lake, Brenden M},
  pages = {11},
  abstract = {People can learn a new concept and use it compositionally, understanding how to ``blicket twice'' after learning how to ``blicket.'' In contrast, powerful sequence-tosequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.},
  langid = {english}
}

@inproceedings{lakeCompositionalGeneralizationMeta2019,
  title = {Compositional Generalization through Meta Sequence-to-Sequence Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lake, Brenden M},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d'{\null} {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {9791--9801},
  urldate = {2020-07-12},
  file = {C:\Users\benja\Zotero\storage\7X4ZTQ8R\9172-compositional-generalization-through-meta-sequence-to-sequence-learning.html}
}

@inproceedings{lakshminarayananSimpleScalablePredictive2017,
  title = {Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  year = {2017},
  month = dec,
  series = {{{NIPS}}'17},
  pages = {6405--6416},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2021-05-18},
  abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
  isbn = {978-1-5108-6096-4}
}

@article{lakshminarayananSimpleScalablePredictive2017a,
  title = {Simple and {{Scalable Predictive Uncertainty Estimation}} Using {{Deep Ensembles}}},
  author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  year = {2017},
  month = nov,
  journal = {arXiv:1612.01474 [cs, stat]},
  eprint = {1612.01474},
  primaryclass = {cs, stat},
  urldate = {2020-12-19},
  abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\LVS9AL8A\1612.html}
}

@inproceedings{lallementHATPHTNPlanner2014,
  title = {{{HATP}}: {{An HTN Planner}} for {{Robotics}}},
  booktitle = {2nd {{ICAPS Workshop}} on {{Planning}} and {{Robotics}}},
  author = {Lallement, Rapha{\"e}l and {de Silva}, Lavindra and Alami, Rachid},
  year = {2014},
  address = {Portsmouth, USA},
  langid = {english}
}

@article{lambProfessorForcingNew2016,
  title = {Professor {{Forcing}}: {{A New Algorithm}} for {{Training Recurrent Networks}}},
  shorttitle = {Professor {{Forcing}}},
  author = {Lamb, Alex and Goyal, Anirudh and Zhang, Ying and Zhang, Saizheng and Courville, Aaron and Bengio, Yoshua},
  year = {2016},
  month = oct,
  journal = {arXiv:1610.09038 [cs, stat]},
  eprint = {1610.09038},
  primaryclass = {cs, stat},
  urldate = {2020-01-01},
  abstract = {The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-stepahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{landinMechanicalEvaluationExpressions1964,
  title = {The {{Mechanical Evaluation}} of {{Expressions}}},
  author = {Landin, P. J.},
  year = {1964},
  month = jan,
  journal = {The Computer Journal},
  volume = {6},
  number = {4},
  pages = {308--320},
  issn = {0010-4620},
  doi = {10.1093/comjnl/6.4.308},
  urldate = {2024-04-26},
  abstract = {This paper is a contribution to the ``theory'' of the activity of using computers. It shows how some forms of expression used in current programming languages can be modelled in Church's {$\lambda$}-notation, and then describes a way of ``interpreting'' such expressions. This suggests a method, of analyzing the things computer users write, that applies to many different problem orientations and to different phases of the activity of using a computer. Also a technique is introduced by which the various composite information structures involved can be formally characterized in their essentials, without commitment to specific written or other representations.},
  file = {C:\Users\benja\Zotero\storage\SFJWYJBE\375725.html}
}

@book{lanFirstorderStochasticOptimization2020,
  title = {First-Order and {{Stochastic Optimization Methods}} for {{Machine Learning}}},
  author = {Lan, Guanghui},
  year = {2020},
  series = {Springer {{Series}} in the {{Data Sciences}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-39568-1},
  urldate = {2024-05-21},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-030-39567-4 978-3-030-39568-1},
  langid = {english},
  keywords = {Distributed and decentralized methods,Machine learning algorithms,Nonconvex optimization methods,Randomized algorithms,Stochastic optimization methods}
}

@article{Lang.2011,
  title = {Multi-Body Dynamics Simulation of Geometrically Exact {{Cosserat}} Rods},
  author = {Lang, Holger and Linn, Joachim and Arnold, Martin},
  year = {2011},
  journal = {Multibody System Dynamics},
  volume = {25},
  number = {3},
  pages = {285--312},
  issn = {1384-5640},
  doi = {10.1007/s11044-010-9223-x},
  pagination = {page}
}

@inproceedings{Lars16,
  title = {Learning {{Representations}} for {{Automatic Colorization}}},
  author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
  year = {2016},
  pages = {577--593}
}

@article{lauriPartiallyObservableMarkov2023,
  title = {Partially {{Observable Markov Decision Processes}} in {{Robotics}}: {{A Survey}}},
  shorttitle = {Partially {{Observable Markov Decision Processes}} in {{Robotics}}},
  author = {Lauri, Mikko and Hsu, David and Pajarinen, Joni},
  year = {2023},
  month = feb,
  journal = {IEEE Transactions on Robotics},
  volume = {39},
  number = {1},
  pages = {21--40},
  issn = {1941-0468},
  doi = {10.1109/TRO.2022.3200138},
  urldate = {2024-04-28},
  abstract = {Noisy sensing, imperfect control, and environment changes are defining characteristics of many real-world robot tasks. The partially observable Markov decision process (POMDP) provides a principled mathematical framework for modeling and solving robot decision and control tasks under uncertainty. Over the last decade, it has seen many successful applications, spanning localization and navigation, search and tracking, autonomous driving, multirobot systems, manipulation, and human--robot interaction. This survey aims to bridge the gap between the development of POMDP models and algorithms at one end and application to diverse robot decision tasks at the other. It analyzes the characteristics of these tasks and connects them with the mathematical and algorithmic properties of the POMDP framework for effective modeling and solution. For practitioners, the survey provides some of the key task characteristics in deciding when and how to apply POMDPs to robot tasks successfully. For POMDP algorithm designers, the survey provides new insights into the unique challenges of applying POMDPs to robot systems and points to promising new directions for further research.},
  keywords = {AI-based methods,autonomous agents,Markov processes,partially observable Markov decision process (POMDP),Planning,planning under uncertainty,Robot kinematics,Robot sensing systems,Robots,scheduling and coordination,Task analysis,Uncertainty},
  file = {C:\Users\benja\Zotero\storage\SQGPEXY4\9899480.html}
}

@inproceedings{laursenAutomaticErrorRecovery2015,
  title = {Automatic Error Recovery in Robot Assembly Operations Using Reverse Execution},
  booktitle = {2015 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Laursen, Johan Sund and Schultz, Ulrik Pagh and Ellekilde, Lars-Peter},
  year = {2015},
  month = sep,
  pages = {1785--1792},
  doi = {10.1109/IROS.2015.7353609},
  abstract = {Robotic assembly tasks are in general difficult to program and require a high degree of precision. As the complexity of the task increases it becomes increasingly unlikely that tasks can always be executed without errors. Preventing errors beyond a certain point is economically infeasible, in particular for small-batch productions. As an alternative, we propose a system for automatically handling certain classes of errors instead of preventing them. Specifically, we show that many operations can be automatically reversed. Errors can be handled through automatic reverse execution of the control program to a safe point, from which forward execution can be resumed. This paper describes the principles behind automatic reversal of robotic assembly operations, and experimentally demonstrates the use of a domain-specific language that supports automatic error handling through reverse execution. Our contribution represents the first experimental demonstration of reversible computing principles applied to industrial robotics.},
  keywords = {Assembly,DSL,Fasteners,Programming,Service robots,Unified modeling language},
  file = {C:\Users\benja\Zotero\storage\EKBXYZSH\7353609.html}
}

@techreport{lavalleRapidlyexploringRandomTrees1998,
  type = {Technical Report},
  title = {Rapidly-Exploring Random Trees: {{A}} New Tool for Path Planning},
  shorttitle = {Rapidly-Exploring Random Trees},
  author = {LaValle, Steven M.},
  year = {1998},
  number = {TR 98-11},
  address = {Ames, IA},
  institution = {Iowa State University},
  urldate = {2024-04-04},
  abstract = {We introduce the concept of a Rapidly-exploring Random Tree (RRT) as a randomized data structure that is designed for a broad class of path planning problems. While they share many of the bene cial properties of existing randomized planning techniques, RRTs are specifically designed to handle nonholonomic constraints (including dynamics) and high degrees of freedom. An RRT is iteratively expanded by applying control inputs that drive the system slightly toward randomly-selected points, as opposed to requiring point-to-point convergence, as in the probabilistic roadmap approach. Several desirable properties and a basic implementation of RRTs are discussed. To date, we have successfully applied RRTs to holonomic, nonholonomic, and kinodynamic planning problems of up to twelve degrees of freedom.}
}

@article{lechnerAdversarialTrainingNot2021,
  title = {Adversarial {{Training}} Is {{Not Ready}} for {{Robot Learning}}},
  author = {Lechner, Mathias and Hasani, Ramin and Grosu, Radu and Rus, Daniela and Henzinger, Thomas A.},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.08187 [cs]},
  eprint = {2103.08187},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {Adversarial training is an effective method to train deep learning models that are resilient to norm-bounded perturbations, with the cost of nominal performance drop. While adversarial training appears to enhance the robustness and safety of a deep model deployed in open-world decision-critical applications, counterintuitively, it induces undesired behaviors in robot learning settings. In this paper, we show theoretically and experimentally that neural controllers obtained via adversarial training are subjected to three types of defects, namely transient, systematic, and conditional errors. We first generalize adversarial training to a safety-domain optimization scheme allowing for more generic specifications. We then prove that such a learning process tends to cause certain error profiles. We support our theoretical results by a thorough experimental safety analysis in a robot-learning task. Our results suggest that adversarial training is not yet ready for robot learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\T5857YBR\2103.html}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  urldate = {2021-09-16},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Reviews\\
Subject\_term: Computer science;Mathematics and computing\\
Subject\_term\_id: computer-science;mathematics-and-computing},
  file = {C:\Users\benja\Zotero\storage\S5URFK98\nature14539.html}
}

@unpublished{lecunPathAutonomousMachine2022,
  title = {A {{Path Towards Autonomous Machine Intelligence}}},
  author = {LeCun, Yann},
  year = {2022},
  month = jun
}

@inproceedings{Lee.2014,
  title = {Unifying Scene Registration and Trajectory Optimization for Learning from Demonstrations with Application to Manipulation of Deformable Objects},
  booktitle = {2014 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems},
  author = {Lee, Alex X. and Huang, Sandy H. and {Hadfield-Menell}, Dylan and Tzeng, Eric and Abbeel, Pieter},
  year = {2014},
  pages = {4402--4407},
  publisher = {IEEE},
  doi = {10.1109/IROS.2014.6943185},
  bookpagination = {page},
  isbn = {978-1-4799-6934-0}
}

@article{leeCausalReasoningSimulation2021,
  title = {Causal {{Reasoning}} in {{Simulation}} for {{Structure}} and {{Transfer Learning}} of {{Robot Manipulation Policies}}},
  author = {Lee, Timothy E. and Zhao, Jialiang and Sawhney, Amrita S. and Girdhar, Siddharth and Kroemer, Oliver},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.16772 [cs]},
  eprint = {2103.16772},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {We present CREST, an approach for causal reasoning in simulation to learn the relevant state space for a robot manipulation policy. Our approach conducts interventions using internal models, which are simulations with approximate dynamics and simplified assumptions. These interventions elicit the structure between the state and action spaces, enabling construction of neural network policies with only relevant states as input. These policies are pretrained using the internal model with domain randomization over the relevant states. The policy network weights are then transferred to the target domain (e.g., the real world) for fine tuning. We perform extensive policy transfer experiments in simulation for two representative manipulation tasks: block stacking and crate opening. Our policies are shown to be more robust to domain shifts, more sample efficient to learn, and scale to more complex settings with larger state spaces. We also show improved zero-shot sim-to-real transfer of our policies for the block stacking task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\DERUTHKJ\2103.html}
}

@article{leeIKEAFurnitureAssembly2019,
  title = {{{IKEA Furniture Assembly Environment}} for {{Long-Horizon Complex Manipulation Tasks}}},
  author = {Lee, Youngwoon and Hu, Edward S. and Yang, Zhengyu and Yin, Alex and Lim, Joseph J.},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.07246 [cs]},
  eprint = {1911.07246},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {The IKEA Furniture Assembly Environment is one of the first benchmarks for testing and accelerating the automation of complex manipulation tasks. The environment is designed to advance reinforcement learning from simple toy tasks to complex tasks requiring both long-term planning and sophisticated low-level control. Our environment supports over 80 different furniture models, Sawyer and Baxter robot simulation, and domain randomization. The IKEA Furniture Assembly Environment is a testbed for methods aiming to solve complex manipulation tasks. The environment is publicly available at https://clvrai.com/furniture},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\GE8X87P4\1911.html}
}

@inproceedings{leeMathematicalReasoningLatent2019,
  title = {Mathematical {{Reasoning}} in {{Latent Space}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Lee, Dennis and Szegedy, Christian and Rabe, Markus and Loos, Sarah and Bansal, Kshitij},
  year = {2019},
  month = sep,
  urldate = {2024-04-27},
  abstract = {We design and conduct a simple experiment to study whether neural networks can perform several steps of approximate reasoning in a fixed dimensional latent space. The set of rewrites (i.e. transformations) that can be successfully performed on a statement represents essential semantic features of the statement. We can compress this information by embedding the formula in a vector space, such that the vector associated with a statement can be used to predict whether a statement can be rewritten by other theorems. Predicting the embedding of a formula generated by some rewrite rule is naturally viewed as approximate reasoning in the latent space. In order to measure the effectiveness of this reasoning, we perform approximate deduction sequences in the latent space and use the resulting embedding to inform the semantic features of the corresponding formal statement (which is obtained by performing the corresponding rewrite sequence using real formulas). Our experiments show that graph neural networks can make non-trivial predictions about the rewrite-success of statements, even when they propagate predicted latent representations for several steps. Since our corpus of mathematical formulas includes a wide variety of mathematical disciplines, this experiment is a strong indicator for the feasibility of deduction in latent space in general.},
  langid = {english}
}

@misc{leeSTAMPDifferentiableTask2024,
  title = {{{STAMP}}: {{Differentiable Task}} and {{Motion Planning}} via {{Stein Variational Gradient Descent}}},
  shorttitle = {{{STAMP}}},
  author = {Lee, Yewon and Huang, Philip and Jatavallabhula, Krishna Murthy and Li, Andrew Z. and Damken, Fabian and Heiden, Eric and Smith, Kevin and Nowrouzezahrai, Derek and Ramos, Fabio and Shkurti, Florian},
  year = {2024},
  month = jan,
  number = {arXiv:2310.01775},
  eprint = {2310.01775},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.01775},
  urldate = {2024-09-07},
  abstract = {Planning for many manipulation tasks, such as using tools or assembling parts, often requires both symbolic and geometric reasoning. Task and Motion Planning (TAMP) algorithms typically solve these problems by conducting a tree search over high-level task sequences while checking for kinematic and dynamic feasibility. This can be inefficient as the width of the tree can grow exponentially with the number of possible actions and objects. In this paper, we propose a novel approach to TAMP that relaxes discrete-and-continuous TAMP problems into inference problems on a continuous domain. Our method, Stein Task and Motion Planning (STAMP) subsequently solves this new problem using a gradient-based variational inference algorithm called Stein Variational Gradient Descent, by obtaining gradients from a parallelized differentiable physics simulator. By introducing relaxations to the discrete variables, leveraging parallelization, and approaching TAMP as an Bayesian inference problem, our method is able to efficiently find multiple diverse plans in a single optimization run. We demonstrate our method on two TAMP problems and benchmark them against existing TAMP baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics,I.2.9},
  file = {C\:\\Users\\benja\\Zotero\\storage\\ZPAZ7U85\\Lee et al. - 2024 - STAMP Differentiable Task and Motion Planning via Stein Variational Gradient Descent.pdf;C\:\\Users\\benja\\Zotero\\storage\\3Z2Q5ETN\\2310.html}
}

@article{leeTSFastformerFastTransformer2024,
  title = {{{TS-Fastformer}}: {{Fast Transformer}} for {{Time-series Forecasting}}},
  shorttitle = {{{TS-Fastformer}}},
  author = {Lee, Sangwon and Hong, Junho and Liu, Ling and Choi, Wonik},
  year = {2024},
  month = feb,
  journal = {ACM Transactions on Intelligent Systems and Technology},
  volume = {15},
  number = {2},
  pages = {24:1--24:20},
  issn = {2157-6904},
  doi = {10.1145/3630637},
  urldate = {2024-03-27},
  abstract = {Many real-world applications require precise and fast time-series forecasting. Recent trends in time-series forecasting models are shifting from LSTM-based models to Transformer-based models. However, the Transformer-based model has a limited ability to represent sequential relationships in time-series data. In addition, the transformer-based model suffers from slow training and inference speed due to the bottleneck incurred by a deep encoder and step-by-step decoder inference. To address these problems, we propose a time-series forecasting optimized Transformer model, called TS-Fastformer. TS-Fastformer introduces three new optimizations: First, we propose a Sub Window Tokenizer for compressing input in a simple manner. The Sub Window Tokenizer reduces the length of input sequences to mitigate the complexity of self-attention and enables both single and multi-sequence learning. Second, we propose Time-series Pre-trained Encoder to extract effective representations through pre-training. This optimization enables TS-Fastformer to capture both seasonal and trend representations as well as to mitigate bottlenecks of conventional transformer models. Third, we propose the Past Attention Decoder to forecast target by incorporating past long short-term dependency patterns. Furthermore, Past Attention Decoder achieves high performance improvement by removing a trend distribution that changes over a long period. We evaluate the efficiency of our model with extensive experiments using seven real-world datasets and compare our model to six representative time-series forecasting approaches. The results show that the proposed TS-Fastformer reduces MSE by 10.1\% compared to state-of-the-art model and demonstrates 21.6\% faster training time compared to the existing fastest transformer, respectively.},
  keywords = {Deep learning,time-series forecasting,time-series representation,transformer}
}

@book{Leibe.2016,
  title = {Computer Vision -- {{ECCV}} 2016},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  year = {2016},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-46484-8},
  isbn = {978-3-319-46483-1}
}

@article{leichtmannEffectsExplainableArtificial2023,
  title = {Effects of {{Explainable Artificial Intelligence}} on {{Trust}} and {{Human Behavior}} in a {{High-Risk Decision Task}}},
  author = {Leichtmann, Benedikt and Humer, Christina and Hinterreiter, Andreas and Streit, Marc and Mara, Martina},
  year = {2023},
  month = feb,
  journal = {Computers in Human Behavior},
  volume = {139},
  pages = {107539},
  issn = {0747-5632},
  doi = {10.1016/j.chb.2022.107539},
  urldate = {2024-02-01},
  abstract = {Understanding the recommendations of an artificial intelligence (AI) based assistant for decision-making is especially important in high-risk tasks, such as deciding whether a mushroom is edible or poisonous. To foster user understanding and appropriate trust in such systems, we assessed the effects of explainable artificial intelligence (XAI) methods and an educational intervention on AI-assisted decision-making behavior in a 2~{\texttimes}~2 between subjects online experiment with N=410 participants. We developed a novel use case in which users go on a virtual mushroom hunt and are tasked with picking edible and leaving poisonous mushrooms. Users were provided with an AI-based app that showed classification results of mushroom images. To manipulate explainability, one subgroup additionally received attribution-based and example-based explanations of the AI's predictions; for the educational intervention one subgroup received additional information on how the AI worked. We found that the group that received explanations outperformed that which did not and showed better calibrated trust levels. Contrary to our expectations, we found that the educational intervention, domain-specific (i.e., mushroom) knowledge, and AI knowledge had no effect on performance. We discuss practical implications and introduce the mushroom-picking task as a promising use case for XAI research.},
  keywords = {AI literacy,Domain-specific knowledge,Mushroom identification,Trust calibration,Visual explanation,XAI},
  file = {C:\Users\benja\Zotero\storage\ELPY589W\S0747563222003594.html}
}

@inproceedings{Leizea.2014,
  title = {Real-Time Deformation, Registration and Tracking of Solids Based on Physical Simulation},
  booktitle = {2014 {{IEEE}} International Symposium on Mixed and Augmented Reality ({{ISMAR}})},
  author = {Leizea, Ibai and Alvarez, Hugo and Aguinaga, Iker and Borro, Diego},
  year = {2014},
  pages = {165--170},
  publisher = {IEEE},
  doi = {10.1109/ISMAR.2014.6948423},
  bookpagination = {page},
  isbn = {978-1-4799-6184-9}
}

@article{lembonoMemoryMotionWarmstarting2020,
  title = {Memory of {{Motion}} for {{Warm-starting Trajectory Optimization}}},
  author = {Lembono, Teguh Santoso and Paolillo, Antonio and Pignat, Emmanuel and Calinon, Sylvain},
  year = {2020},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {2},
  eprint = {1907.01474},
  primaryclass = {cs},
  pages = {2594--2601},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2020.2972893},
  urldate = {2024-06-26},
  abstract = {Trajectory optimization for motion planning requires good initial guesses to obtain good performance. In our proposed approach, we build a memory of motion based on a database of robot paths to provide good initial guesses. The memory of motion relies on function approximators and dimensionality reduction techniques to learn the mapping between the tasks and the robot paths. Three function approximators are compared: k-Nearest Neighbor, Gaussian Process Regression, and Bayesian Gaussian Mixture Regression. In addition, we show that the memory can be used as a metric to choose between several possible goals, and using an ensemble method to combine different function approximators results in a significantly improved warm-starting performance. We demonstrate the proposed approach with motion planning examples on the dual-arm robot PR2 and the humanoid robot Atlas.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\CCXAKHWC\Lembono et al. - 2020 - Memory of Motion for Warm-starting Trajectory Opti.pdf}
}

@book{Lengyel.2004,
  title = {Mathematics for {{3D}} Game Programming and Computer Graphics},
  author = {Lengyel, Eric},
  year = {2004},
  series = {Game Development Series},
  edition = {2. ed.},
  publisher = {River Media},
  address = {Hingham, Mass.},
  isbn = {1-58450-277-0}
}

@article{leokumarStateArtIntenseReview2017,
  title = {State of {{The Art-Intense Review}} on {{Artificial Intelligence Systems Application}} in {{Process Planning}} and {{Manufacturing}}},
  author = {Leo Kumar, S. P.},
  year = {2017},
  month = oct,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {65},
  pages = {294--329},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2017.08.005},
  urldate = {2022-04-05},
  abstract = {Artificial Intelligence (AI) systems applications are widespread due to its domain independent characteristics. In this work, an attempt has been made for review on AI applications in Computer Aided Process Planning (CAPP) and manufacturing. Primarily, uniqueness of present review work addressed by analysis of existing review articles. The review work comprise of three main elements; 1. Feature based design, a primary input for a CAPP system, 2. Expert System (ES) usefulness in Process Planning (PP) and manufacturing and 3. Evolutionary approach applications. The review begins with an overview and the use of AI systems in decision making. Research works exemplified for the past three and half decades (1981--2016) are analyzed in terms of feature based modeling, Standards for Exchange of Product Model data approach, ES in PP, scheduling, manufacturing and miscellaneous applications. Role of Evolutionary Techniques (ET) in intelligent system development, execution of PP activities and manufacturing are described. A statistical analysis on existing review articles, number of publications, domain specific articles and percentage contribution of each area are carried out. Finally, research gaps are identified and the possible future research directions are presented.},
  langid = {english},
  keywords = {Artificial Intelligence,Computer Aided Process Planning,Evolutionary Techniques,Expert System,Manufacturing},
  file = {C:\Users\benja\Zotero\storage\Z4CH7CQB\S0952197617301896.html}
}

@inproceedings{leon-urrutiaDataLiteracyEssential2022,
  title = {Data {{Literacy}}: {{An Essential Skill}} for the {{Industry}}},
  shorttitle = {Data {{Literacy}}},
  booktitle = {Proceedings on 18th {{International Conference}} on {{Industrial Systems}} -- {{IS}}'20},
  author = {{Leon-Urrutia}, Manuel and Taibi, Davide and Pospelova, Vera and Splendore, Sergio and Urbsiene, Laimute and Marjanovic, Ugljesa},
  editor = {Lalic, Bojan and Gracanin, Danijela and Tasic, Nemanja and Simeunovi{\'c}, Nenad},
  year = {2022},
  pages = {326--331},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-97947-8_43},
  abstract = {Data literacy is the ability to read, write, critically assess, communicate and extract value of data in different contexts. As such, data literacy is a key competence in the current digital economy, where most transactions require handling high amounts and variety of data. For a data literate workforce in the data-driven industry, there is a critical need for training and educational curricula that address this key competence, both in Higher Education and professional development programs. Through a systematic desk research spanning over 15 European countries, this paper sheds light on how data literacy is addressed in European Higher Education and professional training. Official syllabuses and educational documents have been examined to determine the strengths and challenges of data literacy enhancement, and identify the needs and opportunities for data literacy-specific training programs that promote a data-driven culture within the European industry ecosystem.},
  isbn = {978-3-030-97947-8},
  langid = {english},
  keywords = {Data knowledge,Data literacy,Information systems},
  file = {C:\Users\benja\Zotero\storage\RLK55HI5\Leon-Urrutia et al. - 2022 - Data Literacy An Essential Skill for the Industry.pdf}
}

@book{Leonardis.2006,
  title = {Computer Vision -- {{ECCV}} 2006},
  editor = {Leonardis, Ale{\v s} and Bischof, Horst and Pinz, Axel},
  year = {2006},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11744023},
  isbn = {978-3-540-33832-1}
}

@article{levenbergMethodSolutionCertain1944,
  title = {A Method for the Solution of Certain Non-Linear Problems in Least Squares},
  author = {Levenberg, Kenneth},
  year = {1944},
  journal = {Quarterly of Applied Mathematics},
  volume = {2},
  number = {2},
  pages = {164--168},
  issn = {0033-569X, 1552-4485},
  doi = {10.1090/qam/10666},
  urldate = {2024-05-21},
  abstract = {The standard method for solving least squares problems which lead to non-linear normal equations depends upon a reduction of the residuals to linear form by first order Taylor approximations taken about an initial or trial solution for the parameters.2 If the usual least squares procedure, performed with these linear approximations, yields new values for the parameters which are not sufficiently close to the initial values, the neglect of second and higher order terms may invalidate the process, and may actually give rise to a larger value of the sum of the squares of the residuals than that corresponding to the initial solution. This failure of the standard method to improve the initial solution has received some notice in statistical applications of least squares3 and has been encountered rather frequently in connection with certain engineering applications involving the approximate representation of one function by another. The purpose of this article is to show how the problem may be solved by an extension of the standard method which insures improvement of the initial solution.4 The process can also be used for solving non-linear simultaneous equations, in which case it may be considered an extension of Newton's method. Let the function to be approximated be h\{x, y, z, {$\bullet$} {$\bullet$} {$\bullet$} ), and let the approximating function be H\{oc, y, z, {$\bullet$} {$\bullet$} \ding{110} ; a, j3, y, \ding{110} {$\bullet$} \ding{110} ), where a, /3, 7, {$\bullet$} \ding{110} \ding{110} are the unknown parameters. Then the residuals at the points, yit zit {$\bullet$} {$\bullet$} {$\bullet$} ), i = 1, 2, \ding{110} {$\bullet$} {$\bullet$} , n, are\vphantom{\}\}}},
  langid = {english}
}

@article{levesqueExpressivenessTractabilityKnowledge1987,
  title = {Expressiveness and Tractability in Knowledge Representation and Reasoning},
  author = {Levesque, Hector J. and Brachman, Ronald J.},
  year = {1987},
  journal = {Computational Intelligence},
  volume = {3},
  number = {1},
  pages = {78--93},
  issn = {1467-8640},
  doi = {10.1111/j.1467-8640.1987.tb00176.x},
  urldate = {2024-07-31},
  abstract = {A fundamental computational limit on automated reasoning and its effect on knowledge representation is examined. Basically, the problem is that it can be more difficult to reason correctly with one representational language than with another and, moreover, that this difficulty increases dramatically as the expressive power of the language increases. This leads to a tradeoff between the expressiveness of a representational language and its computational tractability. Here we show that this tradeoff can be seen to underlie the differences among a number of existing representational formalisms, in addition to motivating many of the current research issues in knowledge representation.},
  langid = {english},
  keywords = {bases de donnees,complexite,complexity of reasoning,databases,description subsumption,du raisonnement,first-order logic,frames,knowledge representation,logique du premier ordre,representation de connaissances,reseaux semantiques,schemas,semantic networks},
  file = {C:\Users\benja\Zotero\storage\C6YVMMES\j.1467-8640.1987.tb00176.html}
}

@article{levineEndtoendTrainingDeep2016,
  title = {End-to-End {{Training}} of {{Deep Visuomotor Policies}}},
  author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  year = {2016},
  month = jan,
  journal = {The Journal of Machine Learning Research},
  volume = {17},
  number = {1},
  pages = {1334--1373},
  issn = {1532-4435},
  urldate = {2019-07-14},
  abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
  keywords = {neural networks,optimal control,reinforcement learning,vision}
}

@article{levineLearningHandeyeCoordination2018,
  title = {Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection},
  author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Ibarz, Julian and Quillen, Deirdre},
  year = {2018},
  month = apr,
  journal = {The International Journal of Robotics Research},
  volume = {37},
  number = {4-5},
  pages = {421--436},
  publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  doi = {10.1177/0278364917710318},
  urldate = {2020-06-30},
  abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images independent of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. We describe two large-scale experiments that we conducted on two separate robotic platforms. In the first experiment, about 800,000 grasp attempts were collected over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and gripper wear and tear. In the second experiment, we used a different robotic platform and 8 robots to collect a dataset consisting of over 900,000 grasp attempts. The second robotic platform was used to test transfer between robots, and the degree to which data from a different set of robots can be used to aid learning. Our experimental results demonstrate that our approach achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing. Our transfer experiment also illustrates that data from different robots can be combined to learn more reliable and effective grasping.},
  langid = {english}
}

@inproceedings{levineLearningNeuralNetwork2014,
  title = {Learning {{Neural Network Policies}} with {{Guided Policy Search}} under {{Unknown Dynamics}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Levine, Sergey and Abbeel, Pieter},
  year = {2014},
  volume = {27},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-04-28},
  abstract = {We present a policy search method that uses iteratively refitted local linear models to optimize trajectory distributions for large, continuous problems. These trajectory distributions can be used within the framework of guided policy search to learn policies with an arbitrary parameterization. Our method fits time-varying linear dynamics models to speed up learning, but does not rely on learning a global model, which can be difficult when the dynamics are complex and discontinuous. We show that this hybrid approach requires many fewer samples than model-free methods, and can handle complex, nonsmooth dynamics that can pose a challenge for model-based techniques. We present experiments showing that our method can be used to learn complex neural network policies that successfully execute simulated robotic manipulation tasks in partially observed environments with numerous contact discontinuities and underactuation.}
}

@book{Levoy.2007,
  title = {{{ACM SIGGRAPH}} 2007 Papers on - {{SIGGRAPH}} '07},
  editor = {Levoy, Marc},
  year = {2007},
  publisher = {ACM Press},
  address = {New York, New York, USA},
  doi = {10.1145/1275808},
  isbn = {978-1-59593-648-6}
}

@inproceedings{Lewis1992,
  title = {Genetic Programming Approach to the Construction of a Neural Network for Control of a Walking Robot},
  booktitle = {Robotics and {{Automation}}, 1992. {{Proceedings}}., 1992 {{IEEE International Conference}} On},
  author = {Lewis, M Anthony and Fagg, Andrew H and Solidum, Alan},
  year = {1992},
  pages = {2618--2623},
  publisher = {IEEE}
}

@inproceedings{lewisRetrievalAugmentedGenerationKnowledgeIntensive2020,
  title = {Retrieval-{{Augmented Generation}} for {{Knowledge-Intensive NLP Tasks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  year = {2020},
  volume = {33},
  pages = {9459--9474},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-08-27},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  file = {C:\Users\benja\Zotero\storage\XR7V8CGY\Lewis et al. - 2020 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf}
}

@inproceedings{Li.2014,
  title = {Real-Time Pose Estimation of Deformable Objects Using a Volumetric Approach},
  booktitle = {2014 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems},
  author = {Li, Yinxiao and Wang, Yan and Case, Michael and Chang, Shih-Fu and Allen, Peter K.},
  year = {2014},
  pages = {1046--1052},
  publisher = {IEEE},
  doi = {10.1109/IROS.2014.6942687},
  bookpagination = {page},
  isbn = {978-1-4799-6934-0}
}

@article{Li.2018,
  title = {Model-Driven Feedforward Prediction for Manipulation of Deformable Objects},
  author = {Li, Yinxiao and Wang, Yan and Yue, Yonghao and Xu, Danfei and Case, Michael and Chang, Shih-Fu and Grinspun, Eitan and Allen, Peter K.},
  year = {2018},
  journal = {IEEE Transactions on Automation Science and Engineering},
  volume = {15},
  number = {4},
  pages = {1621--1638},
  issn = {1545-5955},
  doi = {10.1109/TASE.2017.2766228},
  pagination = {page}
}

@article{Li17,
  title = {Kinematic {{Control}} of {{Redundant Manipulators Using Neural Networks}}},
  author = {Li, Shuai and Zhang, Yunong and Jin, Long},
  year = {2017},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {PP},
  number = {99},
  pages = {1--12}
}

@article{Li2018,
  title = {Massively {{Parallel Hyperparameter Tuning}}},
  author = {Li, Liam and Jamieson, Kevin and Rostamizadeh, Afshin and Gonina, Ekaterina and Hardt, Moritz and Recht, Benjamin and Talwalkar, Ameet},
  year = {2018},
  month = oct,
  eprint = {1810.05934},
  urldate = {2019-02-17},
  abstract = {Modern learning models are characterized by large hyperparameter spaces. In order to adequately explore these large spaces, we must evaluate a large number of configurations, typically orders of magnitude more configurations than available parallel workers. Given the growing costs of model training, we would ideally like to perform this search in roughly the same wall-clock time needed to train a single model. In this work, we tackle this challenge by introducing ASHA, a simple and robust hyperparameter tuning algorithm with solid theoretical underpinnings that exploits parallelism and aggressive early-stopping. Our extensive empirical results show that ASHA outperforms state-of-the hyperparameter tuning methods; scales linearly with the number of workers in distributed settings; converges to a high quality configuration in half the time taken by Vizier (Google's internal hyperparameter tuning service) in an experiment with 500 workers; and beats the published result for a near state-of-the-art LSTM architecture in under 2x the time to train a single model.},
  archiveprefix = {arXiv}
}

@incollection{Li93,
  title = {A {{Neural Network Based Inverse Kinematics Solution}} in {{Robotics}}},
  booktitle = {Neural {{Networks}} in {{Robotics}}},
  author = {Li, Yaotong and Zeng, Nan},
  year = {1993},
  pages = {97--111},
  publisher = {Springer}
}

@inproceedings{liangCodePoliciesLanguage2023,
  title = {Code as {{Policies}}: {{Language Model Programs}} for {{Embodied Control}}},
  shorttitle = {Code as {{Policies}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  year = {2023},
  month = may,
  pages = {9493--9500},
  doi = {10.1109/ICRA48891.2023.10160591},
  abstract = {Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (`faster') depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
  keywords = {Codes,Detectors,Feedback loop,Impedance,Libraries,Natural languages,Process control},
  file = {C:\Users\benja\Zotero\storage\Z4VUWFLD\10160591.html}
}

@inproceedings{liangGPUAcceleratedRoboticSimulation2018,
  title = {{{GPU-Accelerated Robotic Simulation}} for {{Distributed Reinforcement Learning}}},
  booktitle = {{{CoRL}}},
  author = {Liang, Jacky and Makoviychuk, Viktor and Handa, Ankur and Chentanez, Nuttapong and Macklin, Miles and Fox, Dieter},
  year = {2018},
  month = oct,
  pages = {270--282},
  urldate = {2019-07-14},
  abstract = {Most Deep Reinforcement Learning (Deep RL) algorithms require a prohibitively large number of training samples for learning complex tasks. Many recent works on speeding up Deep RL have focused on d...},
  langid = {english}
}

@inproceedings{liangHumanRobotCollaborativeSemiAutonomous2018,
  title = {Human-{{Robot Collaborative Semi-Autonomous Teleoperation}} with {{Force Feedback}}},
  booktitle = {2018 5th {{International Conference}} on {{Soft Computing Machine Intelligence}} ({{ISCMI}})},
  author = {Liang, Ji and Yu, Ge and Guo, Lili},
  year = {2018},
  month = nov,
  pages = {129--134},
  issn = {2640-0146},
  doi = {10.1109/ISCMI.2018.8703223},
  abstract = {Teleoperation is the most important method to control robot in the distance, as complete autonomy still has a long way to go. To facilitate teleoperation efficiency, human-robot collaboration methods have been suggested, in which robot and human work together to accomplish a task. In this paper, we propose a semi-autonomous teleoperation method, with which operator can command the robot to reach the desired location via direct control at the initial phase, by virtue of the intent recognition algorithm, sub-task is recognized, and finally robot itself takes over the control to accomplish the task. A biological experiment is designed, and the direct control and semi-autonomous control strategies are compared through user studies. Execution time, task success rate and a subjective evaluation (NASA TLX) are used to quantify user performance. The trials show that our strategy can reduce workload and enhance operation accuracy significantly.},
  keywords = {End effectors,Force,intent recognition,Predictive models,Robot sensing systems,semi-autonomous,Supervisory control,Task analysis,teleoperation},
  file = {C:\Users\benja\Zotero\storage\YMWB2R8C\8703223.html}
}

@article{liangIRoProInteractiveRobot2022,
  title = {{{iRoPro}}: {{An}} Interactive {{Robot Programming Framework}}},
  shorttitle = {{{iRoPro}}},
  author = {Liang, Ying Siu and Pellier, Damien and Fiorino, Humbert and Pesty, Sylvie},
  year = {2022},
  month = jan,
  journal = {International Journal of Social Robotics},
  volume = {14},
  number = {1},
  pages = {177--191},
  issn = {1875-4805},
  doi = {10.1007/s12369-021-00775-9},
  urldate = {2024-09-17},
  abstract = {The great diversity of end-user tasks ranging from manufacturing environments to personal homes makes pre-programming robots for general purpose applications extremely challenging. In fact, teaching robots new actions from scratch that can be reused for previously unseen tasks remains a difficult challenge and is generally left up to robotics experts. In this work, we present iRoPro, an interactive Robot Programming framework that allows end-users with little to no technical background to teach a robot new reusable actions. We combine Programming by Demonstration and Automated Planning techniques to allow the user to construct the robot's knowledge base by teaching new actions by kinesthetic demonstration. The actions are generalised and reused with a task planner to solve previously unseen problems defined by the user. We implement iRoPro as an end-to-end system on a Baxter Research Robot to simultaneously teach low- and high-level actions by demonstration that the user can customise via a Graphical User Interface to adapt to their specific use case. To evaluate the feasibility of our approach, we first conducted pre-design experiments to better understand the user's adoption of involved concepts and the proposed robot programming process. We compare results with post-design experiments, where we conducted a user study to validate the usability of our approach with real end-users. Overall, we showed that users with different programming levels and educational backgrounds can easily learn and use iRoPro and its robot programming process.},
  langid = {english},
  keywords = {Artificial Intelligence,Automated planning,End-User robot programming,Human-Robot interaction,Programming by demonstration},
  file = {C:\Users\benja\Zotero\storage\2LAP2EVX\Liang et al. - 2022 - iRoPro An interactive Robot Programming Framework.pdf}
}

@inproceedings{liangOptimizationRobotPath2017,
  title = {Optimization of Robot Path Planning Parameters Based on Genetic Algorithm},
  booktitle = {2017 {{IEEE International Conference}} on {{Real-time Computing}} and {{Robotics}} ({{RCAR}})},
  author = {Liang, Y. and Hong, F. and Lin, Q. and Bi, S. and Feng, L.},
  year = {2017},
  month = jul,
  pages = {529--534},
  doi = {10.1109/RCAR.2017.8311917},
  abstract = {The paper introduces a method which is based o n genetic algorithm to select more properly parameters for local path planning of mobile robot. In our experiments, according to experience, we choose properly parameter domain as chromosomes in Genetic algorithm(GA). Then we will extract several different individual cost function from DWA algorithm, adopted by Local path planning, and merge them into a fitness function. Experiments show that the automatic tuning method, proposed in this paper, can generate smoother and faster trajectories than manual tuning method and perform better in avoiding obstacle. This method can also be used for other platforms.},
  keywords = {automatic tuning method,Cost function,DWA algorithm,fitness function,genetic algorithm,genetic algorithms,Genetic algorithms,Heuristic algorithms,local path planning,manual tuning method,mobile robot,mobile robots,Mobile robots,path planning,Robot kinematics,robot path planning,Trajectory}
}

@article{liaoMultifidelityConvolutionalNeural2021,
  title = {Multi-Fidelity Convolutional Neural Network Surrogate Model for Aerodynamic Optimization Based on Transfer Learning},
  author = {Liao, Peng and Song, Wei and Du, Peng and Zhao, Hang},
  year = {2021},
  month = dec,
  journal = {Physics of Fluids},
  volume = {33},
  number = {12},
  pages = {127121},
  publisher = {American Institute of Physics},
  issn = {1070-6631},
  doi = {10.1063/5.0076538},
  urldate = {2022-05-04}
}

@inproceedings{liaoQuestioningAIInforming2020,
  title = {Questioning the {{AI}}: {{Informing Design Practices}} for {{Explainable AI User Experiences}}},
  shorttitle = {Questioning the {{AI}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Liao, Q. Vera and Gruen, Daniel and Miller, Sarah},
  year = {2020},
  month = apr,
  series = {{{CHI}} '20},
  pages = {1--15},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3313831.3376590},
  urldate = {2024-03-25},
  abstract = {A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm-informed XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI.},
  isbn = {978-1-4503-6708-0},
  keywords = {explainable AI,human-AI interaction,user experience}
}

@inproceedings{liAttentionGuidedUnifiedNetwork2019,
  title = {Attention-{{Guided Unified Network}} for {{Panoptic Segmentation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Li, Yanwei and Chen, Xinze and Zhu, Zheng and Xie, Lingxi and Huang, Guan and Du, Dalong and Wang, Xingang},
  year = {2019},
  pages = {7026--7035},
  urldate = {2023-02-27}
}

@article{liDataScienceSkills2021,
  title = {Data Science Skills and Domain Knowledge Requirements in the Manufacturing Industry: {{A}} Gap Analysis},
  shorttitle = {Data Science Skills and Domain Knowledge Requirements in the Manufacturing Industry},
  author = {Li, Guoyan and Yuan, Chenxi and Kamarthi, Sagar and Moghaddam, Mohsen and Jin, Xiaoning},
  year = {2021},
  month = jul,
  journal = {Journal of Manufacturing Systems},
  volume = {60},
  pages = {692--706},
  issn = {0278-6125},
  doi = {10.1016/j.jmsy.2021.07.007},
  urldate = {2024-01-19},
  abstract = {Manufacturing has adopted technologies such as automation, robotics, industrial Internet of Things (IoT), and big data analytics to improve productivity, efficiency, and capabilities in the production environment. Modern manufacturing workers not only need to be adept at the traditional manufacturing technologies but also ought to be trained in the advanced data-rich computer-automated technologies. This study analyzes the data science and analytics (DSA) skills gap in today's manufacturing workforce to identify the critical technical skills and domain knowledge required for data science and intelligent manufacturing-related jobs that are highly in-demand in today's manufacturing industry. The gap analysis conducted in this paper on Emsi job posting and profile data provides insights into the trends in manufacturing jobs that leverage data science, automation, cyber, and sensor technologies. These insights will be helpful for educators and industry to train the next generation manufacturing workforce. The main contribution of this paper includes (1) presenting the overall trend in manufacturing job postings in the U.S., (2) summarizing the critical skills and domain knowledge in demand in the manufacturing sector, (3) summarizing skills and domain knowledge reported by manufacturing job seekers, (4) identifying the gaps between demand and supply of skills and domain knowledge, and (5) recognize opportunities for training and upskilling workforce to address the widening skills and knowledge gap.},
  keywords = {Data science,Industry 4.0,Labor market analysis,Skills gap},
  file = {C:\Users\benja\Zotero\storage\ZDKBXYS3\S0278612521001448.html}
}

@article{liEfficientForceControl2018,
  title = {Efficient {{Force Control Learning System}} for {{Industrial Robots Based}} on {{Variable Impedance Control}}},
  author = {Li, Chao and Zhang, Zhi and Xia, Guihua and Xie, Xinru and Zhu, Qidan},
  year = {2018},
  month = aug,
  journal = {Sensors (Basel, Switzerland)},
  volume = {18},
  number = {8},
  issn = {1424-8220},
  doi = {10.3390/s18082539},
  urldate = {2020-05-12},
  abstract = {Learning variable impedance control is a powerful method to improve the performance of force control. However, current methods typically require too many interactions to achieve good performance. Data-inefficiency has limited these methods to learn force-sensitive tasks in real systems. In order to improve the sampling efficiency and decrease the required interactions during the learning process, this paper develops a data-efficient learning variable impedance control method that enables the industrial robots automatically learn to control the contact force in the unstructured environment. To this end, a Gaussian process model is learned as a faithful proxy of the system, which is then used to predict long-term state evolution for internal simulation, allowing for efficient strategy updates. The effects of model bias are reduced effectively by incorporating model uncertainty into long-term planning. Then the impedance profiles are regulated online according to the learned humanlike impedance strategy. In this way, the flexibility and adaptivity of the system could be enhanced. Both simulated and experimental tests have been performed on an industrial manipulator to verify the performance of the proposed method.},
  pmcid = {PMC6111768},
  pmid = {30081474}
}

@inproceedings{liHRL4INHierarchicalReinforcement2019,
  title = {{{HRL4IN}}: {{Hierarchical Reinforcement Learning}} for {{Interactive Navigation}} with {{Mobile Manipulators}}},
  shorttitle = {{{HRL4IN}}},
  booktitle = {{{CoRL}}},
  author = {Li, Chengshu and Xia, Fei and Martin, Roberto Martin and Savarese, Silvio},
  year = {2019},
  abstract = {Most common navigation tasks in human environments require auxiliary arm interactions, e.g. opening doors, pressing buttons and pushing obstacles away. This type of navigation tasks, which we call Interactive Navigation, requires the use of mobile manipulators: mobile bases with manipulation capabilities. Interactive Navigation tasks are usually long-horizon and composed of heterogeneous phases of pure navigation, pure manipulation, and their combination. Using the wrong part of the embodiment is inefficient and hinders progress. We propose HRL4IN, a novel Hierarchical RL architecture for Interactive Navigation tasks. HRL4IN exploits the exploration benefits of HRL over flat RL for long-horizon tasks thanks to temporally extended commitments towards subgoals. Different from other HRL solutions, HRL4IN handles the heterogeneous nature of the Interactive Navigation task by creating subgoals in different spaces in different phases of the task. Moreover, HRL4IN selects different parts of the embodiment to use for each phase, improving energy efficiency. We evaluate HRL4IN against flat PPO and HAC, a state-of-the-art HRL algorithm, on Interactive Navigation in two environments - a 2D grid-world environment and a 3D environment with physics simulation. We show that HRL4IN significantly outperforms its baselines in terms of task performance and energy efficiency. More information is available at this https URL.}
}

@article{liJointStiffnessIdentification2018,
  title = {Joint {{Stiffness Identification}} and {{Deformation Compensation}} of {{Serial Robots Based}} on {{Dual Quaternion Algebra}}},
  author = {Li, Guozhi and Zhang, Fuhai and Fu, Yili and Wang, Shuguo},
  year = {2018},
  month = dec,
  journal = {Applied Sciences},
  volume = {9},
  pages = {65},
  doi = {10.3390/app9010065},
  abstract = {As the application of industrial robots is limited by low stiffness that causes low precision, a joint stiffness identification algorithm for serial robots is presented. In addition, a deformation compensation algorithm is proposed for the accuracy improvement. Both of these algorithms are formulated by dual quaternion algebra, which offers a compact, efficient, and singularity-free way in robot analysis. The joint stiffness identification algorithm is derived from stiffness modeling, which is the combination of the principle of virtual work and dual quaternion algebra. To validate the effectiveness of the proposed identification algorithm and deformation compensation algorithm, an experiment was conducted on a dual arm industrial robot SDA5F. The robot performed a drilling operation during the experiment, and the forces and torques that acted on the end-effector (EE) of both arms were measured in order to apply the deformation compensation algorithm. The results of the experiment show that the proposed identification algorithm is able to identify the joint stiffness parameters of serial industrial robots, and the deformation compensation algorithm can improve the accuracy of the position and orientation of the EE. Furthermore, the performance of the forces and torques that acted on the EE during the operation were improved as well.}
}

@article{likertTechniqueMeasurementAttitudes1932,
  title = {A Technique for the Measurement of Attitudes},
  author = {Likert, R.},
  year = {1932},
  journal = {Archives of Psychology},
  volume = {22  140},
  pages = {55--55},
  abstract = {The project conceived in 1929 by Gardner Murphy and the writer aimed first to present a wide array of problems having to do with five major "attitude areas"---international relations, race relations, economic conflict, political conflict, and religion. The kind of questionnaire material falls into four classes: yes-no, multiple choice, propositions to be responded to by degrees of approval, and a series of brief newspaper narratives to be approved or disapproved in various degrees. The monograph aims to describe a technique rather than to give results. The appendix, covering ten pages, shows the method of constructing an attitude scale. A bibliography is also given. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C:\Users\benja\Zotero\storage\5M8CUIQC\1933-01885-001.html}
}

@article{liKohnShamEquationsRegularizer2021,
  title = {Kohn-{{Sham Equations}} as {{Regularizer}}: {{Building Prior Knowledge}} into {{Machine-Learned Physics}}},
  shorttitle = {Kohn-{{Sham Equations}} as {{Regularizer}}},
  author = {Li, Li and Hoyer, Stephan and Pederson, Ryan and Sun, Ruoxi and Cubuk, Ekin D. and Riley, Patrick and Burke, Kieron},
  year = {2021},
  month = jan,
  journal = {Physical Review Letters},
  volume = {126},
  number = {3},
  pages = {036401},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.126.036401},
  urldate = {2022-03-26},
  abstract = {Including prior knowledge is important for effective machine learning models in physics and is usually achieved by explicitly adding loss terms or constraints on model architectures. Prior knowledge embedded in the physics computation itself rarely draws attention. We show that solving the Kohn-Sham equations when training neural networks for the exchange-correlation functional provides an implicit regularization that greatly improves generalization. Two separations suffice for learning the entire one-dimensional H2 dissociation curve within chemical accuracy, including the strongly correlated region. Our models also generalize to unseen types of molecules and overcome self-interaction error.},
  file = {C:\Users\benja\Zotero\storage\SMUKS7DK\PhysRevLett.126.html}
}

@misc{liMultimodalFoundationModels2023,
  title = {Multimodal {{Foundation Models}}: {{From Specialists}} to {{General-Purpose Assistants}}},
  shorttitle = {Multimodal {{Foundation Models}}},
  author = {Li, Chunyuan and Gan, Zhe and Yang, Zhengyuan and Yang, Jianwei and Li, Linjie and Wang, Lijuan and Gao, Jianfeng},
  year = {2023},
  month = sep,
  number = {arXiv:2309.10020},
  eprint = {2309.10020},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.10020},
  urldate = {2024-01-05},
  abstract = {This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from specialist models to general-purpose assistants. The research landscape encompasses five core topics, categorized into two classes. (i) We start with a survey of well-established research areas: multimodal foundation models pre-trained for specific purposes, including two topics -- methods of learning vision backbones for visual understanding and text-to-image generation. (ii) Then, we present recent advances in exploratory, open research areas: multimodal foundation models that aim to play the role of general-purpose assistants, including three topics -- unified vision models inspired by large language models (LLMs), end-to-end training of multimodal LLMs, and chaining multimodal tools with LLMs. The target audiences of the paper are researchers, graduate students, and professionals in computer vision and vision-language multimodal communities who are eager to learn the basics and recent advances in multimodal foundation models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\benja\Zotero\storage\QC9CSS4U\2309.html}
}

@article{liMultimodalFoundationModels2024,
  title = {Multimodal {{Foundation Models}}: {{From Specialists}} to {{General-Purpose Assistants}}},
  shorttitle = {Multimodal {{Foundation Models}}},
  author = {Li, Chunyuan and Gan, Zhe and Yang, Zhengyuan and Yang, Jianwei and Li, Linjie and Wang, Lijuan and Gao, Jianfeng},
  year = {2024},
  month = may,
  journal = {Foundations and Trends{\textregistered} in Computer Graphics and Vision},
  volume = {16},
  number = {1-2},
  pages = {1--214},
  publisher = {Now Publishers, Inc.},
  issn = {1572-2740, 1572-2759},
  doi = {10.1561/0600000110},
  urldate = {2024-06-16},
  abstract = {Multimodal Foundation Models: From Specialists to General-Purpose Assistants},
  langid = {english}
}

@article{lindemannAnomalyDetectionDiscrete2019,
  title = {Anomaly Detection in Discrete Manufacturing Using Self-Learning Approaches},
  author = {Lindemann, Benjamin and Fesenmayr, Fabian and Jazdi, Nasser and Weyrich, Michael},
  year = {2019},
  month = jan,
  journal = {Procedia CIRP},
  series = {12th {{CIRP Conference}} on {{Intelligent Computation}} in {{Manufacturing Engineering}}, 18-20 {{July}} 2018, {{Gulf}} of {{Naples}}, {{Italy}}},
  volume = {79},
  pages = {313--318},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2019.02.073},
  urldate = {2021-04-14},
  abstract = {Process anomalies and unexpected failures of manufacturing systems are problems that cause a decreased quality of process and product. Current data analytics approaches show decent results concerning the optimization of single processes but lack in extensibility to plants with high-dimensional data spaces. This paper presents and compares two data-driven self-learning approaches that are used to detect anomalies within large amounts of machine and process data. Models of the machine behavior are generated to capture complex interdependencies and to extract features that represent anomalies. The approaches are tested and evaluated on the basis of real industrial data from metal forming processes.},
  langid = {english},
  keywords = {Anomaly detection,Lstm autoencoder,Predictive maintenance,Unsupervised learning},
  file = {C:\Users\benja\Zotero\storage\H7NPN5WU\S2212827119301908.html}
}

@inproceedings{lindenInversionMultilayerNets1989,
  title = {Inversion of Multilayer Nets},
  booktitle = {International 1989 {{Joint Conference}} on {{Neural Networks}}},
  author = {{Linden} and {Kindermann}},
  year = {1989},
  month = jun,
  pages = {425-430 vol.2},
  doi = {10.1109/IJCNN.1989.118277},
  urldate = {2024-05-23},
  abstract = {The method of inversion for arbitrary continuous multilayer nets is developed. The inversion is done by computing iteratively an input vector which minimizes the least-mean-square errors to approximate a given output target. This inversion is not unique for given targets and depends on the starting point in input space. The inversion method turns out to be a valuable tool for the examination of multilayer nets (MLNs). Applications of the inversion method to constraint satisfaction, feature detection, and the testing of reliability and performance of MLNs are outlined. It is concluded that recurrent nets and even time-delay nets might be invertible.{$<>$}},
  keywords = {Least squares methods,Neural networks},
  file = {C:\Users\benja\Zotero\storage\MEULNGWF\118277.html}
}

@inproceedings{linFeaturePyramidNetworks2017,
  title = {Feature {{Pyramid Networks}} for {{Object Detection}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  year = {2017},
  pages = {2117--2125},
  urldate = {2023-02-28}
}

@inproceedings{linMicrosoftCOCOCommon2014,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2014},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {740--755},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-10602-1_48},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  isbn = {978-3-319-10602-1},
  langid = {english},
  keywords = {Common Object,Object Category,Object Detection,Object Instance,Scene Understanding}
}

@inproceedings{linRoboFlowDatacentricWorkflow2022,
  title = {{{RoboFlow}}: A {{Data-centric Workflow Management System}} for {{Developing AI-enhanced Robots}}},
  shorttitle = {{{RoboFlow}}},
  booktitle = {Proceedings of the 5th {{Conference}} on {{Robot Learning}}},
  author = {Lin, Qinjie and Ye, Guo and Wang, Jiayi and Liu, Han},
  year = {2022},
  month = jan,
  pages = {1789--1794},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2022-04-15},
  abstract = {We propose RoboFlow,  a cloud-based workflow management system orchestrating the pipelines of developing AI-enhanced robots. Unlike most traditional robotic development processes that are essentially process-centric, RoboFlow is data-centric. This striking property makes it especially suitable for developing AI-enhanced robots in which data play a central role. More specifically, RoboFlow models the whole robotic development process into 4 building modules (1. data processing, 2. algorithmic development, 3. back testing and 4. application adaptation) interacting with a centralized data engine. All these building modules are containerized and orchestrated under a unified interfacing framework. Such an architectural design greatly increases the maintainability and re-usability of all the building modules and enables us to develop them in a fully parallel fashion. To demonstrate the efficacy of the developed system, we exploit it to develop two prototype systems named ``Egomobility" and ``Egoplan". Egomobility provides general-purpose navigation functionalities for a wide variety of mobile robots and Egoplan solves path planning problems in high dimensional continuous state and action spaces for robot arms. Our result shows that RoboFlow can significantly streamline the whole development lifecycle and the same workflow is applicable to numerous intelligent robotic applications.},
  langid = {english}
}

@inproceedings{linRobotCoveragePath2017,
  title = {Robot {{Coverage Path}} Planning for General Surfaces Using Quadratic Differentials},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Lin, Y. and Ni, C. and Lei, N. and Gu, X. David and Gao, J.},
  year = {2017},
  month = may,
  pages = {5005--5011},
  doi = {10.1109/ICRA.2017.7989583},
  abstract = {Robot Coverage Path planning (i.e., the process of providing full coverage of a given domain by one or multiple robots) is a classical problem in the field of robotics and motion planning. The goal of such planning is to provide nearly full coverage while also minimize duplicately visited area. In this paper, we focus on the scenario of path planning on general surface, including planar domains with complex topology, complex terrain, and general surface in 3D space. Our approach described in this paper adopts a natural, intrinsic and global parametrization of the surface for robot path planning, namely the holomorphic quadratic differentials. We give each point on the surface a uv-coordinates naturally represented by a complex number, except for a small number of zero points (singularities). We show that natural, efficient robot paths can be obtained by using such coordinate systems. The method is based on intrinsic geometry and thus can be adapted to general surface exploration in 3D.},
  keywords = {3D space,complex number,complex terrain,complex topology,coordinate systems,elliptic equations,general surfaces,global parametrization,graph theory,holomorphic quadratic differentials,intrinsic geometry,intrinsic parametrization,mobile robots,multi-robot systems,natural parametrization,natural robot paths,partial differential equations,path planning,planar domains,Planning,robot coverage path planning,Robot kinematics,surface exploration,Surface treatment,Three-dimensional displays,Trajectory,uv-coordinates}
}

@article{liowTHINKSurgicalTSolutionOne2017,
  title = {{{THINK}} Surgical {{TSolution-One}}{\textregistered} ({{Robodoc}}) Total Knee Arthroplasty},
  author = {Liow, Ming Han Lincoln and Chin, Pak Lin and Pang, Hee Nee and Tay, Darren Keng-Jin and Yeo, Seng-Jin},
  year = {2017},
  month = oct,
  journal = {SICOT-J},
  volume = {3},
  pages = {63},
  issn = {2426-8887},
  doi = {10.1051/sicotj/2017052},
  urldate = {2024-04-11},
  abstract = {THINK Surgical TSolution-One{\textregistered} is an active-autonomous, image-based, robotic milling system which enables the surgeon to attain a consistently accurate implant component positioning. The TSolution-One{\textregistered} system is capable of achieving this through an image-based preoperative planning system which allows the surgeon to create, view and analyse the surgical outcome in 3D. The accuracy and precision of component positioning have been attributed to the following factors: customized distal femoral resection, accurate determination of the femoral rotational alignment, minimization of errors and maintenance of bone temperature with robotic milling. Despite all these advantages, there is still a paucity of long-term, high-quality data that demonstrates the efficacy of robotic-assisted total knee arthroplasty (TKA). Questions regarding radiation risks, prolonged surgical duration and cost-effectiveness remain unanswered. This paper aims to describe: (1) TSolution-One{\textregistered} surgical technique; (2) limitations and complications; (3) clinical and radiological outcomes.},
  pmcid = {PMC5663203},
  pmid = {29087292}
}

@inproceedings{liPanopticPartFormerLearningUnified2022,
  title = {Panoptic-{{PartFormer}}: {{Learning}} a~{{Unified Model}} for~{{Panoptic Part Segmentation}}},
  shorttitle = {Panoptic-{{PartFormer}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2022},
  author = {Li, Xiangtai and Xu, Shilin and Yang, Yibo and Cheng, Guangliang and Tong, Yunhai and Tao, Dacheng},
  editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {729--747},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-19812-0_42},
  abstract = {Panoptic Part Segmentation (PPS) aims to unify panoptic segmentation and part segmentation into one task. Previous work mainly utilizes separated approaches to handle thing, stuff, and part predictions individually without performing any shared computation and task association. In this work, we aim to unify these tasks at the architectural level, designing the first end-to-end unified method named Panoptic-PartFormer. In particular, motivated by the recent progress in Vision Transformer, we model things, stuff, and part as object queries and directly learn to optimize the all three predictions as unified mask prediction and classification problem. We design a decoupled decoder to generate part feature and thing/stuff feature respectively. Then we propose to utilize all the queries and corresponding features to perform reasoning jointly and iteratively. The final mask can be obtained via inner product between queries and the corresponding features. The extensive ablation studies and analysis prove the effectiveness of our framework. Our Panoptic-PartFormer achieves the new state-of-the-art results on both Cityscapes PPS and Pascal Context PPS datasets with around 70\% GFlops and 50\% parameters decrease. Given its effectiveness and conceptual simplicity, we hope the Panoptic-PartFormer can serve as a strong baseline and aid future research in PPS. Our code and models will be available at https://github.com/lxtGH/Panoptic-PartFormer.},
  isbn = {978-3-031-19812-0},
  langid = {english},
  keywords = {Panoptic Part Segmentation,Scene understanding,Vision Transformer}
}

@inproceedings{liPrefixTuningOptimizingContinuous2021,
  title = {Prefix-{{Tuning}}: {{Optimizing Continuous Prompts}} for {{Generation}}},
  shorttitle = {Prefix-{{Tuning}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Li, Xiang Lisa and Liang, Percy},
  year = {2021},
  month = aug,
  pages = {4582--4597},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.353},
  urldate = {2023-10-11},
  abstract = {Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were ``virtual tokens''. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.}
}

@article{liProDMPUnifiedPerspective2023,
  title = {{{ProDMP}}: {{A Unified Perspective}} on {{Dynamic}} and {{Probabilistic Movement Primitives}}},
  shorttitle = {{{ProDMP}}},
  author = {Li, Ge and Jin, Zeqi and Volpp, Michael and Otto, Fabian and Lioutikov, Rudolf and Neumann, Gerhard},
  year = {2023},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {8},
  number = {4},
  pages = {2325--2332},
  issn = {2377-3766},
  doi = {10.1109/LRA.2023.3248443},
  urldate = {2024-04-17},
  abstract = {Movement Primitives (MPs) are a well-known concept to represent and generate modular trajectories. MPs can be broadly categorized into two types: (a) dynamics-based approaches that generate smooth trajectories from any initial state, e. g., Dynamic Movement Primitives (DMPs), and (b) probabilistic approaches that capture higher-order statistics of the motion, e. g., Probabilistic Movement Primitives (ProMPs). To date, however, there is no MP method that unifies both, i. e. that can generate smooth trajectories from an arbitrary initial state while capturing higher-order statistics. In this letter, we introduce a unified perspective of both approaches by solving the ODE underlying the DMPs. We convert expensive online numerical integration of DMPs into position and velocity basis functions that can be used to represent trajectories or trajectory distributions similar to ProMPs while maintaining all the properties of dynamical systems. Since we inherit the properties of both methodologies, we call our proposed model Probabilistic Dynamic Movement Primitives (ProDMPs). Additionally, we embed ProDMPs in deep neural network architecture and propose a new cost function for efficient end-to-end learning of higher-order trajectory statistics. To this end, we leverage Bayesian Aggregation for non-linear iterative conditioning on sensory inputs. Our proposed model achieves smooth trajectory generation, goal-attractor convergence, correlation analysis, nonlinear conditioning, and online re-planing in one framework.},
  keywords = {Computer architecture,Correlation,Dynamical systems,imitation learning,Mathematical models,Movement primitives,probabilistic learning,Probabilistic logic,Robots,Trajectory,trajectory covariance learning},
  file = {C:\Users\benja\Zotero\storage\MBLRA44I\10050558.html}
}

@techreport{Lipt15,
  title = {A {{Critical Review}} of {{Recurrent Neural Networks}} for {{Sequence Learning}}},
  author = {Lipton, Zachary C and Berkowitz, John and Elkan, Charles},
  year = {2015}
}

@article{liReactiveTaskMotion2021,
  title = {Reactive {{Task}} and {{Motion Planning}} under {{Temporal Logic Specifications}}},
  author = {Li, Shen and Park, Daehyung and Sung, Yoonchang and Shah, Julie A. and Roy, Nicholas},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.14464 [cs]},
  eprint = {2103.14464},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {We present a task-and-motion planning (TAMP) algorithm robust against a human operator's cooperative or adversarial interventions. Interventions often invalidate the current plan and require replanning on the fly. Replanning can be computationally expensive and often interrupts seamless task execution. We introduce a dynamically reconfigurable planning methodology with behavior tree-based control strategies toward reactive TAMP, which takes the advantage of previous plans and incremental graph search during temporal logic-based reactive synthesis. Our algorithm also shows efficient recovery functionalities that minimize the number of replanning steps. Finally, our algorithm produces a robust, efficient, and complete TAMP solution. Our experimental results show the algorithm results in superior manipulation performance in both simulated and real-world tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  annotation = {recommend},
  file = {C:\Users\benja\Zotero\storage\LF97I6ZK\2103.html}
}

@article{liRealtimeDetectionSteel2018,
  title = {Real-Time {{Detection}} of {{Steel Strip Surface Defects Based}} on {{Improved YOLO Detection Network}}},
  author = {Li, Jiangyun and Su, Zhenfeng and Geng, Jiahui and Yin, Yixin},
  year = {2018},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {5th {{IFAC Workshop}} on {{Mining}}, {{Mineral}} and {{Metal Processing MMM}} 2018},
  volume = {51},
  number = {21},
  pages = {76--81},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2018.09.412},
  urldate = {2020-10-13},
  abstract = {The surface defects of steel strip have diverse and complex features, and surface defects caused by different production lines tend to have different characteristics. Therefore, the detection algorithms for the surface defects of steel strip should have good generalization performance. Aiming at detecting surface defects of steel strip, we established a dataset of six types of surface defects on cold-rolled steel strip and augmented it in order to reduce over-fitting. We improved the You Only Look Once (YOLO) network and made it all convolutional. Our improved network, which consists of 27 convolution layers, provides an end-to-end solution for the surface defects detection of steel strip. We evaluated the six types of defects with our network and reached performance of 97.55\% mAP and 95.86\% recall rate. Besides, our network achieves 99\% detection rate with speed of 83 FPS, which provides methodological support for real-time surface defects detection of steel strip. It can also predict the location and size information of defect regions, which is of great significance for evaluating the quality of an entire steel strip production line.},
  langid = {english},
  keywords = {Convolutional Neural Network,Defect Detection,Improved YOLO Network,Steel Strip,Surface quality},
  file = {C:\Users\benja\Zotero\storage\YKZCKAA8\S2405896318321001.html}
}

@inproceedings{liRMP2StructuredComposable2021,
  title = {{{RMP2}}: {{A Structured Composable Policy Class}} for {{Robot Learning}}},
  shorttitle = {{{RMP2}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVII}}},
  author = {Li, Anqi and Cheng, Ching-An and Rana, Muhammad Asif and Xie, Man and Wyk, Karl Van and Ratliff, Nathan and Boots, Byron},
  year = {2021},
  month = jul,
  volume = {17},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-7-8},
  file = {C:\Users\benja\Zotero\storage\U7EE7ATC\p092.html}
}

@inproceedings{liSmoothEfficientPolicy2018,
  title = {Smooth and {{Efficient Policy Exploration}} for {{Robot Trajectory Learning}}},
  booktitle = {2018 27th {{IEEE International Symposium}} on {{Robot}} and {{Human Interactive Communication}} ({{RO-MAN}})},
  author = {Li, Shidi and Chew, Chee-Meng and Subramaniam, Velusamy},
  year = {2018},
  month = aug,
  pages = {1087--1092},
  publisher = {IEEE},
  address = {Nanjing},
  doi = {10.1109/ROMAN.2018.8525631},
  urldate = {2019-05-17},
  abstract = {Many policy search algorithms have been proposed for robot learning and proved to be practical in real robot applications. However, there are still hyperparameters in the algorithms, such as the exploration rate, which requires manual tuning. The existing methods to design the exploration rate manually or automatically may not be general enough or hard to apply in the real robot. In this paper, we propose a learning model to update the exploration rate adaptively. We blend the advantages of several previous methods. Smooth trajectories for the robot control system can be produced by the updated exploration rate which maximizes the lower bound of the expected return. Our method is tested in the ball-in-cup problem. The results show that our method can receive the same learning outcome as the previous methods with fewer iterations.},
  isbn = {978-1-5386-7980-7},
  langid = {english}
}

@article{Litman.2014,
  title = {Learning Spectral Descriptors for Deformable Shape Correspondence},
  author = {Litman, R. and Bronstein, A. M.},
  year = {2014},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {36},
  number = {1},
  pages = {171--180},
  issn = {01628828},
  doi = {10.1109/TPAMI.2013.148},
  abstract = {Informative and discriminative feature descriptors play a fundamental role in deformable shape analysis. For example, they have been successfully employed in correspondence, registration, and retrieval tasks. In recent years, significant attention has been devoted to descriptors obtained from the spectral decomposition of the Laplace-Beltrami operator associated with the shape. Notable examples in this family are the heat kernel signature (HKS) and the recently introduced wave kernel signature (WKS). The Laplacian-based descriptors achieve state-of-the-art performance in numerous shape analysis tasks; they are computationally efficient, isometry-invariant by construction, and can gracefully cope with a variety of transformations. In this paper, we formulate a generic family of parametric spectral descriptors. We argue that to be optimized for a specific task, the descriptor should take into account the statistics of the corpus of shapes to which it is applied (the signal) and those of the class of transformations to which it is made insensitive (the noise). While such statistics are hard to model axiomatically, they can be learned from examples. Following the spirit of the Wiener filter in signal processing, we show a learning scheme for the construction of optimized spectral descriptors and relate it to Mahalanobis metric learning. The superiority of the proposed approach in generating correspondences is demonstrated on synthetic and scanned human figures. We also show that the learned descriptors are robust enough to be learned on synthetic data and transferred successfully to scanned shapes.},
  pagination = {page}
}

@article{liTransferLearningComputer2020,
  title = {Transfer Learning in Computer Vision Tasks: {{Remember}} Where You Come~From},
  shorttitle = {Transfer Learning in Computer Vision Tasks},
  author = {Li, Xuhong and Grandvalet, Yves and Davoine, Franck and Cheng, Jingchun and Cui, Yin and Zhang, Hang and Belongie, Serge and Tsai, Yi-Hsuan and Yang, Ming-Hsuan},
  year = {2020},
  month = jan,
  journal = {Image and Vision Computing},
  volume = {93},
  pages = {103853},
  issn = {0262-8856},
  doi = {10.1016/j.imavis.2019.103853},
  urldate = {2021-06-21},
  abstract = {Fine-tuning pre-trained deep networks is a practical way of benefiting from the representation learned on a large database while having relatively few examples to train a model. This adjustment is nowadays routinely performed so as to benefit of the latest improvements of convolutional neural networks trained on large databases. Fine-tuning requires some form of regularization, which is typically implemented by weight decay that drives the network parameters towards zero. This choice conflicts with the motivation for fine-tuning, as starting from a pre-trained solution aims at taking advantage of the previously acquired knowledge. Hence, regularizers promoting an explicit inductive bias towards the pre-trained model have been recently proposed. This paper demonstrates the versatility of this type of regularizer across transfer learning scenarios. We replicated experiments on three state-of-the-art approaches in image classification, image segmentation, and video analysis to compare the relative merits of regularizers. These tests show systematic improvements compared to weight decay. Our experimental protocol put forward the versatility of a regularizer that is easy to implement and to operate that we eventually recommend as the new baseline for future approaches to transfer learning relying on fine-tuning.},
  langid = {english},
  keywords = {Computer vision,Parameter regularization,Transfer learning},
  file = {C:\Users\benja\Zotero\storage\VDF7NXV8\S0262885619304469.html}
}

@incollection{Liu04,
  title = {A {{Dual Neural Network}} for {{Bi-criteria Torque Optimization}} of {{Redundant Robot Manipulators}}},
  booktitle = {Neural {{Information Processing}}: 11th {{International Conference}}, {{ICONIP}} 2004, {{Calcutta}}, {{India}}, {{November}} 22-25, 2004. {{Proceedings}}},
  author = {Liu, Shubao and Wang, Jun},
  editor = {Pal, Nikhil Ranjan and Kasabov, Nik and Mudi, Rajani K and Pal, Srimanta and Parui, Swapan Kumar},
  year = {2004},
  pages = {1142--1147},
  publisher = {Springer}
}

@inproceedings{liuCAGEContextAwareGrasping2020,
  title = {{{CAGE}}: {{Context-Aware Grasping Engine}}},
  shorttitle = {{{CAGE}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Liu, Weiyu and Daruna, Angel and Chernova, Sonia},
  year = {2020},
  month = may,
  pages = {2550--2556},
  issn = {2577-087X},
  doi = {10.1109/ICRA40945.2020.9197289},
  abstract = {Semantic grasping is the problem of selecting stable grasps that are functionally suitable for specific object manipulation tasks. In order for robots to effectively perform object manipulation, a broad sense of contexts, including object and task constraints, needs to be accounted for. We introduce the Context-Aware Grasping Engine, which combines a novel semantic representation of grasp contexts with a neural network structure based on the Wide \& Deep model, capable of capturing complex reasoning patterns. We quantitatively validate our approach against three prior methods on a novel dataset consisting of 14,000 semantic grasps for 44 objects, 7 tasks, and 6 different object states. Our approach outperformed all baselines by statistically significant margins, producing new insights into the importance of balancing memorization and generalization of contexts for semantic grasping. We further demonstrate the effectiveness of our approach on robot experiments in which the presented model successfully achieved 31 of 32 suitable grasps. The code and data are available at: https://github.com/wliu88/railsemanticgrasping.},
  keywords = {Cognition,Context modeling,Feature extraction,Grasping,Robots,Semantics,Task analysis},
  file = {C:\Users\benja\Zotero\storage\AK438ANV\9197289.html}
}

@inproceedings{liuDeepDifferentiableGrasp2020,
  title = {Deep {{Differentiable Grasp Planner}} for {{High-DOF Grippers}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVI}}},
  author = {Liu, Min and Pan, Zherong and Xu, Kai and Ganguly, Kanishka and Manocha, Dinesh},
  year = {2020},
  month = jul,
  volume = {16},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-6-1},
  file = {C:\Users\benja\Zotero\storage\NJHRQZAV\p066.html}
}

@article{liuDeepStructuredReactive2021,
  title = {Deep {{Structured Reactive Planning}}},
  author = {Liu, Jerry and Zeng, Wenyuan and Urtasun, Raquel and Yumer, Ersin},
  year = {2021},
  month = apr,
  journal = {arXiv:2101.06832 [cs]},
  eprint = {2101.06832},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {An intelligent agent operating in the real-world must balance achieving its goal with maintaining the safety and comfort of not only itself, but also other participants within the surrounding scene. This requires jointly reasoning about the behavior of other actors while deciding its own actions as these two processes are inherently intertwined - a vehicle will yield to us if we decide to proceed first at the intersection but will proceed first if we decide to yield. However, this is not captured in most self-driving pipelines, where planning follows prediction. In this paper we propose a novel data-driven, reactive planning objective which allows a self-driving vehicle to jointly reason about its own plans as well as how other actors will react to them. We formulate the problem as an energy-based deep structured model that is learned from observational data and encodes both the planning and prediction problems. Through simulations based on both real-world driving and synthetically generated dense traffic, we demonstrate that our reactive model outperforms a non-reactive variant in successfully completing highly complex maneuvers (lane merges/turns in traffic) faster, without trading off collision rate.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\ZDC7T7CD\2101.html}
}

@inproceedings{liuEndToEndNetworkPanoptic2019,
  title = {An {{End-To-End Network}} for {{Panoptic Segmentation}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Liu, Huanyu and Peng, Chao and Yu, Changqian and Wang, Jingbo and Liu, Xu and Yu, Gang and Jiang, Wei},
  year = {2019},
  pages = {6172--6181},
  urldate = {2023-02-27}
}

@misc{liuGroundingDINOMarrying2024,
  title = {Grounding {{DINO}}: {{Marrying DINO}} with {{Grounded Pre-Training}} for {{Open-Set Object Detection}}},
  shorttitle = {Grounding {{DINO}}},
  author = {Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei},
  year = {2024},
  month = jul,
  number = {arXiv:2303.05499},
  eprint = {2303.05499},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.05499},
  urldate = {2024-08-27},
  abstract = {In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a \$52.5\$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean \$26.1\$ AP. Code will be available at {\textbackslash}url\{https://github.com/IDEA-Research/GroundingDINO\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\benja\\Zotero\\storage\\JN4EI54T\\Liu et al. - 2024 - Grounding DINO Marrying DINO with Grounded Pre-Training for Open-Set Object Detection.pdf;C\:\\Users\\benja\\Zotero\\storage\\CDVJQQKY\\2303.html}
}

@article{liuLimitedMemoryBFGS1989,
  title = {On the Limited Memory {{BFGS}} Method for Large Scale Optimization},
  author = {Liu, Dong C. and Nocedal, Jorge},
  year = {1989},
  month = aug,
  journal = {Mathematical Programming},
  volume = {45},
  number = {1},
  pages = {503--528},
  issn = {1436-4646},
  doi = {10.1007/BF01589116},
  urldate = {2024-03-31},
  abstract = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems.},
  langid = {english},
  keywords = {conjugate gradient method,Large scale nonlinear optimization,limited memory methods,partitioned quasi-Newton method}
}

@inproceedings{liuModelAgnosticMetaLearningFault2023,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fault Diagnosis}} of {{Industrial Robots}}},
  booktitle = {2023 28th {{International Conference}} on {{Automation}} and {{Computing}} ({{ICAC}})},
  author = {Liu, Yuxin and Chen, Chong and Wang, Tao and Cheng, Lianglun and Qin, Jian},
  year = {2023},
  month = aug,
  pages = {1--6},
  doi = {10.1109/ICAC57885.2023.10275255},
  urldate = {2024-06-15},
  abstract = {The success of deep learning in the field of fault diagnosis depends on a large number of training data, but it is a challenge to achieve fault diagnosis of multi-axis industrial robots in the case of few-shot. To address this issue, this paper proposes a method called Model-Agnostic Meta-Learning (MAML) for fault diagnosis of industrial robots. Its goal is to train an effective industrial robot fault classifier using minimal training data. Additionally, it can learn to recognize faults in new scenarios with high accuracy based on the training data. Experimental results based on a six-axis industrial robot dataset show that the proposed method is superior to traditional convolutional neural network (CNN) and transfer learning, and that the diagnostic results with the same amount of data in few-shot cases are better than existing intelligent fault diagnosis methods.},
  keywords = {Computational modeling,Deep Learning,Fault diagnosis,Fault Diagnosis,Industrial Robots,Meta Learning,Metalearning,Scalability,Service robots,Training data,Transfer learning},
  file = {C:\Users\benja\Zotero\storage\J9H99ZFC\10275255.html}
}

@article{liUncertaintyEstimationActive2020,
  title = {On Uncertainty Estimation in Active Learning for Image Segmentation},
  author = {Li, Bo and Alstr{\o}m, Tommy Sonne},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.06364 [cs]},
  eprint = {2007.06364},
  primaryclass = {cs},
  urldate = {2020-12-19},
  abstract = {Uncertainty estimation is important for interpreting the trustworthiness of machine learning models in many applications. This is especially critical in the data-driven active learning setting where the goal is to achieve a certain accuracy with minimum labeling effort. In such settings, the model learns to select the most informative unlabeled samples for annotation based on its estimated uncertainty. The highly uncertain predictions are assumed to be more informative for improving model performance. In this paper, we explore uncertainty calibration within an active learning framework for medical image segmentation, an area where labels often are scarce. Various uncertainty estimation methods and acquisition strategies (regions and full images) are investigated. We observe that selecting regions to annotate instead of full images leads to more well-calibrated models. Additionally, we experimentally show that annotating regions can cut 50\% of pixels that need to be labeled by humans compared to annotating full images.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}.},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan},
  year = {2019},
  journal = {CoRR},
  volume = {abs/1907.11692},
  file = {C:\Users\benja\Zotero\storage\CY5MNU4G\abs-1907-11692.html}
}

@misc{liuVisualAgentBenchLargeMultimodal2024,
  title = {{{VisualAgentBench}}: {{Towards Large Multimodal Models}} as {{Visual Foundation Agents}}},
  shorttitle = {{{VisualAgentBench}}},
  author = {Liu, Xiao and Zhang, Tianjie and Gu, Yu and Iong, Iat Long and Xu, Yifan and Song, Xixuan and Zhang, Shudan and Lai, Hanyu and Liu, Xinyi and Zhao, Hanlin and Sun, Jiadai and Yang, Xinyue and Yang, Yu and Qi, Zehan and Yao, Shuntian and Sun, Xueqiao and Cheng, Siyi and Zheng, Qinkai and Yu, Hao and Zhang, Hanchen and Hong, Wenyi and Ding, Ming and Pan, Lihang and Gu, Xiaotao and Zeng, Aohan and Du, Zhengxiao and Song, Chan Hee and Su, Yu and Dong, Yuxiao and Tang, Jie},
  year = {2024},
  month = aug,
  number = {arXiv:2408.06327},
  eprint = {2408.06327},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.06327},
  urldate = {2024-08-20},
  abstract = {Large Multimodal Models (LMMs) have ushered in a new era in artificial intelligence, merging capabilities in both language and vision to form highly capable Visual Foundation Agents. These agents are postulated to excel across a myriad of tasks, potentially approaching general artificial intelligence. However, existing benchmarks fail to sufficiently challenge or showcase the full potential of LMMs in complex, real-world environments. To address this gap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering benchmark specifically designed to train and evaluate LMMs as visual foundation agents across diverse scenarios, including Embodied, Graphical User Interface, and Visual Design, with tasks formulated to probe the depth of LMMs' understanding and interaction capabilities. Through rigorous testing across nine proprietary LMM APIs and eight open models, we demonstrate the considerable yet still developing agent capabilities of these models. Additionally, VAB constructs a trajectory training set constructed through hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and Human Demonstrations, promoting substantial performance improvements in LMMs through behavior cloning. Our work not only aims to benchmark existing models but also provides a solid foundation for future development into visual foundation agents. Code, train {\textbackslash}\& test data, and part of fine-tuned open LMMs are available at {\textbackslash}url\{https://github.com/THUDM/VisualAgentBench\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\benja\\Zotero\\storage\\CBTE43SD\\Liu et al. - 2024 - VisualAgentBench Towards Large Multimodal Models as Visual Foundation Agents.pdf;C\:\\Users\\benja\\Zotero\\storage\\RZKCQEMW\\2408.html}
}

@misc{liuVisualInstructionTuning2023,
  title = {Visual {{Instruction Tuning}}},
  author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  year = {2023},
  month = dec,
  number = {arXiv:2304.08485},
  eprint = {2304.08485},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.08485},
  urldate = {2024-01-05},
  abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\YLV3ERQ7\2304.html}
}

@article{liuWindTurbineBlade2017,
  title = {Wind Turbine Blade Waste in 2050},
  author = {Liu, Pu and Barlow, Claire Y.},
  year = {2017},
  month = apr,
  journal = {Waste Management},
  volume = {62},
  pages = {229--240},
  issn = {0956-053X},
  doi = {10.1016/j.wasman.2017.02.007},
  urldate = {2024-07-27},
  abstract = {Wind energy has developed rapidly over the last two decades to become one of the most promising and economically viable sources of renewable energy. Although wind energy is claimed to provide clean renewable energy without any emissions during operation, but it is only one side of the coin. The blades, one of the most important components in the wind turbines, made with composite, are currently regarded as unrecyclable. With the first wave of early commercial wind turbine installations now approaching their end of life, the problem of blade disposal is just beginning to emerge as a significant factor for the future. This paper is aimed at discovering the magnitude of the wind turbine blade waste problem, looking not only at disposal but at all stages of a blade's lifecycle. The first stage of the research, the subject of this paper, is to accurately estimate present and future wind turbine blade waste inventory using the most recent and most accurate data available. The result will provide a solid reference point to help the industry and policy makers to understand the size of potential environmental problem and to help to manage it better. This study starts by estimating the annual blade material usage with wind energy installed capacity and average blade weight. The effect of other waste contributing factors in the full lifecycle of wind turbine blades is then included, using industrial data from the manufacturing, testing and in-service stages. The research indicates that there will be 43 million tonnes of blade waste worldwide by 2050 with China possessing 40\% of the waste, Europe 25\%, the United States 16\% and the rest of the world 19\%.},
  keywords = {Composites waste,Decommissioning,End-of-life,Wind turbine blade waste},
  file = {C:\Users\benja\Zotero\storage\UNZSYJGY\S0956053X17300491.html}
}

@inproceedings{liVisionLanguageFoundationModels2023,
  title = {Vision-{{Language Foundation Models}} as {{Effective Robot Imitators}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Li, Xinghang and Liu, Minghuan and Zhang, Hanbo and Yu, Cunjun and Xu, Jie and Wu, Hongtao and Cheang, Chilam and Jing, Ya and Zhang, Weinan and Liu, Huaping and Li, Hang and Kong, Tao},
  year = {2023},
  month = oct,
  urldate = {2024-06-15},
  abstract = {Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control. Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy. Our code will be made public upon acceptance.},
  langid = {english}
}

@article{liVisualizingNeuralNetworks2020,
  title = {Visualizing {{Neural Networks}} with the {{Grand Tour}}},
  author = {Li, Mingwei and Zhao, Zhenge and Scheidegger, Carlos},
  year = {2020},
  month = mar,
  journal = {Distill},
  volume = {5},
  number = {3},
  pages = {e25},
  issn = {2476-0757},
  doi = {10.23915/distill.00025},
  urldate = {2021-01-31},
  abstract = {By focusing on linear dimensionality reduction, we show how to visualize many dynamic phenomena in neural networks.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\T284JJ2C\grand-tour.html}
}

@inproceedings{lizotteAutomaticGaitOptimization2007,
  title = {Automatic Gait Optimization with {{Gaussian}} Process Regression},
  booktitle = {Proceedings of the 20th International Joint Conference on {{Artifical}} Intelligence},
  author = {Lizotte, Daniel and Wang, Tao and Bowling, Michael and Schuurmans, Dale},
  year = {2007},
  month = jan,
  series = {{{IJCAI}}'07},
  pages = {944--949},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  urldate = {2024-05-23},
  abstract = {Gait optimization is a basic yet challenging problem for both quadrupedal and bipedal robots. Although techniques for automating the process exist, most involve local function optimization procedures that suffer from three key drawbacks. Local optimization techniques are naturally plagued by local optima, make no use of the expensive gait evaluations once a local step is taken, and do not explicitly model noise in gait evaluation. These drawbacks increase the need for a large number of gait evaluations, making optimization slow, data inefficient, and manually intensive. We present a Bayesian approach based on Gaussian process regression that addresses all three drawbacks. It uses a global search strategy based on a posterior model inferred from all of the individual noisy evaluations. We demonstrate the technique on a quadruped robot, using it to optimize two different criteria: speed and smoothness. We show in both cases our technique requires dramatically fewer gait evaluations than state-of-the-art local gradient approaches.}
}

@inproceedings{loewPROMPTProbabilisticMotion2021,
  title = {{{PROMPT}}: {{Probabilistic Motion Primitives}} Based {{Trajectory Planning}}},
  shorttitle = {{{PROMPT}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVII}}},
  author = {Loew, Tobias and Bandyopadhyay, Tirthankar and Williams, Jason and Borges, Paulo},
  year = {2021},
  month = jul,
  volume = {17},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-7-8},
  file = {C:\Users\benja\Zotero\storage\KXTKPQAV\p058.html}
}

@inproceedings{longLearningTransferableFeatures2015,
  title = {Learning {{Transferable Features}} with {{Deep Adaptation Networks}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Long, Mingsheng and Cao, Yue and Wang, Jianmin and Jordan, Michael},
  year = {2015},
  month = jun,
  pages = {97--105},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2024-06-14},
  abstract = {Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.},
  langid = {english}
}

@article{longUnsupervisedDomainAdaptation,
  title = {Unsupervised {{Domain Adaptation}} with {{Residual Transfer Networks}}},
  author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I},
  pages = {9},
  abstract = {The recent success of deep neural networks relies on massive amounts of labeled data. For a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain. In this paper, we propose a new approach to domain adaptation in deep networks that can jointly learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain. We relax a shared-classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function. We enable classifier adaptation by plugging several layers into deep network to explicitly learn the residual function with reference to the target classifier. We fuse features of multiple layers with tensor product and embed them into reproducing kernel Hilbert spaces to match distributions for feature adaptation. The adaptation can be achieved in most feed-forward models by extending them with new residual layers and loss functions, which can be trained efficiently via back-propagation. Empirical evidence shows that the new approach outperforms state of the art methods on standard domain adaptation benchmarks.},
  langid = {english}
}

@inproceedings{longWhatAILiteracy2020,
  title = {What Is {{AI Literacy}}? {{Competencies}} and {{Design Considerations}}},
  shorttitle = {What Is {{AI Literacy}}?},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Long, Duri and Magerko, Brian},
  year = {2020},
  month = apr,
  series = {{{CHI}} '20},
  pages = {1--16},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3313831.3376727},
  urldate = {2024-09-18},
  abstract = {Artificial intelligence (AI) is becoming increasingly integrated in user-facing technology, but public understanding of these technologies is often limited. There is a need for additional HCI research investigating a) what competencies users need in order to effectively interact with and critically evaluate AI and b) how to design learner-centered AI technologies that foster increased user understanding of AI. This paper takes a step towards realizing both of these goals by providing a concrete definition of AI literacy based on existing research. We synthesize a variety of interdisciplinary literature into a set of core competencies of AI literacy and suggest several design considerations to support AI developers and educators in creating learner-centered AI. These competencies and design considerations are organized in a conceptual framework thematically derived from the literature. This paper's contributions can be used to start a conversation about and guide future research on AI literacy within the HCI community.},
  isbn = {978-1-4503-6708-0}
}

@article{loquercioGeneralFrameworkUncertainty2020,
  title = {A {{General Framework}} for {{Uncertainty Estimation}} in {{Deep Learning}}},
  author = {Loquercio, Antonio and Seg{\`u}, Mattia and Scaramuzza, Davide},
  year = {2020},
  month = feb,
  journal = {IEEE Robotics and Automation Letters},
  volume = {PP},
  pages = {1--1},
  doi = {10.1109/LRA.2020.2974682},
  abstract = {Neural networks predictions are unreliable when the input sample is out of the training distribution or corrupted by noise. Being able to detect such failures automatically is fundamental to integrate deep learning algorithms into robotics. Current approaches for uncertainty estimation of neural networks require changes to the network and optimization process, typically ignore prior knowledge about the data, and tend to make over-simplifying assumptions which underestimate uncertainty. To address these limitations, we propose a novel framework for uncertainty estimation. Based on Bayesian belief networks and Monte-Carlo sampling, our framework not only fully models the different sources of prediction uncertainty, but also incorporates prior data information, e.g. sensor noise. We show theoretically that this gives us the ability to capture uncertainty better than existing methods. In addition, our framework has several desirable properties: (i) it is agnostic to the network architecture and task; (ii) it does not require changes in the optimization process; (iii) it can be applied to already trained architectures. We thoroughly validate the proposed framework through extensive experiments on both computer vision and control tasks, where we outperform previous methods by up to 23\% in accuracy. The video available at https://youtu.be/X7nbRS5vSM shows qualitative results of our experiments. The project's code is available at: https://tinyurl.com/s3nygw7.}
}

@article{Lorensen.1987,
  title = {Marching Cubes: {{A}} High Resolution {{3D}} Surface Construction Algorithm},
  author = {Lorensen, William E. and Cline, Harvey E.},
  year = {1987},
  journal = {ACM SIGGRAPH Computer Graphics},
  volume = {21},
  number = {4},
  pages = {163--169},
  issn = {00978930},
  doi = {10.1145/37402.37422},
  pagination = {page}
}

@inproceedings{loshchilovDecoupledWeightDecay2018,
  title = {Decoupled {{Weight Decay Regularization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2018},
  month = sep,
  urldate = {2024-03-28},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it ``weight decay'' in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at {\textbackslash}url\{https://github.com/loshchil/AdamW-and-SGDW\}},
  langid = {english}
}

@inproceedings{loshchilovSGDRStochasticGradient2022,
  title = {{{SGDR}}: {{Stochastic Gradient Descent}} with {{Warm Restarts}}},
  shorttitle = {{{SGDR}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2022},
  month = jul,
  urldate = {2024-05-25},
  abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14{\textbackslash}\% and 16.21{\textbackslash}\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at {\textbackslash}{\textbackslash} {\textbackslash}url\{https://github.com/loshchil/SGDR\}},
  langid = {english}
}

@misc{loullingenKukaKRC1,
  title = {Kuka {{KR C1 Anwenderprogrammierung}}},
  author = {Loullingen, Claude},
  urldate = {2024-04-15},
  howpublished = {https://www.loullingen.lu/projekte/kuka/kuka\_anwenderprogramm.php},
  file = {C:\Users\benja\Zotero\storage\X5STFAMD\kuka_anwenderprogramm.html}
}

@article{Low.2004,
  title = {Linear Least-Squares Optimization for Point-to-Plane {{ICP}} Surface Registration},
  author = {Low, Kok-Lim},
  year = {2004},
  journal = {Technical Report TR04-004, Department of COmputer Science}
}

@inproceedings{Lowe.1999,
  title = {Object Recognition from Local Scale-Invariant Features},
  booktitle = {Proceedings of the Seventh {{IEEE}} International Conference on Computer Vision},
  author = {Lowe, D. G.},
  year = {1999},
  pages = {1150-1157 vol.2},
  publisher = {IEEE},
  doi = {10.1109/ICCV.1999.790410},
  bookpagination = {page},
  isbn = {0-7695-0164-8}
}

@article{lozano-perezRobotProgramming1983,
  title = {Robot Programming},
  author = {{Lozano-Perez}, T.},
  year = {1983},
  month = jul,
  journal = {Proceedings of the IEEE},
  volume = {71},
  number = {7},
  pages = {821--841},
  issn = {1558-2256},
  doi = {10.1109/PROC.1983.12681},
  urldate = {2024-02-03},
  abstract = {The industrial robot's principal advantage over traditional automation is programmability. Robots can perform arbitrary sequences of pre-stored motions or of motions computed as functions of sensory input. This paper reviews requirements for and developments in robot programming systems. The key requirements for robot programming systems examined in the paper are in the areas of sensing, world modeling, motion specification, flow of control, and programming support. Existing and proposed robot programming systems fall into three broad categories: guiding systems in which the user leads a robot through the motions to be performed, robot-level programming systems in which the user writes a computer program specifying motion and sensing, and task-level programming systems in which the user specifies operations by their desired effect on objects. A representative sample of systems in each of these categories is surveyed in the paper.},
  keywords = {Robot kinematics,Robot programming,Robot sensing systems,Task analysis},
  file = {C:\Users\benja\Zotero\storage\Z9LZGXQN\Lozano-Perez - 1983 - Robot programming.pdf}
}

@incollection{luckAutonomyVariableGenerative2003,
  title = {Autonomy: {{Variable}} and {{Generative}}},
  shorttitle = {Autonomy},
  booktitle = {Agent {{Autonomy}}},
  author = {Luck, Michael and D'Inverno, Mark and Munroe, Steve},
  editor = {Hexmoor, Henry and Castelfranchi, Cristiano and Falcone, Rino},
  year = {2003},
  series = {Multiagent {{Systems}}, {{Artificial Societies}}, and {{Simulated Organizations}}},
  pages = {11--28},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4419-9198-0_2},
  urldate = {2021-02-05},
  abstract = {In the paper we discuss variable and generative forms of autonomy. Variable autonomy is discussed in terms of the practicalities in designing autonomous agents, dealing as it does with the notion of degrees of autonomy and hence issues of agent control. The major part of the paper discusses an absolute, theoretically grounded notion of autonomy: the ability to generate one's own goals. This theoretical account of autonomy is embedded in the larger SMART framework and is intimately linked with the issue of motivation. Autonomous agents are motivated agents in that for the generation of goals an agent needs a set of higher order, non-derivative sources of action, or in our terminology, motivations. Autonomous agents in the SMART framework form the basis and source of action in multi-agent systems, which can thus propagate through the other entities in the system, such as non-autonomous agents and objects. We conclude with a discussion regarding the situations an autonomous agent would be willing to relinquish its autonomy thus linking the generative and variable notions of autonomy.},
  isbn = {978-1-4419-9198-0},
  langid = {english},
  keywords = {adjustable autonomy,generative autonomy,motivation}
}

@inproceedings{luckcuckSummaryFormalSpecification2019,
  title = {A {{Summary}} of {{Formal Specification}} and {{Verification}} of {{Autonomous Robotic~Systems}}},
  booktitle = {Integrated {{Formal Methods}}},
  author = {Luckcuck, Matt and Farrell, Marie and Dennis, Louise A. and Dixon, Clare and Fisher, Michael},
  editor = {Ahrendt, Wolfgang and Tapia Tarifa, Silvia Lizeth},
  year = {2019},
  pages = {538--541},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-34968-4_33},
  abstract = {Autonomous robotic systems are complex, hybrid, and often safety-critical; this makes their formal specification and verification uniquely challenging. Though commonly used, testing and simulation alone are insufficient to ensure the correctness of, or provide sufficient evidence for the certification of, autonomous robotics. Formal methods for autonomous robotics have received some attention in the literature, but no resource provides a current overview. This short paper summarises the contributions published in~[5], which surveys the state-of-the-art in formal specification and verification for autonomous robotics.},
  isbn = {978-3-030-34968-4},
  langid = {english}
}

@inproceedings{luddeckeLearningSegmentAffordances2017,
  title = {Learning to {{Segment Affordances}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {L{\"u}ddecke, Timo and W{\"o}rg{\"o}tter, Florentin},
  year = {2017},
  month = oct,
  pages = {769--776},
  issn = {2473-9944},
  doi = {10.1109/ICCVW.2017.96},
  abstract = {The goal of this work is to densely predict a comparatively large set of affordances given only single RGB images. We approach this task by using a convolutional neural network based on the well-known ResNet architecture, which we blend with refinement modules recently proposed in the semantic segmentation literature. A novel cost function, capable of handling incomplete data, is introduced, which is necessary because we make use of segmentations of objects and their parts to generate affordance maps. We demonstrate both, quantitatively and qualitatively, that learning a dense predictor of affordances from an object part dataset is indeed possible and show that our model outperforms several baselines.},
  keywords = {Computer architecture,Computer vision,Cost function,Image segmentation,Predictive models,Training},
  file = {C:\Users\benja\Zotero\storage\4IMI5VG5\8265305.html}
}

@article{luIndustry40Survey2017,
  title = {Industry 4.0: {{A}} Survey on Technologies, Applications and Open Research Issues},
  shorttitle = {Industry 4.0},
  author = {Lu, Yang},
  year = {2017},
  month = jun,
  journal = {Journal of Industrial Information Integration},
  volume = {6},
  pages = {1--10},
  issn = {2452-414X},
  doi = {10.1016/j.jii.2017.04.005},
  urldate = {2021-12-20},
  abstract = {Originally initiated in Germany, Industry 4.0, the fourth industrial revolution, has attracted much attention in recent literatures. It is closely related with the Internet of Things (IoT), Cyber Physical System (CPS), information and communications technology (ICT), Enterprise Architecture (EA), and Enterprise Integration (EI). Despite of the dynamic nature of the research on Industry 4.0, however, a systematic and extensive review of recent research on it is has been unavailable. Accordingly, this paper conducts a comprehensive review on Industry 4.0 and presents an overview of the content, scope, and findings of Industry 4.0 by examining the existing literatures in all of the databases within the Web of Science. Altogether, 88 papers related to Industry 4.0 are grouped into five research categories and reviewed. In addition, this paper outlines the critical issue of the interoperability of Industry 4.0, and proposes a conceptual framework of interoperability regarding Industry 4.0. Challenges and trends for future research on Industry 4.0 are discussed.},
  langid = {english},
  keywords = {Big data,Cyber physical system,Enterprise architecture,Enterprise integration,Industry 4.0,Internet of things}
}

@incollection{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Lundberg, Scott M and Lee, Su-In},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {4765--4774},
  publisher = {Curran Associates, Inc.},
  urldate = {2020-10-13},
  file = {C\:\\Users\\benja\\Zotero\\storage\\2ZN535BI\\7062-a-unified-approach-to-interpreting-model-predictions.html;C\:\\Users\\benja\\Zotero\\storage\\49JU7T6N\\7062-a-unified-approach-to-interpreting-model-predictions.html}
}

@misc{lundbyDeepActiveLearning2023,
  title = {Deep Active Learning for Nonlinear System Identification},
  author = {Lundby, Erlend Torje Berg and Rasheed, Adil and Halvorsen, Ivar Johan and Reinhardt, Dirk and Gros, Sebastien and Gravdahl, Jan Tommy},
  year = {2023},
  month = feb,
  number = {arXiv:2302.12667},
  eprint = {2302.12667},
  primaryclass = {cs, eess, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.12667},
  urldate = {2024-01-08},
  abstract = {The exploding research interest for neural networks in modeling nonlinear dynamical systems is largely explained by the networks' capacity to model complex input-output relations directly from data. However, they typically need vast training data before they can be put to any good use. The data generation process for dynamical systems can be an expensive endeavor both in terms of time and resources. Active learning addresses this shortcoming by acquiring the most informative data, thereby reducing the need to collect enormous datasets. What makes the current work unique is integrating the deep active learning framework into nonlinear system identification. We formulate a general static deep active learning acquisition problem for nonlinear system identification. This is enabled by exploring system dynamics locally in different regions of the input space to obtain a simulated dataset covering the broader input space. This simulated dataset can be used in a static deep active learning acquisition scheme referred to as global explorations. The global exploration acquires a batch of initial states corresponding to the most informative state-action trajectories according to a batch acquisition function. The local exploration solves an optimal control problem, finding the control trajectory that maximizes some measure of information. After a batch of informative initial states is acquired, a new round of local explorations from the initial states in the batch is conducted to obtain a set of corresponding control trajectories that are to be applied on the system dynamics to get data from the system. Information measures used in the acquisition scheme are derived from the predictive variance of an ensemble of neural networks. The novel method outperforms standard data acquisition methods used for system identification of nonlinear dynamical systems in the case study performed on simulated data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Electrical Engineering and Systems Science - Systems and Control},
  file = {C:\Users\benja\Zotero\storage\IXKCQRK5\2302.html}
}

@article{lundStateoftheartValueChain2024,
  title = {State-of-the-Art Value Chain Roadmap for Sustainable End-of-Life Wind Turbine Blades},
  author = {Lund, K. W. and Madsen, E. S.},
  year = {2024},
  month = mar,
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {192},
  pages = {114234},
  issn = {1364-0321},
  doi = {10.1016/j.rser.2023.114234},
  urldate = {2024-07-27},
  abstract = {Increasing volumes of wind turbine blades (WTBs) meeting their end-of-life are expected in the coming years, while establishing sustainable value chains for handling the blades is still at an early stage. Through a systematic literature identification and review process, current literature on sustainable end-of-life management and technologies for wind turbine blades is identified to map state-of-the-art. The review resulted in the identification of four overarching research themes with sixteen underlying research topics. The study findings highlight that research so far has mainly been focused on recycling technologies and material properties of recovered WTB material. Furthermore, none of the reviewed literature contained empirical data from end-of-life blade projects. Thus, there is a lack of data and knowledge on the complete value chain, including processes of logistics, sectioning, pre-processing, and refining recovered materials for secondary applications. Furthermore, the understanding of the environmental, social, and cost impacts of these processes are also found to be absent. Based on a consolidation of findings, a complete value chain roadmap of required processes and technologies for sustainable end-of-life routes of WTBs is presented. The roadmap and results illustrate a baseline for current practices but also calls for further research into optimization and impact assessment of value chains. Thus, the presented roadmap has implications for academia, wind turbine producers and owners to aid the assessment of which practices and technologies to utilize when decommissioning WTBs.},
  keywords = {Decommissioning,End-of-Life,Recycling,Sustainability,Value chain,Waste management,Wind turbine blades},
  file = {C:\Users\benja\Zotero\storage\9ENGGQ4Z\S1364032123010924.html}
}

@article{luoLargeLanguageModelbased2024,
  title = {Large Language Model-Based Code Generation for the Control of Construction Assembly Robots: {{A}} Hierarchical Generation Approach},
  shorttitle = {Large Language Model-Based Code Generation for the Control of Construction Assembly Robots},
  author = {Luo, Hanbin and Wu, Jianxin and Liu, Jiajing and {Antwi-Afari}, Maxwell Fordjour},
  year = {2024},
  month = oct,
  journal = {Developments in the Built Environment},
  volume = {19},
  pages = {100488},
  issn = {2666-1659},
  doi = {10.1016/j.dibe.2024.100488},
  urldate = {2024-09-25},
  abstract = {Offline programming (OLP) is a mainstream approach for controlling assembly robots at construction sites. However, existing methods are tailored to specific assembly tasks and workflows, and thus lack flexibility. Additionally, the emerging large language model (LLM)-based OLP cannot effectively handle the code logic of robot programming. Thus, this paper addresses the question: How can robot control programs be generated effectively and accurately for diverse construction assembly tasks using LLM techniques? This paper describes a closed user-on-the-loop control framework for construction assembly robots based on LLM techniques. A hierarchical strategy to generate robot control programs is proposed to logically integrate code generation at high and low levels. Additionally, customized application programming interfaces and a chain of action are combined to enhance the LLM's understanding of assembly action logic. An assembly task set was designed to evaluate the feasibility and reliability of the proposed approach. The results show that the proposed approach (1) is widely applicable to diverse assembly tasks, and (2) can improve the quality of the generated code by decreasing the number of errors. Our approach facilitates the automation of construction assembly tasks by simplifying the robot control process.},
  keywords = {Code generation,Construction assembly robot,Human-robot collaboration,Large language model},
  file = {C:\Users\benja\Zotero\storage\75DXV8B8\S2666165924001698.html}
}

@article{luoSelfImitationLearningPlanning2021,
  title = {Self-{{Imitation Learning}} by {{Planning}}},
  author = {Luo, Sha and Kasaei, Hamidreza and Schomaker, Lambert},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.13834 [cs]},
  eprint = {2103.13834},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {Imitation learning (IL) enables robots to acquire skills quickly by transferring expert knowledge, which is widely adopted in reinforcement learning (RL) to initialize exploration. However, in long-horizon motion planning tasks, a challenging problem in deploying IL and RL methods is how to generate and collect massive, broadly distributed data such that these methods can generalize effectively. In this work, we solve this problem using our proposed approach called \{self-imitation learning by planning (SILP)\}, where demonstration data are collected automatically by planning on the visited states from the current policy. SILP is inspired by the observation that successfully visited states in the early reinforcement learning stage are collision-free nodes in the graph-search based motion planner, so we can plan and relabel robot's own trials as demonstrations for policy learning. Due to these self-generated demonstrations, we relieve the human operator from the laborious data preparation process required by IL and RL methods in solving complex motion planning tasks. The evaluation results show that our SILP method achieves higher success rates and enhances sample efficiency compared to selected baselines, and the policy learned in simulation performs well in a real-world placement task with changing goals and obstacles.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\KAPVP6MH\2103.html}
}

@article{lutterDifferentiablePhysicsModels2020,
  title = {Differentiable {{Physics Models}} for {{Real-world Offline Model-based Reinforcement Learning}}},
  author = {Lutter, Michael and Silberbauer, Johannes and Watson, Joe and Peters, Jan},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.01734 [cs]},
  eprint = {2011.01734},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {A limitation of model-based reinforcement learning (MBRL) is the exploitation of errors in the learned models. Black-box models can fit complex dynamics with high fidelity, but their behavior is undefined outside of the data distribution.Physics-based models are better at extrapolating, due to the general validity of their informed structure, but underfit in the real world due to the presence of unmodeled phenomena. In this work, we demonstrate experimentally that for the offline model-based reinforcement learning setting, physics-based models can be beneficial compared to high-capacity function approximators if the mechanical structure is known. Physics-based models can learn to perform the ball in a cup (BiC) task on a physical manipulator using only 4 minutes of sampled data using offline MBRL. We find that black-box models consistently produce unviable policies for BiC as all predicted trajectories diverge to physically impossible state, despite having access to more data than the physics-based model. In addition, we generalize the approach of physics parameter identification from modeling holonomic multi-body systems to systems with nonholonomic dynamics using end-to-end automatic differentiation. Videos: https://sites.google.com/view/ball-in-a-cup-in-4-minutes/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\QT9YETI9\2011.html}
}

@inproceedings{lutterDifferentiablePhysicsModels2021,
  title = {Differentiable {{Physics Models}} for {{Real-world Offline Model-based Reinforcement Learning}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Lutter, Michael and Silberbauer, Johannes and Watson, Joe and Peters, Jan},
  year = {2021},
  month = may,
  pages = {4163--4170},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9561805},
  abstract = {A limitation of model-based reinforcement learning (MBRL) is the exploitation of errors in the learned models. Blackbox models can fit complex dynamics with high fidelity, but their behavior is undefined outside of the data distribution. Physics-based models are better at extrapolating, due to the general validity of their informed structure, but underfit in the real world due to the presence of unmodeled phenomena. In this work, we demonstrate experimentally that for the offline model-based reinforcement learning setting, physics-based models can be beneficial compared to high-capacity function approximators if the mechanical structure is known. Physics-based models can learn to perform the ball in a cup (BiC) task on a physical manipulator using only 4 minutes of sampled data using offline MBRL. We find that black-box models consistently produce unviable policies for BiC as all predicted trajectories diverge to physically impossible state, despite having access to more data than the physics-based model. In addition, we generalize the approach of physics parameter identification from modeling holonomic multi-body systems to systems with nonholonomic dynamics using end-to-end automatic differentiation.Videos: https://sites.google.com/view/ball-in-a-cup-in-4-minutes/},
  keywords = {Automation,Conferences,Data models,Manipulators,Parameter estimation,Predictive models,Reinforcement learning}
}

@inproceedings{lynchLanguageConditionedImitation2021,
  title = {Language {{Conditioned Imitation Learning Over Unstructured Data}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVII}}},
  author = {Lynch, Corey and Sermanet, Pierre},
  year = {2021},
  month = jul,
  volume = {17},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-7-8},
  file = {C:\Users\benja\Zotero\storage\4EFIRE7W\p047.html}
}

@inproceedings{lynchLearningLatentPlans2019,
  title = {Learning {{Latent Plans}} from {{Play}}},
  booktitle = {Proceedings of {{Machine Learning Research}}},
  author = {Lynch, Corey and Khansari, Mohi and Xiao, Ted and Kumar, Vikash and Tompson, Jonathan and Levine, Sergey and Sermanet, Pierre},
  year = {2019},
  month = mar,
  eprint = {1903.01973},
  urldate = {2019-06-27},
  abstract = {We propose learning from teleoperated play data (LfP) as a way to scale up multi-task robotic skill learning. Learning from play (LfP) offers three main advantages: 1) It is cheap. Large amounts of play data can be collected quickly as it does not require scene staging, task segmenting, or resetting to an initial state. 2) It is general. It contains both functional and non-functional behavior, relaxing the need for a predefined task distribution. 3) It is rich. Play involves repeated, varied behavior and naturally leads to high coverage of the possible interaction space. These properties distinguish play from expert demonstrations, which are rich, but expensive, and scripted unattended data collection, which is cheap, but insufficiently rich. Variety in play, however, presents a multimodality challenge to methods seeking to learn control on top. To this end, we introduce Play-LMP, a method designed to handle variability in the LfP setting by organizing it in an embedding space. Play-LMP jointly learns 1) reusable latent plan representations unsupervised from play data and 2) a single goal-conditioned policy capable of decoding inferred plans to achieve user-specified tasks. We show empirically that Play-LMP, despite not being trained on task-specific data, is capable of generalizing to 18 complex user-specified manipulation tasks with average success of 85.5\%, outperforming individual models trained on expert demonstrations (success of 70.3\%). Furthermore, we find that play-supervised models, unlike their expert-trained counterparts, 1) are more robust to perturbations and 2) exhibit retrying-till-success. Finally, despite never being trained with task labels, we find that our agent learns to organize its latent plan space around functional tasks. Videos of the performed experiments are available at learning-from-play.github.io},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics}
}

@inproceedings{lyuControllableMeshGeneration2023,
  title = {Controllable {{Mesh Generation Through Sparse Latent Point Diffusion Models}}},
  booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Lyu, Zhaoyang and Wang, Jinyi and An, Yuwei and Zhang, Ya and Lin, Dahua and Dai, Bo},
  year = {2023},
  month = jun,
  pages = {271--280},
  issn = {2575-7075},
  doi = {10.1109/CVPR52729.2023.00034},
  urldate = {2024-01-11},
  abstract = {Mesh generation is of great value in various applications involving computer graphics and virtual content, yet designing generative models for meshes is challenging due to their irregular data structure and inconsistent topology of meshes in the same category. In this work, we design a novel sparse latent point diffusion model for mesh generation. Our key insight is to regard point clouds as an intermediate representation of meshes, and model the distribution of point clouds instead. While meshes can be generated from point clouds via techniques like Shape as Points (SAP), the challenges of directly generating meshes can be effectively avoided. To boost the efficiency and controllability of our mesh generation method, we propose to further encode point clouds to a set of sparse latent points with pointwise semantic meaningful features, where two DDPMs are trained in the space of sparse latent points to respectively model the distribution of the latent point positions and features at these latent points. We find that sampling in this latent space is faster than directly sampling dense point clouds. Moreover, the sparse latent points also enable us to explicitly control both the overall structures and local details of the generated meshes. Extensive experiments are conducted on the ShapeNet dataset, where our proposed sparse latent point diffusion model achieves superior performance in terms of generation quality and controllability when compared to existing methods. Project page, code and appendix: https://slide-3d.github.io.},
  file = {C:\Users\benja\Zotero\storage\MTH6P6N2\10204137.html}
}

@article{Maass1997,
  title = {Networks of Spiking Neurons: The Third Generation of Neural Network Models},
  author = {Maass, Wolfgang},
  year = {1997},
  journal = {Neural networks},
  volume = {10},
  number = {9},
  pages = {1659--1671},
  publisher = {Elsevier}
}

@inproceedings{machinoRemoteCollaborationSystemUsing2006,
  title = {Remote-{{Collaboration System Using Mobile Robot}} with {{Camera}} and {{Projector}}},
  booktitle = {Journal of the {{Robotics Society}} of {{Japan}}},
  author = {Machino, Tamotsu and Iwaki, Satoshi and Kawata, Hiroaki and Yanagihara, Yoshimasa and Nanjo, Yoshito and Shimokura, Ken-ichiro},
  year = {2006},
  month = jan,
  volume = {2006},
  pages = {4063--4068},
  doi = {10.1109/ROBOT.2006.1642326},
  abstract = {We have been studying a remote-collaboration system called SCOPE (sight collaboration by projection effect) featuring image projecting and capturing capabilities as implemented in a maintenance robot. With the help of SCOPE and a remote support person, an on-site worker can perform maintenance very efficiently. We propose a mobile SCOPE that can significantly expand the area of activity of the conventional SCOPE. To enable an on-site worker to share the field of view of a remote support person, we developed a technique for aligning the optical axes of a camera and a projector. We present experimental data that demonstrates the validity of our optomechanical design. Finally, we show that the mobile SCOPE enables workers to continuously share their field of view, no matter where the mobile SCOPE moves}
}

@article{mackayEvidenceFrameworkApplied1992,
  title = {The {{Evidence Framework Applied}} to {{Classification Networks}}},
  author = {MacKay, David J. C.},
  year = {1992},
  month = sep,
  journal = {Neural Computation},
  volume = {4},
  number = {5},
  pages = {720--736},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.5.720},
  abstract = {Three Bayesian ideas are presented for supervised adaptive classifiers. First, it is argued that the output of a classifier should be obtained by marginalizing over the posterior distribution of the parameters; a simple approximation to this integral is proposed and demonstrated. This involves a "moderation" of the most probable classifier's outputs, and yields improved performance. Second, it is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems. This framework successfully chooses the magnitude of weight decay terms, and ranks solutions found using different numbers of hidden units. Third, an information-based data selection criterion is derived and demonstrated within this framework.},
  file = {C:\Users\benja\Zotero\storage\NX6S8VT2\6796959.html}
}

@article{mackayPracticalBayesianFramework1992,
  title = {A Practical {{Bayesian}} Framework for Backpropagation Networks},
  author = {MacKay, David J. C.},
  year = {1992},
  month = may,
  journal = {Neural Computation},
  volume = {4},
  number = {3},
  pages = {448--472},
  publisher = {MIT Press},
  issn = {0899-7667},
  urldate = {2020-12-19},
  abstract = {A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian "evidence" automatically embodies "Occam's razor," penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained.},
  file = {C\:\\Users\\benja\\Zotero\\storage\\85NTJJHS\\13793.html;C\:\\Users\\benja\\Zotero\\storage\\Q7YP6RLW\\13793.html}
}

@inproceedings{magdyReviewTrajectorySimilarity2015,
  title = {Review on Trajectory Similarity Measures},
  author = {Magdy, Nehal and Sakr, Mahmoud and Abdelkader, Tamer and Elbahnasy, Khaled},
  year = {2015},
  month = dec,
  doi = {10.1109/IntelCIS.2015.7397286},
  abstract = {The availability of devices that can be used to track moving objects has increased dramatically leading to a great growth in movement data from almost every application domain. Therefore, there has been an increasing interest in proposing new methodologies for indexing, classifying, clustering, querying and measuring similarity between moving objects' data. One of the main functions for a wide range of application domains is to measure the similarity between two moving objects' trajectories. In this paper, we present a comparative study between widely used trajectory similarity measures observing the advantages and disadvantages of these measures.}
}

@incollection{magrenanGaussNewtonMethod2018,
  title = {Gauss--{{Newton Method}}},
  booktitle = {A {{Contemporary Study}} of {{Iterative Methods}}},
  author = {Magre{\~n}{\'a}n, {\'A}. Alberto and Argyros, Ioannis K.},
  year = {2018},
  month = jan,
  pages = {61--67},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-809214-9.00005-X},
  urldate = {2024-05-21},
  abstract = {In this chapter we present the local convergence analysis of Gauss--Newton method using the idea of restricted convergence domains, which allows us to improve previous results. Finally, some special cases and a numerical example are also given, validating the theoretical results.},
  isbn = {978-0-12-809214-9},
  keywords = {Gauss-Newton method,Local convergence,Restricted domains}
}

@article{maier-heinSurgicalDataScience2017,
  title = {Surgical Data Science for Next-Generation Interventions},
  author = {{Maier-Hein}, Lena and Vedula, Swaroop S. and Speidel, Stefanie and Navab, Nassir and Kikinis, Ron and Park, Adrian and Eisenmann, Matthias and Feussner, Hubertus and Forestier, Germain and Giannarou, Stamatia and Hashizume, Makoto and Katic, Darko and Kenngott, Hannes and Kranzfelder, Michael and Malpani, Anand and M{\"a}rz, Keno and Neumuth, Thomas and Padoy, Nicolas and Pugh, Carla and Schoch, Nicolai and Stoyanov, Danail and Taylor, Russell and Wagner, Martin and Hager, Gregory D. and Jannin, Pierre},
  year = {2017},
  month = sep,
  journal = {Nature Biomedical Engineering},
  volume = {1},
  number = {9},
  pages = {691--696},
  publisher = {Nature Publishing Group},
  issn = {2157-846X},
  doi = {10.1038/s41551-017-0132-7},
  urldate = {2024-04-12},
  abstract = {Interventional healthcare will evolve from an artisanal craft based on the individual experiences, preferences and traditions of physicians into a discipline that relies on objective decision-making on the basis of large-scale data from heterogeneous sources.},
  copyright = {2017 Springer Nature Limited},
  langid = {english},
  keywords = {Health care,Surgery}
}

@article{maier-heinSurgicalDataScience2022,
  title = {Surgical Data Science - from Concepts toward Clinical Translation},
  author = {{Maier-Hein}, Lena and Eisenmann, Matthias and Sarikaya, Duygu and M{\"a}rz, Keno and Collins, Toby and Malpani, Anand and Fallert, Johannes and Feussner, Hubertus and Giannarou, Stamatia and Mascagni, Pietro and Nakawala, Hirenkumar and Park, Adrian and Pugh, Carla and Stoyanov, Danail and Vedula, Swaroop S. and Cleary, Kevin and Fichtinger, Gabor and Forestier, Germain and Gibaud, Bernard and Grantcharov, Teodor and Hashizume, Makoto and {Heckmann-N{\"o}tzel}, Doreen and Kenngott, Hannes G. and Kikinis, Ron and M{\"u}ndermann, Lars and Navab, Nassir and Onogur, Sinan and Ro{\ss}, Tobias and Sznitman, Raphael and Taylor, Russell H. and Tizabi, Minu D. and Wagner, Martin and Hager, Gregory D. and Neumuth, Thomas and Padoy, Nicolas and Collins, Justin and Gockel, Ines and Goedeke, Jan and Hashimoto, Daniel A. and Joyeux, Luc and Lam, Kyle and Leff, Daniel R. and Madani, Amin and Marcus, Hani J. and Meireles, Ozanan and Seitel, Alexander and Teber, Dogu and {\"U}ckert, Frank and {M{\"u}ller-Stich}, Beat P. and Jannin, Pierre and Speidel, Stefanie},
  year = {2022},
  month = feb,
  journal = {Medical Image Analysis},
  volume = {76},
  pages = {102306},
  issn = {1361-8423},
  doi = {10.1016/j.media.2021.102306},
  abstract = {Recent developments in data science in general and machine learning in particular have transformed the way experts envision the future of surgery. Surgical Data Science (SDS) is a new research field that aims to improve the quality of interventional healthcare through the capture, organization, analysis and modeling of data. While an increasing number of data-driven approaches and clinical applications have been studied in the fields of radiological and clinical data science, translational success stories are still lacking in surgery. In this publication, we shed light on the underlying reasons and provide a roadmap for future advances in the field. Based on an international workshop involving leading researchers in the field of SDS, we review current practice, key achievements and initiatives as well as available standards and tools for a number of topics relevant to the field, namely (1) infrastructure for data acquisition, storage and access in the presence of regulatory constraints, (2) data annotation and sharing and (3) data analytics. We further complement this technical perspective with (4) a review of currently available SDS products and the translational progress from academia and (5) a roadmap for faster clinical translation and exploitation of the full potential of SDS, based on an international multi-round Delphi process.},
  langid = {english},
  pmcid = {PMC9135051},
  pmid = {34879287},
  keywords = {Artificial intelligence,Clinical translation,Computer aided surgery,Data Science,Deep learning,Humans,Machine Learning,Surgical data science}
}

@misc{MakingTransformerNetworks,
  title = {Making {{Transformer}} Networks Simpler and More Efficient},
  urldate = {2020-12-15},
  abstract = {Facebook AI researchers are sharing an all-attention layer to simplify the Transformer model and an adaptive attention span method to make it more efficient. Even with a much simpler architecture, these methods match or improve state-of-the-art results.},
  howpublished = {https://ai.facebook.com/blog/making-transformer-networks-simpler-and-more-efficient/},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\ECGU6KPJ\making-transformer-networks-simpler-and-more-efficient.html}
}

@article{mallaSocialSTAGESpatioTemporalMultiModal2021,
  title = {Social-{{STAGE}}: {{Spatio-Temporal Multi-Modal Future Trajectory Forecast}}},
  shorttitle = {Social-{{STAGE}}},
  author = {Malla, Srikanth and Choi, Chiho and Dariush, Behzad},
  year = {2021},
  month = mar,
  journal = {arXiv:2011.04853 [cs]},
  eprint = {2011.04853},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {This paper considers the problem of multi-modal future trajectory forecast with ranking. Here, multi-modality and ranking refer to the multiple plausible path predictions and the confidence in those predictions, respectively. We propose Social-STAGE, Social interaction-aware Spatio-Temporal multi-Attention Graph convolution network with novel Evaluation for multi-modality. Our main contributions include analysis and formulation of multi-modality with ranking using interaction and multi-attention, and introduction of new metrics to evaluate the diversity and associated confidence of multi-modal predictions. We evaluate our approach on existing public datasets ETH and UCY and show that the proposed algorithm outperforms the state of the arts on these datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\benja\Zotero\storage\PZ4XXU6N\2011.html}
}

@article{maModelingCalibrationHighorder2018,
  title = {Modeling and Calibration of High-Order Joint-Dependent Kinematic Errors for Industrial Robots},
  author = {Ma, Le and Bazzoli, Patrick and Sammons, Patrick M. and Landers, Robert G. and Bristow, Douglas A.},
  year = {2018},
  month = apr,
  journal = {Robotics and Computer-Integrated Manufacturing},
  volume = {50},
  number = {C},
  pages = {153--167},
  issn = {0736-5845},
  doi = {10.1016/j.rcim.2017.09.006},
  urldate = {2022-08-02},
  abstract = {Robot kinematic errors, both static and joint-dependent, were classified.A new robot calibration and compensation methodology was presented.The new method is capable of reducing high-order joint-dependent kinematic errors. Robot positioning accuracy is critically important in many manufacturing applications. While geometric errors such as imprecise link length and assembly misalignment dominate positioning errors in industrial robots, significant errors also arise from non-uniformities in bearing systems and strain wave gearings. These errors are characteristically more complicated than the fixed geometric errors in link lengths and assembly. Typical robot calibration methods only consider constant kinematic errors, thus, neglecting complex kinematic errors and limiting the accuracy to which robots can be calibrated. In contrast to typical calibration methods, this paper considers models containing both constant and joint-dependent kinematic errors. Constituent robot kinematic error sources are identified and kinematic error models are classified for each error source. The constituent models are generalized into a single robot kinematic error model with both constant and high-order joint-dependent error terms. Maximum likelihood estimation is utilized to identify error model parameters using measurements obtained over the measurable joint space by a laser tracker. Experiments comparing the proposed and traditional calibration methods implemented on a FANUC LR Mate 200i robot are presented and analyzed. While the traditional constant kinematic error model describes 79.4\% of the measured error, the proposed modeling framework, constructed from measurements of 250 poses, describes 97.0\% of the measured error. The results demonstrate that nearly 20\% of the kinematic error in this study can be attributed to complex, joint-dependent error sources.},
  keywords = {Calibration,Industrial robots,Maximum likelihood estimation,Strain wave gearing}
}

@inproceedings{mandiEffectivenessFinetuningMetareinforcement2022,
  title = {On the {{Effectiveness}} of {{Fine-tuning Versus Meta-reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mandi, Zhao and Abbeel, Pieter and James, Stephen},
  year = {2022},
  month = oct,
  urldate = {2024-06-15},
  abstract = {Intelligent agents should have the ability to leverage knowledge from previously learned tasks in order to learn new ones quickly and efficiently. Meta-learning approaches have emerged as a popular solution to achieve this. However, meta-reinforcement learning (meta-RL) algorithms have thus far been restricted to simple environments with narrow task distributions and have seen limited success. Moreover, the paradigm of pretraining followed by fine-tuning to adapt to new tasks has emerged as a simple yet effective solution in supervised learning. This calls into question the benefits of meta learning approaches also in reinforcement learning, which typically come at the cost of high complexity. We therefore investigate meta-RL approaches in a variety of vision-based benchmarks, including Procgen, RLBench, and Atari, where evaluations are made on completely novel tasks. Our findings show that when meta-learning approaches are evaluated on different tasks (rather than different variations of the same task), multi-task pretraining with fine-tuning on new tasks performs equally as well, or better, than meta-pretraining with meta test-time adaptation. This is encouraging for future research, as multi-task pretraining tends to be simpler and computationally cheaper than meta-RL. From these findings, we advocate for evaluating future meta-RL methods on more challenging tasks and including multi-task pretraining with fine-tuning as a simple, yet strong baseline.},
  langid = {english}
}

@article{maniaActiveLearningNonlinear2022,
  title = {Active {{Learning}} for {{Nonlinear System Identification}} with {{Guarantees}}},
  author = {Mania, Horia and Jordan, Michael I. and Recht, Benjamin},
  year = {2022},
  journal = {Journal of Machine Learning Research},
  volume = {23},
  number = {32},
  pages = {1--30},
  issn = {1533-7928},
  urldate = {2024-01-08},
  abstract = {While the identification of nonlinear dynamical systems is a fundamental building block of model-based reinforcement learning and feedback control, its sample complexity is only understood for systems that either have discrete states and actions or for systems that can be identified from data generated by i.i.d. random inputs. Nonetheless, many interesting dynamical systems have continuous states and actions and can only be identified through a judicious choice of inputs. Motivated by practical settings, we study a class of nonlinear dynamical systems whose state transitions depend linearly on a known feature embedding of state-action pairs. To estimate such systems in finite time identification methods must explore all directions in feature space. We propose an active learning approach that achieves this by repeating three steps: trajectory planning, trajectory tracking, and re-estimation of the system from all available data. We show that our method estimates nonlinear dynamical systems at a parametric rate, similar to the statistical rate of standard linear regression.}
}

@inproceedings{maniaImaginationenabledRobotPerception2021,
  title = {Imagination-Enabled {{Robot Perception}}},
  booktitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Mania, Patrick and Kenfack, Franklin Kenghagho and Neumann, Michael and Beetz, Michael},
  year = {2021},
  month = sep,
  pages = {936--943},
  issn = {2153-0866},
  doi = {10.1109/IROS51168.2021.9636359},
  urldate = {2024-04-25},
  abstract = {Many of today's robot perception systems aim at accomplishing perception tasks that are too simplistic and too hard. They are too simplistic because they do not require the perception systems to provide all the information needed to accomplish manipulation tasks. Typically the perception results do not include information about the part structure of objects, articulation mechanisms and other attributes needed for adapting manipulation behavior. On the other hand, the perception problems stated are also too hard because --- unlike humans--- the perception systems cannot leverage the expectations about what they will see to their full potential. Therefore, we investigate a variation of robot perception tasks suitable for robots accomplishing everyday manipulation tasks, such as household robots or a robot in a retail store. In such settings it is reasonable to assume that robots know most objects and have detailed models of them. We propose a perception system that maintains its beliefs about its environment as a scene graph with physics simulation and visual rendering. When detecting objects, the perception system retrieves the model of the object and places it at the corresponding place in a VR-based environment model. The physics simulation ensures that object detections that are physically not possible are rejected and scenes can be rendered to generate expectations at the image level. The result is a perception system that can provide useful information for manipulation tasks.},
  keywords = {Intelligent robots,Object detection,Physics,Rendering (computer graphics),Robots,Task analysis,Visualization},
  file = {C:\Users\benja\Zotero\storage\EPMBW38I\9636359.html}
}

@inproceedings{maniaOpenFlexibleRobot2024,
  title = {An Open and Flexible Robot Perception Framework for Mobile Manipulation Tasks},
  booktitle = {2024 International Conference on Robotics and Automation ({{ICRA}})},
  author = {Mania, Patrick and Stelter, Simon and Kazhoyan, Gayane and Beetz, Michael},
  year = {2024},
  publisher = {IEEE},
  keywords = {easecrc_perception}
}

@article{mannaAutomaticProgramSynthesis1971,
  title = {Toward Automatic Program Synthesis},
  author = {Manna, Zohar and Waldinger, Richard J.},
  year = {1971},
  month = mar,
  journal = {Communications of the ACM},
  volume = {14},
  number = {3},
  pages = {151--165},
  issn = {0001-0782},
  doi = {10.1145/362566.362568},
  urldate = {2024-04-18},
  abstract = {An elementary outline of the theorem-proving approach to automatic program synthesis is given, without dwelling on technical details. The method is illustrated by the automatic construction of both recursive and iterative programs operating on natural numbers, lists, and trees. In order to construct a program satisfying certain specifications, a theorem induced by those specifications is proved, and the desired program is extracted from the proof. The same technique is applied to transform recursively defined functions into iterative programs, frequently with a major gain in efficiency. It is emphasized that in order to construct a program with loops or with recursion, the principle of mathematical induction must be applied. The relation between the version of the induction rule used and the form of the program constructed is explored in some detail.},
  keywords = {answer extraction,artificial intelligence,automatic program synthesis,mathematical induction principle,problem solving,theorem proving}
}

@article{mannaDeductiveApproachProgram1980,
  title = {A {{Deductive Approach}} to {{Program Synthesis}}},
  author = {Manna, Zohar and Waldinger, Richard},
  year = {1980},
  month = jan,
  journal = {ACM Transactions on Programming Languages and Systems},
  volume = {2},
  number = {1},
  pages = {90--121},
  issn = {0164-0925},
  doi = {10.1145/357084.357090},
  urldate = {2024-04-18},
  abstract = {Program synthesis is the systematic derivation of a program from a given specification. A deductive approach to program synthesis is presented for the construction of recursive programs. This approach regards program synthesis as a theorem-proving task and relies on a theorem-proving method that combines the features of transformation rules, unification, and mathematical induction within a single framework.}
}

@article{mannaKnowledgeReasoningProgram1975,
  title = {Knowledge and Reasoning in Program Synthesis},
  author = {Manna, Zohar and Waldinger, Richard},
  year = {1975},
  month = jun,
  journal = {Artificial Intelligence},
  volume = {6},
  number = {2},
  pages = {175--208},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(75)90008-9},
  urldate = {2024-04-18},
  abstract = {Program synthesis is the construction of a computer program from given specifications. An automatic program synthesis system must combine reasoning and programming ability with a good deal of knowledge about the subject matter of the program. This ability and knowledge must be manifested both procedurally (by programs) and structurally (by choice of representation). We describe some of the reasoning and programming capabilities of a projected synthesis system. Special attention is paid to the introduction of conditional tests, loops, and instructions with side effects in the program being constructed. The ability to satisfy several interacting goals simultaneously proves to be important in many contexts. The modification of an already existing program to solve a somewhat different problem has been found to be a powerful approach. We illustrate these concepts with hand simulations of the synthesis of a number of pattern-matching programs. Some of these techniques have already been implemented, some are in the course of implementation, while others seem equivalent to well-known unsolved problems in artificial intelligence.}
}

@article{maqbool2020m2caiseg,
  title = {M2caiseg: {{Semantic}} Segmentation of Laparoscopic Images Using Convolutional Neural Networks},
  author = {Maqbool, Salman and Riaz, Aqsa and Sajid, Hasan and Hasan, Osman},
  year = {2020},
  journal = {arXiv preprint arXiv:2008.10134},
  eprint = {2008.10134},
  archiveprefix = {arXiv}
}

@phdthesis{marco-valleBayesianOptimizationRobot2020,
  title = {Bayesian {{Optimization}} in {{Robot Learning}}: {{Automatic Controller Tuning}} and {{Sample-efficient Methods}}},
  shorttitle = {Bayesian {{Optimization}} in {{Robot Learning}}},
  author = {{Marco-Valle}, Alonso},
  year = {2020},
  googlebooks = {7k580AEACAAJ},
  langid = {english},
  school = {Eberhard Karls Universit{\"a}t T{\"u}bingen}
}

@article{marcusIDEALFrameworkSurgical2024,
  title = {The {{IDEAL}} Framework for Surgical Robotics: Development, Comparative Evaluation and Long-Term Monitoring},
  shorttitle = {The {{IDEAL}} Framework for Surgical Robotics},
  author = {Marcus, Hani J. and Ramirez, Pedro T. and Khan, Danyal Z. and Layard Horsfall, Hugo and Hanrahan, John G. and Williams, Simon C. and Beard, David J. and Bhat, Rani and Catchpole, Ken and Cook, Andrew and Hutchison, Katrina and Martin, Janet and Melvin, Tom and Stoyanov, Danail and Rovers, Maroeska and Raison, Nicholas and Dasgupta, Prokar and Noonan, David and Stocken, Deborah and Sturt, Georgia and Vanhoestenberghe, Anne and Vasey, Baptiste and McCulloch, Peter},
  year = {2024},
  month = jan,
  journal = {Nature Medicine},
  volume = {30},
  number = {1},
  pages = {61--75},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-023-02732-7},
  urldate = {2024-09-17},
  abstract = {The next generation of surgical robotics is poised to disrupt healthcare systems worldwide, requiring new frameworks for evaluation. However, evaluation during a surgical robot's development is challenging due to their complex evolving nature, potential for wider system disruption and integration with complementary technologies like artificial intelligence. Comparative clinical studies require attention to intervention context, learning curves and standardized outcomes. Long-term monitoring needs to transition toward collaborative, transparent and inclusive consortiums for real-world data collection. Here, the Idea, Development, Exploration, Assessment and Long-term monitoring (IDEAL) Robotics Colloquium proposes recommendations for evaluation during development, comparative study and clinical monitoring of surgical robots---providing practical recommendations for developers, clinicians, patients and healthcare systems. Multiple perspectives are considered, including economics, surgical training, human factors, ethics, patient perspectives and sustainability. Further work is needed on standardized metrics, health economic assessment models and global applicability of recommendations.},
  copyright = {2024 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Medical research,Surgery},
  file = {C:\Users\benja\Zotero\storage\4LYV927V\Marcus et al. - 2024 - The IDEAL framework for surgical robotics development, comparative evaluation and long-term monitor.pdf}
}

@article{marcusMakingLeapTranslation2016,
  title = {Making the {{Leap}}: The {{Translation}} of {{Innovative Surgical Devices From}} the {{Laboratory}} to the {{Operating Room}}},
  shorttitle = {Making the {{Leap}}},
  author = {Marcus, Hani J. and Payne, Christopher J. and {Hughes-Hallett}, Archie and Gras, Gauthier and Leibrandt, Konrad and Nandi, Dipankar and Yang, Guang-Zhong},
  year = {2016},
  month = jun,
  journal = {Annals of Surgery},
  volume = {263},
  number = {6},
  pages = {1077},
  issn = {0003-4932},
  doi = {10.1097/SLA.0000000000001532},
  urldate = {2024-04-12},
  abstract = {Objective:~           To determine the rate and extent of translation of innovative surgical devices from the laboratory to first-in-human studies, and to evaluate the factors influencing such translation.           Summary Background Data:~           Innovative surgical devices have preceded many of the major advances in surgical practice. However, the process by which devices arising from academia find their way to translation remains poorly understood.           Methods:~           All biomedical engineering journals, and the 5 basic science journals with the highest impact factor, were searched between January 1993 and January 2000 using the Boolean search term ``surgery OR surgeon OR surgical''. Articles were included if they described the development of a new device and a surgical application was described. A recursive search of all citations to the article was performed using the Web of Science (Thompson-Reuters, New York, NY) to identify any associated first-in-human studies published by January 2015. Kaplan-Meier curves were constructed for the time to first-in-human studies. Factors influencing translation were evaluated using log-rank and Cox proportional hazards models.           Results:~           A total of 8297 articles were screened, and 205 publications describing unique devices were identified. The probability of a first-in-human at 10 years was 9.8\%. Clinical involvement was a significant predictor of a first-in-human study (P = 0.02); devices developed with early clinical collaboration were over 6 times more likely to be translated than those without [RR 6.5 (95\% confidence interval 0.9--48)].           Conclusions:~           These findings support initiatives to increase clinical translation through improved interactions between basic, translational, and clinical researchers.},
  langid = {american},
  file = {C:\Users\benja\Zotero\storage\BLC3QF9W\making_the_leap__the_translation_of_innovative.12.html}
}

@article{margossianReviewAutomaticDifferentiation2019,
  title = {A {{Review}} of Automatic Differentiation and Its Efficient Implementation},
  author = {Margossian, Charles C.},
  year = {2019},
  month = jul,
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {9},
  number = {4},
  eprint = {1811.05031},
  issn = {1942-4787, 1942-4795},
  doi = {10.1002/WIDM.1305},
  urldate = {2022-03-26},
  abstract = {Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. Automatic differentiation is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of automatic differentiation however requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management. Among these techniques are operation overloading, region based memory, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi-analytical derivatives can reduce by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open problems include the extension of current packages to provide more specialized routines, and efficient methods to perform higher-order differentiation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Mathematical Software,Statistics - Computation},
  file = {C:\Users\benja\Zotero\storage\6ISAHN25\1811.html}
}

@patent{marinelarenagalarzaRoboterzelleAnlageUnd2013,
  title = {Roboterzelle, {{Anlage}} Und {{Verfahren}} Zum {{Entgraten}} von {{Teilen}}},
  author = {Marinelarena Galarza, Iker and Etxeberria Martinez, Inigo and Lopez De Alda Arrese, Francisco},
  year = {2013},
  month = jan,
  number = {EP2821872B1},
  urldate = {2021-03-01},
  assignee = {Getting Robotika S.L.},
  annotation = {IPC:}
}

@incollection{markulikQualityAssuranceAutomotive2019,
  title = {Quality {{Assurance}} in the {{Automotive Industry}} and {{Industry}} 4.0},
  booktitle = {Smart {{Technology Trends}} in {{Industrial}} and {{Business Management}}},
  author = {Markulik, {\v S}tefan and Sinay, Juraj and Pa{\v c}aiov{\'a}, Hana},
  editor = {Cag{\'a}{\v n}ov{\'a}, Dagmar and Balog, Michal and Knap{\v c}{\'i}kov{\'a}, Lucia and Soviar, Jakub and Mezarc{\i}{\"o}z, Serkan},
  year = {2019},
  series = {{{EAI}}/{{Springer Innovations}} in {{Communication}} and {{Computing}}},
  pages = {217--225},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-76998-1_15},
  urldate = {2020-10-13},
  abstract = {Industry 4.0 is more than just a phrase. A confluence of trends and technologies promises to reshape the way things are made. An illustrative example of new changes coming from the philosophy of Industry 4.0 is the automotive industry. Automotive industry is not only represented by final manufacturers of the car but the entire network of large and small suppliers who produce components for several automotive manufacturers. In order to implement this production according to customer requirements, they need to communicate with each other in a way and a language they can understand. This means special communication, its tools can be standards or standardized methodology to ensure quality throughout the production chain from the processing of raw materials to final assembly of the vehicle on the production line, its testing and the delivery to the customer. The basic precondition of Industry 4.0 strategy is communication in its broadest understanding. Communication or interconnection is conditioned by the application of generally accepted standards. Part of this connection is the focus on integrated management systems. The philosophy of Industry 4.0 is to minimize the risks as part of functional management systems. Timely and correct communication creates conditions for quality of a product at the end of the production and supply chain, i.e., a car according to customer requirements.},
  isbn = {978-3-319-76998-1},
  langid = {english},
  keywords = {Automotive industry,Industry 4.0,Quality}
}

@incollection{Mart04,
  title = {Electric {{Power System Anomaly Detection Using Neural Networks}}},
  booktitle = {Knowledge-{{Based Intelligent Information}} and {{Engineering Systems}}: 8th {{International Conference}}, {{KES}} 2004, {{Wellington}}, {{New Zealand}}, {{September}} 20-25, 2004, {{Proceedings}}, {{Part I}}},
  author = {Martinelli, Marco and Tronci, Enrico and Dipoppa, Giovanni and Balducelli, Claudio},
  editor = {Negoita, Mircea Gh. and Howlett, Robert J and Jain, Lakhmi C},
  year = {2004},
  pages = {1242--1248},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg}
}

@inproceedings{martensDeepLearningHessianfree2010,
  title = {Deep Learning via {{Hessian-free}} Optimization},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Martens, James},
  year = {2010},
  month = jun,
  series = {{{ICML}}'10},
  pages = {735--742},
  publisher = {Omnipress},
  address = {Madison, WI, USA},
  urldate = {2020-10-20},
  abstract = {We develop a 2nd-order optimization method based on the "Hessian-free" approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton \& Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn't limited in applicability to auto-encoders, or any specific model class. We also discuss the issue of "pathological curvature" as a possible explanation for the difficulty of deep-learning and how 2nd-order optimization, and our method in particular, effectively deals with it.},
  isbn = {978-1-60558-907-7}
}

@article{martin-brevetTaxonomyBasedAnalysis2017,
  title = {Taxonomy Based Analysis of Force Exchanges during Object Grasping and Manipulation},
  author = {{Martin-Brevet}, Sandra and Jarrass{\'e}, Nathana{\"e}l and Burdet, Etienne and {Roby-Brami}, Agn{\`e}s},
  year = {2017},
  month = may,
  journal = {PLOS ONE},
  volume = {12},
  number = {5},
  pages = {e0178185},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0178185},
  urldate = {2021-11-15},
  abstract = {The flexibility of the human hand in object manipulation is essential for daily life activities, but remains relatively little explored with quantitative methods. On the one hand, recent taxonomies describe qualitatively the classes of hand postures for object grasping and manipulation. On the other hand, the quantitative analysis of hand function has been generally restricted to precision grip (with thumb and index opposition) during lifting tasks. The aim of the present study is to fill the gap between these two kinds of descriptions, by investigating quantitatively the forces exerted by the hand on an instrumented object in a set of representative manipulation tasks. The object was a parallelepiped object able to measure the force exerted on the six faces and its acceleration. The grasping force was estimated from the lateral force and the unloading force from the bottom force. The protocol included eleven tasks with complementary constraints inspired by recent taxonomies: four tasks corresponding to lifting and holding the object with different grasp configurations, and seven to manipulating the object (rotation around each of its axis and translation). The grasping and unloading forces and object rotations were measured during the five phases of the actions: unloading, lifting, holding or manipulation, preparation to deposit, and deposit. The results confirm the tight regulation between grasping and unloading forces during lifting, and extend this to the deposit phase. In addition, they provide a precise description of the regulation of force exchanges during various manipulation tasks spanning representative actions of daily life. The timing of manipulation showed both sequential and overlapping organization of the different sub-actions, and micro-errors could be detected. This phenomenological study confirms the feasibility of using an instrumented object to investigate complex manipulative behavior in humans. This protocol will be used in the future to investigate upper-limb dexterity in patients with sensory-motor impairments.},
  langid = {english},
  keywords = {Classical mechanics,Fingers,Hands,Motion,Quantitative analysis,Taxonomy,Thumbs,Wrist},
  file = {C:\Users\benja\Zotero\storage\WZZLJZGH\article.html}
}

@incollection{martinCombiningReinforcementLearning1997,
  title = {Combining Reinforcement Learning and Differential Inverse Kinematics for Collision-Free Motion of Multilink Manipulators},
  booktitle = {Biological and {{Artificial Computation}}: {{From Neuroscience}} to {{Technology}}},
  author = {Martin, Pedro and Mill{\'a}n, Jos{\'e} del R.},
  editor = {Mira, Jos{\'e} and {Moreno-D{\'i}az}, Roberto and Cabestany, Joan and Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan},
  year = {1997},
  volume = {1240},
  pages = {1324--1333},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/BFb0032593},
  urldate = {2019-05-17},
  abstract = {This paper presents a class of neural controllers that learn goaloriented obstacle-avoiding strategies for multilink manipulators. They acquire these strategies on-line through reinforcement learning from local sensory data. These controllers are mainly made of two neural modules: a reinforcement-based action generator and a module for differential inverse kinematics (DIV). The action generator generates actions with regard to a goal vector in the manipulator joint space. Suitable goal vectors are provided by the DIV module. This module is based on the inversion of a neural network that has been previously trained to approximate the manipulator forward kinematics in polar coordinates. Results for two- and three-link planar manipulators are shown. These controllers achieve a good performance quite rapidly and exhibit good generalization capabilities in the face of new environments.},
  isbn = {978-3-540-63047-0 978-3-540-69074-0},
  langid = {english},
  annotation = {Series Editors: \_:n3842}
}

@article{martinez-fernandezSoftwareEngineeringAIBased2022,
  title = {Software {{Engineering}} for {{AI-Based Systems}}: {{A Survey}}},
  shorttitle = {Software {{Engineering}} for {{AI-Based Systems}}},
  author = {{Mart{\'i}nez-Fern{\'a}ndez}, Silverio and Bogner, Justus and Franch, Xavier and Oriol, Marc and Siebert, Julien and Trendowicz, Adam and Vollmer, Anna Maria and Wagner, Stefan},
  year = {2022},
  month = apr,
  journal = {ACM Transactions on Software Engineering and Methodology},
  volume = {31},
  number = {2},
  pages = {37e:1--37e:59},
  issn = {1049-331X},
  doi = {10.1145/3487043},
  urldate = {2024-09-18},
  abstract = {AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.},
  file = {C:\Users\benja\Zotero\storage\4RUX2IHZ\Martnez-Fernndez et al. - 2022 - Software Engineering for AI-Based Systems A Survey.pdf}
}

@article{martinsThermalOptimizationDeposition2023,
  title = {Thermal Optimization of Deposition during Robotic {{3D}} Printing of Large Parts},
  author = {Martins, Diana Filipa Gon{\c c}alves},
  year = {2023},
  month = jul,
  urldate = {2024-04-03},
  copyright = {openAccess},
  langid = {english},
  annotation = {Accepted: 2023-12-18T00:20:17Z}
}

@inproceedings{marvelAutomatedLearningParameter2009,
  title = {Automated Learning for Parameter Optimization of Robotic Assembly Tasks Utilizing Genetic Algorithms},
  booktitle = {2008 {{IEEE International Conference}} on {{Robotics}} and {{Biomimetics}}},
  author = {Marvel, J. A. and Newman, W. S. and Gravel, D. P. and Zhang, G. and {Jianjun Wang} and Fuhlbrigge, T.},
  year = {2009},
  month = feb,
  pages = {179--184},
  doi = {10.1109/ROBIO.2009.4913000},
  abstract = {A challenge for automating mechanical assembly is that cumulative uncertainties typically exceed part clearances, which makes conventional position-based tactics unsuccessful. Force-based assembly strategies offer a potential solution, although such methods are still poorly understood and can be difficult to program. In this paper, we describe a force-based robotic assembly approach that uses fixed strategies with tunable parameters. A generic assembly strategy suitable for execution on an industrial robot is selected by the programmer. Parameters are then self-tuned empirically by the robot using a genetic-algorithm learning process that seeks to minimize assembly time subject to contact-force limits. Results are presented for two automotive part assembly examples using ABB robots with commercial force-control software, showing that the approach is highly effective and suitable for industrial use.},
  keywords = {ABB robots,adaptive control,automated learning,Automatic control,force control,Force control,force-based assembly strategies,generic assembly strategy,genetic algorithms,Genetic algorithms,Industrial control,industrial robot,Manufacturing automation,Orbital robotics,parameter optimization,Robot control,robotic assembly,Robotic assembly,robotic assembly tasks,Robotics and automation,self-adjusting systems,self-tuning,Service robots,tunable parameters}
}

@inproceedings{marvinPromptEngineeringLarge2024,
  title = {Prompt {{Engineering}} in {{Large Language Models}}},
  booktitle = {Data {{Intelligence}} and {{Cognitive Informatics}}},
  author = {Marvin, Ggaliwango and Hellen, Nakayiza and Jjingo, Daudi and {Nakatumba-Nabende}, Joyce},
  editor = {Jacob, I. Jeena and Piramuthu, Selwyn and {Falkowski-Gilski}, Przemyslaw},
  year = {2024},
  pages = {387--402},
  publisher = {Springer Nature},
  address = {Singapore},
  doi = {10.1007/978-981-99-7962-2_30},
  abstract = {With the undeniable rapid development of Conversational Artificial Intelligence (AI) particularly Large Language Models (LLMs), prompt engineering has become an obligatory skill for effective communication and interaction with language driven tools like ChatGPT. It can be leveraged in enforcing rules and process automation for ensuring good quality and quantity of output from LLMs. Moreover, the order of providing examples within prompts, automatic instruction generation, and selection methods has been proven to significantly impact the performance of LLMs. Prompts can be optimized to maximize a chosen score function by searching a pool of instruction candidates within LLMs. No wonder automatically generated instructions give better or similar performance than human annotated instructions and outperform baselines of LLMs, this makes prompt engineering a programming procedure for customizing outputs and interactions of LLMs. In this chapter, we provide thorough understanding of prompt engineering, latest prompt engineering techniques with relevant exercises for putting the techniques in practice. We also discuss current and future trends of LLMs and prompt engineering research, including the rise of automatic instruction generation and selection methods. These are very important for prompt and NLP engineers, conversational AI researchers, and all information seekers or users of LLMs and prompt engineering tools in sensitive domains like health care, security, education~among others.~The chapter provides indepth understanding of prompt engineering principles and techniques for responsible coversational AI.},
  isbn = {978-981-9979-62-2},
  langid = {english},
  keywords = {Automatic instruction generation,Conversational AI,Large language models (LLMs),Natural language processing (NLP),Program synthesis,Prompt engineering}
}

@misc{mastersRevisitingSmallBatch2018,
  title = {Revisiting {{Small Batch Training}} for {{Deep Neural Networks}}},
  author = {Masters, Dominic and Luschi, Carlo},
  year = {2018},
  month = apr,
  number = {arXiv:1804.07612},
  eprint = {1804.07612},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1804.07612},
  urldate = {2024-03-28},
  abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput. In this paper, we review common assumptions on learning rate scaling and training duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size \$m\$. The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient calculations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between \$m = 2\$ and \$m = 32\$, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\U3YKV87N\1804.html}
}

@misc{maSurveyVisionLanguageActionModels2024,
  title = {A {{Survey}} on {{Vision-Language-Action Models}} for {{Embodied AI}}},
  author = {Ma, Yueen and Song, Zixing and Zhuang, Yuzheng and Hao, Jianye and King, Irwin},
  year = {2024},
  month = may,
  number = {arXiv:2405.14093},
  eprint = {2405.14093},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.14093},
  urldate = {2024-07-20},
  abstract = {Deep learning has demonstrated remarkable success across many domains, including computer vision, natural language processing, and reinforcement learning. Representative artificial neural networks in these fields span convolutional neural networks, Transformers, and deep Q-networks. Built upon unimodal neural networks, numerous multi-modal models have been introduced to address a range of tasks such as visual question answering, image captioning, and speech recognition. The rise of instruction-following robotic policies in embodied AI has spurred the development of a novel category of multi-modal models known as vision-language-action models (VLAs). Their multi-modality capability has become a foundational element in robot learning. Various methods have been proposed to enhance traits such as versatility, dexterity, and generalizability. Some models focus on refining specific components through pretraining. Others aim to develop control policies adept at predicting low-level actions. Certain VLAs serve as high-level task planners capable of decomposing long-horizon tasks into executable subtasks. Over the past few years, a myriad of VLAs have emerged, reflecting the rapid advancement of embodied AI. Therefore, it is imperative to capture the evolving landscape through a comprehensive survey.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\LJRYG28A\2405.html}
}

@inproceedings{mateenThoracicSurgeryVideo2024,
  title = {Thoracic {{Surgery Video Analysis}} for {{Surgical Phase Recognition}}},
  booktitle = {2nd {{Robot-Assisted Medical Imaging Workshop}}},
  author = {Mateen, Syed Abdul and Malvia, Niharika and Khader, Syed Abdul and Wang, Danny and Srinivasan, Deepti and Yang, Chi-Fu Jeffrey and Schumacher, Lana and Manjanna, Sandeep},
  year = {2024},
  month = jun,
  eprint = {2406.09185},
  primaryclass = {cs},
  publisher = {arXiv},
  address = {Yokohama, Japan},
  doi = {10.48550/arXiv.2406.09185},
  urldate = {2024-06-14},
  abstract = {This paper presents an approach for surgical phase recognition using video data, aiming to provide a comprehensive understanding of surgical procedures for automated workflow analysis. The advent of robotic surgery, digitized operating rooms, and the generation of vast amounts of data have opened doors for the application of machine learning and computer vision in the analysis of surgical videos. Among these advancements, Surgical Phase Recognition(SPR) stands out as an emerging technology that has the potential to recognize and assess the ongoing surgical scenario, summarize the surgery, evaluate surgical skills, offer surgical decision support, and facilitate medical training. In this paper, we analyse and evaluate both frame-based and video clipping-based phase recognition on thoracic surgery dataset consisting of 11 classes of phases. Specifically, we utilize ImageNet ViT for image-based classification and VideoMAE as the baseline model for video-based classification. We show that Masked Video Distillation(MVD) exhibits superior performance, achieving a top-1 accuracy of 72.9\%, compared to 52.31\% achieved by ImageNet ViT. These findings underscore the efficacy of video-based classifiers over their image-based counterparts in surgical phase recognition tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\benja\Zotero\storage\6NUH4TUW\2406.html}
}

@misc{matlUrdfpy2020,
  title = {Urdfpy},
  author = {Matl, Matthew},
  year = {2020},
  month = sep,
  urldate = {2020-10-12},
  abstract = {Python parser for URDFs. Contribute to mmatl/urdfpy development by creating an account on GitHub.},
  copyright = {MIT License         ,                 MIT License},
  keywords = {kinematics,robotics,ros,urdf-descriptions,urdf-models,urdf-visualizer}
}

@article{Matsuno.2006,
  title = {Manipulation of Deformable Linear Objects Using Knot Invariants to Classify the Object Condition Based on Image Sensor Information},
  author = {Matsuno, T. and Tamaki, D. and Arai, F. and Fukuda, T.},
  year = {2006},
  journal = {IEEE/ASME Transactions on Mechatronics},
  volume = {11},
  number = {4},
  pages = {401--408},
  issn = {1083-4435},
  doi = {10.1109/TMECH.2006.878557},
  pagination = {page}
}

@article{MattAustern.,
  title = {Why You Shouldn't Use Set (and What You Should Use Instead)},
  author = {{Matt Austern}}
}

@article{mawIADAImprovedAnytime2020,
  title = {{{iADA}}*: {{Improved Anytime Path Planning}} and {{Replanning Algorithm}} for {{Autonomous Vehicle}}},
  shorttitle = {{{iADA}}*},
  author = {Maw, Aye Aye and Tyan, Maxim and Lee, Jae-Woo},
  year = {2020},
  month = dec,
  journal = {Journal of Intelligent \& Robotic Systems},
  volume = {100},
  number = {3},
  pages = {1005--1013},
  issn = {1573-0409},
  doi = {10.1007/s10846-020-01240-x},
  urldate = {2021-03-03},
  abstract = {Path planning of autonomous mobile robots in a real-world environment presents several challenges which are usually not raised in other areas. The real world is inherently complex, uncertain and dynamic. Therefore, accurate models of path planning are difficult to obtain and quickly become outdated. Anytime planners are ideal for this type of problem as they can find an initial solution very quickly and then improve it as time allows. This paper proposes a new anytime incremental search algorithm named improved Anytime Dynamic A*(iADA*). The algorithm is based on the currently popular anytime heuristic search algorithm, which is Anytime Dynamic A*(ADA*). The iADA* algorithm improves the calculation of the path lengths and decreases the calculating frequency of the path throughout the search, making it significantly faster. The algorithm is designed to provide an efficient solution to a complex, dynamic search environment when the locally changes affected. Our study shows that the two-dimensional path-planning iADA* experiments were between 2.0 to 3.7 times faster than ADA*, both in partially known and fully unknown dynamic environments. Additionally, in this paper shows the experiment results of the comparison with other four existing algorithms based on computing time and path lengths. iADA* was an average 2.57 times reduced on the computational time for the environment which locally changes effected. For the path length is little increase, but it is not the worst case. According to the experiments, the more the environmental problems and complexity increases, the more iADA* provides a rapid in-search time and total time to obtain the final solution.},
  langid = {english}
}

@book{Mayb99,
  title = {Stochastic {{Models}}, {{Estimation}}, and {{Control}}},
  author = {Maybeck, Peter S},
  year = {1999},
  volume = {1},
  publisher = {Academic Press}
}

@article{mayrSkillbasedMultiobjectiveReinforcement2022,
  title = {Skill-Based {{Multi-objective Reinforcement Learning}} of {{Industrial Robot Tasks}} with {{Planning}} and {{Knowledge Integration}}},
  author = {Mayr, Matthias and Ahmad, Faseeh and Chatzilygeroudis, Konstantinos and Nardi, Luigi and Krueger, Volker},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.10033 [cs]},
  eprint = {2203.10033},
  primaryclass = {cs},
  urldate = {2022-05-06},
  abstract = {In modern industrial settings with small batch sizes it should be easy to set up a robot system for a new task. Strategies exist, e.g. the use of skills, but when it comes to handling forces and torques, these systems often fall short. We introduce an approach that provides a combination of task-level planning with targeted learning of scenario-specific parameters for skill-based systems. We propose the following pipeline: (1) the user provides a task goal in the planning language PDDL, (2) a plan (i.e., a sequence of skills) is generated and the learnable parameters of the skills are automatically identified. An operator then chooses (3) reward functions and hyperparameters for the learning process. Two aspects of our methodology are critical: (a) learning is tightly integrated with a knowledge framework to support symbolic planning and to provide priors for learning, (b) using multi-objective optimization. This can help to balance key performance indicators (KPIs) such as safety and task performance since they can often affect each other. We adopt a multi-objective Bayesian optimization approach and learn entirely in simulation. We demonstrate the efficacy and versatility of our approach by learning skill parameters for two different contact-rich tasks. We show their successful execution on a real 7-DOF KUKA-iiwa manipulator and outperform the manual parameterization by human robot operators.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\BHWXKN5J\2203.html}
}

@article{McAdams.2011,
  title = {Computing the {{Singular Value Decomposition}} of 3 x 3 Matrices with Minimal Branching and Elementary Floating Point Operations: {{Technical Report}} \#1690},
  author = {McAdams, Aleka and Selle, Andrew and Tamstorf, Rasmus and Teran, Joseph and Sifakis, Eftychios},
  year = {2011},
  publisher = {Computer Sciences Department},
  address = {University of Wisconsin Madison}
}

@article{mccannReviewConvolutionalNeural2017,
  title = {A {{Review}} of {{Convolutional Neural Networks}} for {{Inverse Problems}} in {{Imaging}}},
  author = {McCann, Michael T. and Jin, Kyong Hwan and Unser, Michael},
  year = {2017},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {6},
  eprint = {1710.04011},
  pages = {85--95},
  issn = {1053-5888},
  doi = {10.1109/MSP.2017.2739299},
  urldate = {2019-05-17},
  abstract = {In this survey paper, we review recent uses of convolution neural networks (CNNs) to solve inverse problems in imaging. It has recently become feasible to train deep CNNs on large databases of images, and they have shown outstanding performance on object classification and segmentation tasks. Motivated by these successes, researchers have begun to apply CNNs to the resolution of inverse problems such as denoising, deconvolution, super-resolution, and medical image reconstruction, and they have started to report improvements over state-of-the-art methods, including sparsity-based techniques such as compressed sensing. Here, we review the recent experimental work in these areas, with a focus on the critical design decisions: Where does the training data come from? What is the architecture of the CNN? and How is the learning problem formulated and solved? We also bring together a few key theoretical papers that offer perspective on why CNNs are appropriate for inverse problems and point to some next steps in the field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing}
}

@techreport{mcdermottReactivePlanLanguage1991,
  type = {Research {{Report}}},
  title = {A Reactive Plan Language},
  author = {McDermott, D.},
  year = {1991},
  number = {YALEU/DCS/RR-864},
  address = {Yale University},
  institution = {Yale University},
  urldate = {2023-02-14},
  abstract = {1 Overview RPL (Reactive Plan Language) 1 belongs to the family of notations for writing reactive plans for agents RAP notation. Many of Firby's concepts have been carried over, but there are some diierences: (1) The syntax is more {\textbackslash}recursive," more in the style of Lisp (2) More high-level concepts (interrupts, monitors) have been made into explicit constructs. (3) The interpreter does not attempt to maintain a world model that tracks the situation outside the robot. RPL is being applied to high-level robot planning, and to representing advisory plans for emergency situations (McDermott et al. 1991). A RPL plan looks like a Lisp program. (Indeed, I will use the terms {\textbackslash}plan" and {\textbackslash}program" interchangeably .) A plan consists of a set of procedure calls, of the form (f \{args\{), glued together with syntactic constructs like (SEQ \{steps\{), (IF condition act true act false), and the like. Variable bindings in RPL are lexically scoped. But Lisp global variables can be accessed, too. There are special facilities for binding variables to the tasks created by the plan itself. Any Lisp procedure can be called from a RPL plan. In addition, RPL procedures can deened using (DEF-INTERP-PROC name (\{formals\{) \{body\{) There are no local procedures (i.e., no LABELS). RPL looks so much like Lisp that many Lisp programs are valid RPL plans. However, the intent is not to encourage users to think of plans as typical computer programs, whose operation hinges on the maintenance of complex datastructures. RPL plans are intended to spell out how the behavior of an agent is to be driven by events around it, and to do so in a notation so transparent that a planner can reason about how well the plans will work. A key mechanism to achieve these aims is the idea of a uent, or time-varying quantity. Of course, all program variables are time-varying quantities, but in plans we want behavior to be governed by the temporal changes. For example, in RPL we can say (FILTER c e) to mean, {\textbackslash}Execute e while the uent 1 Thanks to Mark Drummond and Sean Engelson for comments on previous drafts of this paper.\vphantom{\}\}\}\}\}\}\}\}}}
}

@article{Meagher.1982,
  title = {Geometric Modeling Using Octree Encoding},
  author = {Meagher, Donald},
  year = {1982},
  journal = {Computer Graphics and Image Processing},
  volume = {19},
  number = {2},
  pages = {129--147},
  issn = {0146664X},
  doi = {10.1016/0146-664X(82)90104-6},
  pagination = {page}
}

@article{meesAdversarialSkillNetworks2020,
  title = {Adversarial {{Skill Networks}}: {{Unsupervised Robot Skill Learning}} from {{Video}}},
  shorttitle = {Adversarial {{Skill Networks}}},
  author = {Mees, Oier and Merklinger, Markus and Kalweit, Gabriel and Burgard, Wolfram},
  year = {2020},
  month = feb,
  journal = {ICRA},
  eprint = {1910.09430},
  urldate = {2020-06-30},
  abstract = {Key challenges for the deployment of reinforcement learning (RL) agents in the real world are the discovery, representation and reuse of skills in the absence of a reward function. To this end, we propose a novel approach to learn a task-agnostic skill embedding space from unlabeled multi-view videos. Our method learns a general skill embedding independently from the task context by using an adversarial loss. We combine a metric learning loss, which utilizes temporal video coherence to learn a state representation, with an entropy regularized adversarial skill-transfer loss. The metric learning loss learns a disentangled representation by attracting simultaneous viewpoints of the same observations and repelling visually similar frames from temporal neighbors. The adversarial skill-transfer loss enhances re-usability of learned skill embeddings over multiple task domains. We show that the learned embedding enables training of continuous control policies to solve novel tasks that require the interpolation of previously seen skills. Our extensive evaluation with both simulation and real world data demonstrates the effectiveness of our method in learning transferable skills from unlabeled interaction videos and composing them for new tasks. Code, pretrained models and dataset are available at http://robotskills.cs.uni-freiburg.de},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics}
}

@misc{meierDifferentiableLearnableRobot2022,
  title = {Differentiable and {{Learnable Robot Models}}},
  author = {Meier, Franziska and Wang, Austin and Sutanto, Giovanni and Lin, Yixin and Shah, Paarth},
  year = {2022},
  month = feb,
  number = {arXiv:2202.11217},
  eprint = {2202.11217},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-31},
  abstract = {Building differentiable simulations of physical processes has recently received an increasing amount of attention. Specifically, some efforts develop differentiable robotic physics engines motivated by the computational benefits of merging rigid body simulations with modern differentiable machine learning libraries. Here, we present a library that focuses on the ability to combine data driven methods with analytical rigid body computations. More concretely, our library {\textbackslash}emph\{Differentiable Robot Models\} implements both {\textbackslash}emph\{differentiable\} and {\textbackslash}emph\{learnable\} models of the kinematics and dynamics of robots in Pytorch. The source-code is available at {\textbackslash}url\{https://github.com/facebookresearch/differentiable-robot-model\}},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\FRYS8XSS\2202.html}
}

@incollection{meloun8NonlinearRegression2011,
  title = {8 - {{Nonlinear Regression Models}}},
  booktitle = {Statistical {{Data Analysis}}},
  author = {Meloun, Milan and Militk{\'y}, Ji{\v r}{\'i}},
  editor = {Militk{\'y}, Ji{\v r}{\'i}},
  year = {2011},
  month = jan,
  pages = {667--762},
  publisher = {Woodhead Publishing India},
  doi = {10.1533/9780857097200.667},
  urldate = {2024-05-21},
  isbn = {978-0-85709-109-3},
  file = {C:\Users\benja\Zotero\storage\EWXCH8LE\B978085709109350008X.html}
}

@article{memmertComplexProblemSolving2022,
  title = {Complex {{Problem Solving}} through {{Human-AI Collaboration}}: {{Literature Review}} on {{Research Contexts}}},
  shorttitle = {Complex {{Problem Solving}} through {{Human-AI Collaboration}}},
  author = {Memmert, Lucas and Bittner, Eva},
  year = {2022},
  month = jan,
  journal = {Hawaii International Conference on System Sciences 2022 (HICSS-55)},
  file = {C:\Users\benja\Zotero\storage\AHJZBG87\3.html}
}

@inproceedings{mendoncaGuidedMetaPolicySearch2019,
  title = {Guided {{Meta-Policy Search}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mendonca, Russell and Gupta, Abhishek and Kralev, Rosen and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d'{\null} {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {9656--9667},
  urldate = {2020-07-01},
  file = {C:\Users\benja\Zotero\storage\N2D3X7VE\9160-guided-meta-policy-search.html}
}

@misc{MenschRoboterKollaborationVorteileFuer2020,
  title = {{Mensch-Roboter-Kollaboration: Vorteile f{\"u}r die manuelle Fertigung}},
  shorttitle = {{Mensch-Roboter-Kollaboration}},
  year = {2020},
  month = sep,
  journal = {Produktion Online},
  urldate = {2020-10-07},
  abstract = {Entdecken Sie die Vorteile von Mensch-Roboter-Kollaboration in der manuellen Montage: etwa die effiziente Fertigung von Kleinstserien.},
  langid = {ngerman},
  file = {C:\Users\benja\Zotero\storage\2CKKBB7C\mensch-roboter-kollaboration-vorteile-fuer-die-manuelle-fertigung-295.html}
}

@inproceedings{mericliInteractiveApproachSituated2014,
  title = {An Interactive Approach for Situated Task Specification through Verbal Instructions},
  booktitle = {Proceedings of the 2014 International Conference on {{Autonomous}} Agents and Multi-Agent Systems},
  author = {Mericli, Cetin and Klee, Steven D. and Paparian, Jack and Veloso, Manuela},
  year = {2014},
  month = may,
  series = {{{AAMAS}} '14},
  pages = {1069--1076},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  address = {Richland, SC},
  urldate = {2024-04-24},
  abstract = {The ability to specify a task without having to write special software is an important and prominent feature for a mobile service robot deployed in a crowded office environment, working around and interacting with people. In this paper, we contribute an interactive approach for enabling the users to instruct tasks to a mobile service robot through verbal commands.The input is given as typed or spoken instructions, which are then mapped to the available sensing and actuation primitives on the robot. The main contributions of this work are the addition of conditionals on sensory information that the specified actions to be executed in a closed-loop manner, and a correction mode that allows an existing task to be modified or corrected at a later time by providing a replacement action during the test execution.We describe all the components of our approach along with the implementation details and illustrative examples in depth. We also discuss the extensibility of the presented approach, and point out potential future extensions.},
  isbn = {978-1-4503-2738-1},
  keywords = {human-robot interaction,robot task specification}
}

@article{mertenChirurgiePersonalmangelProgrammiert2005,
  title = {{Chirurgie: Personalmangel programmiert}},
  shorttitle = {{Chirurgie}},
  author = {Merten, Martina},
  year = {2005},
  month = jan,
  journal = {Deutsches {\"A}rzteblatt},
  urldate = {2024-04-12},
  abstract = {Arbeitszeiten, Verg{\"u}tung und Verwaltungsaufwand machen Fach unattraktiv. Der Deutschen Gesellschaft f{\"u}r Chirurgie (DGCH) zufolge steht Deutschland ein Chirurgenmangel bevor: ,,Angehende {\"A}rzte bevorzugen zunehmend F{\"a}cher, die einen kontrollierbaren...},
  langid = {ngerman},
  file = {C:\Users\benja\Zotero\storage\TT99IZL8\Chirurgie-Personalmangel-programmiert.html}
}

@article{messnerConvolutionalNeuralNetwork2020,
  title = {Convolutional {{Neural Network Surrogate Models}} for the {{Mechanical Properties}} of {{Periodic Structures}}},
  author = {Messner, M.},
  year = {2020},
  journal = {Journal of Mechanical Design},
  volume = {142},
  number = {2},
  doi = {10.1115/1.4045040},
  abstract = {Improvements will be required to develop robust, efficient neural network-based surrogate models for calculating effective mechanical properties of a periodic composites and several directions for future research are highlighted here.   This work describes neural network surrogate models for calculating the effective mechanical properties of a periodic composites. The models achieve good accuracy even when only provided with training data sampling a small portion of the design space. As an example, the surrogate models are applied to solving the inverse design problem of finding structures with optimal mechanical properties. The surrogate models are sufficiently accurate to recover optimal solutions in general agreement with established topology optimization methods. However, improvements will be required to develop robust, efficient neural network-based surrogate models and several directions for future research are highlighted here.}
}

@article{Metaxas.1993,
  title = {Shape and Nonrigid Motion Estimation through Physics-Based Synthesis},
  author = {Metaxas, D. and Terzopoulos, D.},
  year = {1993},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {15},
  number = {6},
  pages = {580--591},
  issn = {01628828},
  doi = {10.1109/34.216727},
  pagination = {page}
}

@article{Meteyard2012,
  title = {Coming of Age: {{A}} Review of Embodiment and the Neuroscience of Semantics},
  author = {Meteyard, Lotte and Cuadrado, Sara Rodriguez and Bahrami, Bahador and Vigliocco, Gabriella},
  year = {2012},
  journal = {cortex},
  volume = {48},
  number = {7},
  pages = {788--804},
  publisher = {Elsevier}
}

@article{mettesHypersphericalPrototypeNetworks2019,
  title = {Hyperspherical {{Prototype Networks}}},
  author = {Mettes, Pascal and {van der Pol}, Elise and Snoek, Cees G. M.},
  year = {2019},
  month = oct,
  journal = {arXiv:1901.10514 [cs, stat]},
  eprint = {1901.10514},
  primaryclass = {cs, stat},
  urldate = {2021-02-07},
  abstract = {This paper introduces hyperspherical prototype networks, which unify classification and regression with prototypes on hyperspherical output spaces. For classification, a common approach is to define prototypes as the mean output vector over training examples per class. Here, we propose to use hyperspheres as output spaces, with class prototypes defined a priori with large margin separation. We position prototypes through data-independent optimization, with an extension to incorporate priors from class semantics. By doing so, we do not require any prototype updating, we can handle any training size, and the output dimensionality is no longer constrained to the number of classes. Furthermore, we generalize to regression, by optimizing outputs as an interpolation between two prototypes on the hypersphere. Since both tasks are now defined by the same loss function, they can be jointly trained for multi-task problems. Experimentally, we show the benefit of hyperspherical prototype networks for classification, regression, and their combination over other prototype methods, softmax cross-entropy, and mean squared error approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\benja\\Zotero\\storage\\VR3ARN46\\Mettes et al. - 2019 - Hyperspherical Prototype Networks.pdf;C\:\\Users\\benja\\Zotero\\storage\\N46W5VVV\\1901.html}
}

@article{metznerHighprecisionAssemblyElectronic2021,
  title = {High-Precision Assembly of Electronic Devices with Lightweight Robots through Sensor-Guided Insertion},
  author = {Metzner, Maximilian and Leurer, Sebastian and Handwerker, Andreas and Karlidag, Engin and Blank, Andreas and Hefner, Florian and Franke, J{\"o}rg},
  year = {2021},
  month = jan,
  journal = {Procedia CIRP},
  series = {8th {{CIRP Conference}} of {{Assembly Technology}} and {{Systems}}},
  volume = {97},
  pages = {337--341},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2020.05.247},
  urldate = {2021-08-06},
  abstract = {Through-hole devices still play an important role in power electronics production. Due to a high number of variants in this field, an automated placement by special machines is often not economically feasible. The resulting manual process is characterized by monotony, ergonomic strain and high labor cost. As a result, a partial automation, using lightweight robots in hybrid assembly system, yields high potential. The lower stiffness and resulting accuracy deficiency of lightweight robots compared to common industrial robots, combined with the precision requirements and inherent fragility of the devices, prohibits the deployment of such systems. In this contribution, a scalable setup and strategies for visual and force-torque guided tactile insertion of such devices is presented and evaluated. A library, based on a system for the offline-programming of such tasks, is used to decrease implementation efforts. A performance evaluation of the system on printed circuit boards with multiple different devices is conducted.},
  langid = {english},
  keywords = {Electronics production,Lightweight robots,Offline programming,Peg in Hole},
  file = {C:\Users\benja\Zotero\storage\8B8VJ9GE\S2212827120314694.html}
}

@book{meyerEmpiricalSoftwareEngineering2012,
  title = {Empirical {{Software Engineering}} and {{Verification}}},
  editor = {Meyer, Bertrand and Nordio, Martin},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {7007},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-25231-0},
  urldate = {2023-06-01},
  isbn = {978-3-642-25230-3 978-3-642-25231-0},
  keywords = {concurrent programming,experimental replicaction,formal specification,operational semantics,software testing}
}

@article{meyerLaserFlowEfficientProbabilistic2020,
  title = {{{LaserFlow}}: {{Efficient}} and {{Probabilistic Object Detection}} and {{Motion Forecasting}}},
  shorttitle = {{{LaserFlow}}},
  author = {Meyer, Gregory P. and Charland, Jake and Pandey, Shreyash and Laddha, Ankit and Gautam, Shivam and {Vallespi-Gonzalez}, Carlos and Wellington, Carl K.},
  year = {2020},
  month = oct,
  journal = {arXiv:2003.05982 [cs]},
  eprint = {2003.05982},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {In this work, we present LaserFlow, an efficient method for 3D object detection and motion forecasting from LiDAR. Unlike the previous work, our approach utilizes the native range view representation of the LiDAR, which enables our method to operate at the full range of the sensor in real-time without voxelization or compression of the data. We propose a new multi-sweep fusion architecture, which extracts and merges temporal features directly from the range images. Furthermore, we propose a novel technique for learning a probability distribution over future trajectories inspired by curriculum learning. We evaluate LaserFlow on two autonomous driving datasets and demonstrate competitive results when compared to the existing state-of-the-art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\ILVH5EQI\2003.html}
}

@inproceedings{meywerkSymbolicFaultInjection2022,
  title = {Symbolic {{Fault Injection}} for {{Plan-based Robotics}}},
  booktitle = {2022 22nd {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Meywerk, Tim and Herdt, Vladimir and Drechsler, Rolf},
  year = {2022},
  month = nov,
  pages = {1710--1715},
  issn = {2642-3901},
  doi = {10.23919/ICCAS55662.2022.10003719},
  urldate = {2024-07-21},
  abstract = {Autonomous robots are being used increasingly in safety-critical environments. Due to their dynamic nature and uncertainty, failures of low-level actions are common. In plan-based robotics, these failures are handled inside the higher-level plans, using a multitude of failure handling strategies. With the increasing complexity of robotic plans, failures may be accidentally left unhandled. This will usually stop the plan entirely. To avoid such a situation, the failure handling should ideally be complete, i.e. there should be no failure that can reach the top-level of the execution. In this paper, we propose to use formal methods, in particular symbolic fault injection to tackle the problem of finding unhandled failures or proving that no such failure exists. We implement symbolic fault injection for the CRAM Planning Language (CPL). We base our work on the worst-case assumption that any low-level action may fail at any time with any of its possible types of failure. Our work builds upon an existing symbolic execution engine for CPL and extends it to reason about CPL's failure handling mechanism. We also present a way to implement the worst-case assumption directly into CPL. Our experimental evaluation suggests that symbolic fault injection is a suitable and scalable method to find unhandled failures in robotic plans.},
  keywords = {Autonomous Robots,Complexity theory,Control systems,Fault Tolerance,Flow graphs,Formal Verification,Manuals,Planning,Runtime,Uncertainty},
  file = {C:\Users\benja\Zotero\storage\63J4ZMAG\10003719.html}
}

@article{MichelePirovano.2012,
  title = {Kinfu -- an Open Source Implementation of {{Kinect Fusion}} + Case Study: Implementing a {{3D}} Scanner with {{PCL}}},
  author = {{Michele Pirovano}},
  year = {2012}
}

@inproceedings{micikeviciusMixedPrecisionTraining2018,
  title = {Mixed {{Precision Training}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  year = {2018},
  month = feb,
  urldate = {2023-03-03},
  abstract = {Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.},
  langid = {english}
}

@misc{micropsiindustriesMicropsiIndustriesTraditional2024,
  type = {Company Website},
  title = {Micropsi {{Industries}} {\textbar} {{From}} Traditional Vision to {{AI}} Guidance},
  author = {{Micropsi Industries}},
  year = {2024},
  journal = {Micropsi Industries},
  urldate = {2024-09-18},
  abstract = {Explore the advantages of AI for robotic vision systems and learn how you can benefit from this new technology. Download our white paper and stay ahead of the competition.},
  howpublished = {https://www.micropsi-industries.com/blog/white-paper-from-traditional-vision-to-ai-guidance},
  file = {C:\Users\benja\Zotero\storage\U59LJGF6\white-paper-from-traditional-vision-to-ai-guidance.html}
}

@misc{microsoftMicrosoftCopilot2023,
  title = {Microsoft {{Copilot}}},
  author = {{Microsoft}},
  year = {2023},
  urldate = {2024-01-05},
  howpublished = {https://copilot.microsoft.com/},
  file = {C:\Users\benja\Zotero\storage\8UG46WQF\copilot.microsoft.com.html}
}

@inproceedings{miglaniSkinLesionClassification2021,
  title = {Skin {{Lesion Classification}}: {{A Transfer Learning Approach Using EfficientNets}}},
  shorttitle = {Skin {{Lesion Classification}}},
  booktitle = {Advanced {{Machine Learning Technologies}} and {{Applications}}},
  author = {Miglani, Vandana and Bhatia, {\relax MPS}},
  editor = {Hassanien, Aboul Ella and Bhatnagar, Roheet and Darwish, Ashraf},
  year = {2021},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {315--324},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-15-3383-9_29},
  abstract = {This paper studies the ability of deep convolutional neural networks (DCNNs) to classify skin lesions belonging to seven different categories. Two pre-trained state-of-the-art architectures for computer vision tasks ResNet-50 and Google's recently proposed, EfficientNet-B0, were fine-tuned for the classification task on the HAM10000 dataset. The dataset comprises 10015 dermatoscopic images belonging to seven classes of skin cancer melanocytic nevus, melanoma, benign keratosis, basal cell carcinoma, actinic keratosis, vascular lesions, and dermatofibroma. The aim of the study was to establish how well the EfficientNet family of models (which result in up to 8.4{\texttimes}{\texttimes}{\textbackslash}times parameter reduction and 16{\texttimes}{\texttimes}{\textbackslash}times FLOPS reduction) transfers to the skin classification task in comparison with the ResNet architecture. Overall, it was found that the EfficientNet-B0 model, with fewer parameters, outperformed the ResNet-50 model. EfficientNet-B0 model produced better ROC AUC values for each classification category and also achieved higher macro and micro averaged AUC values for the overall classification, 0.93 and 0.97, respectively (in comparison with, 0.91 and 0.96 of the ResNet-50 model).},
  isbn = {9789811533839},
  langid = {english},
  keywords = {Deep learning,EfficieNtnet,Lesion classification,ResNet,Skin cancer,Transfer learning}
}

@article{mildenhallNeRFRepresentingScenes2021,
  title = {{{NeRF}}: Representing Scenes as Neural Radiance Fields for View Synthesis},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  year = {2021},
  month = dec,
  journal = {Communications of the ACM},
  volume = {65},
  number = {1},
  pages = {99--106},
  issn = {0001-0782},
  doi = {10.1145/3503250},
  urldate = {2024-01-11},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction ({\texttheta}, {$\phi$})) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.}
}

@article{mirzaPhysicallyEmbeddedPlanning2020,
  title = {Physically {{Embedded Planning Problems}}: {{New Challenges}} for {{Reinforcement Learning}}},
  shorttitle = {Physically {{Embedded Planning Problems}}},
  author = {Mirza, Mehdi and Jaegle, Andrew and Hunt, Jonathan J. and Guez, Arthur and Tunyasuvunakool, Saran and Muldal, Alistair and Weber, Th{\'e}ophane and Karkus, Peter and Racani{\`e}re, S{\'e}bastien and Buesing, Lars and Lillicrap, Timothy and Heess, Nicolas},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.05524 [cs]},
  eprint = {2009.05524},
  primaryclass = {cs},
  urldate = {2020-10-11},
  abstract = {Recent work in deep reinforcement learning (RL) has produced algorithms capable of mastering challenging games such as Go, chess, or shogi. In these works the RL agent directly observes the natural state of the game and controls that state directly with its actions. However, when humans play such games, they do not just reason about the moves but also interact with their physical environment. They understand the state of the game by looking at the physical board in front of them and modify it by manipulating pieces using touch and fine-grained motor control. Mastering complicated physical systems with abstract goals is a central challenge for artificial intelligence, but it remains out of reach for existing RL algorithms. To encourage progress towards this goal we introduce a set of physically embedded planning problems and make them publicly available. We embed challenging symbolic tasks (Sokoban, tic-tac-toe, and Go) in a physics engine to produce a set of tasks that require perception, reasoning, and motor control over long time horizons. Although existing RL algorithms can tackle the symbolic versions of these tasks, we find that they struggle to master even the simplest of their physically embedded counterparts. As a first step towards characterizing the space of solution to these tasks, we introduce a strong baseline that uses a pre-trained expert game player to provide hints in the abstract space to an RL agent's policy while training it on the full sensorimotor control task. The resulting agent solves many of the tasks, underlining the need for methods that bridge the gap between abstract planning and embodied control.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\8W2MI8WW\2009.html}
}

@inproceedings{misraLearningAskingQuestions2018,
  title = {Learning by {{Asking Questions}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Misra, Ishan and Girshick, Ross and Fergus, Rob and Hebert, Martial and Gupta, Abhinav and {van der Maaten}, Laurens},
  year = {2018},
  pages = {11--20},
  urldate = {2024-01-10}
}

@article{miyatoVirtualAdversarialTraining2018,
  title = {Virtual {{Adversarial Training}}: {{A Regularization Method}} for {{Supervised}} and {{Semi-Supervised Learning}}},
  shorttitle = {Virtual {{Adversarial Training}}},
  author = {Miyato, T. and Maeda, S. and Ishii, S. and Koyama, M.},
  year = {2018},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2018.2858821},
  abstract = {We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the conditional label distribution given input. Virtual adversarial loss is defined as the robustness of the conditional label distribution around each input data point against local perturbation. Unlike adversarial training, our method defines the adversarial direction without label information and is hence applicable to semi-supervised learning. Because the directions in which we smooth the model are only "virtually" adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward- and back-propagations. In our experiments, we applied VAT to supervised and semi-supervised learning tasks on multiple benchmark datasets. With a simple enhancement of the algorithm based on the entropy minimization principle, our VAT achieves state-of-the-art performance for semi-supervised learning tasks on SVHN and CIFAR-10.},
  keywords = {Artificial neural networks,Computational modeling,Data models,Perturbation methods,Robustness,Semisupervised learning,Training},
  file = {C:\Users\benja\Zotero\storage\3M5Q2D7H\8417973.html}
}

@article{mnihAsynchronousMethodsDeep2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  journal = {arXiv:1602.01783 [cs]},
  eprint = {1602.01783},
  primaryclass = {cs},
  urldate = {2021-04-30},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\benja\\Zotero\\storage\\EETVW362\\Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf;C\:\\Users\\benja\\Zotero\\storage\\QIT4Q68E\\1602.html}
}

@inproceedings{mnihAsynchronousMethodsDeep2016a,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  pages = {1928--1937},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2020-09-18},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present as...},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\KKAX9MSY\mniha16.html}
}

@book{mockusBayesianApproachGlobal1989,
  title = {Bayesian {{Approach}} to {{Global Optimization}}: {{Theory}} and {{Applications}}},
  shorttitle = {Bayesian {{Approach}} to {{Global Optimization}}},
  author = {Mockus, Jonas},
  editor = {Hazewinkel, M.},
  year = {1989},
  series = {Mathematics and {{Its Applications}}},
  volume = {37},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-009-0909-0},
  urldate = {2024-05-23},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-94-010-6898-7 978-94-009-0909-0},
  langid = {english},
  keywords = {global optimization,optimization}
}

@incollection{mockusGlobalOptimizationBayesian1989,
  title = {Global {{Optimization}} and the {{Bayesian Approach}}},
  booktitle = {Bayesian {{Approach}} to {{Global Optimization}}: {{Theory}} and {{Applications}}},
  author = {Mockus, Jonas},
  editor = {Mockus, Jonas},
  year = {1989},
  pages = {1--3},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-009-0909-0_1},
  urldate = {2024-05-23},
  abstract = {Any decision problem (with an objective function f to be minimized or maximized) may be classified as a global optimization problem, if there is no additional information indicating that there is only one minimum (or maximum). This definition also includes the case of discrete optimization, when the (quantifiable) decision variables must assume discrete values. In this book we shall consider only the case of continuous variables because for the purpose of developing solutions the discrete optimization problems are better regarded separately.},
  isbn = {978-94-009-0909-0},
  langid = {english}
}

@article{mohanEfficientPSEfficientPanoptic2021,
  title = {{{EfficientPS}}: {{Efficient Panoptic Segmentation}}},
  shorttitle = {{{EfficientPS}}},
  author = {Mohan, Rohit and Valada, Abhinav},
  year = {2021},
  month = may,
  journal = {International Journal of Computer Vision},
  volume = {129},
  number = {5},
  pages = {1551--1579},
  issn = {1573-1405},
  doi = {10.1007/s11263-021-01445-z},
  urldate = {2021-10-25},
  abstract = {Understanding the scene in which an autonomous robot operates is critical for its competent functioning. Such scene comprehension necessitates recognizing instances of traffic participants along with general scene semantics which can be effectively addressed by the panoptic segmentation task. In this paper, we introduce the Efficient Panoptic Segmentation (EfficientPS) architecture that consists of a shared backbone which efficiently encodes and fuses semantically rich multi-scale features. We incorporate a new semantic head that aggregates fine and contextual features coherently and a new variant of Mask R-CNN as the instance head. We also propose a novel panoptic fusion module that congruously integrates the output logits from both the heads of our EfficientPS architecture to yield the final panoptic segmentation output. Additionally, we introduce the KITTI panoptic segmentation dataset that contains panoptic annotations for the popularly challenging KITTI benchmark. Extensive evaluations on Cityscapes, KITTI, Mapillary Vistas and Indian Driving Dataset demonstrate that our proposed architecture consistently sets the new state-of-the-art on all these four benchmarks while being the most efficient and fast panoptic segmentation architecture to date.},
  langid = {english}
}

@article{mohrMLPlanAutomatedMachine2018,
  title = {{{ML-Plan}}: {{Automated}} Machine Learning via Hierarchical Planning},
  shorttitle = {{{ML-Plan}}},
  author = {Mohr, Felix and Wever, Marcel and H{\"u}llermeier, Eyke},
  year = {2018},
  month = sep,
  journal = {Machine Learning},
  volume = {107},
  number = {8-10},
  pages = {1495--1515},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-018-5735-z},
  urldate = {2019-05-20},
  abstract = {Automated machine learning (AutoML) seeks to automatically select, compose, and parametrize machine learning algorithms, so as to achieve optimal performance on a given task (dataset). Although current approaches to AutoML have already produced impressive results, the field is still far from mature, and new techniques are still being developed. In this paper, we present ML-Plan, a new approach to AutoML based on hierarchical planning. To highlight the potential of this approach, we compare ML-Plan to the state-of-the-art frameworks Auto-WEKA, auto-sklearn, and TPOT. In an extensive series of experiments, we show that ML-Plan is highly competitive and often outperforms existing approaches.},
  langid = {english}
}

@inproceedings{mohseni-kabirCollaborativeLearningHierarchical2014,
  title = {Collaborative {{Learning}} of {{Hierarchical Task Networks}} from {{Demonstration}} and {{Instruction}}},
  booktitle = {Artificial {{Intelligence}} for {{Human-Robot Interaction}}},
  author = {{Mohseni-Kabir}, Anahita and Chernova, Sonia and Rich, Charles},
  year = {2014},
  pages = {115--117},
  publisher = {Association for the Advancement of Artificial Intelligence},
  abstract = {In this work, we focus on advancing the state of the art in intelligent agents that can learn complex procedural tasks from humans. Our main innovation is to view the interaction between the human and the robot as a mixedinitiative collaboration. Our contribution is to integrate hierarchical task networks and collaborative discourse theory into the learning from demonstration paradigm to enable robots to learn complex tasks in collaboration with the human teacher.},
  langid = {english}
}

@article{mohsinPathPlanningForce2019,
  title = {Path {{Planning}} under {{Force Control}} in {{Robotic Polishing}} of the {{Complex Curved Surfaces}}},
  author = {Mohsin, Imran and He, Kai and Li, Zheng and Du, Ruxu},
  year = {2019},
  month = jan,
  journal = {Applied Sciences},
  volume = {9},
  number = {24},
  pages = {5489},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/app9245489},
  urldate = {2021-02-25},
  abstract = {Surface polishing is required in many manufacturing sectors. Currently, it demands a large amount of manual work, which is time-consuming, error-prone, and costly. Additionally, it creates hazards for the workers as it may result in many deadly respiratory diseases. Robotic polishing is the solution to these problems. It can improve productivity, eliminate the defects, and provides consistent product quality. In this paper, an effective approach is presented for the robotic polishing of the complex curved surfaces. The key part of the presented method is the tool path planning with controlled force and polishing parameters optimization evaluated using design of experiments (DOE). The tool path planning is aimed at improving the surface quality and the contact area per path. The constraints of joint limits and productivity are also considered. Moreover, its jerk avoidance strategy allows the robot to move swiftly while ensuring a smooth trajectory. The presented method is verified for the polishing of an eyeglass frame. A considerable improvement of 90\% on the average roughness is achieved with the maximum acceptable roughness set at 0.02 \&micro;m. The polishing operation takes just 79 secs and the average glossiness of 76 G is achieved on the final product along with the successful elimination of scratches on the surface.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {force control,glossiness,path planning,robotic polishing,surface roughness}
}

@article{Moll.2006,
  title = {Path Planning for Deformable Linear Objects},
  author = {Moll, M. and Kavraki, L. E.},
  year = {2006},
  journal = {IEEE Transactions on Robotics},
  volume = {22},
  number = {4},
  pages = {625--636},
  issn = {1552-3098},
  doi = {10.1109/TRO.2006.878933},
  pagination = {page}
}

@misc{molschlDifferentiableForwardKinematics2023,
  title = {Differentiable {{Forward Kinematics}} for {{TensorFlow}} 2},
  author = {M{\"o}lschl, Lukas and Hollenstein, Jakob J. and Piater, Justus},
  year = {2023},
  month = mar,
  number = {arXiv:2301.09954},
  eprint = {2301.09954},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.09954},
  urldate = {2024-03-31},
  abstract = {Robotic systems are often complex and depend on the integration of a large number of software components. One important component in robotic systems provides the calculation of forward kinematics, which is required by both motion-planning and perception related components. End-to-end learning systems based on deep learning require passing gradients across component boundaries.Typical software implementations of forward kinematics are not differentiable, and thus prevent the construction of gradient-based, end-to-end learning systems. In this paper we present a library compatible with ROS-URDF that computes forward kinematics while simultaneously giving access to the gradients w.r.t. joint configurations and model parameters, allowing gradient-based learning and model identification. Our Python library is based on Tensorflow{\textasciitilde}2 and is auto-differentiable. It supports calculating a large number of kinematic configurations on the GPU in parallel, yielding a considerable performance improvement compared to sequential CPU-based calculation. https://github.com/lumoe/dlkinematics.git},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics,Computer Science - Software Engineering},
  file = {C:\Users\benja\Zotero\storage\W93V2N4N\2301.html}
}

@book{monarchHumanintheLoopMachineLearning2021,
  title = {Human-in-the-{{Loop Machine Learning}}: {{Active}} Learning and Annotation for Human-Centered {{AI}}},
  shorttitle = {Human-in-the-{{Loop Machine Learning}}},
  author = {Monarch, Robert},
  year = {2021},
  month = jul,
  publisher = {Manning},
  address = {Sherlter Island, NY},
  abstract = {Human-in-the-Loop Machine Learning lays out methods for humans and machines to work together effectively.Summary Most machine learning systems that are deployed in the world today learn from human feedback. However, most machine learning courses focus almost exclusively on the algorithms, not the human-computer interaction part of the systems. This can leave a big knowledge gap for data scientists working in real-world machine learning, where data scientists spend more time on data management than on building algorithms. Human-in-the-Loop Machine Learning is a practical guide to optimizing the entire machine learning process, including techniques for annotation, active learning, transfer learning, and using machine learning to optimize every step of the process.  Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.  About the technology Machine learning applications perform better with human feedback. Keeping the right people in the loop improves the accuracy of models, reduces errors in data, lowers costs, and helps you ship models faster.  About the book Human-in-the-Loop Machine Learning lays out methods for humans and machines to work together effectively. You'll find best practices on selecting sample data for human feedback, quality control for human annotations, and designing annotation interfaces. You'll learn to create training data for labeling, object detection, and semantic segmentation, sequence labeling, and more. The book starts with the basics and progresses to advanced techniques like transfer learning and self-supervision within annotation workflows.  What's inside  Identifying the right training and evaluation data Finding and managing people to annotate data Selecting annotation quality control strategies Designing interfaces to improve accuracy and efficiency About the author Robert (Munro) Monarch is a data scientist and engineer who has built machine learning data for companies such as Apple, Amazon, Google, and IBM. He holds a PhD from Stanford. Robert holds a PhD from Stanford focused on Human-in-the-Loop machine learning for healthcare and disaster response, and is a disaster response professional in addition to being a machine learning professional. A worked example throughout this text is classifying disaster-related messages from real disasters that Robert has helped respond to in the past. Table of Contents PART 1 - FIRST STEPS 1 Introduction to human-in-the-loop machine learning 2 Getting started with human-in-the-loop machine learning PART 2 - ACTIVE LEARNING 3 Uncertainty sampling 4 Diversity sampling 5 Advanced active learning 6 Applying active learning to different machine learning tasks PART 3 - ANNOTATION 7 Working with the people annotating your data 8 Quality control for data annotation 9 Advanced data annotation and augmentation 10 Annotation quality for different machine learning tasks PART 4 - HUMAN--COMPUTER INTERACTION FOR MACHINE LEARNING 11 Interfaces for data annotation 12 Human-in-the-loop machine learning products},
  isbn = {978-1-61729-674-1},
  langid = {english}
}

@misc{mongodbMongoDB2023,
  title = {{{MongoDB}}},
  author = {{MongoDB}},
  year = {2023},
  howpublished = {MongoDB, Inc.}
}

@article{montgomeryResetFreeGuidedPolicy2016,
  title = {Reset-{{Free Guided Policy Search}}: {{Efficient Deep Reinforcement Learning}} with {{Stochastic Initial States}}},
  shorttitle = {Reset-{{Free Guided Policy Search}}},
  author = {Montgomery, William and Ajay, Anurag and Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  year = {2016},
  month = oct,
  journal = {arXiv:1610.01112 [cs]},
  eprint = {1610.01112},
  primaryclass = {cs},
  urldate = {2019-07-26},
  abstract = {Autonomous learning of robotic skills can allow general-purpose robots to learn wide behavioral repertoires without requiring extensive manual engineering. However, robotic skill learning methods typically make one of several trade-offs to enable practical real-world learning, such as requiring manually designed policy or value function representations, initialization from human-provided demonstrations, instrumentation of the training environment, or extremely long training times. In this paper, we propose a new reinforcement learning algorithm for learning manipulation skills that can train general-purpose neural network policies with minimal human engineering, while still allowing for fast, efficient learning in stochastic environments. Our approach builds on the guided policy search (GPS) algorithm, which transforms the reinforcement learning problem into supervised learning from a computational teacher (without human demonstrations). In contrast to prior GPS methods, which require a consistent set of initial states to which the system must be reset after each episode, our approach can handle randomized initial states, allowing it to be used in environments where deterministic resets are impossible. We compare our method to existing policy search techniques in simulation, showing that it can train high-dimensional neural network policies with the same sample efficiency as prior GPS methods, and present real-world results on a PR2 robotic manipulator.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics}
}

@inproceedings{moralesDSLAIEngineering2022,
  title = {Towards a~{{DSL}} for~{{AI Engineering Process Modeling}}},
  booktitle = {Product-{{Focused Software Process Improvement}}},
  author = {Morales, Sergio and Claris{\'o}, Robert and Cabot, Jordi},
  editor = {Taibi, Davide and Kuhrmann, Marco and Mikkonen, Tommi and Kl{\"u}nder, Jil and Abrahamsson, Pekka},
  year = {2022},
  pages = {53--60},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-21388-5_4},
  abstract = {Many modern software products embed AI components. As a result, their development requires multidisciplinary teams with diverse skill sets. Diversity may lead to communication issues or misapplication of best practices. Process models, which prescribe how software should be developed within an organization, can alleviate this problem. In this paper, we introduce a domain-specific language for modeling AI engineering processes. The DSL concepts stem from our analysis of scientific and gray literature that describes how teams are developing AI-based software. This DSL contributes a structured framework and a common ground for designing, enacting and automating AI engineering processes.},
  isbn = {978-3-031-21388-5},
  langid = {english},
  keywords = {AI engineering,Domain-specific language,Process modeling},
  file = {C:\Users\benja\Zotero\storage\Z8VN9JHH\Morales et al. - 2022 - Towards aDSL forAI Engineering Process Modeling.pdf}
}

@inproceedings{moraPODSPolicyOptimization2021,
  title = {{{PODS}}: {{Policy Optimization}} via {{Differentiable Simulation}}},
  shorttitle = {{{PODS}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Mora, Miguel Angel Zamora and Peychev, Momchil and Ha, Sehoon and Vechev, Martin and Coros, Stelian},
  year = {2021},
  month = jul,
  pages = {7805--7817},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-09-07},
  abstract = {Current reinforcement learning (RL) methods use simulation models as simple black-box oracles. In this paper, with the goal of improving the performance exhibited by RL algorithms, we explore a systematic way of leveraging the additional information provided by an emerging class of differentiable simulators. Building on concepts established by Deterministic Policy Gradients (DPG) methods, the neural network policies learned with our approach represent deterministic actions. In a departure from standard methodologies, however, learning these policies does not hinge on approximations of the value function that must be learned concurrently in an actor-critic fashion. Instead, we exploit differentiable simulators to directly compute the analytic gradient of a policy's value function with respect to the actions it outputs. This, in turn, allows us to efficiently perform locally optimal policy improvement iterations. Compared against other state-of-the-art RL methods, we show that with minimal hyper-parameter tuning our approach consistently leads to better asymptotic behavior across a set of payload manipulation tasks that demand a high degree of accuracy and precision.},
  langid = {english},
  file = {C\:\\Users\\benja\\Zotero\\storage\\794JPNV4\\Mora et al. - 2021 - PODS Policy Optimization via Differentiable Simulation.pdf;C\:\\Users\\benja\\Zotero\\storage\\ZUJZIJBN\\Mora et al. - 2021 - PODS Policy Optimization via Differentiable Simulation.pdf}
}

@book{moreckiBasicsRoboticsTheory1999,
  title = {Basics of {{Robotics}}: {{Theory}} and {{Components}} of {{Manipulators}} and {{Robots}}},
  shorttitle = {Basics of Robotics},
  editor = {Morecki, Adam and Knapczyk, Jozef},
  year = {1999},
  series = {{{CISM}} Courses and Lectures / {{International Centre}} for {{Mechanical Sciences}}},
  number = {402},
  publisher = {Springer},
  address = {Wien},
  isbn = {978-3-211-83150-2},
  langid = {english},
  annotation = {OCLC: 247354207}
}

@article{morganModelPredictiveActorCritic2021,
  title = {Model {{Predictive Actor-Critic}}: {{Accelerating Robot Skill Acquisition}} with {{Deep Reinforcement Learning}}},
  shorttitle = {Model {{Predictive Actor-Critic}}},
  author = {Morgan, Andrew S. and Nandha, Daljeet and Chalvatzaki, Georgia and D'Eramo, Carlo and Dollar, Aaron M. and Peters, Jan},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.13842 [cs]},
  eprint = {2103.13842},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {Substantial advancements to model-based reinforcement learning algorithms have been impeded by the model-bias induced by the collected data, which generally hurts performance. Meanwhile, their inherent sample efficiency warrants utility for most robot applications, limiting potential damage to the robot and its environment during training. Inspired by information theoretic model predictive control and advances in deep reinforcement learning, we introduce Model Predictive Actor-Critic (MoPAC), a hybrid model-based/model-free method that combines model predictive rollouts with policy optimization as to mitigate model bias. MoPAC leverages optimal trajectories to guide policy learning, but explores via its model-free method, allowing the algorithm to learn more expressive dynamics models. This combination guarantees optimal skill learning up to an approximation error and reduces necessary physical interaction with the environment, making it suitable for real-robot training. We provide extensive results showcasing how our proposed method generally outperforms current state-of-the-art and conclude by evaluating MoPAC for learning on a physical robotic hand performing valve rotation and finger gaiting--a task that requires grasping, manipulation, and then regrasping of an object.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\IDTN8524\2103.html}
}

@inproceedings{Morita.2003,
  title = {Knot Planning from Observation},
  booktitle = {2003 {{IEEE}} International Conference on Robotics and Automation (Cat. {{No}}.{{03CH37422}})},
  author = {Morita, T. and Takamatsu, J. and Ogawara, K. and Kimura, H. and Ikeuchi, K.},
  year = {2003},
  pages = {3887--3892},
  publisher = {IEEE},
  doi = {10.1109/ROBOT.2003.1242193},
  bookpagination = {page},
  isbn = {0-7803-7736-2}
}

@article{morrisonYoureNotBoss2023,
  title = {You're {{Not}} the {{Boss}} of Me, {{Algorithm}}: {{Increased User Control}} and {{Positive Implicit Attitudes Are Related}} to {{Greater Adherence}} to an {{Algorithmic Aid}}},
  shorttitle = {You're {{Not}} the {{Boss}} of Me, {{Algorithm}}},
  author = {Morrison, Ben W. and Kelson, Joshua N. and Morrison, Natalie M. V. and Innes, John Michael and Zelic, Gregory and {Al-Saggaf}, Yeslam and Paul, Manoranjan},
  year = {2023},
  journal = {Interacting with Computers},
  volume = {35},
  number = {3},
  pages = {452--460},
  doi = {10.1093/IWC/IWAD028},
  urldate = {2024-08-08}
}

@inproceedings{morrisRoboticAssemblyConstraints1987,
  title = {Robotic Assembly by Constraints},
  booktitle = {1987 {{IEEE International Conference}} on {{Robotics}} and {{Automation Proceedings}}},
  author = {Morris, G. and Haynes, L.},
  year = {1987},
  month = mar,
  volume = {4},
  pages = {1507--1515},
  doi = {10.1109/ROBOT.1987.1087907},
  abstract = {Off-line programming of robots will become increasingly more important. This paper describes a robot programming system which is based upon the use of geometrical constraints on the degrees of freedom of a component for specifying robotic assembly actions. Two pieces of software have been developed which allow easy definition of these constraints and the order of execution of the constraints.},
  keywords = {Assembly systems,Control systems,Design automation,Educational institutions,Fixtures,Formal languages,Mechanical engineering,NIST,Robot programming,Robotic assembly},
  file = {C:\Users\benja\Zotero\storage\UH5CKCJU\1087907.html}
}

@article{mortensenIntestinalAnastomosis2008,
  title = {Intestinal {{Anastomosis}}},
  author = {Mortensen, Neil and Ashraf, Shazad},
  year = {2008},
  month = jan,
  journal = {ACS surgery: principles and practice, 6th edition, Section 5, Chapter 29: intestinal anastomosis},
  pages = {10--11},
  doi = {10.2310/SURG.2085},
  abstract = {The creation of a join between two bowel ends is an operative procedure that is of central importance in the practice of a general surgeon. Leakage from an intestinal anastomosis can be disastrous, resulting in prolonged hospital stays and increased risk of mortality. To minimize the risk of potential complications, it is important to create a tension-free join with good apposition of the bowel edges in the presence of an excellent blood supply. This review discusses the factors that influence intestinal anastomotic healing, the various technical operations for creating anastomoses, and operative techniques currently used in constructing anastomoses. Tables review the principles of successful intestinal anastomosis, consequences of postoperative dehiscence, factors linked with dehiscence, anastomotic techniques ranked by best blood flow to the healing site, comparison of hand and stapled techniques, leak rates from the Rectal Cancer Trial on Defunctioning Stoma and the Contant and colleagues mechanical bowel obstruction trial, leak and wound infection rates from mechanical bowel obstruction meta-analyses, diseases and systemic factors associated with poor anastomotic healing, lifestyle-associated leakage rates, salvage after anastomotic leakage, standard checks for creation of anastomoses, and steps for left-sided stapled colorectal anastomoses for cancer. Figures show the phases of wound healing, the tissue layers of the jejunum, interrupted and continuous suture techniques, stitches commonly used in fashioning intestinal anastomoses, double-layer end-to-end anastomosis, traction sutures, anatomic relations between the colon and the retroperitoneal organs, single-layer sutured side-to-side enteroenterostomy, Finney strictureplasty, double-layer sutured end-to-side enterocolostomy, double-stapled end-to-end coloanal anastomosis, use of a ``glove'' port in laparoscopic surgery, and perfusion assessment at the time of anastomotic creation. This review contains 14 figures, 13 tables, and 85 references.}
}

@inproceedings{mosbachStabilityFinetuningBERT2020,
  title = {On the {{Stability}} of {{Fine-tuning BERT}}: {{Misconceptions}}, {{Explanations}}, and {{Strong Baselines}}},
  shorttitle = {On the {{Stability}} of {{Fine-tuning BERT}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Mosbach, Marius and Andriushchenko, Maksym and Klakow, Dietrich},
  year = {2020},
  month = sep,
  urldate = {2021-06-21},
  abstract = {Fine-tuning pre-trained transformer-based language models such as BERT has become a common practice dominating leaderboards across various NLP benchmarks. Despite the strong empirical performance...},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\FUCRFZFR\forum.html}
}

@article{mosqueira-reyHumanintheLoopMachineLearning2023,
  title = {Human-in-the-{{Loop Machine Learning}}: {{A State}} of the {{Art}}},
  shorttitle = {Human-in-the-Loop Machine Learning},
  author = {{Mosqueira-Rey}, Eduardo and {Hern{\'a}ndez-Pereira}, Elena and {Alonso-R{\'i}os}, David and {Bobes-Bascar{\'a}n}, Jos{\'e} and {Fern{\'a}ndez-Leal}, {\'A}ngel},
  year = {2023},
  month = apr,
  journal = {Artificial Intelligence Review},
  volume = {56},
  number = {4},
  pages = {3005--3054},
  issn = {1573-7462},
  doi = {10.1007/s10462-022-10246-w},
  urldate = {2024-01-25},
  abstract = {Researchers are defining new types of interactions between humans and machine learning algorithms generically called human-in-the-loop machine learning. Depending on who is in control of the learning process, we can identify: active learning, in which the system remains in control; interactive machine learning, in which there is a closer interaction between users and learning systems; and machine teaching, where human domain experts have control over the learning process. Aside from control, humans can also be involved in the learning process in other ways. In curriculum learning human domain experts try to impose some structure on the examples presented to improve the learning; in explainable AI the focus is on the ability of the model to explain to humans why a given solution was chosen. This collaboration between AI models and humans should not be limited only to the learning process; if we go further, we can see other terms that arise such as Usable and Useful AI. In this paper we review the state of the art of the techniques involved in the new forms of relationship between humans and ML algorithms. Our contribution is not merely listing the different approaches, but to provide definitions clarifying confusing, varied and sometimes contradictory terms; to elucidate and determine the boundaries between the different methods; and to correlate all the techniques searching for the connections and influences between them.},
  langid = {english},
  keywords = {Active learning,Curriculum learning,Explainable AI,Human-in-the-loop machine learning,Interactive machine learning,Machine teaching}
}

@article{mouEndtoEndProgramGeneration2015,
  title = {On {{End-to-End Program Generation}} from {{User Intention}} by {{Deep Neural Networks}}},
  author = {Mou, Lili and Men, Rui and Li, Ge and Zhang, Lu and Jin, Zhi},
  year = {2015},
  journal = {ArXiv},
  abstract = {This paper envisions an end-to-end program generation scenario using recurrent neural networks (RNNs): Users can express their intention in natural language; an RNN then automatically generates corresponding code in a characterby-by-character fashion. This paper envisions an end-to-end program generation scenario using recurrent neural networks (RNNs): Users can express their intention in natural language; an RNN then automatically generates corresponding code in a characterby-by-character fashion. We demonstrate its feasibility through a case study and empirical analysis. To fully make such technique useful in practice, we also point out several cross-disciplinary challenges, including modeling user intention, providing datasets, improving model architectures, etc. Although much long-term research shall be addressed in this new field, we believe end-to-end program generation would become a reality in future decades, and we are looking forward to its practice.}
}

@misc{muheReverseengineeringKUKARobot2010,
  title = {On Reverse-Engineering the {{KUKA Robot Language}}},
  author = {M{\"u}he, Henrik and Angerer, Andreas and Hoffmann, Alwin and Reif, Wolfgang},
  year = {2010},
  month = sep,
  number = {arXiv:1009.5004},
  eprint = {1009.5004},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1009.5004},
  urldate = {2024-04-15},
  abstract = {Most commercial manufacturers of industrial robots require their robots to be programmed in a proprietary language tailored to the domain - a typical domain-specific language (DSL). However, these languages oftentimes suffer from shortcomings such as controller-specific design, limited expressiveness and a lack of extensibility. For that reason, we developed the extensible Robotics API for programming industrial robots on top of a general-purpose language. Although being a very flexible approach to programming industrial robots, a fully-fledged language can be too complex for simple tasks. Additionally, legacy support for code written in the original DSL has to be maintained. For these reasons, we present a lightweight implementation of a typical robotic DSL, the KUKA Robot Language (KRL), on top of our Robotics API. This work deals with the challenges in reverse-engineering the language and mapping its specifics to the Robotics API. We introduce two different approaches of interpreting and executing KRL programs: tree-based and bytecode-based interpretation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\BG9QWZE6\1009.html}
}

@inproceedings{Muja.2009,
  title = {Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration},
  booktitle = {International Conference on Computer Vision Theory and Application {{VISSAPP}}'09)},
  author = {Muja, Marius and Lowe, David G.},
  year = {2009},
  pages = {331--340},
  publisher = {INSTICC Press},
  bookpagination = {page}
}

@article{mukadamContinuousTimeGaussianProcess2018,
  title = {Continuous-{{Time Gaussian Process Motion Planning}} via {{Probabilistic Inference}}},
  author = {Mukadam, Mustafa and Dong, Jing and Yan, Xinyan and Dellaert, Frank and Boots, Byron},
  year = {2018},
  month = sep,
  journal = {The International Journal of Robotics Research},
  volume = {37},
  number = {11},
  eprint = {1707.07383},
  pages = {1319--1340},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364918790369},
  urldate = {2020-03-17},
  abstract = {We introduce a novel formulation of motion planning, for continuous-time trajectories, as probabilistic inference. We first show how smooth continuous-time trajectories can be represented by a small number of states using sparse Gaussian process (GP) models. We next develop an efficient gradient-based optimization algorithm that exploits this sparsity and Gaussian process interpolation. We call this algorithm the Gaussian Process Motion Planner (GPMP). We then detail how motion planning problems can be formulated as probabilistic inference on a factor graph. This forms the basis for GPMP2, a very efficient algorithm that combines GP representations of trajectories with fast, structure-exploiting inference via numerical optimization. Finally, we extend GPMP2 to an incremental algorithm, iGPMP2, that can efficiently replan when conditions change. We benchmark our algorithms against several samplingbased and trajectory optimization-based motion planning algorithms on planning problems in multiple environments. Our evaluation reveals that GPMP2 is several times faster than previous algorithms while retaining robustness. We also benchmark iGPMP2 on replanning problems, and show that it can find successful solutions in a fraction of the time required by GPMP2 to replan from scratch.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics}
}

@inproceedings{mukadamGaussianProcessMotion2016,
  title = {Gaussian {{Process Motion}} Planning},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Mukadam, Mustafa and Yan, Xinyan and Boots, Byron},
  year = {2016},
  month = may,
  pages = {9--15},
  doi = {10.1109/ICRA.2016.7487091},
  abstract = {Motion planning is a fundamental tool in robotics, used to generate collision-free, smooth, trajectories, while satisfying task-dependent constraints. In this paper, we present a novel approach to motion planning using Gaussian processes. In contrast to most existing trajectory optimization algorithms, which rely on a discrete state parameterization in practice, we represent the continuous-time trajectory as a sample from a Gaussian process (GP) generated by a linear time-varying stochastic differential equation. We then provide a gradient-based optimization technique that optimizes continuous-time trajectories with respect to a cost functional. By exploiting GP interpolation, we develop the Gaussian Process Motion Planner (GPMP), that finds optimal trajectories parameterized by a small number of states. We benchmark our algorithm against recent trajectory optimization algorithms by solving 7-DOF robotic arm planning problems in simulation and validate our approach on a real 7-DOF WAM arm.},
  keywords = {Gaussian processes,Interpolation,Planning,Robots,Trajectory optimization},
  file = {C:\Users\benja\Zotero\storage\PBWTBSPM\7487091.html}
}

@article{mundhenkEfficientSaliencyMaps2020,
  title = {Efficient {{Saliency Maps}} for {{Explainable AI}}},
  author = {Mundhenk, T. Nathan and Chen, Barry Y. and Friedland, Gerald},
  year = {2020},
  month = mar,
  journal = {arXiv:1911.11293 [cs]},
  eprint = {1911.11293},
  primaryclass = {cs},
  urldate = {2020-10-07},
  abstract = {We describe an explainable AI saliency map method for use with deep convolutional neural networks (CNN) that is much more efficient than popular fine-resolution gradient methods. It is also quantitatively similar or better in accuracy. Our technique works by measuring information at the end of each network scale which is then combined into a single saliency map. We describe how saliency measures can be made more efficient by exploiting Saliency Map Order Equivalence. We visualize individual scale/layer contributions by using a Layer Ordered Visualization of Information. This provides an interesting comparison of scale information contributions within the network not provided by other saliency map methods. Using our method instead of Guided Backprop, coarse-resolution class activation methods such as Grad-CAM and Grad-CAM++ seem to yield demonstrably superior results without sacrificing speed. This will make fine-resolution saliency methods feasible on resource limited platforms such as robots, cell phones, low-cost industrial devices, astronomy and satellite imagery.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Performance,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\L6XCJEIN\1911.html}
}

@inproceedings{munkhdalaiMetaNetworks2017,
  title = {Meta {{Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Munkhdalai, Tsendsuren and Yu, Hong},
  year = {2017},
  month = jul,
  pages = {2554--2563},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-09-29},
  abstract = {Neural networks have been successfully applied in applications with a large amount of labeled data. However, the task of rapid generalization on new concepts with small training data while preserving performances on previously learned ones still presents a significant challenge to neural network models. In this work, we introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve a near human-level performance and outperform the baseline approaches by up to 6{\textbackslash}\% accuracy. We demonstrate several appealing properties of MetaNet relating to generalization and continual learning.},
  langid = {english},
  file = {C\:\\Users\\benja\\Zotero\\storage\\6VZBNYWC\\Munkhdalai und Yu - 2017 - Meta Networks.pdf;C\:\\Users\\benja\\Zotero\\storage\\XAL7RKK6\\Munkhdalai und Yu - 2017 - Meta Networks.pdf}
}

@inproceedings{muraliNeuralSketchLearning2018,
  title = {Neural {{Sketch Learning}} for {{Conditional Program Generation}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Murali, Vijayaraghavan and Qi, Letao and Chaudhuri, Swarat and Jermaine, Chris},
  year = {2018},
  month = feb,
  urldate = {2022-04-02},
  abstract = {We give a method for generating type-safe programs in a Java-like language, given a small amount of syntactic information about the desired code.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\5LU5E8LU\forum.html}
}

@misc{muraliPyRobotOpensourceRobotics2019,
  title = {{{PyRobot}}: {{An Open-source Robotics Framework}} for {{Research}} and {{Benchmarking}}},
  shorttitle = {{{PyRobot}}},
  author = {Murali, Adithyavairavan and Chen, Tao and Alwala, Kalyan Vasudev and Gandhi, Dhiraj and Pinto, Lerrel and Gupta, Saurabh and Gupta, Abhinav},
  year = {2019},
  month = jun,
  number = {arXiv:1906.08236},
  eprint = {1906.08236},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.08236},
  urldate = {2024-09-17},
  abstract = {This paper introduces PyRobot, an open-source robotics framework for research and benchmarking. PyRobot is a light-weight, high-level interface on top of ROS that provides a consistent set of hardware independent mid-level APIs to control different robots. PyRobot abstracts away details about low-level controllers and inter-process communication, and allows non-robotics researchers (ML, CV researchers) to focus on building high-level AI applications. PyRobot aims to provide a research ecosystem with convenient access to robotics datasets, algorithm implementations and models that can be used to quickly create a state-of-the-art baseline. We believe PyRobot, when paired up with low-cost robot platforms such as LoCoBot, will reduce the entry barrier into robotics, and democratize robotics. PyRobot is open-source, and can be accessed via https://pyrobot.org.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\benja\\Zotero\\storage\\N8ZZ8EQ5\\Murali et al. - 2019 - PyRobot An Open-source Robotics Framework for Research and Benchmarking.pdf;C\:\\Users\\benja\\Zotero\\storage\\844M5EB3\\1906.html}
}

@inproceedings{murthyGradSimDifferentiableSimulation2020,
  title = {{{gradSim}}: {{Differentiable}} Simulation for System Identification and Visuomotor Control},
  shorttitle = {{{gradSim}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Murthy, J. Krishna and Macklin, Miles and Golemo, Florian and Voleti, Vikram and Petrini, Linda and Weiss, Martin and Considine, Breandan and {Parent-L{\'e}vesque}, J{\'e}r{\^o}me and Xie, Kevin and Erleben, Kenny and Paull, Liam and Shkurti, Florian and Nowrouzezahrai, Derek and Fidler, Sanja},
  year = {2020},
  month = sep,
  urldate = {2022-03-30},
  abstract = {In this paper, we tackle the problem of estimating object physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is...},
  langid = {english}
}

@article{muxfeldtAutomaticErrorRecovery2018,
  title = {{Automatic Error Recovery During Industrial Assembly Operations Based on Human Demonstrations}},
  author = {Muxfeldt, Arne},
  year = {2018},
  month = dec,
  doi = {10.24355/dbbs.084-201812171027-0},
  urldate = {2021-12-20},
  abstract = {Based on a scenario where humans and robots share their workspace, a system for automatically error handling during an automated industrial assembly is presented. If an error occurs, it is first detected and then classified. If it is a previously unknown error, the human closest to the robot will be asked to perform error handling by interacting with the robot. This interaction is recorded so that it can be reapplied if the same error occurs again. If the error is already known, an appropriate error handling is selected and applied without any further human interaction required. Thus, the interaction rate decreases over time and the system learns to handle more and more errors independently. In addition, it is presented how different recorded error handlings can be optimized according to given performance criteria. For this purpose, a suitable input device for performing the error handling is required first. In addition, the Hierarchical Decomposition (HD) is introduced as the abstract representation of an assembly operation. In this case, an assembly is subdivided into different states at multiple hierarchical levels. This is done by a domain export which also defines conditions for state transition. Thus, the HD allows assembly progress monitoring, error detection and classification as well as error prediction. A strategy presentation is introduced to store and reuse demonstrated error handling interactions. One particular feature of this representation is that a strategy is always related to the robot's end-effector pose at that point of time when an error occurs. Thus, a strategy describes the movements which have been performed for error handling. The strategy's invariance against rotation or translation allows significant reduction in the amount of strategies needed to be demonstrated by a human via interaction. Four selection criteria are introduced in order to decide if a strategy matches an error. Thereby, it is possible to make a selection based on one criterion or to perform a multi-criteria optimization using all available information. By introducing a strategy optimization approach, the overall system performance can be improved.  In a subsequent experiment, it is shown that the presented error handling approach can be successfully applied.},
  copyright = {Alle Rechte vorbehalten},
  isbn = {9781047542951},
  langid = {ngerman},
  file = {C:\Users\benja\Zotero\storage\4NIWEJNA\dbbs_mods_00066129.html}
}

@inproceedings{myersGoalRepresentationsInstruction2023,
  title = {Goal {{Representations}} for {{Instruction Following}}: {{A Semi-Supervised Language Interface}} to {{Control}}},
  shorttitle = {Goal {{Representations}} for {{Instruction Following}}},
  booktitle = {7th {{Annual Conference}} on {{Robot Learning}}},
  author = {Myers, Vivek and He, Andre Wang and Fang, Kuan and Walke, Homer Rich and {Hansen-Estruch}, Philippe and Cheng, Ching-An and Jalobeanu, Mihai and Kolobov, Andrey and Dragan, Anca and Levine, Sergey},
  year = {2023},
  month = aug,
  urldate = {2024-04-29},
  abstract = {Our goal is for robots to follow natural language instructions like ``put the towel next to the microwave.'' But getting large amounts of labeled data, i.e. data that contains demonstrations of tasks labeled with the language instruction, is prohibitive. In contrast, obtaining policies that respond to image goals is much easier, because any autonomous trial or demonstration can be labeled in hindsight with its final state as the goal. In this work, we contribute a method that taps into joint image- and goal- conditioned policies with language using only a small amount of language data. Prior work has made progress on this using vision-language models or by jointly training language-goal-conditioned policies, but so far neither method has scaled effectively to real-world robot tasks without significant human annotation. Our method achieves robust performance in the real world by learning an embedding from the labeled data that aligns language not to the goal image, but rather to the desired change between the start and goal images that the instruction corresponds to. We then train a policy on this embedding: the policy benefits from all the unlabeled data, but the aligned embedding provides an *interface* for language to steer the policy. We show instruction following across a variety of manipulation tasks in different scenes, with generalization to language instructions outside of the labeled data.},
  langid = {english}
}

@article{Myronenko.2010,
  title = {Point Set Registration: Coherent Point Drift},
  author = {Myronenko, Andriy and Song, Xubo},
  year = {2010},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {32},
  number = {12},
  pages = {2262--2275},
  issn = {01628828},
  doi = {10.1109/TPAMI.2010.46},
  abstract = {Point set registration is a key component in many computer vision tasks. The goal of point set registration is to assign correspondences between two sets of points and to recover the transformation that maps one point set to the other. Multiple factors, including an unknown nonrigid spatial transformation, large dimensionality of point set, noise, and outliers, make the point set registration a challenging problem. We introduce a probabilistic method, called the Coherent Point Drift (CPD) algorithm, for both rigid and nonrigid point set registration. We consider the alignment of two point sets as a probability density estimation problem. We fit the Gaussian mixture model (GMM) centroids (representing the first point set) to the data (the second point set) by maximizing the likelihood. We force the GMM centroids to move coherently as a group to preserve the topological structure of the point sets. In the rigid case, we impose the coherence constraint by reparameterization of GMM centroid locations with rigid parameters and derive a closed form solution of the maximization step of the EM algorithm in arbitrary dimensions. In the nonrigid case, we impose the coherence constraint by regularizing the displacement field and using the variational calculus to derive the optimal transformation. We also introduce a fast algorithm that reduces the method computation complexity to linear. We test the CPD algorithm for both rigid and nonrigid transformations in the presence of noise, outliers, and missing points, where CPD shows accurate results and outperforms current state-of-the-art methods.},
  pagination = {page}
}

@patent{nadererRobotergestuetztesSchleifverfahrenUnd2016,
  title = {{Robotergest{\"u}tztes Schleifverfahren und Vorrichtung zum robotergest{\"u}tzten Schleifen}},
  author = {Naderer, Ronald},
  year = {2016},
  month = nov,
  number = {DE102014119532B4},
  urldate = {2021-03-01},
  abstract = {Robotergest{\"u}tztes Schleifverfahren, das folgendes umfasst: Ber{\"u}hren einer Oberfl{\"a}che (303) eines Werkst{\"u}cks (301) mit einem rotierenden Schleifwerkzeug (203), wobei entweder das Schleifwerkzeug (203) oder das Werkst{\"u}ck (301) mit einem Tool-Center-Point (TCP) eines Manipulators (100) mechanisch gekoppelt ist; Ansteuern eines auf das Schleifwerkzeug (203) oder das Werkst{\"u}ck (301) wirkenden Aktors (200) zur Erzeugung einer Schleifkraft (FS) zwischen dem Schleifwerkzeug (203) und dem Werkst{\"u}ck (301), Messen einer Ist-Auslenkung (a) des Aktors (200); Anpassen einer Rotationsgeschwindigkeit des Schleifwerkzeuges (203), abh{\"a}ngig von der gemessenen Ist-Auslenkung (a) des Aktors (200) und einer Referenz-Auslenkung (a0) des Aktors (200).},
  assignee = {Ferrobotics Compliant Robot Technology GmbH},
  langid = {ngerman},
  nationality = {DE},
  keywords = {actuator,deflection,grinding,manipulator,workpiece}
}

@inproceedings{nairContextualImaginedGoals2019,
  title = {Contextual {{Imagined Goals}} for {{Self-Supervised Robotic Learning}}},
  booktitle = {{{CoRL}}},
  author = {Nair, Ashvin and Bahl, Shikhar and Khazatsky, Alexander and Pong, Vitchyr and Berseth, Glen and Levine, Sergey},
  year = {2019},
  eprint = {1910.11670},
  urldate = {2020-09-18},
  abstract = {While reinforcement learning provides an appealing formalism for learning individual skills, a general-purpose robotic system must be able to master an extensive repertoire of behaviors. Instead of learning a large collection of skills individually, can we instead enable a robot to propose and practice its own behaviors automatically, learning about the affordances and behaviors that it can perform in its environment, such that it can then repurpose this knowledge once a new task is commanded by the user? In this paper, we study this question in the context of self-supervised goal-conditioned reinforcement learning. A central challenge in this learning regime is the problem of goal setting: in order to practice useful skills, the robot must be able to autonomously set goals that are feasible but diverse. When the robot's environment and available objects vary, as they do in most open-world settings, the robot must propose to itself only those goals that it can accomplish in its present setting with the objects that are at hand. Previous work only studies self-supervised goal-conditioned RL in a single-environment setting, where goal proposals come from the robot's past experience or a generative model are sufficient. In more diverse settings, this frequently leads to impossible goals and, as we show experimentally, prevents effective learning. We propose a conditional goal-setting model that aims to propose goals that are feasible from the robot's current state. We demonstrate that this enables self-supervised goal-conditioned off-policy learning with raw image observations in the real world, enabling a robot to manipulate a variety of objects and generalize to new objects that were not seen during training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\IV97HJCL\1910.html}
}

@inproceedings{nairTimeReversalSelfSupervision2020,
  title = {Time {{Reversal}} as {{Self-Supervision}}},
  booktitle = {{{ICRA}}},
  author = {Nair, Suraj and Babaeizadeh, Mohammad and Finn, Chelsea and Levine, Sergey and Kumar, Vikash},
  year = {2020},
  eprint = {1810.01128},
  urldate = {2019-11-03},
  abstract = {A longstanding challenge in robot learning for manipulation tasks has been the ability to generalize to varying initial conditions, diverse objects, and changing objectives. Learning based approaches have shown promise in producing robust policies, but require heavy supervision to efficiently learn precise control, especially from visual inputs. We propose a novel self-supervision technique that uses time-reversal to learn goals and provide a high level plan to reach them. In particular, we introduce the time-reversal model (TRM), a self-supervised model which explores outward from a set of goal states and learns to predict these trajectories in reverse. This provides a high level plan towards goals, allowing us to learn complex manipulation tasks with no demonstrations or exploration at test time. We test our method on the domain of assembly, specifically the mating of tetris-style block pairs. Using our method operating atop visual model predictive control, we are able to assemble tetris blocks on a physical robot using only uncalibrated RGB camera input, and generalize to unseen block pairs. sites.google.com/view/time-reversal},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics}
}

@inproceedings{narodytskaFormalVerificationDeep2018,
  title = {Formal {{Verification}} of {{Deep Neural Networks}}},
  booktitle = {2018 {{Formal Methods}} in {{Computer Aided Design}} ({{FMCAD}})},
  author = {Narodytska, Nina},
  year = {2018},
  month = oct,
  pages = {1--1},
  doi = {10.23919/FMCAD.2018.8603017},
  urldate = {2024-04-19},
  abstract = {Deep neural networks are among the most successful artificial intelligence technologies making impact in a variety of practical applications. However, many concerns were raised about the `magical' power of these networks. It is disturbing that we are really lacking of understanding of the decision making process behind this technology. Therefore, a natural question is whether we can trust decisions that neural networks make. One way to address this issue is to define properties that we want a neural network to satisfy. Verifying whether a neural network fulfills these properties sheds light on the properties of the function that it represents. In this tutorial, we overview several approaches to verifying neural networks properties. The first set of methods encode neural networks into Integer Linear Programs or Satisfiability Modulo Theory formulas. They come up with domain-specific algorithms to solve verification problems. The second approach is to treat the neural network as a non-linear function and to use global optimization techniques for verification. The third line of work uses abstract interpretation to certify neural networks. Finally, we consider a special class of neural networks - Binarized Neural Networks - that can be represented and analyzed using Boolean Satisfiability. We discuss how we can take advantage of the structure of neural networks in the search procedure.},
  keywords = {Artificial neural networks,Decision making,Encoding,Optimization,Perturbation methods,Tutorials},
  file = {C:\Users\benja\Zotero\storage\JRBKMWE9\8603017.html}
}

@inproceedings{naumannFlexibelAutomatisierenMit2017,
  title = {Flexibel {{Automatisieren}} Mit Drag\&bot},
  booktitle = {Roboter in Der {{Intralogistik}}},
  author = {Naumann, Martin},
  year = {2017},
  publisher = {Stuttgarter Produktionsakademie},
  address = {Stuttgart, Germany},
  urldate = {2024-04-26},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\AH38GNGJ\details.html}
}

@inproceedings{nauSHOPSimpleHierarchical1999,
  title = {{{SHOP}}: {{Simple Hierarchical Ordered Planner}}},
  shorttitle = {{{SHOP}}},
  booktitle = {International {{Joint Conference}} on {{Artificial Intelligence}}},
  author = {Nau, Dana S. and Cao, Yue and Lotem, A. and {Mu{\~n}oz-Avila}, Hector},
  year = {1999},
  month = jul,
  urldate = {2024-04-24},
  abstract = {SHOP (Simple Hierarchical Ordered Planner) is a domain-independent HTN planning system with the following characteristics. {$\bullet$} SHOP plans for tasks in the same order that they will later be executed. This avoids some goal-interaction issues that arise in other HTN planners, so that the planning algorithm is relatively simple. {$\bullet$} Since SHOP knows the complete world-state at each step of the planning process, it can use highly expressive domain representations. For example, it can do planning problems that require complex numeric computations. {$\bullet$} In our tests, SHOP was several orders of magnitude faster man Blackbox and several times faster than TLpian, even though SHOP is coded in Lisp and the other planners are coded in C.}
}

@article{NavarroAlarcon.2016,
  title = {Automatic 3-{{D}} Manipulation of Soft Objects by Robotic Arms with an Adaptive Deformation Model},
  author = {{Navarro-Alarcon}, David and Yip, Hiu Man and Wang, Zerui and Liu, Yun-hui and Zhong, Fangxun and Zhang, Tianxue and Li, Peng},
  year = {2016},
  journal = {IEEE Transactions on Robotics},
  volume = {32},
  number = {2},
  pages = {429--441},
  issn = {1552-3098},
  doi = {10.1109/TRO.2016.2533639},
  pagination = {page}
}

@phdthesis{Nebehay.2016,
  title = {A Deformable Part Model for One-Shot Object Tracking},
  author = {Nebehay, Georg},
  year = {2016},
  address = {Graz},
  school = {Graz University of Technology}
}

@misc{neumannURoboSimEpisodicSimulation2020,
  title = {{{URoboSim}} -- {{An Episodic Simulation Framework}} for {{Prospective Reasoning}} in {{Robotic Agents}}},
  author = {Neumann, Michael and Koralewski, Sebastian and Beetz, Michael},
  year = {2020},
  month = dec,
  number = {arXiv:2012.04442},
  eprint = {2012.04442},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.04442},
  urldate = {2024-09-21},
  abstract = {Anticipating what might happen as a result of an action is an essential ability humans have in order to perform tasks effectively. On the other hand, robots capabilities in this regard are quite lacking. While machine learning is used to increase the ability of prospection it is still limiting for novel situations. A possibility to improve the prospection ability of robots is through simulation of imagined motions and the physical results of these actions. Therefore, we present URoboSim, a robot simulator that allows robots to perform tasks as mental simulation before performing this task in reality. We show the capabilities of URoboSim in form of mental simulations, generating data for machine learning and the usage as belief state for a real robot.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\benja\\Zotero\\storage\\7GRLBNQM\\Neumann et al. - 2020 - URoboSim -- An Episodic Simulation Framework for Prospective Reasoning in Robotic Agents.pdf;C\:\\Users\\benja\\Zotero\\storage\\P9XGEQ4C\\2012.html}
}

@misc{neuraroboticsTechnologies2024,
  type = {Company Website},
  title = {Technologies},
  author = {{NEURA Robotics}},
  year = {2024},
  journal = {NEURA Robotics},
  urldate = {2024-09-18},
  abstract = {Software and hardware developed in-house, made to bring cognitive skills to robots. From AI and force-torque sensors to safe human detection.},
  langid = {american},
  file = {C:\Users\benja\Zotero\storage\I8HGKWS7\technologies.html}
}

@article{Newcombe.2011a,
  title = {Presentation Slides: {{KinectFusion}}: {{Real-time 3D}} Reconstruction and Interaction Using a Moving Depth Camera},
  author = {Newcombe, Richard and Izadi, Sharam and Hilliges, Otmar},
  year = {2011}
}

@article{Newcombe.2015,
  title = {{{DynamicFusion}}: {{Reconstruction}} and Tracking of Non-Rigid Scenes in Real-Time},
  author = {Newcombe, Richard A. and Fox, Dieter and Seitz, Steven M.},
  year = {2015},
  journal = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {343--352},
  doi = {10.1109/CVPR.2015.7298631},
  abstract = {2015 IEEE Conference on Computer Vision and Pattern Recognition},
  pagination = {page}
}

@article{newellComputerScienceEmpirical1976,
  title = {Computer Science as Empirical Inquiry: Symbols and Search},
  shorttitle = {Computer Science as Empirical Inquiry},
  author = {Newell, Allen and Simon, Herbert A.},
  year = {1976},
  month = mar,
  journal = {Communications of the ACM},
  volume = {19},
  number = {3},
  pages = {113--126},
  issn = {0001-0782},
  doi = {10.1145/360018.360022},
  urldate = {2024-04-17},
  abstract = {Computer science is the study of the phenomena surrounding computers. The founders of this society understood this very well when they called themselves the Association for Computing Machinery. The machine---not just the hardware, but the programmed, living machine---is the organism we study.},
  keywords = {artificial intelligence,cognition,computer science,empirical,heuristics,list processing,problem solving,science,search,symbols,Turing}
}

@incollection{newellComputerScienceEmpirical1997,
  title = {Computer {{Science}} as {{Empirical Inquiry}}: {{Symbols}} and {{Search}}},
  shorttitle = {Computer {{Science}} as {{Empirical Inquiry}}},
  booktitle = {Mind {{Design II}}},
  author = {Newell, Allen and Simon, Herbert A.},
  editor = {Haugeland, John},
  year = {1997},
  month = mar,
  pages = {81--110},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/4626.003.0004},
  urldate = {2024-04-18},
  isbn = {978-0-262-27507-1},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\8MZFM2Y4\document.pdf}
}

@article{newellPhysicalSymbolSystems1980,
  title = {Physical Symbol Systems},
  author = {Newell, Allen},
  year = {1980},
  month = apr,
  journal = {Cognitive Science},
  volume = {4},
  number = {2},
  pages = {135--183},
  issn = {0364-0213},
  doi = {10.1016/S0364-0213(80)80015-2},
  urldate = {2024-04-17},
  abstract = {On the occasion of a first conference on Cognitive Science, it seems appropriate to review the basis of common understanding between the various disciplines. In my estimate, the most fundamental contribution so far of artificial intelligence and computer science to the joint enterprise of cognitive science has been the notion of a physical symbol system, i.e., the concept of a broad class of systems capable of having and manipulating symbols, yet realizable in the physical universe. The notion of symbol so defined is internal to this concept, so it becomes a hypothesis that this notion of symbols includes the symbols that we humans use every day of our lives. In this paper we attempt systematically, but plainly, to lay out the nature of physical symbol systems. Such a review is in ways familiar, but not thereby useless. Restatement of fundamentals is an important exercise.The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency, or the U.S. Government.Herb Simon would be a co-author of this paper, except that he is giving his own paper at this conference. The key ideas are entirely joint, as the references indicate.},
  file = {C:\Users\benja\Zotero\storage\FDN3YS9B\S0364021380800152.html}
}

@book{newellUnifiedTheoriesCognition1994,
  title = {{Unified Theories of Cognition}},
  author = {Newell, Allen},
  year = {1994},
  month = jan,
  edition = {Reprint Edition},
  publisher = {Harvard University Press},
  address = {Cambridge, Mass},
  isbn = {978-0-674-92101-6},
  langid = {Englisch}
}

@misc{NexeedManufacturingExecution2020,
  title = {{Nexeed Manufacturing Execution System}},
  year = {2020},
  month = may,
  journal = {Bosch Connected Industry},
  urldate = {2020-05-15},
  abstract = {Nexeed Manufacturing Execution System},
  howpublished = {https://www.bosch-connected-industry.com/connected-manufacturing/single-solutions/nexeed-manufacturing-execution-system/},
  langid = {ngerman},
  file = {C:\Users\benja\Zotero\storage\Y43V8A92\nexeed-manufacturing-execution-system.html}
}

@article{neyerEntwicklungUndValidierung2012,
  title = {Entwicklung Und {{Validierung}} Einer {{Kurzskala}} Zur {{Erfassung}} von {{Technikbereitschaft}}},
  author = {Neyer, Franz J. and Felber, Juliane and Gebhardt, Claudia},
  year = {2012},
  month = apr,
  journal = {Diagnostica},
  volume = {58},
  number = {2},
  pages = {87--99},
  publisher = {Hogrefe Verlag},
  issn = {0012-1924},
  doi = {10.1026/0012-1924/a000067},
  urldate = {2024-03-25},
  abstract = {Zusammenfassung. Die Autoren stellen ein Modell der Technikbereitschaft vor, nach dem individuelle Unterschiede in der Bereitschaft zum Umgang mit Technik in drei unterscheidbare Facetten untergliedert werden k{\"o}nnen: Technikakzeptanz, Technikkompetenz- und Technikkontroll{\"u}berzeugungen. Technikbereitschaft soll den erfolgreichen Umgang mit neuen Technologien insbesondere im h{\"o}heren Lebensalter vorhersagen. Die Messeigenschaften einer neu entwickelten Skala wurden in drei Studien (N = 825) {\"u}berpr{\"u}ft. Die Ergebnisse zeigen, dass das Modell der Technikbereitschaft empirisch best{\"a}tigt werden kann und das Instrument gute psychometrische Eigenschaften besitzt. Die Konstruktvalidit{\"a}t wurde {\"u}ber Zusammenh{\"a}nge mit theoretisch einschl{\"a}gigen Referenzkonstrukten (Techniknutzung, Pers{\"o}nlichkeit, Indikatoren erfolgreichen Alterns) sowie konkurrierend gegen{\"u}ber anderen Ma{\ss}en der Technikakzeptanz {\"u}berpr{\"u}ft.},
  keywords = {Technikakzeptanz,Technikbereitschaft,Technikkompetenzuberzeugungen,Technikkontrolle,Techniknutzung,technology acceptance,technology commitment,technology competence,technology control,technology use}
}

@article{ngConceptualizingAILiteracy2021,
  title = {Conceptualizing {{AI}} Literacy: {{An}} Exploratory Review},
  shorttitle = {Conceptualizing {{AI}} Literacy},
  author = {Ng, Davy Tsz Kit and Leung, Jac Ka Lok and Chu, Samuel Kai Wah and Qiao, Maggie Shen},
  year = {2021},
  month = jan,
  journal = {Computers and Education: Artificial Intelligence},
  volume = {2},
  pages = {100041},
  issn = {2666-920X},
  doi = {10.1016/j.caeai.2021.100041},
  urldate = {2024-09-11},
  abstract = {Artificial Intelligence (AI) has spread across industries (e.g., business, science, art, education) to enhance user experience, improve work efficiency, and create many future job opportunities. However, public understanding of AI technologies and how to define AI literacy is under-explored. This vision poses upcoming challenges for our next generation to learn about AI. On this note, an exploratory review was conducted to conceptualize the newly emerging concept ``AI literacy'', in search for a sound theoretical foundation to define, teach and evaluate AI literacy. Grounded in literature on 30 existing peer-reviewed articles, this review proposed four aspects (i.e., know and understand, use and apply, evaluate and create, and ethical issues) for fostering AI literacy based on the adaptation of classic literacies. This study sheds light on the consolidated definition, teaching, and ethical concerns on AI literacy, establishing the groundwork for future research such as competency development and assessment criteria on AI literacy.},
  keywords = {AI ethics,AI in education,AI learning and teaching,AI literacy,AI literacy questionnaire},
  file = {C:\Users\benja\Zotero\storage\8I3LFSJ2\S2666920X21000357.html}
}

@inproceedings{ngoAlignmentProblemDeep2023,
  title = {The {{Alignment Problem}} from a {{Deep Learning Perspective}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Ngo, Richard and Chan, Lawrence and Mindermann, S{\"o}ren},
  year = {2023},
  month = oct,
  urldate = {2024-09-29},
  abstract = {AI systems based on deep learning have reached or surpassed human performance in a range of narrow domains. In coming years or decades, artificial general intelligence (AGI) may surpass human capabilities at many critical tasks. In this position paper, we examine the technical difficulty of fine-tuning hypothetical AGI systems based on pretrained deep models to pursue goals that are aligned with human interests. We argue that, if trained like today's most capable models, AGI systems could learn to act deceptively to receive higher reward, learn internally-represented goals which generalize beyond their fine-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. AGIs with these properties would be difficult to align and may appear aligned even when they are not.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\R7QJ96P6\Ngo et al. - 2023 - The Alignment Problem from a Deep Learning Perspective.pdf}
}

@inproceedings{nguyenProbabilisticTaskModelling2021,
  title = {Probabilistic Task Modelling for Meta-Learning},
  booktitle = {Proceedings of the {{Thirty-Seventh Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Nguyen, Cuong C. and Do, Thanh-Toan and Carneiro, Gustavo},
  year = {2021},
  month = dec,
  pages = {781--791},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-15},
  abstract = {We propose probabilistic task modelling -- a generative probabilistic model for collections of tasks used in meta-learning. The proposed model combines variational auto-encoding and latent Dirichlet allocation to model each task as a mixture of Gaussian distribution in an embedding space. Such modelling provides an explicit representation of a task through its task-theme mixture. We present an efficient approximation inference technique based on variational inference method for empirical Bayes parameter estimation. We perform empirical evaluations to validate the task uncertainty and task distance produced by the proposed method through correlation diagrams of the prediction accuracy on testing tasks. We also carry out experiments of task selection in meta-learning to demonstrate how the task relatedness inferred from the proposed model help to facilitate meta-learning algorithms.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\FEZUC2B2\Nguyen et al. - 2021 - Probabilistic task modelling for meta-learning.pdf}
}

@article{nicholFirstOrderMetaLearningAlgorithms2018,
  title = {On {{First-Order Meta-Learning Algorithms}}},
  author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
  year = {2018},
  month = oct,
  journal = {arXiv:1803.02999 [cs]},
  eprint = {1803.02999},
  primaryclass = {cs},
  urldate = {2021-06-21},
  abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\MNWAMHJG\1803.html}
}

@inproceedings{niekumIncrementalSemanticallyGrounded2013,
  title = {Incremental {{Semantically Grounded Learning}} from {{Demonstration}}},
  booktitle = {Robotics: {{Science}} and {{Systems}}},
  author = {Niekum, Scott and Chitta, Sachin and Barto, Andrew and Marthi, Bhaskara and Osentoski, Sarah},
  year = {2013},
  month = jun,
  doi = {10.15607/RSS.2013.IX.048},
  file = {C:\Users\benja\Zotero\storage\TDLBQA4P\Niekum et al. - 2013 - Incremental Semantically Grounded Learning from De.pdf}
}

@incollection{nilssonPhysicalSymbolSystem2007,
  title = {The {{Physical Symbol System Hypothesis}}: {{Status}} and {{Prospects}}},
  shorttitle = {The {{Physical Symbol System Hypothesis}}},
  booktitle = {50 {{Years}} of {{Artificial Intelligence}}: {{Essays Dedicated}} to the 50th {{Anniversary}} of {{Artificial Intelligence}}},
  author = {Nilsson, Nils J.},
  editor = {Lungarella, Max and Iida, Fumiya and Bongard, Josh and Pfeifer, Rolf},
  year = {2007},
  pages = {9--17},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-77296-5_2},
  urldate = {2024-04-18},
  abstract = {I analyze some of the attacks against the Physical Symbol System Hypothesis---attacks based on the presumed need for symbol-grounding and non-symbolic processing for intelligent behavior and on the supposed non-computational and ``mindless'' aspects of brains.},
  isbn = {978-3-540-77296-5},
  langid = {english},
  keywords = {Deep Belief Network,Intelligent Action,Intelligent Behavior,Physical Symbol System,Symbol Grounding}
}

@inproceedings{niNTFieldsNeuralTime2022,
  title = {{{NTFields}}: {{Neural Time Fields}} for {{Physics-Informed Robot Motion Planning}}},
  shorttitle = {{{NTFields}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Ni, Ruiqi and Qureshi, Ahmed H.},
  year = {2022},
  month = sep,
  urldate = {2024-06-26},
  abstract = {Neural Motion Planners (NMPs) have emerged as a promising tool for solving robot navigation tasks in complex environments. However, these methods often require expert data for learning, which limits their application to scenarios where data generation is time-consuming. Recent developments have also led to physics-informed deep neural models capable of representing complex dynamical Partial Differential Equations (PDEs). Inspired by these developments, we propose Neural Time Fields (NTFields) for robot motion planning in cluttered scenarios. Our framework represents a wave propagation model generating continuous arrival time to find path solutions informed by a nonlinear first-order PDE called Eikonal Equation. We evaluate our method in various cluttered 3D environments, including the Gibson dataset, and demonstrate its ability to solve motion planning problems for 4-DOF and 6-DOF robot manipulators where the traditional grid-based Eikonal planners often face the curse of dimensionality. Furthermore, the results show that our method exhibits high success rates and significantly lower computational times than the state-of-the-art methods, including NMPs that require training data from classical planners.},
  langid = {english}
}

@inproceedings{niPhysicsinformedNeuralMotion2024,
  title = {Physics-Informed {{Neural Motion Planning}} on {{Constraint Manifolds}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Ni, Ruiqi and Qureshi, Ahmed H.},
  year = {2024},
  month = mar,
  publisher = {IEEE},
  address = {Yokohama, Japan},
  doi = {0.1109/ICRA57147.2024.10610883},
  urldate = {2024-04-29},
  abstract = {Constrained Motion Planning (CMP) aims to find a collision-free path between the given start and goal configurations on the kinematic constraint manifolds. These problems appear in various scenarios ranging from object manipulation to legged-robot locomotion. However, the zero-volume nature of manifolds makes the CMP problem challenging, and the state-of-the-art methods still take several seconds to find a path and require a computationally expansive path dataset for imitation learning. Recently, physics-informed motion planning methods have emerged that directly solve the Eikonal equation through neural networks for motion planning and do not require expert demonstrations for learning. Inspired by these approaches, we propose the first physics-informed CMP framework that solves the Eikonal equation on the constraint manifolds and trains neural function for CMP without expert data. Our results show that the proposed approach efficiently solves various CMP problems in both simulation and real-world, including object manipulation under orientation constraints and door opening with a high-dimensional 6-DOF robot manipulator. In these complex settings, our method exhibits high success rates and finds paths in sub-seconds, which is many times faster than the state-of-the-art CMP methods.},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\3D8N4QIQ\2403.html}
}

@book{Nischwitz.2012,
  title = {Computergrafik Und Bildverarbeitung: {{Band I}}: {{Computergrafik}}},
  author = {Nischwitz, Alfred and Fischer, Max and Haber{\"a}cker, Peter and Socher, Gudrun},
  year = {2012},
  series = {Studium},
  edition = {3., neu bearb. Aufl. 2011},
  publisher = {Vieweg+Teubner Verlag},
  address = {Wiesbaden},
  doi = {10.1007/978-3-8348-8323-0},
  abstract = {Im Buch Computergrafik und Bildverarbeitung finden Sie alles, was Sie f{\"u}r Studium und Praxis {\"u}ber Generierung und Verarbeitung von digitalen Bildern wissen m{\"o}chten und wie Sie es anwenden. Das erfolgreiche didaktische Konzept wurde weiterentwickelt und liegt ab dieser dritten Auflage in zwei Teilen vor. Computergrafik und Bildverarbeitung Band I f{\"u}hrt den Leser durch die Themen der Computergrafik. Dabei werden das alte und neue OpenGL parallel dargestellt, um einen guten Zugang f{\"u}r Einsteiger und einen leichteren {\"U}bergang f{\"u}r Fortgeschrittene zu gew{\"a}hrleisten. Profitieren Sie von dem kostenlosen Online-Service: Bildverarbeitungswerkzeuge, Beispiel-Software und interaktive Vorlesungen (als HTML-Seiten mit Java-Applets und Praktikumsaufgaben).},
  isbn = {978-3-8348-1304-6}
}

@incollection{Noack2015,
  title = {Treatment of {{Dependent Information}} in {{Multisensor Kalman Filtering}} and {{Data Fusion}}},
  booktitle = {Multisensor {{Data Fusion}}},
  author = {Noack, Benjamin and Sijs, Joris and Reinhardt, Marc and Hanebeck, Uwe D.},
  editor = {Fourati, Hassen},
  year = {2015},
  edition = {1st},
  pages = {169--191},
  publisher = {CRC Press},
  address = {Boca Raton},
  isbn = {978-1-4822-6375-6}
}

@article{Noack2017,
  title = {Decentralized {{Data Fusion}} with {{Inverse Covariance Intersection}}},
  author = {Noack, Benjamin and Sijs, Joris and Reinhardt, Marc and Hanebeck, Uwe D.},
  year = {2017},
  month = may,
  journal = {Automatica},
  volume = {79},
  pages = {35--41},
  publisher = {Pergamon},
  issn = {0005-1098},
  doi = {10.1016/J.AUTOMATICA.2017.01.019},
  urldate = {2018-05-01},
  abstract = {In distributed and decentralized state estimation systems, fusion methods are employed to systematically combine multiple estimates of the state into a single, more accurate estimate. An often encountered problem in the fusion process relates to unknown common information that is shared by the estimates to be fused and is responsible for correlations. If the correlation structure is unknown to the fusion method, conservative strategies are typically pursued. As such, the parameterization introduced by the ellipsoidal intersection method has been a novel approach to describe unknown correlations, though suitable values for these parameters with proven consistency have not been identified yet. In this article, an extension of ellipsoidal intersection is proposed that guarantees consistent fusion results in the presence of unknown common information. The bound used by the novel approach corresponds to computing an outer ellipsoidal bound on the intersection of inverse covariance ellipsoids. As a major advantage of this inverse covariance intersection method, fusion results prove to be more accurate than those provided by the well-known covariance intersection method.}
}

@inproceedings{Noack2017a,
  title = {Inverse {{Covariance Intersection}}: {{New Insights}} and {{Properties}}},
  booktitle = {2017 20th {{International Conference}} on {{Information Fusion}} ({{Fusion}})},
  author = {Noack, Benjamin and Sijs, Joris and Hanebeck, Uwe D.},
  year = {2017},
  month = jul,
  pages = {1--8},
  publisher = {IEEE},
  doi = {10.23919/ICIF.2017.8009694},
  urldate = {2018-06-02},
  isbn = {978-0-9964527-0-0}
}

@article{nofreWhenTechnologyBecame2014,
  title = {When {{Technology Became Language}}: {{The Origins}} of the {{Linguistic Conception}} of {{Computer Programming}}, 1950--1960},
  shorttitle = {When {{Technology Became Language}}},
  author = {Nofre, David and Priestley, Mark and Alberts, Gerard},
  year = {2014},
  journal = {Technology and Culture},
  volume = {55},
  number = {1},
  eprint = {24468397},
  eprinttype = {jstor},
  pages = {40--75},
  publisher = {[The Johns Hopkins University Press, Society for the History of Technology]},
  issn = {0040-165X},
  urldate = {2024-02-24},
  abstract = {Language is one of the central metaphors around which the discipline of computer science has been built. The language metaphor entered modern computing as part of a cybernetic discourse, but during the second half of the 1950s acquired a more abstract meaning, closely related to the formal languages of logic and linguistics. The article argues that this transformation was related to the appearance of the commercial computer in the mid-1950s. Managers of computing installations and specialists on computer programming in academic computer centers, confronted with an increasing variety of machines, called for the creation of "common" or "universal languages" to enable the migration of computer code from machine to machine. Finally, the article shows how the idea of a universal language was a decisive step in the emergence of programming languages, in the recognition of computer programming as a proper field of knowledge, and eventually in the way we think of the computer.},
  file = {C:\Users\benja\Zotero\storage\AWADHADK\Nofre et al. - 2014 - When Technology Became Language The Origins of th.pdf}
}

@inproceedings{nordmannSurveyDomainSpecificLanguages2014,
  title = {A {{Survey}} on {{Domain-Specific Languages}} in {{Robotics}}},
  booktitle = {Simulation, {{Modeling}}, and {{Programming}} for {{Autonomous Robots}}},
  author = {Nordmann, Arne and Hochgeschwender, Nico and Wrede, Sebastian},
  editor = {Brugali, Davide and Broenink, Jan F. and Kroeger, Torsten and MacDonald, Bruce A.},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {195--206},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-11900-7_17},
  abstract = {The design, simulation and programming of robotics systems is challenging as expertise from multiple domains needs to be integrated conceptually and technically. Domain-specific modeling promises an efficient and flexible concept for developing robotics applications that copes with this challenge. It allows to raise the level of abstraction through the use of specific concepts that are closer to the respective domain concerns and easier to understand and validate. Furthermore, it focuses on increasing the level of automation, e.g. through code generation, to bridge the gap between the modeling and the implementation levels and to improve the efficiency and quality of the software development process. Within this contribution, we survey the literature available on domain-specific (modeling) languages in robotics required to realize a state-of-the-art real-world example from the RoboCup@Work competition. We classify 41 publications in the field as reference for potential DSL users. Furthermore, we analyze these contributions from a DSL-engineering viewpoint and discuss quantitative and qualitative aspects such as the methods and tools used for DSL implementation as well as their documentation status and platform integration. Finally, we conclude with some recommendations for discussion in the robotics programming and simulation community based on the insights gained with this survey.},
  isbn = {978-3-319-11900-7},
  langid = {english},
  keywords = {Eclipse Modeling Framework,Pirical Software,Robot Platform,Robotic Application,Robotic System}
}

@inproceedings{noseworthyActiveLearningAbstract2021,
  title = {Active {{Learning}} of {{Abstract Plan Feasibility}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVII}}},
  author = {Noseworthy, Michael and Brand, Isaiah and Moses, Caris and Castro, Sebastian and Kaelbling, Leslie and {Lozano-Perez}, Tomas and Roy, Nicholas},
  year = {2021},
  month = jul,
  volume = {17},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-7-8},
  file = {C:\Users\benja\Zotero\storage\IJ6SE6BC\p043.html}
}

@article{notomistaSafetyPassivityFilter2021,
  title = {A {{Safety}} and {{Passivity Filter}} for {{Robot Teleoperation Systems}}},
  author = {Notomista, Gennaro and Cai, Xiaoyi},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.08630 [cs, math]},
  eprint = {2102.08630},
  primaryclass = {cs, math},
  urldate = {2021-09-16},
  abstract = {In this paper, we present a way of enforcing safety and passivity properties of robot teleoperation systems, where a human operator interacts with a dynamical system modeling the robot. The approach does so in a holistic fashion, by combining safety and passivity constraints in a single optimization-based controller which effectively filters the desired control input before supplying it to the system. The result is a safety and passivity filter implemented as a convex quadratic program which can be solved efficiently and employed in an online fashion in many robotic teleoperation applications. Simulation results show the benefits of the approach developed in this paper applied to the human teleoperation of a second-order dynamical system.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics,Mathematics - Dynamical Systems},
  file = {C:\Users\benja\Zotero\storage\N92WWGB6\2102.html}
}

@article{nubarMinimalPrincipleBiomechanics1961,
  title = {A Minimal Principle in Biomechanics},
  author = {Nubar, Yves and Contini, Renato},
  year = {1961},
  month = dec,
  journal = {The bulletin of mathematical biophysics},
  volume = {23},
  number = {4},
  pages = {377--391},
  issn = {1522-9602},
  doi = {10.1007/BF02476493},
  urldate = {2020-08-26},
  abstract = {Expenditure of energy under several simultaneous forms (mechanical, chemical, etc.) is associated with all muscular activity. The energy is directly related to what is commonly called exertion or effort. This paper defines ``muscular effort'' quantitatively in terms of some of the elements of the dynamics of the human (and animal) body. It postulates that in all likelihood the individual will, consciously or otherwise, determine his motion (or his posture, if at rest) in such a manner as to reduce his total muscular effort to a minimum consistent with imposed conditions, or ``constraints''.},
  langid = {english}
}

@misc{nvidiaIsaacSim2024,
  type = {Company Website},
  title = {Isaac {{Sim}}},
  author = {{NVIDIA}},
  year = {2024},
  journal = {NVIDIA Developer},
  urldate = {2024-09-21},
  abstract = {Design, simulate, test, and train AI-based robots in a physically-based virtual environment.},
  howpublished = {https://developer.nvidia.com/isaac/sim},
  langid = {american},
  file = {C:\Users\benja\Zotero\storage\ZRQ6S5XJ\sim.html}
}

@misc{nvidiaProjectGR00T2024,
  title = {Project {{GR00T}}},
  author = {{NVIDIA}},
  year = {2024},
  month = mar,
  journal = {NVIDIA Developer},
  urldate = {2024-07-20},
  abstract = {Enables humanoid robots to learn from human demonstrations with imitation learning.},
  howpublished = {https://developer.nvidia.com/project-gr00t},
  langid = {american},
  file = {C:\Users\benja\Zotero\storage\9P7755E4\project-gr00t.html}
}

@article{nyeLearningCompositionalRules2020,
  title = {Learning {{Compositional Rules}} via {{Neural Program Synthesis}}},
  author = {Nye, Maxwell I. and {Solar-Lezama}, Armando and Tenenbaum, Joshua B. and Lake, Brenden M.},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.05562 [cs]},
  eprint = {2003.05562},
  primaryclass = {cs},
  urldate = {2020-04-27},
  abstract = {Many aspects of human reasoning, including language, require learning rules from very little data. Humans can do this, often learning systematic rules from very few examples, and combining these rules to form compositional rule-based systems. Current neural architectures, on the other hand, often fail to generalize in a compositional manner, especially when evaluated in ways that vary systematically from training. In this work, we present a neuro-symbolic model which learns entire rule systems from a small set of examples. Instead of directly predicting outputs from inputs, we train our model to induce the explicit system of rules governing a set of previously seen examples, drawing upon techniques from the neural program synthesis literature. Our rule-synthesis approach outperforms neural meta-learning techniques in three domains: an artificial instructionlearning domain used to evaluate human learning, the SCAN challenge datasets, and learning rulebased translations of number words into integers for a wide range of human languages.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@inproceedings{nygaGroundingRobotPlans2018,
  title = {Grounding {{Robot Plans}} from {{Natural Language Instructions}} with {{Incomplete World Knowledge}}},
  booktitle = {Proceedings of {{The}} 2nd {{Conference}} on {{Robot Learning}}},
  author = {Nyga, Daniel and Roy, Subhro and Paul, Rohan and Park, Daehyung and Pomarlan, Mihai and Beetz, Michael and Roy, Nicholas},
  year = {2018},
  month = oct,
  pages = {714--723},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-08-02},
  abstract = {Our goal is to enable robots to interpret and execute high-level tasks conveyed using natural language instructions. For example, consider tasking a household robot to, ``prepare my breakfast'', ``clear the boxes on the table'' or ``make me a fruit milkshake''. Interpreting such underspecified instructions requires environmental context and background knowledge about how to accomplish complex tasks. Further, the robot's workspace knowledge may be incomplete: the environment may only be partially-observed or background knowledge may be missing causing a failure in plan synthesis. We introduce a probabilistic model that utilizes background knowledge to infer latent or missing plan constituents based on semantic co-associations learned from noisy textual corpora of task descriptions. The ability to infer missing plan constituents enables information-seeking actions such as visual exploration or dialogue with the human to acquire new knowledge to fill incomplete plans. Results indicate robust plan inference from under-specified instructions in partially-known worlds.},
  langid = {english}
}

@incollection{nyholmAIRobotCoworkers2024,
  title = {{{AI}}, {{Robot Co-workers}} and {{Humans}}},
  booktitle = {The {{De Gruyter Handbook}} of {{Artificial Intelligence}}, {{Identity}} and {{Technology Studies}}},
  author = {Nyholm, Sven},
  year = {2024},
  month = jul,
  pages = {101--120},
  publisher = {De Gruyter},
  doi = {10.1515/9783110721751-006},
  urldate = {2024-07-22},
  abstract = {Das Kapitel 6 AI, Robot Co-workers and Humans erschien in The De Gruyter Handbook of Artificial Intelligence, Identity and Technology Studies auf Seite 101.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  isbn = {978-3-11-072175-1},
  langid = {english}
}

@misc{OASIS:UIMA:2009,
  title = {Unstructured Information Management Architecture ({{UIMA}}) Version 1.0},
  author = {Ferrucci, David and Lally, Adam and Verspoor, Karin and Nyberg, Eric},
  year = {2009},
  month = mar,
  howpublished = {OASIS Standard}
}

@misc{Octopuz,
  title = {Octopuz},
  urldate = {2021-03-24},
  howpublished = {https://octopuz.com/},
  file = {C:\Users\benja\Zotero\storage\JXX7898M\octopuz.com.html}
}

@misc{OCTOPUZa,
  title = {{{OCTOPUZ}}},
  urldate = {2020-10-22},
  file = {C:\Users\benja\Zotero\storage\IYIS2GY3\octopuz.com.html}
}

@article{OHara.2011,
  title = {Introduction to the Bag of Features Paradigm for Image Classification and Retrieval},
  author = {O'Hara, Stephen and Draper, Bruce A.},
  year = {2011},
  abstract = {The past decade has seen the growing popularity of Bag of Features (BoF) approaches to many computer vision tasks, including image classification, video search, robot localization, and texture recognition. Part of the appeal is simplicity. BoF methods are based on orderless collections of quantized local image descriptors; they discard spatial information and are therefore conceptually and computationally simpler than many alternative methods. Despite this, or perhaps because of this, BoF-based systems have set new performance standards on popular image classification benchmarks and have achieved scalability breakthroughs in image retrieval. This paper presents an introduction to BoF image representations, describes critical design choices, and surveys the BoF literature. Emphasis is placed on recent techniques that mitigate quantization errors, improve feature detection, and speed up image retrieval. At the same time, unresolved issues and fundamental challenges are raised. Among the unresolved issues are determining the best techniques for sampling images, describing local image features, and evaluating system performance. Among the more fundamental challenges are how and whether BoF methods can contribute to localizing objects in complex images, or to associating high-level semantics with natural images. This survey should be useful both for introducing new investigators to the field and for providing existing researchers with a consolidated reference to related work.}
}

@inproceedings{oistadColleagueToolInteractivity2016,
  title = {Colleague or {{Tool}}? {{Interactivity Increases Positive Perceptions}} of and {{Willingness}} to {{Interact}} with a {{Robotic Co-worker}}},
  shorttitle = {Colleague or {{Tool}}?},
  booktitle = {Social {{Robotics}}},
  author = {Oistad, Benjamin C. and Sembroski, Catherine E. and Gates, Kathryn A. and Krupp, Margaret M. and Fraune, Marlena R. and {\v S}abanovi{\'c}, Selma},
  editor = {Agah, Arvin and Cabibihan, John-John and Howard, Ayanna M. and Salichs, Miguel A. and He, Hongsheng},
  year = {2016},
  pages = {774--785},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-47437-3_76},
  abstract = {Human-robot interaction is increasingly likely in workplaces in the near future. In ``co-working'' relationships, humans may appreciate socially interactive robots for their anthropomorphic likeability, or functional robots for their strict task orientation. The current study examines the comparative perceived advantages of robots that behave interactively or functionally towards humans during a task with a superordinate goal. Survey results from 33 participants assessed perceptions of, and perceived cooperation with, robots during the task. Results indicated that participants stood physically closer to and rated Interactive robots as more anthropomorphic, sympathetic, and respected than Functional robots, but they did not rate the two types of robots differently in terms of cooperation. The more participants anthropomorphized, sympathized with, and respected the robots, the more willingness they reported to working with robots in the future.},
  isbn = {978-3-319-47437-3},
  langid = {english}
}

@article{okadaPathIntegralNetworks2017,
  title = {Path {{Integral Networks}}: {{End-to-End Differentiable Optimal Control}}},
  shorttitle = {Path {{Integral Networks}}},
  author = {Okada, Masashi and Rigazio, Luca and Aoshima, Takenobu},
  year = {2017},
  month = jun,
  journal = {arXiv:1706.09597 [cs]},
  eprint = {1706.09597},
  primaryclass = {cs},
  urldate = {2020-04-03},
  abstract = {In this paper, we introduce Path Integral Networks (PI-Net), a recurrent network representation of the Path Integral optimal control algorithm. The network includes both system dynamics and cost models, used for optimal control based planning. PI-Net is fully differentiable, learning both dynamics and cost models end-to-end by back-propagation and stochastic gradient descent. Because of this, PI-Net can learn to plan. PI-Net has several advantages: it can generalize to unseen states thanks to planning, it can be applied to continuous control tasks, and it allows for a wide variety learning schemes, including imitation and reinforcement learning. Preliminary experiment results show that PI-Net, trained by imitation learning, can mimic control demonstrations for two simulated problems; a linear system and a pendulum swing-up problem. We also show that PI-Net is able to learn dynamics and cost models latent in the demonstrations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Electrical Engineering and Systems Science - Systems and Control}
}

@misc{okadaPathIntegralNetworks2017a,
  title = {Path {{Integral Networks}}: {{End-to-End Differentiable Optimal Control}}},
  shorttitle = {Path {{Integral Networks}}},
  author = {Okada, Masashi and Rigazio, Luca and Aoshima, Takenobu},
  year = {2017},
  month = jun,
  number = {arXiv:1706.09597},
  eprint = {1706.09597},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.09597},
  urldate = {2024-06-25},
  abstract = {In this paper, we introduce Path Integral Networks (PI-Net), a recurrent network representation of the Path Integral optimal control algorithm. The network includes both system dynamics and cost models, used for optimal control based planning. PI-Net is fully differentiable, learning both dynamics and cost models end-to-end by back-propagation and stochastic gradient descent. Because of this, PI-Net can learn to plan. PI-Net has several advantages: it can generalize to unseen states thanks to planning, it can be applied to continuous control tasks, and it allows for a wide variety learning schemes, including imitation and reinforcement learning. Preliminary experiment results show that PI-Net, trained by imitation learning, can mimic control demonstrations for two simulated problems; a linear system and a pendulum swing-up problem. We also show that PI-Net is able to learn dynamics and cost models latent in the demonstrations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Electrical Engineering and Systems Science - Systems and Control},
  file = {C:\Users\benja\Zotero\storage\4GEVYYTP\1706.html}
}

@misc{Olah15,
  type = {Personal Blog},
  title = {Understanding {{LSTM Networks}}},
  author = {Olah, Christopher},
  year = {2015},
  month = aug,
  journal = {colah's blog},
  urldate = {2017-07-04}
}

@inproceedings{oleariEnhancingSurgicalProcess2019,
  title = {Enhancing {{Surgical Process Modeling}} for {{Artificial Intelligence}} Development in Robotics: The {{SARAS}} Case Study for {{Minimally Invasive Procedures}}},
  shorttitle = {Enhancing {{Surgical Process Modeling}} for {{Artificial Intelligence}} Development in Robotics},
  booktitle = {2019 13th {{International Symposium}} on {{Medical Information}} and {{Communication Technology}} ({{ISMICT}})},
  author = {Oleari, Elettra and Leporini, Alice and Trojaniello, Diana and Sanna, Alberto and Capitanio, Umberto and Deh{\'o}, Federico and Larcher, Alessandro and Montorsi, Francesco and Salonia, Andrea and Setti, Francesco and Muradore, Riccardo},
  year = {2019},
  month = may,
  pages = {1--6},
  issn = {2326-8301},
  doi = {10.1109/ISMICT.2019.8743931},
  urldate = {2024-09-17},
  abstract = {Nowadays Minimally Invasive Surgery (MIS) is playing an increasingly major role in the clinical practice also thanks to a rapid evolution of the available medical technologies, especially surgical robotics. A new challenge in this respect is to equip robots with cognitive capabilities, in order to make them able to act autonomously and cooperate with human surgeons. In this paper we describe the methodological approach developed to comprehensively describe a specific surgical knowledge, to be transferred to a complex Artificial Intelligence (AI) integrating Perception, Cognitive and Planning modules. Starting from desk researches and a strict cooperation with expert surgeons, the surgical process is framed on a high-level perspective, which is then deepened into a granular model through a Surgical Process Modelling approach, so as to embed all of the needed information by the AI to properly work. The model is eventually completed adding the corresponding Process Risk Analysis. We present the results obtained with the application of the aforementioned methodology to a Laparoscopic Radical Nephrectomy (LRN) procedure and discuss on the next technical implementation of this model.},
  keywords = {artificial intelligence,Artificial intelligence,cognitive control,Frequency modulation,laparoscopy,minimally invasive surgery,Minimally invasive surgery,Planning,Robots,Surgical process modeling,Task analysis},
  file = {C\:\\Users\\benja\\Zotero\\storage\\BG4VQRS3\\Oleari et al. - 2019 - Enhancing Surgical Process Modeling for Artificial Intelligence development in robotics the SARAS c.pdf;C\:\\Users\\benja\\Zotero\\storage\\R8J4XVY5\\8743931.html}
}

@inproceedings{Olfati-Saber2007,
  title = {Distributed {{Kalman Filtering}} for {{Sensor Networks}}},
  booktitle = {2007 46th {{IEEE Conference}} on {{Decision}} and {{Control}}},
  author = {{Olfati-Saber}, R.},
  year = {2007},
  pages = {5492--5498},
  publisher = {IEEE},
  doi = {10.1109/CDC.2007.4434303},
  urldate = {2018-05-12},
  isbn = {978-1-4244-1497-0}
}

@misc{OlisRoboticsRemote,
  title = {Olis {{Robotics}} - {{Remote Monitoring}} and {{Control}} for {{Robots}}},
  journal = {Olis Robotics},
  urldate = {2023-04-11},
  abstract = {Olis Connect is a plug and play product for remote error recovery of industrial robots. Access directly in browser. Compatible with all UR cobots.},
  howpublished = {https://olisrobotics.com},
  langid = {american}
}

@article{olivaGeneralVisualImpedanceFramework2021,
  title = {A {{General Visual-Impedance Framework}} for {{Effectively Combining Vision}} and {{Force Sensing}} in {{Feature Space}}},
  author = {Oliva, Alexander Antonio and Giordano, Paolo Robuffo and Chaumette, Fran{\c c}ois},
  year = {2021},
  month = jul,
  journal = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {3},
  pages = {4441--4448},
  issn = {2377-3766},
  doi = {10.1109/LRA.2021.3068911},
  urldate = {2024-04-11},
  abstract = {Robotic systems are increasingly used to work in dynamic and/or unstructured environments and to operate with a high degree of safety and autonomy. Consequently, they are often equipped with external sensors capable of perceiving the environment (e.g., cameras) and the contacts that may arise (e.g. force/torque sensors). This letter proposes a general framework for combining force and visual information in the visual feature space. By leveraging recent results on the derivation of visual servo dynamics, we generalize the treatment regardless of the visual features chosen. Vision and force sensing are coupled in the feature space, avoiding both the convergence to a local minimum and the arising of inconsistencies at the actuation level. Any task space direction is simultaneously controlled by both vision and force. Compliance against interaction forces is achieved in feature space along the features defining the visual task. Experiments on a real platform are carried out to evaluate the effectiveness of the proposed framework.},
  keywords = {Aerospace electronics,Cameras,compliance and impedance control,Force,force control,Robots,sensor-based control,Sensors,Task analysis,Visual servoing,Visualization},
  file = {C:\Users\benja\Zotero\storage\DE65PI43\9387109.html}
}

@article{olivares-alarcosReviewComparisonOntologybased2019,
  title = {A Review and Comparison of Ontology-Based Approaches to Robot Autonomy},
  author = {{Olivares-Alarcos}, Alberto and Be{\ss}ler, Daniel and Khamis, Alaa and Goncalves, Paulo and Habib, Maki K. and {Bermejo-Alonso}, Julita and Barreto, Marcos and Diab, Mohammed and Rosell, Jan and Quintas, Jo{\~a}o and Olszewska, Joanna and Nakawala, Hirenkumar and Pignaton, Edison and Gyrard, Amelie and Borgo, Stefano and Aleny{\`a}, Guillem and Beetz, Michael and Li, Howard},
  year = {2019},
  journal = {The Knowledge Engineering Review},
  volume = {34},
  pages = {e29},
  issn = {0269-8889, 1469-8005},
  doi = {10.1017/S0269888919000237},
  urldate = {2020-09-03},
  abstract = {Within the next decades, robots will need to be able to execute a large variety of tasks autonomously in a large variety of environments. To relax the resulting programming effort, a knowledge-enabled approach to robot programming can be adopted to organize information in re-usable knowledge pieces. However, for the ease of re-use, there needs to be an agreement on the meaning of terms. A common approach is to represent these terms using ontology languages that conceptualize the respective domain. In this work, we will review projects that use ontologies to support robot autonomy. We will systematically search for projects that fulfill a set of inclusion criteria, and compare them with each other with respect to the scope of their ontology, what types of cognitive capabilities are supported by the use of ontologies, and which is their application domain.},
  langid = {english}
}

@inproceedings{olivares-alarcosRobotExplanatoryNarratives2023,
  title = {Robot Explanatory Narratives of Collaborative and Adaptive Experiences},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {{Olivares-Alarcos}, Alberto and Andriella, Antonio and Foix, Sergi and Aleny{\`a}, Guillem},
  year = {2023},
  month = may,
  publisher = {IEEE},
  address = {London, United Kingdom},
  doi = {10.1109/ICRA48891.2023.10161359},
  urldate = {2024-09-17},
  abstract = {In the future, robots are expected to autonomously interact and/or collaborate with humans, who will increase the uncertainty during the execution of tasks, provoking online adaptations of robots' plans. Hence, trustworthy robots must be able to store, retrieve and narrate important knowledge about their collaborations and adaptations. In this article, it is proposed a sound methodology that integrates three main elements. First, an ontology for collaborative robotics and adaptation to model the domain knowledge. Second, an episodic memory for time-indexed knowledge storage and retrieval. Third, a novel algorithm to extract the relevant knowledge and generate textual explanatory narratives. The algorithm produces three different types of outputs, varying the specificity, for diverse uses and preferences. A pilot study was conducted to evaluate the usefulness of the narratives, obtaining promising results. Finally, we discuss how the methodology can be generalized to other ontologies and experiences. This work boosts robot explainability, especially in cases where robots need to narrate the details of their short and long-term past experiences.},
  langid = {american},
  file = {C:\Users\benja\Zotero\storage\IIKA5SPR\10161359.html}
}

@incollection{OMahony.2019,
  title = {Computer Vision for {{3D}} Perception},
  booktitle = {Intelligent Systems and Applications},
  author = {O' Mahony, Niall and Campbell, Sean and Krpalkova, Lenka and Riordan, Daniel and Walsh, Joseph and Murphy, Aidan and Ryan, Conor},
  editor = {Arai, Kohei and Kapoor, Supriya and Bhatia, Rahul},
  year = {2019},
  series = {Advances in Intelligent Systems and Computing},
  volume = {869},
  pages = {788--804},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-01057-7_59},
  bookpagination = {page},
  isbn = {978-3-030-01056-0}
}

@inproceedings{oneillOpenXEmbodimentRobotic2024,
  title = {Open {{X-Embodiment}}: {{Robotic Learning Datasets}} and {{RT-X Models}} : {{Open X-Embodiment Collaboration0}}},
  shorttitle = {Open {{X-Embodiment}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {O'Neill, Abby and Rehman, Abdul and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and Jain, Ajinkya and Tung, Albert and Bewley, Alex and Herzog, Alex and Irpan, Alex and Khazatsky, Alexander and Rai, Anant and Gupta, Anchit and Wang, Andrew and Singh, Anikait and Garg, Animesh and Kembhavi, Aniruddha and Xie, Annie and Brohan, Anthony and Raffin, Antonin and Sharma, Archit and Yavary, Arefeh and Jain, Arhan and Balakrishna, Ashwin and Wahid, Ayzaan and {Burgess-Limerick}, Ben and Kim, Beomjoon and Sch{\"o}lkopf, Bernhard and Wulfe, Blake and Ichter, Brian and Lu, Cewu and Xu, Charles and Le, Charlotte and Finn, Chelsea and Wang, Chen and Xu, Chenfeng and Chi, Cheng and Huang, Chenguang and Chan, Christine and Agia, Christopher and Pan, Chuer and Fu, Chuyuan and Devin, Coline and Xu, Danfei and Morton, Daniel and Driess, Danny and Chen, Daphne and Pathak, Deepak and Shah, Dhruv and B{\"u}chler, Dieter and Jayaraman, Dinesh and Kalashnikov, Dmitry and Sadigh, Dorsa and Johns, Edward and Foster, Ethan and Liu, Fangchen and Ceola, Federico and Xia, Fei and Zhao, Feiyu and Stulp, Freek and Zhou, Gaoyue and Sukhatme, Gaurav S. and Salhotra, Gautam and Yan, Ge and Feng, Gilbert and Schiavi, Giulio and Berseth, Glen and Kahn, Gregory and Wang, Guanzhi and Su, Hao and Fang, Hao-Shu and Shi, Haochen and Bao, Henghui and Ben Amor, Heni and Christensen, Henrik I and Furuta, Hiroki and Walke, Homer and Fang, Hongjie and Ha, Huy and Mordatch, Igor and Radosavovic, Ilija and Leal, Isabel and Liang, Jacky and {Abou-Chakra}, Jad and Kim, Jaehyung and Drake, Jaimyn and Peters, Jan and Schneider, Jan and Hsu, Jasmine and Bohg, Jeannette and Bingham, Jeffrey and Wu, Jeffrey and Gao, Jensen and Hu, Jiaheng and Wu, Jiajun and Wu, Jialin and Sun, Jiankai and Luo, Jianlan and Gu, Jiayuan and Tan, Jie and Oh, Jihoon and Wu, Jimmy and Lu, Jingpei and Yang, Jingyun and Malik, Jitendra and Silv{\'e}rio, Jo{\~a}o and Hejna, Joey and Booher, Jonathan and Tompson, Jonathan and Yang, Jonathan and Salvador, Jordi and Lim, Joseph J. and Han, Junhyek and Wang, Kaiyuan and Rao, Kanishka and Pertsch, Karl and Hausman, Karol and Go, Keegan and Gopalakrishnan, Keerthana and Goldberg, Ken and Byrne, Kendra and Oslund, Kenneth and Kawaharazuka, Kento and Black, Kevin and Lin, Kevin and Zhang, Kevin and Ehsani, Kiana and Lekkala, Kiran and Ellis, Kirsty and Rana, Krishan and Srinivasan, Krishnan and Fang, Kuan and Singh, Kunal Pratap and Zeng, Kuo-Hao and Hatch, Kyle and Hsu, Kyle and Itti, Laurent and Chen, Lawrence Yunliang and Pinto, Lerrel and {Fei-Fei}, Li and Tan, Liam and Fan, Linxi Jim and Ott, Lionel and Lee, Lisa and Weihs, Luca and Chen, Magnum and Lepert, Marion and Memmel, Marius and Tomizuka, Masayoshi and Itkina, Masha and Castro, Mateo Guaman and Spero, Max and Du, Maximilian and Ahn, Michael and Yip, Michael C. and Zhang, Mingtong and Ding, Mingyu and Heo, Minho and Srirama, Mohan Kumar and Sharma, Mohit and Kim, Moo Jin and Kanazawa, Naoaki and Hansen, Nicklas and Heess, Nicolas and Joshi, Nikhil J and Suenderhauf, Niko and Liu, Ning and Di Palo, Norman and Shafiullah, Nur Muhammad Mahi and Mees, Oier and Kroemer, Oliver and Bastani, Osbert and Sanketi, Pannag R and Miller, Patrick Tree and Yin, Patrick and Wohlhart, Paul and Xu, Peng and Fagan, Peter David and Mitrano, Peter and Sermanet, Pierre and Abbeel, Pieter and Sundaresan, Priya and Chen, Qiuyu and Vuong, Quan and Rafailov, Rafael and Tian, Ran and Doshi, Ria and {Mart{\'i}n-Mart{\'i}n}, Roberto and Baijal, Rohan and Scalise, Rosario and Hendrix, Rose and Lin, Roy and Qian, Runjia and Zhang, Ruohan and Mendonca, Russell and Shah, Rutav and Hoque, Ryan and Julian, Ryan and Bustamante, Samuel and Kirmani, Sean and Levine, Sergey and Lin, Shan and Moore, Sherry and Bahl, Shikhar and Dass, Shivin and Sonawani, Shubham and Song, Shuran and Xu, Sichun and Haldar, Siddhant and Karamcheti, Siddharth and Adebola, Simeon and Guist, Simon and Nasiriany, Soroush and Schaal, Stefan and Welker, Stefan and Tian, Stephen and Ramamoorthy, Subramanian and Dasari, Sudeep and Belkhale, Suneel and Park, Sungjae and Nair, Suraj and Mirchandani, Suvir and Osa, Takayuki and Gupta, Tanmay and Harada, Tatsuya and Matsushima, Tatsuya and Xiao, Ted and Kollar, Thomas and Yu, Tianhe and Ding, Tianli and Davchev, Todor and Zhao, Tony Z. and Armstrong, Travis and Darrell, Trevor and Chung, Trinity and Jain, Vidhi and Vanhoucke, Vincent and Zhan, Wei and Zhou, Wenxuan and Burgard, Wolfram and Chen, Xi and Wang, Xiaolong and Zhu, Xinghao and Geng, Xinyang and Liu, Xiyuan and Liangwei, Xu and Li, Xuanlin and Lu, Yao and Ma, Yecheng Jason and Kim, Yejin and Chebotar, Yevgen and Zhou, Yifan and Zhu, Yifeng and Wu, Yilin and Xu, Ying and Wang, Yixuan and Bisk, Yonatan and Cho, Yoonyoung and Lee, Youngwoon and Cui, Yuchen and Cao, Yue and Wu, Yueh-Hua and Tang, Yujin and Zhu, Yuke and Zhang, Yunchu and Jiang, Yunfan and Li, Yunshuang and Li, Yunzhu and Iwasawa, Yusuke and Matsuo, Yutaka and Ma, Zehan and Xu, Zhuo and Cui, Zichen Jeff and Zhang, Zichen and Lin, Zipeng},
  year = {2024},
  month = may,
  pages = {6892--6903},
  doi = {10.1109/ICRA57147.2024.10611477},
  urldate = {2024-09-29},
  abstract = {Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train "generalist" X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. The project website is robotics-transformer-x.github.io.},
  keywords = {Adaptation models,Collaboration,Computational modeling,Computer vision,Data models,Learning systems,Task analysis},
  file = {C:\Users\benja\Zotero\storage\VEBPG27W\10611477.html}
}

@article{oordRepresentationLearningContrastive2019,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
  year = {2019},
  month = jan,
  journal = {arXiv:1807.03748 [cs, stat]},
  eprint = {1807.03748},
  primaryclass = {cs, stat},
  urldate = {2019-12-15},
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{oordWaveNetGenerativeModel2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = sep,
  journal = {arXiv:1609.03499 [cs]},
  eprint = {1609.03499},
  primaryclass = {cs},
  urldate = {2019-12-04},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound}
}

@misc{openaiChatGPT2023,
  type = {Large {{Language Model}}},
  title = {{{ChatGPT}}},
  shorttitle = {{{ChatGPT}}},
  author = {{OpenAI}},
  year = {2023},
  month = sep,
  journal = {ChatGPT},
  howpublished = {https://chat.openai.com}
}

@misc{openaiDALLE3,
  title = {{{DALL}}{$\cdot$}{{E}} 3},
  author = {{OpenAI}},
  journal = {DALL{$\cdot$}E 3},
  urldate = {2024-01-05},
  abstract = {DALL{$\cdot$}E~3 understands significantly more nuance and detail than our previous systems, allowing you to easily translate your ideas into exceptionally accurate images.},
  howpublished = {https://openai.com/dall-e-3},
  langid = {american},
  file = {C:\Users\benja\Zotero\storage\FWESHHM4\dall-e-3.html}
}

@misc{openaiGPT4TechnicalReport2023,
  title = {{{GPT-4 Technical Report}}},
  author = {{OpenAI}},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08774},
  urldate = {2023-10-30},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\benja\Zotero\storage\5G8WDHMB\2303.html}
}

@misc{openaiGPT4TechnicalReport2023a,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mo and Belgum, Jeff and Bello, Irwan and Berdine, Jake and {Bernadett-Shapiro}, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Sim{\'o}n Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and {Gontijo-Lopes}, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, {\L}ukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, {\L}ukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and M{\'e}ly, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cer{\'o}n and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
  year = {2023},
  month = dec,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08774},
  urldate = {2024-01-05},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\benja\Zotero\storage\S3Y2QJV5\2303.html}
}

@misc{openaiHelloGPT4o2024,
  title = {Hello {{GPT-4o}}},
  author = {{OpenAI}},
  year = {2024},
  month = may,
  urldate = {2024-08-26},
  abstract = {We're announcing GPT-4 Omni, our new flagship model which can reason across audio, vision, and text in real time.},
  howpublished = {https://openai.com/index/hello-gpt-4o/},
  langid = {american},
  file = {C:\Users\benja\Zotero\storage\JDQ5GBJL\hello-gpt-4o.html}
}

@misc{openaiOpenAIAPIReference2024,
  title = {{{OpenAI API Reference}}},
  author = {{OpenAI}},
  year = {2024},
  month = aug,
  urldate = {2024-08-26},
  abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\JRZPXH7Q\authentication.html}
}

@article{OpenGL.2018,
  title = {{{OpenGL}} Wiki},
  author = {{OpenGL}},
  year = {2018},
  lastvisited = {2019-07-22}
}

@article{oquabDINOv2LearningRobust2023,
  title = {{{DINOv2}}: {{Learning Robust Visual Features}} without {{Supervision}}},
  shorttitle = {{{DINOv2}}},
  author = {Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy V. and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and {El-Nouby}, Alaaeldin and Assran, Mido and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Herve and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  year = {2023},
  month = jul,
  journal = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  urldate = {2024-06-15},
  abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP on most of the benchmarks at image and pixel levels.},
  langid = {english}
}

@article{ortenziHybridMotionForce2017,
  title = {Hybrid Motion/Force Control: A Review},
  shorttitle = {Hybrid Motion/Force Control},
  author = {Ortenzi, V. and Stolkin, R. and Kuo, J. and Mistry, M.},
  year = {2017},
  month = oct,
  journal = {Advanced Robotics},
  volume = {31},
  number = {19-20},
  pages = {1102--1113},
  issn = {0169-1864, 1568-5535},
  doi = {10.1080/01691864.2017.1364168},
  urldate = {2020-05-12},
  langid = {english}
}

@article{osaMultimodalTrajectoryOptimization2020,
  title = {Multimodal Trajectory Optimization for Motion Planning},
  author = {Osa, Takayuki},
  year = {2020},
  month = jul,
  journal = {The International Journal of Robotics Research},
  volume = {39},
  number = {8},
  pages = {983--1001},
  publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  doi = {10.1177/0278364920918296},
  urldate = {2024-09-06},
  abstract = {Existing motion planning methods often have two drawbacks: (1) goal configurations need to be specified by a user, and (2) only a single solution is generated under a given condition. In practice, multiple possible goal configurations exist to achieve a task. Although the choice of the goal configuration significantly affects the quality of the resulting trajectory, it is not trivial for a user to specify the optimal goal configuration. In addition, the objective function used in the trajectory optimization is often non-convex, and it can have multiple solutions that achieve comparable costs. In this study, we propose a framework that determines multiple trajectories that correspond to the different modes of the cost function. We reduce the problem of identifying the modes of the cost function to that of estimating the density induced by a distribution based on the cost function. The proposed framework enables users to select a preferable solution from multiple candidate trajectories, thereby making it easier to tune the cost function and obtain a satisfactory solution. We evaluated our proposed method with motion planning tasks in 2D and 3D space. Our experiments show that the proposed algorithm is capable of determining multiple solutions for those tasks.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\YE8YPXWY\Osa - 2020 - Multimodal trajectory optimization for motion planning.pdf}
}

@article{ostaninInteractiveRobotPrograming2018,
  title = {Interactive {{Robot Programing Using Mixed Reality}}},
  author = {Ostanin, M. and Klimchik, A.},
  year = {2018},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {12th {{IFAC Symposium}} on {{Robot Control SYROCO}} 2018},
  volume = {51},
  number = {22},
  pages = {50--55},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2018.11.517},
  urldate = {2024-03-06},
  abstract = {The number of robots in industry is increasing every year, they are used to automate various processes. It is necessary to create a system for intuitive and effective interaction between robot and human. This paper represents a system for interactive programming of industrial robots based on Mixed Reality (MR). Microsoft HoloLens glasses were used for immersion in MR. By the developed system, a user without programming skills can assign a task to the manipulator. The main system parts are intuitive geometric path planning, time-optimal trajectory planning, and simulator based on MR. Via MR users are able to interact with robots and the spatial environment through the highly intuitive interface. The system comprises two KUKA robot with different kinematic models, with and without redundancy. The works show features and advantages of the MR-based system comparing to Augmented and Virtual Reality. Experiments on three different path cases show the system performance.},
  keywords = {HoloLens,Industrial robots,Mixed Reality,Robot programming},
  file = {C:\Users\benja\Zotero\storage\6LLKGFT3\S2405896318332233.html}
}

@inproceedings{ostNeuralSceneGraphs2021,
  title = {Neural {{Scene Graphs}} for {{Dynamic Scenes}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ost, Julian and Mannan, Fahim and Thuerey, Nils and Knodt, Julian and Heide, Felix},
  year = {2021},
  month = jun,
  pages = {2855--2864},
  issn = {2575-7075},
  doi = {10.1109/CVPR46437.2021.00288},
  urldate = {2024-07-21},
  abstract = {Recent implicit neural rendering methods have demonstrated that it is possible to learn accurate view synthesis for complex scenes by predicting their volumetric density and color supervised solely by a set of RGB images. However, existing methods are restricted to learning efficient representations of static scenes that encode all scene objects into a single neural network, and they lack the ability to represent dynamic scenes and decompose scenes into individual objects. In this work, we present the first neural rendering method that represents multi-object dynamic scenes as scene graphs. We propose a learned scene graph representation, which encodes object transformations and radiance, allowing us to efficiently render novel arrangements and views of the scene. To this end, we learn implicitly encoded scenes, combined with a jointly learned latent representation to describe similar objects with a single implicit function. We assess the proposed method on synthetic and real automotive data, validating that our approach learns dynamic scenes -- only by observing a video of this scene -- and allows for rendering novel photo-realistic views of novel scene compositions with unseen sets of objects at unseen poses.},
  keywords = {Computer vision,Extrapolation,Image color analysis,Neural networks,Object detection,Training,Training data},
  file = {C:\Users\benja\Zotero\storage\92Q7KR9C\9577399.html}
}

@article{osullivanOperationalFrameworkTraining2020,
  title = {Operational Framework and Training Standard Requirements for {{AI-empowered}} Robotic Surgery},
  author = {O'Sullivan, Shane and Leonard, Simon and Holzinger, Andreas and Allen, Colin and Battaglia, Fiorella and Nevejans, Nathalie and {van Leeuwen}, Fijs W. B. and Sajid, Mohammed Imran and Friebe, Michael and Ashrafian, Hutan and Heinsen, Helmut and Wichmann, Dominic and Hartnett, Margaret and Gallagher, Anthony G.},
  year = {2020},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  volume = {16},
  number = {5},
  pages = {e2020},
  issn = {1478-596X},
  doi = {10.1002/rcs.2020},
  urldate = {2024-09-17},
  abstract = {Background For autonomous robot-delivered surgeries to ever become a feasible option, we recommend the combination of human-centered artificial intelligence (AI) and transparent machine learning (ML), with integrated Gross anatomy models. This can be supplemented with medical imaging data of cadavers for performance evaluation. Methods We reviewed technological advances and state-of-the-art documented developments. We undertook a literature search on surgical robotics and skills, tracing agent studies, relevant frameworks, and standards for AI. This embraced transparency aspects of AI. Conclusion We recommend ``a procedure/skill template'' for teaching AI that can be used by a surgeon. Similar existing methodologies show that when such a metric-based approach is used for training surgeons, cardiologists, and anesthetists, it results in a {$>$}40\% error reduction in objectively assessed intraoperative procedures. The integration of Explainable AI and ML, and novel tissue characterization sensorics to tele-operated robotic-assisted procedures with medical imaged cadavers, provides robotic guidance and refines tissue classifications at a molecular level.},
  copyright = {{\copyright} 2020 John Wiley \& Sons Ltd},
  langid = {english},
  keywords = {autonomous robotic surgery,dexterity,explainable artificial intelligence xai,supervised autonomy,surgical navigation,surgical skills},
  file = {C\:\\Users\\benja\\Zotero\\storage\\N8UQTBDE\\O'Sullivan et al. - 2020 - Operational framework and training standard requirements for AI-empowered robotic surgery.pdf;C\:\\Users\\benja\\Zotero\\storage\\NX5N8HYF\\rcs.html}
}

@techreport{ottoIndustrialDataSpace2016,
  title = {Industrial {{Data Space}}. {{Digitale Souver{\"a}nit{\"a}t}} {\"U}ber {{Daten}}},
  author = {Otto, Boris and J{\"u}rjens, Jan and Schon, Jochen and {al}, et},
  year = {2016},
  address = {M{\"u}nchen},
  institution = {Fraunhofer-Gesellschaft},
  file = {C:\Users\benja\Zotero\storage\P9YMXLB8\N-399869.html}
}

@article{Ouyang2017,
  title = {Gaussian {{Process Decentralized Data Fusion Meets Transfer Learning}} in {{Large-Scale Distributed Cooperative Perception}}},
  author = {Ouyang, Ruofei and Low, Kian Hsiang},
  year = {2017},
  month = nov,
  eprint = {1711.06064},
  urldate = {2018-05-02},
  abstract = {This paper presents novel Gaussian process decentralized data fusion algorithms exploiting the notion of agent-centric support sets for distributed cooperative perception of large-scale environmental phenomena. To overcome the limitations of scale in existing works, our proposed algorithms allow every mobile sensing agent to choose a different support set and dynamically switch to another during execution for encapsulating its own data into a local summary that, perhaps surprisingly, can still be assimilated with the other agents' local summaries (i.e., based on their current choices of support sets) into a globally consistent summary to be used for predicting the phenomenon. To achieve this, we propose a novel transfer learning mechanism for a team of agents capable of sharing and transferring information encapsulated in a summary based on a support set to that utilizing a different support set with some loss that can be theoretically bounded and analyzed. To alleviate the issue of information loss accumulating over multiple instances of transfer learning, we propose a new information sharing mechanism to be incorporated into our algorithms in order to achieve memory-efficient lazy transfer learning. Empirical evaluation on real-world datasets show that our algorithms outperform the state-of-the-art methods.},
  archiveprefix = {arXiv}
}

@inproceedings{ouyangTrainingLanguageModels2024,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2024},
  month = apr,
  series = {{{NIPS}} '22},
  pages = {27730--27744},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2024-07-20},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  isbn = {978-1-71387-108-8}
}

@article{owenTranstibialProstheticSocket2020,
  title = {Transtibial {{Prosthetic Socket Strength}}: {{The Use}} of {{ISO}} 10328 in the {{Comparison}} of {{Standard}} and {{3D-Printed Sockets}}},
  shorttitle = {Transtibial {{Prosthetic Socket Strength}}},
  author = {Owen, Meredith and DesJardins, John},
  year = {2020},
  month = feb,
  journal = {Journal of Prosthetics and Orthotics},
  volume = {32},
  pages = {1},
  doi = {10.1097/JPO.0000000000000306},
  abstract = {Introduction Prosthetic sockets must withstand instances of maximum loading without failure. The goal of the present study is to evaluate the strength at failure and the failure mechanism for 3D-printed transtibial sockets, thermoplastic transtibial check sockets, and carbon-fiber definitive laminated transtibial sockets. Materials and Methods Three clinically available materials (carbon fiber, PETG thermoplastic, and 3D print polylactic acid [PLA]) were used to produce identically shaped sockets. In final assembly, the carbon-fiber sockets were designed for use both with and without a pin-lock system. Thermoplastic sockets and 3D-printed sockets were designed for use without a pin-lock system. These socket systems were then tested following International Standards Organization (ISO) 10328. Ultimate strength (US) at failure, maximum deflection at failure, and failure mechanism were determined. Results Both designs of carbon-fiber sockets had higher US values at failure than thermoplastic sockets and 3D-printed PLA sockets. Thermoplastic sockets had a slightly higher average US than 3D-printed sockets, but 3D-printed sockets exhibited a greater strength-to-weight ratio. Four distinct socket failure mechanisms were determined. Conclusions Failure mechanisms and US values differed for all socket types. Carbon-fiber sockets exhibited the highest US, but 3D-printed PLA sockets showed comparable strength to current thermoplastic sockets. Although the ISO 10328 testing standard was sufficient to complete these evaluations, the method lacked some socket-specific measures and loading conditions that could have improved comparisons between socket types.}
}

@inproceedings{pagesTIAGoModularRobot2016,
  title = {{{TIAGo}}: The Modular Robot That Adapts to Different Research Needs},
  booktitle = {International Workshop on Robot Modularity},
  author = {Pages, Jordi and Marchionni, Luca and Ferro, Francesco},
  year = {2016},
  abstract = {This paper describes TIAGo, the mobile manipulator created by PAL Robotics, which was conceived since its very begining as a modular robot. TIAGo inherits several modular components from its eldest brothers REEM, the service robot, and REEM-C, the biped humanoid robot, and incorporates new modular parts that can be used to build up other robots. The modularity techniques used to create TIAGo are introduced in this paper.},
  langid = {english}
}

@inproceedings{pahicDeepEncoderDecoderNetworks2018,
  title = {Deep {{Encoder-Decoder Networks}} for {{Mapping Raw Images}} to {{Dynamic Movement Primitives}}},
  booktitle = {{{ICRA}}},
  author = {Pahi{\v c}, R. and Gams, A. and Ude, A. and Morimoto, J.},
  year = {2018},
  month = may,
  pages = {1--6},
  doi = {10.1109/ICRA.2018.8460954},
  abstract = {In this paper we propose a new approach for learning perception-action couplings. We show that by collecting a suitable set of raw images and the associated movement trajectories, a deep encoder-decoder network can be trained that takes raw images as input and outputs the corresponding dynamic movement primitives. We propose suitable cost functions for training the network and describe how to calculate their gradients to enable effective training by back-propagation. We tested the proposed approach both on a synthetic dataset and on a widely used MNIST database to generate handwriting movements from raw images of digits. The calculated movements were also applied for digit writing with a real robot.},
  keywords = {associated movement trajectories,backpropagation,calculated movements,Cost function,cost functions,deep encoder-decoder network,Differential equations,dynamic movement primitives,encoder-decoder networks,handwriting movements,handwriting recognition,handwritten character recognition,MNIST database,neural nets,Neural networks,perception-action couplings,raw image mapping,Robot kinematics,Training,Trajectory}
}

@article{pairetPathPlanningManipulation2021,
  title = {Path {{Planning}} for {{Manipulation}} Using {{Experience-driven Random Trees}}},
  author = {Pairet, {\`E}ric and Chamzas, Constantinos and Petillot, Yvan and Kavraki, Lydia E.},
  year = {2021},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {2},
  pages = {3295--3302},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2021.3063063},
  urldate = {2021-06-03},
  abstract = {Robotic systems may frequently come across similar manipulation planning problems that result in similar motion plans. Instead of planning each problem from scratch, it is preferable to leverage previously computed motion plans, i.e., experiences, to ease the planning. Different approaches have been proposed to exploit prior information on novel task instances. These methods, however, rely on a vast repertoire of experiences and fail when none relates closely to the current problem. Thus, an open challenge is the ability to generalise prior experiences to task instances that do not necessarily resemble the prior. This work tackles the above challenge with the proposition that experiences are "decomposable" and "malleable", i.e., parts of an experience are suitable to relevantly explore the connectivity of the robot-task space even in non-experienced regions. Two new planners result from this insight: experience-driven random trees (ERT) and its bi-directional version ERTConnect. These planners adopt a tree sampling-based strategy that incrementally extracts and modulates parts of a single path experience to compose a valid motion plan. We demonstrate our method on task instances that significantly differ from the prior experiences, and compare with related state-of-the-art experience-based planners. While their repairing strategies fail to generalise priors of tens of experiences, our planner, with a single experience, significantly outperforms them in both success rate and planning time. Our planners are implemented and freely available in the Open Motion Planning Library.},
  keywords = {Computer Science - Robotics},
  annotation = {recommend},
  file = {C:\Users\benja\Zotero\storage\H4VZMMC8\2103.html}
}

@inproceedings{pakhomov2019deep,
  title = {Deep Residual Learning for Instrument Segmentation in Robotic Surgery},
  booktitle = {International Workshop on Machine Learning in Medical Imaging},
  author = {Pakhomov, Daniil and Premachandran, Vittal and Allan, Max and Azizian, Mahdi and Navab, Nassir},
  year = {2019},
  pages = {566--573},
  organization = {Springer}
}

@inproceedings{panchenkoBuildingWebScaleDependencyParsed2018,
  title = {Building a {{Web-Scale Dependency-Parsed Corpus}} from {{CommonCrawl}}},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  author = {Panchenko, Alexander and Ruppert, Eugen and Faralli, Stefano and Ponzetto, Simone P. and Biemann, Chris},
  year = {2018},
  month = may,
  publisher = {European Language Resources Association (ELRA)},
  address = {Miyazaki, Japan},
  urldate = {2021-06-21}
}

@article{pandeyMassProducedSociableHumanoid2018,
  title = {A {{Mass-Produced Sociable Humanoid Robot}}: {{Pepper}}: {{The First Machine}} of {{Its Kind}}},
  shorttitle = {A {{Mass-Produced Sociable Humanoid Robot}}},
  author = {Pandey, Amit Kumar and Gelin, Rodolphe},
  year = {2018},
  month = jul,
  journal = {IEEE Robotics \& Automation Magazine},
  volume = {PP},
  pages = {1--1},
  doi = {10.1109/MRA.2018.2833157},
  abstract = {As robotics technology evolves, we believe that personal social robots will be one of the next big expansions in the robotics sector. Based on the accelerated advances in this multidisciplinary domain and the growing number of use cases, we can posit that robots will play key roles in everyday life and will soon coexist with us, leading all people to a smarter, safer, healthier, and happier existence.}
}

@inproceedings{panEffectsRewardMisspecification2021,
  title = {The {{Effects}} of {{Reward Misspecification}}: {{Mapping}} and {{Mitigating Misaligned Models}}},
  shorttitle = {The {{Effects}} of {{Reward Misspecification}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Pan, Alexander and Bhatia, Kush and Steinhardt, Jacob},
  year = {2021},
  month = oct,
  urldate = {2024-05-02},
  abstract = {Reward hacking---where RL agents exploit gaps in misspecified proxy rewards---has been widely observed, but not yet systematically studied. To understand reward hacking, we construct four RL environments with different misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, and observation space noise. Typically, more capable agents are able to better exploit reward misspecifications, causing them to attain higher proxy reward and lower true reward. Moreover, we find instances of {\textbackslash}emph\{phase transitions\}: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To encourage further research on reward misspecification, address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors.},
  langid = {english}
}

@incollection{panekUserAcceptanceMobile2012,
  title = {User {{Acceptance}} of a {{Mobile LED Projector}} on a {{Socially Assistive Robot}}},
  booktitle = {Ambient {{Assisted Living}}: 5. {{AAL-Kongress}} 2012 {{Berlin}}, {{Germany}}, {{January}} 24-25, 2012},
  author = {Panek, Paul and Edelmayer, Georg and Mayer, Peter and Beck, Christian and Rauhala, Marjo},
  editor = {Wichert, Reiner and Eberhardt, Birgid},
  year = {2012},
  series = {Advanced {{Technologies}} and {{Societal Change}}},
  pages = {77--91},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-27491-6_6},
  urldate = {2021-01-26},
  abstract = {A prototype of a LED projector module mounted on and carried by the humanoid robot NAO was iteratively developed in the KSERA project. It offers an additional visual information channel for the users (text, graphic, and video) by being able to come to the user where ever they are. Several mock ups were built to explore advantages and challenges of the mobile projector solution. Ten users, comprising six older persons and four experts from the care domain participated in laboratory tests to explore the user acceptance and expected usefulness. The novel projector solution was also compared with already existing solutions such as stationary screens. It could be shown that the novel solution of the mobile projector was well accepted bringing added value from the points of view of older persons as well as care experts. Challenges remain regarding suitable surfaces needed for projection and the currently limited brightness.},
  isbn = {978-3-642-27491-6},
  langid = {english},
  keywords = {AAL,assistive technology,COPD,mobile user interface,robot,socially assistive robotics}
}

@article{paneReinforcementLearningBased2019,
  title = {Reinforcement Learning Based Compensation Methods for Robot Manipulators},
  author = {Pane, Yudha P. and Nageshrao, Subramanya P. and Kober, Jens and Babu{\v s}ka, Robert},
  year = {2019},
  month = feb,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {78},
  pages = {236--247},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2018.11.006},
  urldate = {2022-08-01},
  abstract = {Smart robotics will be a core feature while migrating from Industry 3.0 (i.e., mass manufacturing) to Industry 4.0 (i.e., customized or social manufacturing). A key characteristic of a smart system is its ability to learn. For smart manufacturing, this means incorporating learning capabilities into the current fixed, repetitive, task-oriented industrial manipulators, thus rendering them `smart'. In this paper we introduce two reinforcement learning (RL) based compensation methods. The learned correction signal, which compensates for unmodeled aberrations, is added to the existing nominal input with an objective to enhance the control performance. The proposed learning algorithms are evaluated on a 6-DoF industrial robotic manipulator arm to follow different kinds of reference paths, such as square or a circular path, or to track a trajectory on a three dimensional surface. In an extensive experimental study we compare the performance of our learning-based methods with well-known tracking controllers, namely, proportional-derivative (PD), model predictive control (MPC), and iterative learning control (ILC). The experimental results show a considerable performance improvement thanks to our RL-based methods when compared to PD, MPC, and ILC.},
  langid = {english},
  keywords = {Actor-critic scheme,Reinforcement learning,Robotics,Tracking control},
  file = {C:\Users\benja\Zotero\storage\5594UL53\S0952197618302446.html}
}

@inproceedings{pangercicSemanticObjectMaps2012,
  title = {Semantic {{Object Maps}} for Robotic Housework - Representation, Acquisition and Use},
  booktitle = {2012 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Pangercic, Dejan and Pitzer, Benjamin and Tenorth, Moritz and Beetz, Michael},
  year = {2012},
  month = oct,
  pages = {4644--4651},
  issn = {2153-0866},
  doi = {10.1109/IROS.2012.6385603},
  urldate = {2024-07-07},
  abstract = {In this article we investigate the representation and acquisition of Semantic Objects Maps (SOMs) that can serve as information resources for autonomous service robots performing everyday manipulation tasks in kitchen environments. These maps provide the robot with information about its operation environment that enable it to perform fetch and place tasks more efficiently and reliably. To this end, the semantic object maps can answer queries such as the following ones: ``What do parts of the kitchen look like?'', ``How can a container be opened and closed?'', ``Where do objects of daily use belong?'', ``What is inside of cupboards/drawers?'', etc. The semantic object maps presented in this article, which we call SOM+, extend the first generation of SOMs presented by Rusu et al. [1] in that the representation of SOM+ is designed more thoroughly and that SOM+ also include knowledge about the appearance and articulation of furniture objects. Also, the acquisition methods for SOM+ substantially advance those developed in [1] in that SOM+ are acquired autonomously and with low-cost (Kinect) instead of very accurate (laser-based) 3D sensors. In addition, perception methods are more general and are demonstrated to work in different kitchen environments.},
  keywords = {Refrigerators,Robot sensing systems},
  file = {C:\Users\benja\Zotero\storage\KBL8F6EF\6385603.html}
}

@article{panSurveyTransferLearning2010,
  title = {A {{Survey}} on {{Transfer Learning}}},
  author = {Pan, Sinno Jialin and Yang, Qiang},
  year = {2010},
  month = oct,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {22},
  number = {10},
  pages = {1345--1359},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2009.191},
  urldate = {2021-06-21},
  abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
  keywords = {data mining.,machine learning,survey,Transfer learning,Transfer learning survey machine learning data mining.}
}

@misc{pantanoCapabilitybasedFrameworksIndustrial2022,
  title = {Capability-Based {{Frameworks}} for {{Industrial Robot Skills}}: A {{Survey}}},
  shorttitle = {Capability-Based {{Frameworks}} for {{Industrial Robot Skills}}},
  author = {Pantano, Matteo and Eiband, Thomas and Lee, Dongheui},
  year = {2022},
  month = jun,
  number = {arXiv:2203.00538},
  eprint = {2203.00538},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.00538},
  urldate = {2022-08-01},
  abstract = {The research community is puzzled with words like skill, action, atomic unit and others when describing robots' capabilities. However, for giving the possibility to integrate capabilities in industrial scenarios, a standardization of these descriptions is necessary. This work uses a structured review approach to identify commonalities and differences in the research community of robots' skill frameworks. Through this method, 210 papers were analyzed and three main results were obtained. First, the vast majority of authors agree on a taxonomy based on task, skill and primitive. Second, the most investigated robots' capabilities are pick and place. Third, industrial oriented applications focus more on simple robots' capabilities with fixed parameters while ensuring safety aspects. Therefore, this work emphasizes that a taxonomy based on task, skill and primitives should be used by future works to align with existing literature. Moreover, further research is needed in the industrial domain for parametric robots' capabilities while ensuring safety.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\HYQW6D6I\2203.html}
}

@article{pantanoCapabilitybasedFrameworksIndustrial2022a,
  title = {Capability-Based {{Frameworks}} for {{Industrial Robot Skills}}: A {{Survey}}},
  shorttitle = {Capability-Based {{Frameworks}} for {{Industrial Robot Skills}}},
  author = {Pantano, Matteo and Eiband, Thomas and Lee, Dongheui},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.00538 [cs]},
  eprint = {2203.00538},
  primaryclass = {cs},
  urldate = {2022-05-06},
  abstract = {The research community is puzzled with words like skill, action, atomic unit and others when trying to describe robot capabilities. However, for giving the possibility to integrate such in the industrial scenario a standardization of their description is necessary. This work, through a structured review approach, tries to identify commonalities in the research community. From this review it was possible to perceive that most of the industrially focused research work targets simple capabilities like pick and place, the large amount of authors agree on a structure consisting of task, skill and primitive, the Robot Operating System is a common framework both in industrial and non-industrial domains and skills are a main enabler for high mix - low volume productions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\52MVKGJW\2203.html}
}

@inproceedings{pantanoCapabilitybasedFrameworksIndustrial2022b,
  title = {Capability-Based {{Frameworks}} for {{Industrial Robot Skills}}: A {{Survey}}},
  shorttitle = {Capability-Based {{Frameworks}} for {{Industrial Robot Skills}}},
  booktitle = {2022 {{IEEE}} 18th {{International Conference}} on {{Automation Science}} and {{Engineering}} ({{CASE}})},
  author = {Pantano, Matteo and Eiband, Thomas and Lee, Dongheui},
  year = {2022},
  month = aug,
  pages = {2355--2362},
  publisher = {IEEE Press},
  address = {Mexico City, Mexico},
  doi = {10.1109/CASE49997.2022.9926648},
  urldate = {2024-03-08},
  abstract = {The research community is puzzled with words like skill, action, atomic unit and others when describing robots\&\#x2019; capabilities. However, for giving the possibility to integrate capabilities in industrial scenarios, a standardization of these descriptions is necessary. This work uses a structured review approach to identify commonalities and differences in the research community of robots\&\#x2019; skill frameworks. Through this method, 210 papers were analyzed and three main results were obtained. First, the vast majority of authors agree on a taxonomy based on task, skill and primitive. Second, the most investigated robots\&\#x2019; capabilities are pick and place. Third, industrial oriented applications focus more on simple robots\&\#x2019; capabilities with fixed parameters while ensuring safety aspects. Therefore, this work emphasizes that a taxonomy based on task, skill and primitives should be used by future works to align with existing literature. Moreover, further research is needed in the industrial domain for parametric robots\&\#x2019; capabilities while ensuring safety.}
}

@article{panzerDeepReinforcementLearning2022,
  title = {Deep Reinforcement Learning in Production Systems: A Systematic Literature Review},
  shorttitle = {Deep Reinforcement Learning in Production Systems},
  author = {Panzer, Marcel and Bender, Benedict},
  year = {2022},
  month = jul,
  journal = {International Journal of Production Research},
  volume = {60},
  number = {13},
  pages = {4316--4341},
  publisher = {Taylor \& Francis},
  issn = {0020-7543},
  doi = {10.1080/00207543.2021.1973138},
  urldate = {2022-08-18},
  abstract = {Shortening product development cycles and fully customisable products pose major challenges for production systems. These not only have to cope with an increased product diversity but also enable high throughputs and provide a high adaptability and robustness to process variations and unforeseen incidents. To overcome these challenges, deep Reinforcement Learning (RL) has been increasingly applied for the optimisation of production systems. Unlike other machine learning methods, deep RL operates on recently collected sensor-data in direct interaction with its environment and enables real-time responses to system changes. Although deep RL is already being deployed in production systems, a systematic review of the results has not yet been established. The main contribution of this paper is to provide researchers and practitioners an overview of applications and to motivate further implementations and research of deep RL supported production systems. Findings reveal that deep RL is applied in a variety of production domains, contributing to data-driven and flexible processes. In most applications, conventional methods were outperformed and implementation efforts or dependence on human experience were reduced. Nevertheless, future research must focus more on transferring the findings to real-world systems to analyse safety aspects and demonstrate reliability under prevailing conditions.},
  keywords = {Machine learning,manufacturing processes,production control,production planning,reinforcement learning,systematic literature review}
}

@article{papanastasiouSeamlessHumanRobot2019,
  title = {Towards Seamless Human Robot Collaboration: Integrating Multimodal Interaction},
  shorttitle = {Towards Seamless Human Robot Collaboration},
  author = {Papanastasiou, Stergios and Kousi, Niki and Karagiannis, Panagiotis and Gkournelos, Christos and Papavasileiou, Apostolis and Dimoulas, Konstantinos and Baris, Konstantinos and Koukas, Spyridon and Michalos, George and Makris, Sotiris},
  year = {2019},
  month = dec,
  journal = {The International Journal of Advanced Manufacturing Technology},
  volume = {105},
  number = {9},
  pages = {3881--3897},
  issn = {1433-3015},
  doi = {10.1007/s00170-019-03790-3},
  urldate = {2020-10-07},
  abstract = {This paper discusses the challenges in the collaboration between human operators and industrial robots for assembly operations focusing on safety and simplified interaction. A case study is presented, involving perception technologies for the robot in conjunction with wearable devices used by the operator. In terms of robot perception, a manual guidance module, an air pressor contact sensor namely skin, and a vision system for recognition and tracking of objects have been developed and integrated. Concerning the wearable devices, an advanced user interface including audio and haptic commands accompanied by augmented reality technology are used to support the operator and provide awareness by visualizing information related to production and safety aspects. In parallel, safety functionalities are implemented through collision detection technologies such as a safety skin and safety monitored regions delimiting the area of the robot activities. The complete system is coordinated under a common integration platform and it is validated in a case study of the white goods industry.},
  langid = {english}
}

@inproceedings{paraschosProbabilisticMovementPrimitives2013,
  title = {Probabilistic {{Movement Primitives}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Paraschos, Alexandros and Daniel, Christian and Peters, Jan R and Neumann, Gerhard},
  year = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-08},
  abstract = {Movement Primitives (MP) are a well-established approach for representing modular and re-usable robot movement generators. Many state-of-the-art robot learning successes are based MPs, due to their compact representation of the inherently continuous and high dimensional robot movements. A major goal in robot learning is to combine multiple MPs as building blocks in a modular control architecture to solve complex tasks. To this effect, a MP representation has to allow for blending between motions, adapting to altered task variables, and co-activating multiple MPs in parallel. We present a probabilistic formulation of the MP concept that maintains a distribution over trajectories. Our probabilistic approach allows for the derivation of new operations which are essential for implementing all aforementioned properties in one framework. In order to use such a trajectory distribution for robot movement control, we analytically derive a stochastic feedback controller which reproduces the given trajectory distribution. We evaluate and compare our approach to existing methods on several simulated as well as real robot scenarios.}
}

@article{parcolletQuaternionRecurrentNeural2018,
  title = {Quaternion {{Recurrent Neural Networks}}},
  author = {Parcollet, Titouan and Ravanelli, Mirco and Morchid, Mohamed and Linar{\`e}s, Georges and Trabelsi, Chiheb and De Mori, Renato and Bengio, Yoshua},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.04418 [cs, stat]},
  eprint = {1806.04418},
  primaryclass = {cs, stat},
  urldate = {2019-05-26},
  abstract = {Recurrent neural networks (RNNs) are powerful architectures to model sequential data, due to their capability to learn short and long-term dependencies between the basic elements of a sequence. Nonetheless, popular tasks such as speech or images recognition, involve multi-dimensional input features that are characterized by strong internal dependencies between the dimensions of the input vector. We propose a novel quaternion recurrent neural network (QRNN), alongside with a quaternion long-short term memory neural network (QLSTM), that take into account both the external relations and these internal structural dependencies with the quaternion algebra. Similarly to capsules, quaternions allow the QRNN to code internal dependencies by composing and processing multidimensional features as single entities, while the recurrent operation reveals correlations between the elements composing the sequence. We show that both QRNN and QLSTM achieve better performances than RNN and LSTM in a realistic application of automatic speech recognition. Finally, we show that QRNN and QLSTM reduce by a maximum factor of 3.3x the number of free parameters needed, compared to real-valued RNNs and LSTMs to reach better results, leading to a more compact representation of the relevant information.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{parisottoNeuroSymbolicProgramSynthesis2016,
  title = {Neuro-{{Symbolic Program Synthesis}}},
  author = {Parisotto, Emilio and Mohamed, Abdel-rahman and Singh, Rishabh and Li, Lihong and Zhou, Dengyong and Kohli, Pushmeet},
  year = {2016},
  month = nov,
  journal = {arXiv:1611.01855 [cs]},
  eprint = {1611.01855},
  primaryclass = {cs},
  urldate = {2019-07-11},
  abstract = {Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the RecursiveReverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Programming Languages}
}

@inproceedings{Park.2006,
  title = {Capturing and Animating Skin Deformation in Human Motion},
  booktitle = {{{ACM SIGGRAPH}} 2006 Papers on - {{SIGGRAPH}} '06},
  author = {Park, Sang Il and Hodgins, Jessica K.},
  editor = {Finnegan, John and Dorsey, Julie},
  year = {2006},
  pages = {881},
  publisher = {ACM Press},
  address = {New York, New York, USA},
  doi = {10.1145/1179352.1141970},
  bookpagination = {page},
  isbn = {1-59593-364-6}
}

@inproceedings{Parker.1998,
  title = {Interactive Ray Tracing for Isosurface Rendering},
  booktitle = {Proceedings Visualization '98 (Cat. {{No}}.{{98CB36276}})},
  author = {Parker, S. and Shirley, P. and Livnat, Y. and Hansen, C. and Sloan, P.-P.},
  year = {1998},
  pages = {233--238},
  publisher = {IEEE},
  doi = {10.1109/VISUAL.1998.745713},
  bookpagination = {page},
  isbn = {0-8186-9176-X}
}

@inproceedings{pascanuDifficultyTrainingRecurrent2013,
  title = {On the {{Difficulty}} of {{Training Recurrent Neural Networks}}},
  booktitle = {{{ICML}}},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  year = {2013},
  pages = {III-1310--III-1318},
  address = {Atlanta, GA, USA},
  urldate = {2019-08-03},
  abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.}
}

@inproceedings{paszkeAutomaticDifferentiationPyTorch2017,
  title = {Automatic Differentiation in {{PyTorch}}},
  booktitle = {{{NIPS}} 2017 {{Workshop Autodiff}}},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year = {2017},
  month = oct,
  urldate = {2019-08-12},
  abstract = {In this article, we describe an automatic differentiation module of PyTorch --- a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua...}
}

@incollection{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {d'Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {8026--8037},
  publisher = {Curran Associates, Inc.},
  urldate = {2020-10-26},
  file = {C:\Users\benja\Zotero\storage\IZ6IID6Z\9015-pytorch-an-imperative-style-high-performance-deep-learning-library.html}
}

@incollection{paszkePyTorchImperativeStyle2019a,
  title = {{{PyTorch}}: An Imperative Style, High-Performance Deep Learning Library},
  shorttitle = {{{PyTorch}}},
  booktitle = {Proceedings of the 33rd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  month = dec,
  number = {721},
  pages = {8026--8037},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2023-01-31},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.}
}

@inproceedings{patacchiolaComparingEfficacyFineTuning2023,
  title = {Comparing the {{Efficacy}} of {{Fine-Tuning}} and {{Meta-Learning}} for {{Few-Shot Policy Imitation}}},
  booktitle = {Proceedings of {{The}} 2nd {{Conference}} on {{Lifelong Learning Agents}}},
  author = {Patacchiola, Massimiliano and Sun, Mingfei and Hofmann, Katja and Turner, Richard E.},
  year = {2023},
  month = nov,
  pages = {878--908},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-15},
  abstract = {In this paper we explore few-shot imitation learning for control problems, which involves learning to imitate a target policy by accessing a limited set of offline rollouts. This setting has been relatively under-explored despite its relevance to robotics and control applications. State-of-the-art methods developed to tackle few-shot imitation rely on meta-learning, which is expensive to train as it requires access to a distribution over tasks (rollouts from many target policies and variations of the base environment). Given this limitation we investigate an alternative approach, fine-tuning, a family of methods that pretrain on a single dataset and then fine-tune on unseen domain-specific data. Recent work has shown that fine-tuners outperform meta-learners in few-shot image classification tasks, especially when the data is out-of-domain. Here we evaluate to what extent this is true for control problems, proposing a simple yet effective baseline which relies on two stages: (i) training a base policy online via reinforcement learning (e.g. Soft Actor-Critic) on a single base environment, (ii) fine-tuning the base policy via behavioral cloning on a few offline rollouts of the target policy. Despite its simplicity this baseline is competitive with meta-learning methods on a variety of conditions and is able to imitate target policies trained on unseen variations of the original environment. Importantly, the proposed approach is practical and easy to implement, as it does not need any complex meta-training protocol. As a further contribution, we release an open source dataset called iMuJoCo (iMitation MuJoCo) consisting of 154 variants of popular OpenAI-Gym MuJoCo environments with associated pretrained target policies and rollouts, which can be used by the community to study few-shot imitation learning and offline reinforcement learning.},
  langid = {english}
}

@article{pateriaHierarchicalReinforcementLearning2021,
  title = {Hierarchical {{Reinforcement Learning}}: {{A Comprehensive Survey}}},
  shorttitle = {Hierarchical {{Reinforcement Learning}}},
  author = {Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-hwee and Quek, Chai},
  year = {2021},
  month = jun,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {5},
  pages = {109:1--109:35},
  issn = {0360-0300},
  doi = {10.1145/3453160},
  urldate = {2021-09-16},
  abstract = {Hierarchical Reinforcement Learning (HRL) enables autonomous decomposition of challenging long-horizon decision-making tasks into simpler subtasks. During the past years, the landscape of HRL research has grown profoundly, resulting in copious approaches. A comprehensive overview of this vast landscape is necessary to study HRL in an organized manner. We provide a survey of the diverse HRL approaches concerning the challenges of learning hierarchical policies, subtask discovery, transfer learning, and multi-agent learning using HRL. The survey is presented according to a novel taxonomy of the approaches. Based on the survey, a set of important open problems is proposed to motivate the future research in HRL. Furthermore, we outline a few suitable task domains for evaluating the HRL approaches and a few interesting examples of the practical applications of HRL in the Supplementary Material.},
  keywords = {Hierarchical reinforcement learning,hierarchical reinforcement learning survey,hierarchical reinforcement learning taxonomy,skill discovery,subtask discovery}
}

@misc{PathZerodefectProduction,
  title = {On the Path toward Zero-Defect Production with {{Bosch AI}}},
  journal = {Bosch Media Service},
  urldate = {2021-04-14},
  abstract = {AI system to be rolled out at Bosch plants worldwide},
  howpublished = {https://www.bosch-presse.de/pressportal/de/en/on-the-path-toward-zero-defect-production-with-bosch-ai-225472.html},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\3MVZFKKM\on-the-path-toward-zero-defect-production-with-bosch-ai-225472.html}
}

@article{pattonProgrammingbyDemonstrationLongHorizonRobot2024,
  title = {Programming-by-{{Demonstration}} for {{Long-Horizon Robot Tasks}}},
  author = {Patton, Noah and Rahmani, Kia and Missula, Meghana and Biswas, Joydeep and Dillig, I{\c s}{\i}l},
  year = {2024},
  month = jan,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {8},
  number = {POPL},
  pages = {18:512--18:545},
  doi = {10.1145/3632860},
  urldate = {2024-04-17},
  abstract = {The goal of programmatic Learning from Demonstration (LfD) is to learn a policy in a programming language that can be used to control a robot's behavior from a set of user demonstrations. This paper presents a new programmatic LfD algorithm that targets long-horizon robot tasks which require synthesizing programs with complex control flow structures, including nested loops with multiple conditionals. Our proposed method first learns a program sketch that captures the target program's control flow and then completes this sketch using an LLM-guided search procedure that incorporates a novel technique for proving unrealizability of programming-by-demonstration problems. We have implemented our approach in a new tool called PROLEX and present the results of a comprehensive experimental evaluation on 120 benchmarks involving complex tasks and environments. We show that, given a 120 second time limit, PROLEX can find a program consistent with the demonstrations in 80\% of the cases. Furthermore, for 81\% of the tasks for which a solution is returned, PROLEX is able to find the ground truth program with just one demonstration. In comparison, CVC5, a syntax-guided synthesis tool, is only able to solve 25\% of the cases even when given the ground truth program sketch, and an LLM-based approach, GPT-Synth, is unable to solve any of the tasks due to the environment complexity.},
  keywords = {Abstract Interpretation,Learning from Demonstrations,Program Synthesis}
}

@article{pattonProgrammingbyDemonstrationLongHorizonRobot2024a,
  title = {Programming-by-{{Demonstration}} for {{Long-Horizon Robot Tasks}}},
  author = {Patton, Noah and Rahmani, Kia and Missula, Meghana and Biswas, Joydeep and Dillig, I{\c s}{\i}l},
  year = {2024},
  month = jan,
  journal = {Programming-by-Demonstration for Long-Horizon Robot Tasks},
  volume = {8},
  number = {POPL},
  pages = {18:512--18:545},
  doi = {10.1145/3632860},
  urldate = {2024-07-19},
  abstract = {The goal of programmatic Learning from Demonstration (LfD) is to learn a policy in a programming language that can be used to control a robot's behavior from a set of user demonstrations. This paper presents a new programmatic LfD algorithm that targets long-horizon robot tasks which require synthesizing programs with complex control flow structures, including nested loops with multiple conditionals. Our proposed method first learns a program sketch that captures the target program's control flow and then completes this sketch using an LLM-guided search procedure that incorporates a novel technique for proving unrealizability of programming-by-demonstration problems. We have implemented our approach in a new tool called PROLEX and present the results of a comprehensive experimental evaluation on 120 benchmarks involving complex tasks and environments. We show that, given a 120 second time limit, PROLEX can find a program consistent with the demonstrations in 80\% of the cases. Furthermore, for 81\% of the tasks for which a solution is returned, PROLEX is able to find the ground truth program with just one demonstration. In comparison, CVC5, a syntax-guided synthesis tool, is only able to solve 25\% of the cases even when given the ground truth program sketch, and an LLM-based approach, GPT-Synth, is unable to solve any of the tasks due to the environment complexity.}
}

@article{paulWAVEModelBased1977,
  title = {{{WAVE A}} Model Based Language for Manipulator Control},
  author = {Paul, R.},
  year = {1977},
  month = jan,
  journal = {Industrial Robot: An International Journal},
  volume = {4},
  number = {1},
  pages = {10--17},
  publisher = {MCB UP Ltd},
  issn = {0143-991X},
  doi = {10.1108/eb004473},
  urldate = {2024-04-15},
  abstract = {This paper describes a symbolic manipulator control language, WAVE. WAVE resembles a computer machine language and has been used for performing such tasks as a the assembly of a ``Model T'' water pump, the block manipulation needed to solve the ``Instant Insanity'' puzzle, bruch caligraphy, crank turning, the two handed mounting of a door hinge, and the assembly of a pencil sharpener. The language specifies motions, provides force and touch control and is capable of interacting with external vision systems. A model of the manipulator is maintained to translate positions in rectangular co-ordinates into joint angles and to predict joint inertias and gravity loads. Programming is interpretative and uses a PDP-10 computer running under time-sharing. The planning programm calculates manipulator trajectories and dynamic constants and writes an executable program on a disk.},
  file = {C:\Users\benja\Zotero\storage\3PT9SP9Q\html.html}
}

@article{pavlickSymbolsGroundingLarge2023,
  title = {Symbols and Grounding in Large Language Models},
  author = {Pavlick, Ellie},
  year = {2023},
  month = jun,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {381},
  number = {2251},
  pages = {20220041},
  publisher = {Royal Society},
  doi = {10.1098/rsta.2022.0041},
  urldate = {2024-08-16},
  abstract = {Large language models (LLMs) are one of the most impressive achievements of artificial intelligence in recent years. However, their relevance to the study~of language more broadly remains unclear. This article considers the potential of LLMs to serve as models of language understanding in humans. While debate on this question typically centres around models' performance on challenging language understanding tasks, this article argues that the answer depends on models' underlying competence, and thus that the focus of the debate should be on empirical work which seeks to characterize the representations and processing algorithms that underlie model behaviour. From this perspective, the article offers counterarguments to two commonly cited reasons why LLMs cannot serve as plausible models of language in humans: their lack of symbolic structure and their lack of grounding. For each, a case is made that recent empirical trends undermine the common assumptions about LLMs, and thus that it is premature to draw conclusions about LLMs' ability (or lack thereof) to offer insights on human language representation and understanding. This article is part of a discussion meeting issue `Cognitive artificial intelligence'.},
  keywords = {cognitive science,language models,natural language processing}
}

@inproceedings{paxtonCombiningNeuralNetworks2017,
  title = {Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments},
  booktitle = {2017 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Paxton, Chris and Raman, Vasumathi and Hager, Gregory D. and Kobilarov, Marin},
  year = {2017},
  month = sep,
  pages = {6059--6066},
  issn = {2153-0866},
  doi = {10.1109/IROS.2017.8206505},
  abstract = {Task and motion planning subject to Linear Temporal Logic (LTL) specifications in complex, dynamic environments requires efficient exploration of many possible future worlds. Model-free reinforcement learning has proven successful in a number of challenging tasks, but shows poor performance on tasks that require long-term planning. In this work, we integrate Monte Carlo Tree Search with hierarchical neural net policies trained on expressive LTL specifications. We use reinforcement learning to find deep neural networks representing both low-level control policies and task-level ``option policies'' that achieve high-level goals. Our combined architecture generates safe and responsive motion plans that respect the LTL constraints. We demonstrate our approach in a simulated autonomous driving setting, where a vehicle must drive down a road in traffic, avoid collisions, and navigate an intersection, all while obeying rules of the road.},
  keywords = {Acceleration,Dynamics,Heuristic algorithms,Neural networks,Planning,Roads},
  file = {C:\Users\benja\Zotero\storage\ZR4833IN\8206505.html}
}

@inproceedings{paxtonRepresentingRobotTask2019,
  title = {Representing {{Robot Task Plans}} as {{Robust Logical-Dynamical Systems}}},
  booktitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Paxton, C. and Ratliff, N. and Eppner, C. and Fox, D.},
  year = {2019},
  month = nov,
  pages = {5588--5595},
  issn = {2153-0866},
  doi = {10.1109/IROS40897.2019.8967861},
  abstract = {It is difficult to create robust, reusable, and reactive behaviors for robots that can be easily extended and combined. Frameworks such as Behavior Trees are flexible but difficult to characterize, especially when designing reactions and recovery behaviors to consistently converge to a desired goal condition. We propose a framework which we call Robust Logical-Dynamical Systems (RLDS), which combines the advantages of task representations like behavior trees with theoretical guarantees on performance. RLDS can also be constructed automatically from simple sequential task plans and will still achieve robust, reactive behavior in dynamic real-world environments. In this work, we describe both our proposed framework and a case study on a simple household manipulation task, with examples for how specific pieces can be implemented to achieve robust behavior. Finally, we show how in the context of these manipulation tasks, a combination of an RLDS with planning can achieve better results under adversarial conditions.},
  keywords = {behavior trees,manipulation tasks,manipulators,mobile robots,path planning,RLDS,robot task representation,robust logical-dynamical systems,simple household manipulation task,trees (mathematics)}
}

@article{paxtonRepresentingRobotTask2019a,
  title = {Representing {{Robot Task Plans}} as {{Robust Logical-Dynamical Systems}}},
  author = {Paxton, Chris and Ratliff, Nathan and Eppner, Clemens and Fox, Dieter},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.01896 [cs]},
  eprint = {1908.01896},
  primaryclass = {cs},
  urldate = {2020-01-08},
  abstract = {It is difficult to create robust, reusable, and reactive behaviors for robots that can be easily extended and combined. Frameworks such as Behavior Trees are flexible but difficult to characterize, especially when designing reactions and recovery behaviors to consistently converge to a desired goal condition. We propose a framework which we call Robust Logical-Dynamical Systems (RLDS), which combines the advantages of task representations like behavior trees with theoretical guarantees on performance. RLDS can also be constructed automatically from simple sequential task plans and will still achieve robust, reactive behavior in dynamic real-world environments. In this work, we describe both our proposed framework and a case study on a simple household manipulation task, with examples for how specific pieces can be implemented to achieve robust behavior. Finally, we show how in the context of these manipulation tasks, a combination of an RLDS with planning can achieve better results under adversarial conditions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\TCMSZ6NJ\1908.html}
}

@article{pearlSevenToolsCausal2019,
  title = {The Seven Tools of Causal Inference, with Reflections on Machine Learning},
  author = {Pearl, Judea},
  year = {2019},
  month = feb,
  journal = {Communications of the ACM},
  volume = {62},
  number = {3},
  pages = {54--60},
  issn = {0001-0782},
  doi = {10.1145/3241036},
  urldate = {2022-05-03},
  abstract = {The kind of causal inference seen in natural human thought can be "algorithmitized" to help produce human-level machine intelligence.}
}

@article{pedersenRobotSkillsManufacturing2016,
  title = {Robot Skills for Manufacturing: {{From}} Concept to Industrial Deployment},
  shorttitle = {Robot Skills for Manufacturing},
  author = {Pedersen, Mikkel Rath and Nalpantidis, Lazaros and Andersen, Rasmus Skovgaard and Schou, Casper and B{\o}gh, Simon and Kr{\"u}ger, Volker and Madsen, Ole},
  year = {2016},
  month = feb,
  journal = {Robotics and Computer-Integrated Manufacturing},
  volume = {37},
  pages = {282--291},
  issn = {0736-5845},
  doi = {10.1016/j.rcim.2015.04.002},
  urldate = {2022-04-15},
  abstract = {Due to a general shift in manufacturing paradigm from mass production towards mass customization, reconfigurable automation technologies, such as robots, are required. However, current industrial robot solutions are notoriously difficult to program, leading to high changeover times when new products are introduced by manufacturers. In order to compete on global markets, the factories of tomorrow need complete production lines, including automation technologies that can effortlessly be reconfigured or repurposed, when the need arises. In this paper we present the concept of general, self-asserting robot skills for manufacturing. We show how a relatively small set of skills are derived from current factory worker instructions, and how these can be transferred to industrial mobile manipulators. General robot skills can not only be implemented on these robots, but also be intuitively concatenated to program the robots to perform a variety of tasks, through the use of simple task-level programming methods. We demonstrate various approaches to this, extensively tested with several people inexperienced in robotics. We validate our findings through several deployments of the complete robot system in running production facilities at an industrial partner. It follows from these experiments that the use of robot skills, and associated task-level programming framework, is a viable solution to introducing robots that can intuitively and on the fly be programmed to perform new tasks by factory workers.},
  langid = {english},
  keywords = {Automated production,Human-Robot interaction,Industrial robots,Mass customization,Robot skills},
  file = {C:\Users\benja\Zotero\storage\UDBVYCGV\S0736584515000575.html}
}

@book{Pei.2009,
  title = {Aperture Problem},
  author = {Pei, Yu-Cheng},
  year = {2009},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-1-4614-6675-8_100029},
  isbn = {978-3-540-23735-8}
}

@article{peiPlanningPurposeTaskSpecific2024,
  title = {Planning {{With Purpose}}: {{Task-Specific Trajectory Optimization}}},
  shorttitle = {Planning {{With Purpose}}},
  author = {Pei, Yinan and Ivanov, Yuri},
  year = {2024},
  month = may,
  journal = {IEEE Robotics and Automation Letters},
  volume = {9},
  number = {5},
  pages = {4551--4558},
  issn = {2377-3766},
  doi = {10.1109/LRA.2024.3381013},
  urldate = {2024-09-06},
  abstract = {In this letter, we propose an approach to trajectory planning based on the purpose of the task. For a redundant manipulator, many end effector poses in the task space can be achieved with multiple joint configurations. In planning the motion, we are free to choose the configuration that is optimal for the particular task requirement. Many previous motion planning approaches have been proposed for the sole purpose of maximizing manipulability, or minimizing effort. However, there is a lack of formulation that is flexible enough to allow the designer to purposefully define the motion and force priority of the planned trajectory. Our approach exploits both velocity and force manipulability, depending on the purpose of the task. In this formulation, the purpose of the task is defined by the motion preference (``fast'' or ``strong''), which can be characterized by a direction of the desired motion, or force. These two directions can be used to evaluate the compatibility of a chosen configuration with the given task. We first demonstrate the possibility of generating two distinct motion plans by the kinematic alignment of desired velocity and force directions with the manipulator's velocity and force manipulability ellipses. Next, this configuration selection strategy is incorporated into a task-specific trajectory optimization formulation to generate dynamically feasible trajectories. Two distinct motions (force-oriented lifting motion and velocity-oriented ballistic motion) are planned. We also propose a blending method to generate a single motion plan that considers both force and velocity, each to a specified degree. Together the three motions (force, velocity, and blended) are successfully planned and executed on a three-link serial robotic manipulator. The letter concludes with discussion and future directions.},
  keywords = {Ellipsoids,Force,Kinematics,Manipulability,Manipulators,motion planning,Planning,pose optimization,redundant manipulator,Robots,Task analysis,trajectory optimization},
  file = {C:\Users\benja\Zotero\storage\VEWGMCN5\10478143.html}
}

@article{peller-konradMemorySystemRobot2023,
  title = {A Memory System of a Robot Cognitive Architecture and Its Implementation in {{ArmarX}}},
  author = {{Peller-Konrad}, Fabian and Kartmann, Rainer and Dreher, Christian R.G. and Meixner, Andre and Reister, Fabian and Grotz, Markus and Asfour, Tamim},
  year = {2023},
  month = jun,
  journal = {Robotics and Autonomous Systems},
  volume = {164},
  number = {C},
  issn = {0921-8890},
  doi = {10.1016/j.robot.2023.104415},
  urldate = {2024-01-24},
  abstract = {Cognitive agents such as humans and robots perceive their environment through an abundance of sensors producing streams of data that need to be processed to generate intelligent behavior. A key question of cognition-enabled and AI-driven robotics is how to organize and manage such data and knowledge efficiently in a cognitive robot control architecture. We argue, that memory is a central active component of such architectures that mediates between semantic and sensorimotor representations, orchestrates the flow of data streams and events between different processes and provides the components of a cognitive architecture with data-driven services for learning semantics from sensorimotor data, the parametrization of symbolic plans for execution and prediction of action effects. Based on related work, and the experience gained in developing our ARMAR humanoid robot systems, we identified conceptual and technical requirements of a memory system as central component of cognitive robot control architecture that facilitate the realization of high-level cognitive abilities such as explaining, reasoning, prospection, simulation and augmentation. Conceptually, a memory should be active, support multi-modal data representations, associate knowledge, be introspective, and have an inherently episodic structure. Technically, the memory should support a distributed design, be access-efficient and capable of long-term data storage. We introduce the memory system for our cognitive robot control architecture and its implementation in the robot software framework ArmarX. We evaluate the efficiency of the memory system with respect to transfer speeds, compression, reproduction and prediction capabilities. {$\bullet$} Memory-based and hybrid robot cognitive architecture {$\bullet$} Memory as mediator between semantic and sensorimotor representations {$\bullet$} Conceptual and technical requirements for the memory {$\bullet$} Implementation in the robot software framework ArmarX},
  keywords = {Episodic memory,Humanoid robotics,Knowledge representation,Long-term memory,Memory-driven cognitive architecture,Working memory}
}

@inproceedings{pengTransferLearningBiomedical2019,
  title = {Transfer {{Learning}} in {{Biomedical Natural Language Processing}}: {{An Evaluation}} of {{BERT}} and {{ELMo}} on {{Ten Benchmarking Datasets}}},
  shorttitle = {Transfer {{Learning}} in {{Biomedical Natural Language Processing}}},
  booktitle = {Proceedings of the 18th {{BioNLP Workshop}} and {{Shared Task}}},
  author = {Peng, Yifan and Yan, Shankai and Lu, Zhiyong},
  year = {2019},
  month = aug,
  pages = {58--65},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/W19-5006},
  urldate = {2021-06-21},
  abstract = {Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The benchmark consists of five tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difficulties. We also evaluate several baselines based on BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes publicly available at https://github.com/ ncbi-nlp/BLUE\_Benchmark.}
}

@misc{pereiraMPCInspiredNeuralNetwork2018,
  title = {{{MPC-Inspired Neural Network Policies}} for {{Sequential Decision Making}}},
  author = {Pereira, Marcus and Fan, David D. and An, Gabriel Nakajima and Theodorou, Evangelos},
  year = {2018},
  month = mar,
  number = {arXiv:1802.05803},
  eprint = {1802.05803},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.05803},
  urldate = {2024-06-25},
  abstract = {In this paper we investigate the use of MPC-inspired neural network policies for sequential decision making. We introduce an extension to the DAgger algorithm for training such policies and show how they have improved training performance and generalization capabilities. We take advantage of this extension to show scalable and efficient training of complex planning policy architectures in continuous state and action spaces. We provide an extensive comparison of neural network policies by considering feed forward policies, recurrent policies, and recurrent policies with planning structure inspired by the Path Integral control framework. Our results suggest that MPC-type recurrent policies have better robustness to disturbances and modeling error.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\4FNC2LT8\1802.html}
}

@article{peresIndustrialArtificialIntelligence2020,
  title = {Industrial {{Artificial Intelligence}} in {{Industry}} 4.0 - {{Systematic Review}}, {{Challenges}} and {{Outlook}}},
  author = {Peres, Ricardo Silva and Jia, Xiaodong and Lee, Jay and Sun, Keyi and Colombo, Armando Walter and Barata, Jose},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {220121--220139},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3042874},
  abstract = {The advent of the Industry 4.0 initiative has made it so that manufacturing environments are becoming more and more dynamic, connected but also inherently more complex, with additional inter-dependencies, uncertainties and large volumes of data being generated. Recent advances in Industrial Artificial Intelligence have showcased the potential of this technology to assist manufacturers in tackling the challenges associated with this digital transformation of Cyber-Physical Systems, through its data-driven predictive analytics and capacity to assist decision-making in highly complex, non-linear and often multistage environments. However, the industrial adoption of such solutions is still relatively low beyond the experimental pilot stage, as real environments provide unique and difficult challenges for which organizations are still unprepared. The aim of this paper is thus two-fold. First, a systematic review of current Industrial Artificial Intelligence literature is presented, focusing on its application in real manufacturing environments to identify the main enabling technologies and core design principles. Then, a set of key challenges and opportunities to be addressed by future research efforts are formulated along with a conceptual framework to bridge the gap between research in this field and the manufacturing industry, with the goal of promoting industrial adoption through a successful transition towards a digitized and data-driven company-wide culture. This paper is among the first to provide a clear definition and holistic view of Industrial Artificial Intelligence in the Industry 4.0 landscape, identifying and analysing its fundamental building blocks and ongoing trends. Its findings are expected to assist and empower researchers and manufacturers alike to better understand the requirements and steps necessary for a successful transition into Industry 4.0 supported by AI, as well as the challenges that may arise during this process.},
  keywords = {Artificial intelligence,Decision making,digital transformation,framework,guidelines,Industries,Industry 4.0,manufacturing,Manufacturing,Robots,Service robots,systematic review,Systematics}
}

@inproceedings{Peric2017,
  title = {Exact Spike Timing Computational Model of Convolutional Associative Memories},
  booktitle = {2017 {{IEEE}} 16th {{International Conference}} on {{Cognitive Informatics}} \& {{Cognitive Computing}} ({{ICCI}}* {{CC}})},
  author = {Peric, Igor and Schneider, Felix and Price, Cameron H and Ulbrich, Stefan and Roennau, Ame and Zoellner, Marius and Dillmann, Ruediger},
  year = {2017},
  pages = {182--190},
  publisher = {IEEE}
}

@incollection{perlisForeword1996,
  title = {{Foreword}},
  booktitle = {{Structure and Interpretation of Computer Programs}},
  author = {Perlis, Alan J.},
  year = {1996},
  month = jul,
  edition = {2nd ed},
  publisher = {The MIT Press},
  address = {Cambridge, Mass.},
  abstract = {Structure and Interpretation of Computer Programs has had a dramatic impact on computer science curricula over the past decade. This long-awaited revision contains changes throughout the text. There are new implementations of most of the major programming systems in the book, including the interpreters and compilers, and the authors have incorporated many small changes that reflect their experience teaching the course at MIT since the first edition was published. A new theme has been introduced that emphasizes the central role played by different approaches to dealing with time in computational models: objects with state, concurrent programming, functional programming and lazy evaluation, and nondeterministic programming. There are new example sections on higher-order procedures in graphics and on applications of stream processing in numerical programming, and many new exercises. In addition, all the programs have been reworked to run in any Scheme implementation that adheres to the IEEE standard.},
  isbn = {978-0-262-51087-5},
  langid = {Englisch}
}

@inproceedings{pervezLearningDeepMovement2017,
  title = {Learning Deep Movement Primitives Using Convolutional Neural Networks},
  booktitle = {Humanoids},
  author = {Pervez, Affan and Mao, Yuecheng and Lee, Dongheui},
  year = {2017},
  month = nov,
  pages = {191--197},
  issn = {2164-0580},
  doi = {10.1109/HUMANOIDS.2017.8246874},
  abstract = {Dynamic Movement Primitives (DMPs) are widely used for encoding motion data. Task parameterized DMP (TP-DMP) can adapt a learned skill to different situations. Mostly a customized vision system is used to extract task specific variables. This limits the use of such systems to real world scenarios. This paper proposes a method for combining the DMP with a Convolutional Neural Network (CNN). Our approach preserves the generalization properties associated with a DMP, while the CNN learns the task specific features from the camera images. This eliminates the need to extract the task parameters, by directly utilizing the camera image during the motion reproduction. The performance of the developed approach is demonstrated through a trash cleaning task, executed with a real robot. We also show that by using the data augmentation, the learned sweeping skill can be generalized for arbitrary objects. The experiments show the robustness of our approach for several different settings.},
  keywords = {camera image,Cameras,Clocks,CNN,Convolution,Convolutional Neural Network,convolutional neural networks,customized vision system,data augmentation,deep movement primitives,Dynamic Movement Primitives,Feature extraction,feedforward neural nets,generalisation (artificial intelligence),generalization properties,image motion analysis,learned skill,learned sweeping skill,learning (artificial intelligence),motion data,motion reproduction,Robot vision systems,task parameters,task specific variables,TP-DMP,trash cleaning task},
  file = {C:\Users\benja\Zotero\storage\SWPWRD6G\8246874.html}
}

@article{pervezLearningTaskparameterizedDynamic2018,
  title = {Learning Task-Parameterized Dynamic Movement Primitives Using Mixture of {{GMMs}}},
  author = {Pervez, Affan and Lee, Dongheui},
  year = {2018},
  month = jan,
  journal = {Intelligent Service Robotics},
  volume = {11},
  number = {1},
  pages = {61--78},
  issn = {1861-2784},
  doi = {10.1007/s11370-017-0235-8},
  urldate = {2024-03-08},
  abstract = {Task-parameterized skill learning aims at adaptive motion encoding to new situations. While existing approaches for task-parameterized skill learning have demonstrated good adaptation within the demonstrated region, the extrapolation problem of task-parameterized skills has not been investigated enough. In this work, with the aim of good adaptation not only within the demonstrated region but also outside of the region, we propose to combine a generative model with a dynamic movement primitive by formulating learning as a density estimation problem. Moreover, for efficient learning from relatively few demonstrations, we propose to augment training data with additional incomplete data. The proposed method is tested and compared with existing works in simulations and real robot experiments. Experimental results verified its generalization in the extrapolation region.},
  langid = {english},
  keywords = {Dynamic movement primitives,Programming by demonstration,Task-parameterized movement}
}

@inproceedings{perzyloIntuitiveInstructionIndustrial2016,
  title = {Intuitive Instruction of Industrial Robots: {{Semantic}} Process Descriptions for Small Lot Production},
  shorttitle = {Intuitive Instruction of Industrial Robots},
  booktitle = {2016 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Perzylo, Alexander and Somani, Nikhil and Profanter, Stefan and Kessler, Ingmar and Rickert, Markus and Knoll, Alois},
  year = {2016},
  month = oct,
  pages = {2293--2300},
  issn = {2153-0866},
  doi = {10.1109/IROS.2016.7759358},
  abstract = {In this paper, we introduce a novel robot programming paradigm. It focuses on reducing the required expertise in robotics to a level that allows shop floor workers to use robots in their application domain without the need of extensive training. Our approach is user-centric and can interpret underspecified robot tasks, enabling communication on an abstract level. Such high-level task descriptions make the system amenable for users that are experts in a particular domain, but have limited knowledge about robotics and are thus not able to specify low-level details and instructions. Semantic models for all involved entities, i.e., processes, workpieces, and workcells, enable automatic reasoning about underspecified tasks and missing pieces of information. We showcase and evaluate this methodology on two industrial use cases from the domains of assembly and woodworking, comparing it to state-of-the-art solutions provided by robot manufacturers.},
  keywords = {Education,Programming,Robot kinematics,Semantics,Service robots,Solid modeling}
}

@inproceedings{perzyloOntologyCADData2015,
  title = {An Ontology for {{CAD}} Data and Geometric Constraints as a Link between Product Models and Semantic Robot Task Descriptions},
  booktitle = {2015 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Perzylo, Alexander and Somani, Nikhil and Rickert, Markus and Knoll, Alois},
  year = {2015},
  month = sep,
  pages = {4197--4203},
  doi = {10.1109/IROS.2015.7353971},
  abstract = {In this paper, we introduce an approach for leveraging CAD description to a semantic level, in order to link additional knowledge to CAD models and to exploit resulting synergy effects. This has been achieved by designing a description language, based on the Web Ontology Language (OWL), that is used to define boundary representations (BREP) of objects. This involves representing geometric entities in a semantic meaningful way, e.g., a circle is defined by a coordinate frame and a radius instead of a set of polygons. Furthermore, the scope of this semantic description language also covers geometric constraints between multiple objects. Constraints can be specified not only on the object level, but down to single edges or faces of an object. This semantic representation is used to improve a variety of applications, ranging from shape-based object recognition to constraint-based robot task descriptions. Results from a quantitative evaluation are presented to assess the practicability of this approach.},
  keywords = {Design automation,Mathematical model,Ontologies,OWL,Semantics,Solid modeling,Wires},
  file = {C:\Users\benja\Zotero\storage\V5JNHBB5\7353971.html}
}

@article{perzyloOntologyCADData2015a,
  title = {An Ontology for {{CAD}} Data and Geometric Constraints as a Link between Product Models and Semantic Robot Task Descriptions},
  author = {Perzylo, A. and Somani, N. and Rickert, Markus and Knoll, A.},
  year = {2015},
  journal = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS.2015.7353971},
  abstract = {This paper introduces an approach for leveraging CAD description to a semantic level, in order to link additional knowledge to CAD models and to exploit resulting synergy effects, by designing a description language based on the Web Ontology Language that is used to define boundary representations of objects. In this paper, we introduce an approach for leveraging CAD description to a semantic level, in order to link additional knowledge to CAD models and to exploit resulting synergy effects. This has been achieved by designing a description language, based on the Web Ontology Language (OWL), that is used to define boundary representations (BREP) of objects. This involves representing geometric entities in a semantic meaningful way, e.g., a circle is defined by a coordinate frame and a radius instead of a set of polygons. Furthermore, the scope of this semantic description language also covers geometric constraints between multiple objects. Constraints can be specified not only on the object level, but down to single edges or faces of an object. This semantic representation is used to improve a variety of applications, ranging from shape-based object recognition to constraint-based robot task descriptions. Results from a quantitative evaluation are presented to assess the practicability of this approach.}
}

@misc{petrovUniversalInContextApproximation2024,
  title = {Universal {{In-Context Approximation By Prompting Fully Recurrent Models}}},
  author = {Petrov, Aleksandar and Lamb, Tom A. and Paren, Alasdair and Torr, Philip H. S. and Bibi, Adel},
  year = {2024},
  month = jun,
  number = {arXiv:2406.01424},
  eprint = {2406.01424},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.01424},
  urldate = {2024-09-20},
  abstract = {Zero-shot and in-context learning enable solving tasks without model fine-tuning, making them essential for developing generative model solutions. Therefore, it is crucial to understand whether a pretrained model can be prompted to approximate any function, i.e., whether it is a universal in-context approximator. While it was recently shown that transformer models do possess this property, these results rely on their attention mechanism. Hence, these findings do not apply to fully recurrent architectures like RNNs, LSTMs, and the increasingly popular SSMs. We demonstrate that RNNs, LSTMs, GRUs, Linear RNNs, and linear gated architectures such as Mamba and Hawk/Griffin can also serve as universal in-context approximators. To streamline our argument, we introduce a programming language called LSRL that compiles to these fully recurrent architectures. LSRL may be of independent interest for further studies of fully recurrent models, such as constructing interpretability benchmarks. We also study the role of multiplicative gating and observe that architectures incorporating such gating (e.g., LSTMs, GRUs, Hawk/Griffin) can implement certain operations more stably, making them more viable candidates for practical in-context universal approximation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\benja\\Zotero\\storage\\WPX8GZNB\\Petrov et al. - 2024 - Universal In-Context Approximation By Prompting Fully Recurrent Models.pdf;C\:\\Users\\benja\\Zotero\\storage\\MDTAVSXE\\2406.html}
}

@article{petrucciExpressiveOntologyLearning2018,
  title = {Expressive Ontology Learning as Neural Machine Translation},
  author = {Petrucci, Giulio and Rospocher, Marco and Ghidini, Chiara},
  year = {2018},
  month = oct,
  journal = {Journal of Web Semantics},
  volume = {52--53},
  pages = {66--82},
  issn = {1570-8268},
  doi = {10.1016/j.websem.2018.10.002},
  urldate = {2024-07-21},
  abstract = {Automated ontology learning from unstructured textual sources has been proposed in literature as a way to support the difficult and time-consuming task of knowledge modeling for semantic applications. In this paper we propose a system, based on a neural network in the encoder--decoder configuration, to translate natural language definitions into Description Logics formul{\ae} through syntactic transformation. The model has been evaluated to assess its capacity to generalize over different syntactic structures, tolerate unknown words, and improve its performance by enriching the training set with new annotated examples. The results obtained in our evaluation show how approaching the ontology learning problem as a neural machine translation task can be a valid way to tackle long term expressive ontology learning challenges such as language variability, domain independence, and high engineering costs.},
  keywords = {Natural language processing,Neural networks,Ontology learning},
  file = {C:\Users\benja\Zotero\storage\QAA3ZC4D\S1570826818300507.html}
}

@inproceedings{petrucciOntologyLearningDeep2016,
  title = {Ontology {{Learning}} in the {{Deep}}},
  booktitle = {Knowledge {{Engineering}} and {{Knowledge Management}}},
  author = {Petrucci, Giulio and Ghidini, Chiara and Rospocher, Marco},
  editor = {Blomqvist, Eva and Ciancarini, Paolo and Poggi, Francesco and Vitali, Fabio},
  year = {2016},
  pages = {480--495},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-49004-5_31},
  abstract = {Recent developments in the area of deep learning have been proved extremely beneficial for several natural language processing tasks, such as sentiment analysis, question answering, and machine translation. In this paper we exploit such advances by tailoring the ontology learning problem as a transductive reasoning task that learns to convert knowledge from natural language to a logic-based specification. More precisely, using a sample of definitory sentences generated starting by a synthetic grammar, we trained Recurrent Neural Network (RNN) based architectures to extract OWL formulae from text. In addition to the low feature engineering costs, our system shows good generalisation capabilities over the lexicon and the syntactic structure. The encouraging results obtained in the paper provide a first evidence of the potential of deep learning techniques towards long term ontology learning challenges such as improving domain independence, reducing engineering costs, and dealing with variable language forms.},
  isbn = {978-3-319-49004-5},
  langid = {english},
  keywords = {Cardinality Restriction,Function Word,Ontology Learning,Recurrent Neural Network,Statistical Machine Translation}
}

@mastersthesis{pfeiferOnlinefaehigeErreichbarkeitsanalyseFuer2019,
  title = {{Onlinef{\"a}hige Erreichbarkeitsanalyse f{\"u}r serielle Kinematiken}},
  author = {Pfeifer, Rainer},
  year = {2019},
  month = apr,
  address = {Karlsruhe, Germany},
  langid = {ngerman},
  school = {Karlsruhe Institute of Technology}
}

@article{pfeifferLearningSoftTissue2019,
  title = {Learning Soft Tissue Behavior of Organs for Surgical Navigation with Convolutional Neural Networks},
  author = {Pfeiffer, Micha and Riediger, Carina and Weitz, J{\"u}rgen and Speidel, Stefanie},
  year = {2019},
  month = jul,
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  volume = {14},
  number = {7},
  pages = {1147--1155},
  issn = {1861-6429},
  doi = {10.1007/s11548-019-01965-7},
  urldate = {2024-04-12},
  abstract = {In surgical navigation, pre-operative organ models are presented to surgeons during the intervention to help them in efficiently finding their target. In the case of soft tissue, these models need to be deformed and adapted to the current situation by using intra-operative sensor data. A promising method to realize this are real-time capable biomechanical models.},
  langid = {english},
  keywords = {Biomechanical model,Convolutional neural network,Organ deformation,Soft tissue,Surgical navigation}
}

@phdthesis{picklumProbabilisticActionProspection2024,
  title = {Probabilistic Action Prospection Based on Experiences - Representation, Learning and Reasoning in Autonomous Robotic Agents},
  author = {Picklum, Mareike},
  year = {2024},
  doi = {10.26092/elib/2990},
  urldate = {2024-09-21},
  abstract = {Probabilistic action prospection based on experiences - representation, learning and reasoning in autonomous robotic agents, Picklum, Mareike, 2024, Universit{\"a}t Bremen. Copyright by the author unless stated otherwise.},
  langid = {english},
  school = {Universit{\"a}t Bremen},
  file = {C:\Users\benja\Zotero\storage\AKHZNKVX\record.html}
}

@article{piersonDeepLearningRobotics2017,
  title = {Deep Learning in Robotics: A Review of Recent Research},
  shorttitle = {Deep Learning in Robotics},
  author = {Pierson, Harry A. and Gashler, Michael S.},
  year = {2017},
  month = aug,
  journal = {Advanced Robotics},
  volume = {31},
  number = {16},
  pages = {821--835},
  publisher = {Taylor \& Francis},
  issn = {0169-1864},
  doi = {10.1080/01691864.2017.1365009},
  urldate = {2024-04-27},
  abstract = {Advances in deep learning over the last decade have led to a flurry of research in the application of deep artificial neural networks to robotic systems, with at least 30 papers published on the subject between 2014 and the present. This review discusses the applications, benefits, and limitations of deep learning vis-{\`a}-vis physical robotic systems, using contemporary research as exemplars. It is intended to communicate recent advances to the wider robotics community and inspire additional interest in and application of deep learning in robotics.},
  keywords = {artificial intelligence,Deep neural networks,human-robot interaction}
}

@article{piginiServiceRobotsElderly2012,
  title = {Service Robots in Elderly Care at Home: {{Users}}' Needs and Perceptions as a Basis for Concept Development},
  shorttitle = {Service Robots in Elderly Care at Home},
  author = {Pigini, Lucia and Facal, David and Blasi, Lorenzo and Andrich, Renzo},
  year = {2012},
  journal = {Technology and Disability},
  volume = {24},
  number = {4},
  pages = {303--311},
  publisher = {IOS Press},
  issn = {1055-4181},
  doi = {10.3233/TAD-120361},
  urldate = {2021-02-05},
  langid = {english}
}

@article{pillaiAdoptionAIempoweredIndustrial2021,
  title = {Adoption of {{AI-empowered}} Industrial Robots in Auto Component Manufacturing Companies},
  author = {Pillai, Rajasshrie and Sivathanu, Brijesh and Mariani, Marcello and Rana, Nripendra P. and Yang, Bai and Dwivedi, Yogesh K.},
  year = {2021},
  month = feb,
  journal = {Production Planning \& Control},
  volume = {0},
  number = {0},
  pages = {1--17},
  publisher = {Taylor \& Francis},
  issn = {0953-7287},
  doi = {10.1080/09537287.2021.1882689},
  urldate = {2022-05-06},
  abstract = {The usage of AI-empowered Industrial Robots (InRos) is booming in the Auto Component Manufacturing Companies (ACMCs) across the globe. Based on a model leveraging the Technology, Organisation, and Environment (TOE) framework, this work examines the adoption of InRos in ACMCs in the context of an emerging economy. This research scrutinises the adoption intention and potential use of InRos in ACMCs through a survey of 460 senior managers and owners of ACMCs in India. The findings indicate that perceived compatibility, external pressure, perceived benefits and support from vendors are critical predictors of InRos adoption intention. Interestingly, the study also reveals that IT infrastructure and government support do not influence InRos adoption intention. Furthermore, the analysis suggests that perceived cost issues negatively moderate the relationship between the adoption intention and potential use of InRos in ACMCs. This study offers a theoretical contribution as it deploys the traditional TOE framework and discovers counter-intuitively that IT resources are not a major driver of technology adoption: as such, it suggests that a more comprehensive framework than the traditional RBV should be adopted. The work provides managerial recommendations for managers, shedding light on the antecedents of adoption intention and potential use of InRos at ACMCs in a country where the adoption of InRos is in a nascent stage.},
  keywords = {adoption,auto component manufacturing,Industrial robots,potential use,TOE},
  file = {C:\Users\benja\Zotero\storage\V3VKASTT\09537287.2021.html}
}

@misc{pillonettoDeepNetworksSystem2023,
  title = {Deep Networks for System Identification: A {{Survey}}},
  shorttitle = {Deep Networks for System Identification},
  author = {Pillonetto, Gianluigi and Aravkin, Aleksandr and Gedon, Daniel and Ljung, Lennart and Ribeiro, Ant{\^o}nio H. and Sch{\"o}n, Thomas B.},
  year = {2023},
  month = jan,
  number = {arXiv:2301.12832},
  eprint = {2301.12832},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.12832},
  urldate = {2024-01-08},
  abstract = {Deep learning is a topic of considerable current interest. The availability of massive data collections and powerful software resources has led to an impressive amount of results in many application areas that reveal essential but hidden properties of the observations. System identification learns mathematical descriptions of dynamic systems from input-output data and can thus benefit from the advances of deep neural networks to enrich the possible range of models to choose from. For this reason, we provide a survey of deep learning from a system identification perspective. We cover a wide spectrum of topics to enable researchers to understand the methods, providing rigorous practical and theoretical insights into the benefits and challenges of using them. The main aim of the identified model is to predict new data from previous observations. This can be achieved with different deep learning based modelling techniques and we discuss architectures commonly adopted in the literature, like feedforward, convolutional, and recurrent networks. Their parameters have to be estimated from past data trying to optimize the prediction performance. For this purpose, we discuss a specific set of first-order optimization tools that is emerged as efficient. The survey then draws connections to the well-studied area of kernel-based methods. They control the data fit by regularization terms that penalize models not in line with prior assumptions. We illustrate how to cast them in deep architectures to obtain deep kernel-based methods. The success of deep learning also resulted in surprising empirical observations, like the counter-intuitive behaviour of models with many parameters. We discuss the role of overparameterized models, including their connection to kernels, as well as implicit regularization mechanisms which affect generalization, specifically the interesting phenomena of benign overfitting ...},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control},
  file = {C:\Users\benja\Zotero\storage\8X847FUQ\2301.html}
}

@misc{pingInteractiveTeachingConversational2020,
  title = {Interactive {{Teaching}} for {{Conversational AI}}},
  author = {Ping, Qing and Niu, Feiyang and Thattai, Govind and Chengottusseriyil, Joel and Gao, Qiaozi and Reganti, Aishwarya and Rajagopal, Prashanth and Tur, Gokhan and {Hakkani-Tur}, Dilek and Nataraja, Prem},
  year = {2020},
  month = dec,
  number = {arXiv:2012.00958},
  eprint = {2012.00958},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.00958},
  urldate = {2024-01-10},
  abstract = {Current conversational AI systems aim to understand a set of pre-designed requests and execute related actions, which limits them to evolve naturally and adapt based on human interactions. Motivated by how children learn their first language interacting with adults, this paper describes a new Teachable AI system that is capable of learning new language nuggets called concepts, directly from end users using live interactive teaching sessions. The proposed setup uses three models to: a) Identify gaps in understanding automatically during live conversational interactions, b) Learn the respective interpretations of such unknown concepts from live interactions with users, and c) Manage a classroom sub-dialogue specifically tailored for interactive teaching sessions. We propose state-of-the-art transformer based neural architectures of models, fine-tuned on top of pre-trained models, and show accuracy improvements on the respective components. We demonstrate that this method is very promising in leading way to build more adaptive and personalized language understanding models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\benja\Zotero\storage\THMRH8RM\2012.html}
}

@inproceedings{pinslerSampleFeedbackEfficient2018,
  title = {Sample and {{Feedback Efficient Hierarchical Reinforcement Learning}} from {{Human Preferences}}},
  booktitle = {{{ICRA}}},
  author = {Pinsler, Robert and Akrour, Riad and Osa, Takayuki and Peters, Jan and Neumann, Gerhard},
  year = {2018},
  month = may,
  pages = {596--601},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2018.8460907},
  abstract = {While reinforcement learning has led to promising results in robotics, defining an informative reward function is challenging. Prior work considered including the human in the loop to jointly learn the reward function and the optimal policy. Generating samples from a physical robot and requesting human feedback are both taxing efforts for which efficiency is critical. We propose to learn reward functions from both the robot and the human perspectives to improve on both efficiency metrics. Learning a reward function from the human perspective increases feedback efficiency by assuming that humans rank trajectories according to a low-dimensional outcome space. Learning a reward function from the robot perspective circumvents the need for a dynamics model while retaining the sample efficiency of model-based approaches. We provide an algorithm that incorporates bi-perspective reward learning into a general hierarchical reinforcement learning framework and demonstrate the merits of our approach on a toy task and a simulated robot grasping task.},
  keywords = {bi-perspective reward learning,Context modeling,Customer relationship management,feedback efficiency,general hierarchical reinforcement learning framework,Grasping,human feedback,human preferences,human-robot interaction,informative reward function,learning (artificial intelligence),Learning (artificial intelligence),physical robot,reward function,robot perspective,Robots,simulated robot grasping task,Task analysis,Trajectory},
  file = {C:\Users\benja\Zotero\storage\Q92EG6XX\8460907.html}
}

@inproceedings{pitisFailureModesLearning2023,
  title = {Failure {{Modes}} of {{Learning Reward Models}} for {{LLMs}} and Other {{Sequence Models}}},
  booktitle = {{{ICML}} 2023 {{Workshop The Many Facets}} of {{Preference-Based Learning}}},
  author = {Pitis, Silviu},
  year = {2023},
  month = oct,
  urldate = {2024-05-02},
  abstract = {To align large language models (LLMs) and other sequence-based models with human values, we typically assume that human preferences can be well represented using a "reward model". We infer the parameters of this reward model from data, and then train our models to maximize reward. Effective alignment with this approach relies on a strong reward model, and reward modeling becomes increasingly important as the dominion of deployed models grows. Yet in practice, we often assume the existence of a particular reward model, without regard to its potential shortcomings. In this preliminary work, I survey several failure modes of learned reward models, which may be organized into three broad categories: model misspecification, ambiguous preferences, and reward misgeneralization. Several avenues for future work are identified. It is likely that I have missed several points and related works; to that end, I greatly appreciate your correspondence.},
  langid = {english}
}

@article{Pitti2013a,
  title = {Neural Model for Learning-to-Learn of Novel Task Sets in the Motor Domain.},
  author = {Pitti, Alexandre and Braud, Rapha{\"e}l and Mah{\'e}, Sylvain and Quoy, Mathias and Gaussier, Philippe},
  year = {2013},
  journal = {Frontiers in psychology},
  volume = {4},
  eprint = {24155736},
  eprinttype = {pubmed},
  pages = {771},
  publisher = {Frontiers Media SA},
  doi = {10.3389/fpsyg.2013.00771},
  urldate = {2018-05-10},
  abstract = {During development, infants learn to differentiate their motor behaviors relative to various contexts by exploring and identifying the correct structures of causes and effects that they can perform; these structures of actions are called task sets or internal models. The ability to detect the structure of new actions, to learn them and to select on the fly the proper one given the current task set is one great leap in infants cognition. This behavior is an important component of the child's ability of learning-to-learn, a mechanism akin to the one of intrinsic motivation that is argued to drive cognitive development. Accordingly, we propose to model a dual system based on (1) the learning of new task sets and on (2) their evaluation relative to their uncertainty and prediction error. The architecture is designed as a two-level-based neural system for context-dependent behavior (the first system) and task exploration and exploitation (the second system). In our model, the task sets are learned separately by reinforcement learning in the first network after their evaluation and selection in the second one. We perform two different experimental setups to show the sensorimotor mapping and switching between tasks, a first one in a neural simulation for modeling cognitive tasks and a second one with an arm-robot for motor task learning and switching. We show that the interplay of several intrinsic mechanisms drive the rapid formation of the neural populations with respect to novel task sets.},
  pmid = {24155736}
}

@article{pittinoAutomaticAnomalyDetection2020,
  title = {Automatic {{Anomaly Detection}} on {{In-Production Manufacturing Machines Using Statistical Learning Methods}}},
  author = {Pittino, Federico and Puggl, Michael and Moldaschl, Thomas and Hirschl, Christina},
  year = {2020},
  month = jan,
  journal = {Sensors},
  volume = {20},
  number = {8},
  pages = {2344},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/s20082344},
  urldate = {2021-04-14},
  abstract = {Anomaly detection is becoming increasingly important to enhance reliability and resiliency in the Industry 4.0 framework. In this work, we investigate different methods for anomaly detection on in-production manufacturing machines taking into account their variability, both in operation and in wear conditions. We demonstrate how the nature of the available data, featuring any anomaly or not, is of importance for the algorithmic choice, discussing both statistical machine learning methods and control charts. We finally develop methods for automatic anomaly detection, which obtain a recall close to one on our data. Our developed methods are designed not to rely on a continuous recalibration and hand-tuning by the machine user, thereby allowing their deployment in an in-production environment robustly and efficiently.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {anomaly detection,in-production data,statistical machine learning}
}

@incollection{pittMentalRepresentation2022,
  title = {Mental {{Representation}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Pitt, David},
  editor = {Zalta, Edward N. and Nodelman, Uri},
  year = {2022},
  edition = {Fall 2022},
  publisher = {Metaphysics Research Lab, Stanford University},
  urldate = {2024-04-17},
  abstract = {The notion of a ``mental representation'' is, arguably, inthe first instance a theoretical construct of cognitive science. Assuch, it is a basic concept of the Computational Theory of Mind,according to which cognitive states and processes are constituted bythe occurrence, transformation and storage (in the mind/brain) ofinformation-bearing structures (representations) of one kind oranother.},
  keywords = {artificial intelligence: logic-based,cognition: embodied,cognitive science,concepts,connectionism,consciousness: and intentionality,consciousness: representational theories of,externalism about the mind,folk psychology: as mental simulation,information: semantic conceptions of,intentionality,intentionality: phenomenal,language of thought hypothesis,materialism: eliminative,mental content: causal theories of,mental content: narrow,mental content: teleological theories of,mental imagery,mental representation: in medieval philosophy,mind: computational theory of,neuroscience philosophy of,perception: the contents of,perception: the problem of,qualia,reference},
  file = {C:\Users\benja\Zotero\storage\8WGR7KDD\mental-representation.html}
}

@misc{PlantSimulationThroughput2020,
  title = {Plant {{Simulation}} and {{Throughput Optimization}}},
  year = {2020},
  month = may,
  journal = {Siemens Digital Industries Software},
  urldate = {2020-05-12},
  abstract = {Plant Simulation and Throughput Optimization},
  howpublished = {https://www.plm.automation.siemens.com/global/en/products/manufacturing-planning/plant-simulation-throughput-optimization.html},
  file = {C:\Users\benja\Zotero\storage\WR5IB42W\plant-simulation-throughput-optimization.html}
}

@techreport{plattformindustrie4.02019ProgressReport2019,
  title = {2019 {{Progress Report}}: {{Shaping Industrie}} 4.0},
  author = {{Plattform Industrie 4.0}},
  year = {2019},
  institution = {{Federal Ministry for Economic Affairs and Energy (BMWi)}},
  urldate = {2021-12-20},
  file = {C:\Users\benja\Zotero\storage\5XS3JBT6\2019-progress-report.pdf}
}

@misc{plestedDeepTransferLearning2022,
  title = {Deep Transfer Learning for Image Classification: A Survey},
  shorttitle = {Deep Transfer Learning for Image Classification},
  author = {Plested, Jo and Gedeon, Tom},
  year = {2022},
  month = may,
  number = {arXiv:2205.09904},
  eprint = {2205.09904},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.09904},
  urldate = {2024-06-14},
  abstract = {Deep neural networks such as convolutional neural networks (CNNs) and transformers have achieved many successes in image classification in recent years. It has been consistently demonstrated that best practice for image classification is when large deep models can be trained on abundant labelled data. However there are many real world scenarios where the requirement for large amounts of training data to get the best performance cannot be met. In these scenarios transfer learning can help improve performance. To date there have been no surveys that comprehensively review deep transfer learning as it relates to image classification overall. However, several recent general surveys of deep transfer learning and ones that relate to particular specialised target image classification tasks have been published. We believe it is important for the future progress in the field that all current knowledge is collated and the overarching patterns analysed and discussed. In this survey we formally define deep transfer learning and the problem it attempts to solve in relation to image classification. We survey the current state of the field and identify where recent progress has been made. We show where the gaps in current knowledge are and make suggestions for how to progress the field to fill in these knowledge gaps. We present a new taxonomy of the applications of transfer learning for image classification. This taxonomy makes it easier to see overarching patterns of where transfer learning has been effective and, where it has failed to fulfill its potential. This also allows us to suggest where the problems lie and how it could be used more effectively. We show that under this new taxonomy, many of the applications where transfer learning has been shown to be ineffective or even hinder performance are to be expected when taking into account the source and target datasets and the techniques used.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\benja\Zotero\storage\TTGITDWL\2205.html}
}

@inproceedings{pogancicDifferentiationBlackboxCombinatorial2020,
  title = {Differentiation of {{Blackbox Combinatorial Solvers}}},
  booktitle = {Eighth {{International Conference}} on {{Learning Representations}}},
  author = {Pogan{\v c}i{\'c}, Marin Vlastelica and Paulus, Anselm and Musil, Vit and Martius, Georg and Rolinek, Michal},
  year = {2020},
  month = apr,
  urldate = {2024-06-26},
  abstract = {Achieving fusion of deep learning with combinatorial algorithms promises transformative changes to artificial intelligence. One possible approach is to introduce combinatorial building blocks into neural networks. Such end-to-end architectures have the potential to tackle combinatorial problems on raw input data such as ensuring global consistency in multi-object tracking or route planning on maps in robotics. In this work, we present a method that implements an efficient backward pass through blackbox implementations of combinatorial solvers with linear objective functions. We provide both theoretical and experimental backing. In particular, we incorporate the Gurobi MIP solver, Blossom V algorithm, and Dijkstra's algorithm into architectures that extract suitable features from raw inputs for the traveling salesman problem, the min-cost perfect matching problem and the shortest path problem.},
  langid = {english}
}

@inproceedings{pogodinBiologicallyPlausibleConvolutional2021,
  title = {Towards {{Biologically Plausible Convolutional Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pogodin, Roman and Mehta, Yash and Lillicrap, Timothy and Latham, Peter E},
  year = {2021},
  volume = {34},
  pages = {13924--13936},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-04-19},
  abstract = {Convolutional networks are ubiquitous in deep learning. They are particularly useful for images, as they reduce the number of parameters, reduce training time, and increase accuracy. However, as a model of the brain they are seriously problematic, since they require weight sharing - something real neurons simply cannot do. Consequently, while neurons in the brain can be locally connected (one of the features of convolutional networks), they cannot be convolutional. Locally connected but non-convolutional networks, however, significantly underperform convolutional ones. This is troublesome for studies that use convolutional networks to explain activity in the visual system. Here we study plausible alternatives to weight sharing that aim at the same regularization principle, which is to make each neuron within a pool react similarly to identical inputs. The most natural way to do that is by showing the network multiple translations of the same image, akin to saccades in animal vision. However, this approach requires many translations, and doesn't remove the performance gap. We propose instead to add lateral connectivity to a locally connected network, and allow learning via Hebbian plasticity. This requires the network to pause occasionally for a sleep-like phase of "weight sharing". This method enables locally connected networks to achieve nearly convolutional performance on ImageNet and improves their fit to the ventral stream data, thus supporting convolutional networks as a model of the visual stream.}
}

@misc{poldrackAIassistedCodingExperiments2023,
  title = {{{AI-assisted}} Coding: {{Experiments}} with {{GPT-4}}},
  shorttitle = {{{AI-assisted}} Coding},
  author = {Poldrack, Russell A. and Lu, Thomas and Begu{\v s}, Ga{\v s}per},
  year = {2023},
  month = apr,
  number = {arXiv:2304.13187},
  eprint = {2304.13187},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.13187},
  urldate = {2024-08-26},
  abstract = {Artificial intelligence (AI) tools based on large language models have acheived human-level performance on some computer programming tasks. We report several experiments using GPT-4 to generate computer code. These experiments demonstrate that AI code generation using the current generation of tools, while powerful, requires substantial human validation to ensure accurate performance. We also demonstrate that GPT-4 refactoring of existing code can significantly improve that code along several established metrics for code quality, and we show that GPT-4 can generate tests with substantial coverage, but that many of the tests fail when applied to the associated code. These findings suggest that while AI coding tools are very powerful, they still require humans in the loop to ensure validity and accuracy of the results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {C\:\\Users\\benja\\Zotero\\storage\\LL993KQU\\Poldrack et al. - 2023 - AI-assisted coding Experiments with GPT-4.pdf;C\:\\Users\\benja\\Zotero\\storage\\6QE5HWZN\\2304.html}
}

@incollection{pooleRobotLanguages1989,
  title = {Robot {{Languages}}},
  booktitle = {Fundamentals of {{Robotics Engineering}}},
  author = {Poole, Harry H.},
  editor = {Poole, Harry H.},
  year = {1989},
  pages = {249--270},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-011-7050-5_10},
  urldate = {2024-04-15},
  abstract = {This chapter starts with a historical review of robot languages and then summarizes 28 available languages. It gives examples of robot commands and an overview of robot language approaches and limitations. Because so many languages are described, we make no attempt to supply enough information to actually use the language. (This information can only be obtained by attending special courses offered by the manufacturer and/or studying the relevant programming manuals.) The languages are not equally important---some never left the research laboratory. However, taken in their entirety, they give us an understanding of the needs of robotic systems and the approaches for satisfying those needs.},
  isbn = {978-94-011-7050-5},
  langid = {english}
}

@misc{PoseEstimationTensorFlow,
  title = {Pose Estimation {\textbar} {{TensorFlow Lite}}},
  journal = {TensorFlow},
  urldate = {2021-01-26},
  howpublished = {https://www.tensorflow.org/lite/models/pose\_estimation/overview},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\V945FA8K\overview.html}
}

@article{powerKeepItSimple2021,
  title = {Keep It {{Simple}}: {{Data-efficient Learning}} for {{Controlling Complex Systems}} with {{Simple Models}}},
  shorttitle = {Keep It {{Simple}}},
  author = {Power, Thomas and Berenson, Dmitry},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.02493 [cs]},
  eprint = {2102.02493},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {When manipulating a novel object with complex dynamics, a state representation is not always available, for example for deformable objects. Learning both a representation and dynamics from observations requires large amounts of data. We propose Learned Visual Similarity Predictive Control (LVSPC), a novel method for data-efficient learning to control systems with complex dynamics and high-dimensional state spaces from images. LVSPC leverages a given simple model approximation from which image observations can be generated. We use these images to train a perception model that estimates the simple model state from observations of the complex system online. We then use data from the complex system to fit the parameters of the simple model and learn where this model is inaccurate, also online. Finally, we use Model Predictive Control and bias the controller away from regions where the simple model is inaccurate and thus where the controller is less reliable. We evaluate LVSPC on two tasks; manipulating a tethered mass and a rope. We find that our method performs comparably to state-of-the-art reinforcement learning methods with an order of magnitude less data. LVSPC also completes the rope manipulation task on a real robot with 80\% success rate after only 10 trials, despite using a perception system trained only on images from simulation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  annotation = {recommend},
  file = {C:\Users\benja\Zotero\storage\GJI9PSTF\2102.html}
}

@inproceedings{prakashPracticalFirstOrderBayesian2024,
  title = {Practical {{First-Order Bayesian Optimization Algorithms}}},
  booktitle = {Proceedings of the 7th {{Joint International Conference}} on {{Data Science}} \& {{Management}} of {{Data}} (11th {{ACM IKDD CODS}} and 29th {{COMAD}})},
  author = {Prakash, Utkarsh and Chollera, Aryan and Khatwani, Kushagra and K. J., Prabuchandran and Bodas, Tejas},
  year = {2024},
  month = jan,
  series = {{{CODS-COMAD}} '24},
  pages = {173--181},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3632410.3632418},
  urldate = {2024-05-25},
  abstract = {First Order Bayesian Optimization (FOBO) is a sample efficient sequential approach to find the global maxima of an expensive-to-evaluate black-box objective function by suitably querying for the function and its gradient evaluations. Such methods assume Gaussian process (GP) models for both, the function and its gradient, and use them to construct an acquisition function that identifies the next query point. In this paper, we propose a class of practical FOBO algorithms that efficiently utilizes the information from the gradient GP to identify potential query points with zero gradients. We construct a multi-level acquisition function where in the first step, we optimize a lower level acquisition function with multiple restarts to identify potential query points with zero gradient value. We then use the upper level acquisition function to rank these query points based on their function values to potentially identify the global maximum. As a final step, the potential point of maximum is chosen as the actual query point. We validate the performance of our proposed algorithms on several test functions and show that our algorithms outperform state-of-the-art FOBO algorithms. We also illustrate the application of our algorithms in finding optimal set of hyper-parameters in machine learning and in learning the optimal policy in reinforcement learning tasks.},
  isbn = {9798400716348},
  keywords = {Bayesian optimization,First Order,Hyper Parameter Optimization,Reinforcement Learning}
}

@article{prapasContinuousTrainingDeployment2021,
  title = {Continuous {{Training}} and {{Deployment}} of {{Deep Learning Models}}},
  author = {Prapas, Ioannis and Derakhshan, Behrouz and Mahdiraji, Alireza Rezaei and Markl, Volker},
  year = {2021},
  month = nov,
  journal = {Datenbank-Spektrum},
  volume = {21},
  number = {3},
  pages = {203--212},
  issn = {1610-1995},
  doi = {10.1007/s13222-021-00386-8},
  urldate = {2022-02-23},
  abstract = {Deep Learning (DL) has consistently surpassed other Machine Learning methods and achieved state-of-the-art performance in multiple cases. Several modern applications like financial and recommender systems require models that are constantly updated with fresh data. The prominent approach for keeping a DL model fresh is to trigger full retraining from scratch when enough new data are available. However, retraining large and complex DL models is time-consuming and compute-intensive. This makes full retraining costly, wasteful, and slow. In this paper, we present an approach to continuously train and deploy DL models. First, we enable continuous training through proactive training that combines samples of historical data with new streaming data. Second, we enable continuous deployment through gradient sparsification that allows us to send a small percentage of the model updates per training iteration. Our experimental results with LeNet5 on MNIST and modern DL models on CIFAR-10 show that proactive training keeps models fresh with comparable---if not superior---performance to full retraining at a fraction of the time. Combined with gradient sparsification, sparse proactive training enables very fast updates of a deployed model with arbitrarily large sparsity, reducing communication per iteration up to four orders of magnitude, with minimal---if any---losses in model quality. Sparse training, however, comes at a price; it incurs overhead on the training that depends on the size of the model and increases the training time by factors ranging from 1.25 to 3 in our experiments. Arguably, a small price to pay for successfully enabling the continuous training and deployment of large DL models.},
  langid = {english}
}

@misc{Productive40Anomaly,
  title = {Productive 4.0 - {{Anomaly Detection Framework}}},
  urldate = {2021-04-14},
  langid = {american}
}

@article{prokhorenkoSurgeonRobotInterface2020,
  title = {Surgeon--Robot Interface Development Framework},
  author = {Prokhorenko, Leonid and Klimov, Daniil and Mishchenkov, Denis and Poduraev, Yuri},
  year = {2020},
  month = may,
  journal = {Computers in Biology and Medicine},
  volume = {120},
  pages = {103717},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2020.103717},
  urldate = {2024-09-17},
  abstract = {The progress of robotic medicine leads to the emergence of an increasing number of highly specialized automated systems based on specialized software. In any such system, there is the task of translating the surgeon's requests into the process of automated procedure execution. The hardware and software system that provides the translation is the interface between the surgeon and the robot. This paper proposes a generalized framework architecture for the development of such software --- the surgeon--robot interface. Existing implementations of such an interface are considered, solutions for the internal structure design of the framework are proposed. Experiments were performed using a prototype of the proposed framework. Such a development framework will allow one to effectively implement the surgeon--robot interfaces at all stages of the robotization of medical procedures, from prototype to final use in the operating room.},
  keywords = {Medical robotics,Robotic systems,Software architecture,Software framework},
  file = {C:\Users\benja\Zotero\storage\HNF6ME97\S0010482520301025.html}
}

@inproceedings{puiuttaExplainableReinforcementLearning2020,
  title = {Explainable {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Explainable {{Reinforcement Learning}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  author = {Puiutta, Erika and Veith, Eric M. S. P.},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {77--95},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-57321-8_5},
  abstract = {Explainable Artificial Intelligence (XAI), i.e., the development of more transparent and interpretable AI models, has gained increased traction over the last few years. This is due to the fact that, in conjunction with their growth into powerful and ubiquitous tools, AI models exhibit one detrimental characteristic: a performance-transparency trade-off. This describes the fact that the more complex a model's inner workings, the less clear it is how its predictions or decisions were achieved. But, especially considering Machine Learning (ML) methods like Reinforcement Learning (RL) where the system learns autonomously, the necessity to understand the underlying reasoning for their decisions becomes apparent. Since, to the best of our knowledge, there exists no single work offering an overview of Explainable Reinforcement Learning (XRL) methods, this survey attempts to address this gap. We give a short summary of the problem, a definition of important terms, and offer a classification and assessment of current XRL methods. We found that a) the majority of XRL methods function by mimicking and simplifying a complex model instead of designing an inherently simple one, and b) XRL (and XAI) methods often neglect to consider the human side of the equation, not taking into account research from related fields like psychology or philosophy. Thus, an interdisciplinary effort is needed to adapt the generated explanations to a (non-expert) human user in order to effectively progress in the field of XRL and XAI in general.},
  isbn = {978-3-030-57321-8},
  langid = {english},
  keywords = {Explainable,Human-computer interaction,Interpretable,Machine learning,Reinforcement Learning}
}

@article{pulvermullerThinkingCircuitsNeurobiological2014,
  title = {Thinking in Circuits: Toward Neurobiological Explanation in Cognitive Neuroscience},
  shorttitle = {Thinking in Circuits},
  author = {Pulverm{\"u}ller, Friedemann and Garagnani, Max and Wennekers, Thomas},
  year = {2014},
  journal = {Biological Cybernetics},
  volume = {108},
  number = {5},
  pages = {573--593},
  issn = {0340-1200},
  doi = {10.1007/s00422-014-0603-9},
  urldate = {2024-04-21},
  abstract = {Cognitive theory has decomposed human mental abilities into cognitive (sub) systems, and cognitive neuroscience succeeded in disclosing a host of relationships between cognitive systems and specific structures of the human brain. However, an explanation of why specific functions are located in specific brain loci had still been missing, along with a neurobiological model that makes concrete the neuronal circuits that carry thoughts and meaning. Brain theory, in particular the Hebb-inspired neurocybernetic proposals by Braitenberg, now offers an avenue toward explaining brain--mind relationships and to spell out cognition in terms of neuron circuits in a neuromechanistic sense. Central to this endeavor is the theoretical construct of an elementary functional neuronal unit above the level of individual neurons and below that of whole brain areas and systems: the distributed neuronal assembly (DNA) or thought circuit (TC). It is shown that DNA/TC theory of cognition offers an integrated explanatory perspective on brain mechanisms of perception, action, language, attention, memory, decision and conceptual thought. We argue that DNAs carry all of these functions and that their inner structure (e.g., core and halo subcomponents), and their functional activation dynamics (e.g., ignition and reverberation processes) answer crucial localist questions, such as why memory and decisions draw on prefrontal areas although memory formation is normally driven by information in the senses and in the motor system. We suggest that the ability of building DNAs/TCs spread out over different cortical areas is the key mechanism for a range of specifically human sensorimotor, linguistic and conceptual capacities and that the cell assembly mechanism of overlap reduction is crucial for differentiating a vocabulary of actions, symbols and concepts.},
  pmcid = {PMC4228116},
  pmid = {24939580}
}

@inproceedings{pumacayCOLOSSEUMBenchmarkEvaluating2024,
  title = {{{THE COLOSSEUM}}: {{A Benchmark}} for {{Evaluating Generalization}} for {{Robotic Manipulation}}},
  shorttitle = {{{THE COLOSSEUM}}},
  booktitle = {Robotics: {{Science}} and {{Systems XX}}},
  author = {Pumacay, Wilbert and Singh, Ishika and Duan, Jiafei and Krishna, Ranjay and Thomason, Jesse and Fox, Dieter},
  year = {2024},
  month = jul,
  address = {Delft, Netherlands},
  doi = {10.15607/RSS.2024.XX.133}
}

@article{putzkyRecurrentInferenceMachines2017,
  title = {Recurrent {{Inference Machines}} for {{Solving Inverse Problems}}},
  author = {Putzky, Patrick and Welling, Max},
  year = {2017},
  month = jun,
  journal = {arXiv:1706.04008 [cs]},
  eprint = {1706.04008},
  primaryclass = {cs},
  urldate = {2019-05-17},
  abstract = {Much of the recent research on solving iterative inference problems focuses on moving away from hand-chosen inference algorithms and towards learned inference. In the latter, the inference process is unrolled in time and interpreted as a recurrent neural network (RNN) which allows for joint learning of model and inference parameters with back-propagation through time. In this framework, the RNN architecture is directly derived from a hand-chosen inference algorithm, effectively limiting its capabilities. We propose a learning framework, called Recurrent Inference Machines (RIM), in which we turn algorithm construction the other way round: Given data and a task, train an RNN to learn an inference algorithm. Because RNNs are Turing complete [1, 2] they are capable to implement any inference algorithm. The framework allows for an abstraction which removes the need for domain knowledge. We demonstrate in several image restoration experiments that this abstraction is effective, allowing us to achieve state-of-the-art performance on image denoising and super-resolution tasks and superior across-task generalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\benja\Zotero\storage\UHT48RCN\1706.html}
}

@misc{pyto17,
  title = {{{PyTorch}}: {{Tensors}} and {{Dynamic Neural Networks}} in {{Python}} with {{Strong GPU Acceleration}}},
  year = {2017},
  journal = {PyTorch},
  urldate = {2017-08-27}
}

@inproceedings{qi2017pointnet,
  title = {Pointnet: {{Deep}} Learning on Point Sets for 3d Classification and Segmentation},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},
  year = {2017},
  pages = {652--660}
}

@article{qi2017pointnet++,
  title = {Pointnet++: {{Deep}} Hierarchical Feature Learning on Point Sets in a Metric Space},
  author = {Qi, Charles R and Yi, Li and Su, Hao and Guibas, Leonidas J},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30}
}

@inproceedings{qiaoEfficientDifferentiableSimulation2021,
  title = {Efficient {{Differentiable Simulation}} of {{Articulated Bodies}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Qiao, Yi-Ling and Liang, Junbang and Koltun, Vladlen and Lin, Ming C.},
  year = {2021},
  month = jul,
  pages = {8661--8671},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2022-03-30},
  abstract = {We present a method for efficient differentiable simulation of articulated bodies. This enables integration of articulated body dynamics into deep learning frameworks, and gradient-based optimization of neural networks that operate on articulated bodies. We derive the gradients of the contact solver using spatial algebra and the adjoint method. Our approach is an order of magnitude faster than autodiff tools. By only saving the initial states throughout the simulation process, our method reduces memory requirements by two orders of magnitude. We demonstrate the utility of efficient differentiable dynamics for articulated bodies in a variety of applications. We show that reinforcement learning with articulated systems can be accelerated using gradients provided by our method. In applications to control and inverse problems, gradient-based optimization enabled by our work accelerates convergence by more than an order of magnitude.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\67FS86B8\Qiao et al. - 2021 - Efficient Differentiable Simulation of Articulated.pdf}
}

@inproceedings{qiaoScalableDifferentiablePhysics2020,
  title = {Scalable {{Differentiable Physics}} for {{Learning}} and {{Control}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Qiao, Yi-Ling and Liang, Junbang and Koltun, Vladlen and Lin, Ming},
  year = {2020},
  month = nov,
  pages = {7847--7856},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2022-03-30},
  abstract = {Differentiable physics is a powerful approach to learning and control problems that involve physical objects and environments. While notable progress has been made, the capabilities of differentiable physics solvers remain limited. We develop a scalable framework for differentiable physics that can support a large number of objects and their interactions. To accommodate objects with arbitrary geometry and topology, we adopt meshes as our representation and leverage the sparsity of contacts for scalable differentiable collision handling. Collisions are resolved in localized regions to minimize the number of optimization variables even when the number of simulated objects is high. We further accelerate implicit differentiation of optimization with nonlinear constraints. Experiments demonstrate that the presented framework requires up to two orders of magnitude less memory and computation in comparison to recent particle-based methods. We further validate the approach on inverse problems and control scenarios, where it outperforms derivative-free and model-free baselines by at least an order of magnitude.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\USHLCKQ6\Qiao et al. - 2020 - Scalable Differentiable Physics for Learning and C.pdf}
}

@inproceedings{qiGeneralInhandObject2023,
  title = {General {{In-hand Object Rotation}} with {{Vision}} and {{Touch}}},
  booktitle = {7th {{Annual Conference}} on {{Robot Learning}}},
  author = {Qi, Haozhi and Yi, Brent and Suresh, Sudharshan and Lambeta, Mike and Ma, Yi and Calandra, Roberto and Malik, Jitendra},
  year = {2023},
  month = aug,
  urldate = {2024-04-28},
  abstract = {We introduce Rotateit, a system that enables fingertip-based object rotation along multiple axes by leveraging multimodal sensory inputs. Our system is trained in simulation, where it has access to ground-truth object shapes and physical properties. Then we distill it to operate on realistic yet noisy simulated visuotactile and proprioceptive sensory inputs. These multimodal inputs are fused via a visuotactile transformer, enabling online inference of object shapes and physical properties during deployment. We show significant performance improvements over prior methods and highlight the importance of visual and tactile sensing.},
  langid = {english}
}

@article{qinHowConvolutionalNeural2018,
  title = {How {{Convolutional Neural Networks See}} the {{World}} : {{A Survey}} of {{Convolutional Neural Network Visualization Methods}}},
  author = {Qin, Zhuwei and Yu, Fuxun and Liu, Chenchen and Chen, Xiang},
  year = {2018},
  month = apr,
  eprint = {1804.11191},
  urldate = {2019-02-19},
  abstract = {Nowadays, the Convolutional Neural Networks (CNNs) have achieved impressive performance on many computer vision related tasks, such as object detection, image recognition, image retrieval, etc. These achievements benefit from the CNNs outstanding capability to learn the input features with deep layers of neuron structures and iterative training process. However, these learned features are hard to identify and interpret from a human vision perspective, causing a lack of understanding of the CNNs internal working mechanism. To improve the CNN interpretability, the CNN visualization is well utilized as a qualitative analysis method, which translates the internal features into visually perceptible patterns. And many CNN visualization works have been proposed in the literature to interpret the CNN in perspectives of network structure, operation, and semantic concept. In this paper, we expect to provide a comprehensive survey of several representative CNN visualization methods, including Activation Maximization, Network Inversion, Deconvolutional Neural Networks (DeconvNet), and Network Dissection based visualization. These methods are presented in terms of motivations, algorithms, and experiment results. Based on these visualization methods, we also discuss their practical applications to demonstrate the significance of the CNN interpretability in areas of network design, optimization, security enhancement, etc.},
  archiveprefix = {arXiv}
}

@misc{QualityAssuranceAutomotive,
  title = {Quality {{Assurance}} in the {{Automotive Industry}} - {{Fraunhofer ITWM}}},
  journal = {Fraunhofer Institute for Industrial Mathematics ITWM},
  urldate = {2020-10-13},
  abstract = {Image processing is an important part of industrial production. In the automotive industry especially the quality assurance and optimization sector has grown.},
  howpublished = {https://www.itwm.fraunhofer.de/en/departments/bv/quality-assurance-and-optimization/quality-assurance-in-the-automotive-industry.html},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\QK69E2YI\quality-assurance-in-the-automotive-industry.html}
}

@inproceedings{quigleyROSOpensourceRobot2009,
  title = {{{ROS}}: An Open-Source {{Robot Operating System}}},
  shorttitle = {{{ROS}}},
  booktitle = {{{ICRA}} 2009},
  author = {Quigley, M.},
  year = {2009},
  abstract = {This paper gives an overview of ROS, an opensource robot operating system. ROS is not an operating system in the traditional sense of process management and scheduling; rather, it provides a structured communications layer above the host operating systems of a heterogenous compute cluster. In this paper, we discuss how ROS relates to existing robot software frameworks, and briefly overview some of the available application software which uses ROS.}
}

@inproceedings{quinteroRobotProgrammingAugmented2018,
  title = {Robot {{Programming Through Augmented Trajectories}} in {{Augmented Reality}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Quintero, Camilo Perez and Li, Sarah and Pan, Matthew KXJ and Chan, Wesley P. and {Machiel Van der Loos}, H.F. and Croft, Elizabeth},
  year = {2018},
  month = oct,
  pages = {1838--1844},
  issn = {2153-0866},
  doi = {10.1109/IROS.2018.8593700},
  urldate = {2024-08-15},
  abstract = {This paper presents a future-focused approach for robot programming based on augmented trajectories. Using a mixed reality head-mounted display (Microsoft Hololens) and a 7-DOF robot arm, we designed an augmented reality (AR) robotic interface with four interactive functions to ease the robot programming task: 1) Trajectory specification. 2) Virtual previews of robot motion. 3) Visualization of robot parameters. 4) Online reprogramming during simulation and execution. We validate our AR-robot teaching interface by comparing it with a kinesthetic teaching interface in two different scenarios as part of a pilot study: creation of contact surface path and free space path. Furthermore, we present an industrial case study that illustrates our AR manufacturing paradigm by interacting with a 7-DOF robot arm to reduce wrinkles during the pleating step of the carbon-fiber-reinforcement-polymer vacuum bagging process in a simulated scenario.},
  keywords = {End effectors,Service robots,Task analysis,Trajectory,Visualization}
}

@article{qureshiNeuralManipulationPlanning2020,
  title = {Neural {{Manipulation Planning}} on {{Constraint Manifolds}}},
  author = {Qureshi, Ahmed H. and Dong, Jiangeng and Choe, Austin and Yip, Michael C.},
  year = {2020},
  month = oct,
  journal = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {4},
  pages = {6089--6096},
  issn = {2377-3766},
  doi = {10.1109/LRA.2020.3010220},
  urldate = {2024-04-29},
  abstract = {The presence of task constraints imposes a significant challenge to motion planning. Despite all recent advancements, existing algorithms are still computationally expensive for most planning problems. In this letter, we present Constrained Motion Planning Networks (CoMPNet), the first neural planner for multimodal kinematic constraints. Our approach comprises the following components: i) constraint and environment perception encoders; ii) neural robot configuration generator that outputs configurations on/near the constraint manifold(s), and iii) a bidirectional planning algorithm that takes the generated configurations to create a feasible robot motion trajectory. We show that CoMPNet solves practical motion planning tasks involving both unconstrained and constrained problems. Furthermore, it generalizes to new unseen locations of the objects, i.e., not seen during training, in the given environments with high success rates. When compared to the state-of-the-art constrained motion planning algorithms, CoMPNet outperforms by order of magnitude improvement in computational speed with a significantly lower variance.},
  keywords = {Collision avoidance,Kinematics,learning from demonstration,Manifolds,Manipulation planning,motion and path planning,Planning,Robots,Switched mode power supplies,Task analysis},
  file = {C:\Users\benja\Zotero\storage\QQGQT8JQ\9143433.html}
}

@inproceedings{rabinovichAbstractSyntaxNetworks2017,
  title = {Abstract {{Syntax Networks}} for {{Code Generation}} and {{Semantic Parsing}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Rabinovich, Maxim and Stern, Mitchell and Klein, Dan},
  year = {2017},
  month = jul,
  pages = {1139--1149},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/P17-1105},
  urldate = {2019-07-16},
  abstract = {Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7\% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1\%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.}
}

@inproceedings{raccaInteractiveTuningRobot2020,
  title = {Interactive {{Tuning}} of {{Robot Program Parameters}} via {{Expected Divergence Maximization}}},
  booktitle = {Proceedings of the 2020 {{ACM}}/{{IEEE International Conference}} on {{Human-Robot Interaction}}},
  author = {Racca, Mattia and Kyrki, Ville and Cakmak, Maya},
  year = {2020},
  month = mar,
  series = {{{HRI}} '20},
  pages = {629--638},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3319502.3374784},
  urldate = {2021-11-04},
  abstract = {Enabling diverse users to program robots for different applications is critical for robots to be widely adopted. Most of the new collaborative robot manipulators come with intuitive programming interfaces that allow novice users to compose robot programs and tune their parameters. However, parameters like motion speeds or exerted forces cannot be easily demonstrated and often require manual tuning, resulting in a tedious trial-and-error process. To address this problem, we formulate tuning of one-dimensional parameters as an Active Learning problem where the learner iteratively refines its estimate of the feasible range of parameter values, by selecting informative queries. By executing the parametrized actions, the learner gathers the user's feedback, in the form of directional answers ("higher,'' "lower,'' or "fine''), and integrates it in the estimate. We propose an Active Learning approach based on Expected Divergence Maximization for this setting and compare it against two baselines with synthetic data. We further compare the approaches on a real-robot dataset obtained from programs written with a simple Domain-Specific Language for a robot arm and manually tuned by expert users (N=8) to perform four manipulation tasks. We evaluate the effectiveness and usability of our interactive tuning approach against manual tuning with a user study where novice users (N=8) tuned parameters of a human-robot hand-over program.},
  isbn = {978-1-4503-6746-2},
  keywords = {active learning,end-user programming,human-robot interaction}
}

@inproceedings{radfordRobustSpeechRecognition2023,
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  year = {2023},
  month = jul,
  series = {{{ICML}}'23},
  volume = {202},
  pages = {28492--28518},
  publisher = {JMLR.org},
  address = {Honolulu, Hawaii, USA},
  urldate = {2024-08-26},
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.}
}

@article{raghuRapidLearningFeature2020,
  title = {Rapid {{Learning}} or {{Feature Reuse}}? {{Towards Understanding}} the {{Effectiveness}} of {{MAML}}},
  shorttitle = {Rapid {{Learning}} or {{Feature Reuse}}?},
  author = {Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
  year = {2020},
  month = feb,
  journal = {arXiv:1909.09157 [cs, stat]},
  eprint = {1909.09157},
  primaryclass = {cs, stat},
  urldate = {2021-02-07},
  abstract = {An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of a MAML-trained network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\benja\\Zotero\\storage\\82683JLV\\Raghu et al. - 2020 - Rapid Learning or Feature Reuse Towards Understan.pdf;C\:\\Users\\benja\\Zotero\\storage\\EEHKZBN3\\1909.html}
}

@article{raghuTransfusionUnderstandingTransfer2019,
  title = {Transfusion: {{Understanding Transfer Learning}} for {{Medical Imaging}}},
  shorttitle = {Transfusion},
  author = {Raghu, Maithra and Zhang, Chiyuan and Kleinberg, Jon and Bengio, Samy},
  year = {2019},
  month = oct,
  journal = {arXiv:1902.07208 [cs, stat]},
  eprint = {1902.07208},
  primaryclass = {cs, stat},
  urldate = {2019-12-06},
  abstract = {Transfer learning from natural image datasets, particularly ImageNet, using standard large models and corresponding pretrained weights has become a de-facto method for deep learning applications to medical imaging. However, there are fundamental differences in data sizes, features and task specifications between natural image classification and the target medical tasks, and there is little understanding of the effects of transfer. In this paper, we explore properties of transfer learning for medical imaging. A performance evaluation on two large scale medical imaging tasks shows that surprisingly, transfer offers little benefit to performance, and simple, lightweight models can perform comparably to ImageNet architectures. Investigating the learned representations and features, we find that some of the differences from transfer learning are due to the over-parametrization of standard models rather than sophisticated feature reuse. We isolate where useful feature reuse occurs, and outline the implications for more efficient model exploration. We also explore feature independent benefits of transfer arising from weight scalings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{rahimiFrameworkSoftwareSafety1991,
  title = {A Framework for Software Safety Verification of Industrial Robot Operations},
  author = {Rahimi, Mansour and Xiadong, Xia},
  year = {1991},
  month = jan,
  journal = {Computers \& Industrial Engineering},
  volume = {20},
  number = {2},
  pages = {279--287},
  issn = {0360-8352},
  doi = {10.1016/0360-8352(91)90032-2},
  urldate = {2024-04-16},
  abstract = {The role of safety of machines and operators in advanced automation and robotics is emphasized. The dominance of software in controlling robotic operations requires a systematic analysis for software safety and verification. Past research in this area shows lack of a modeling framework by which safety applications can be implemented. The present paper explains a design framework for the software safety module for industrial probotic operations. The module proposed includes preconditions and postconditions for each robot action based on a static model of the robot physical configurations and the environment. A generic software safety verification and encoding is presented for safety-critical actions of the robot. The software safety criteria and verification language is presented.},
  file = {C:\Users\benja\Zotero\storage\8AFW4QF3\0360835291900322.html}
}

@inproceedings{raibleArtificialNeuralNetwork2023,
  title = {Artificial {{Neural Network Guided Compensation}} of {{Nonlinear Payload}} and {{Wear Effects}} for {{Industrial Robots}}},
  booktitle = {2023 {{IEEE}} 19th {{International Conference}} on {{Automation Science}} and {{Engineering}} ({{CASE}})},
  author = {Raible, Julian and Rettig, Oliver and Alt, Benjamin and Yaman, Alper and Gauger, Isabelle and Biasi, Lorenzo and M{\"u}ller, Silvan and Katic, Darko and Strand, Marcus and Huber, Marco F.},
  year = {2023},
  month = aug,
  pages = {1--8},
  issn = {2161-8089},
  doi = {10.1109/CASE56687.2023.10260559},
  urldate = {2023-12-16},
  abstract = {The absolute accuracy of industrial robots is influ-enced by numerous geometric and non-geometric errors. Most state-of-the-art calibration and compensation methods consider only the geometric errors and neglect the non-geometric ones. In this paper, a hybrid compensation approach is proposed that combines well-known kinematic models with a model-free data-driven component using a neural network. This allows the use of established calibration methods for the geometric influences captured in the kinematic model to improve the non-geometric error compensation by the neural network. The proposed approach is applied in two use cases: payload and wear compensation. Simulations and real experiments show the improved absolute accuracy of the hybrid compensation approach compared to classical calibration methods.},
  copyright = {All rights reserved},
  keywords = {my},
  file = {C:\Users\benja\Zotero\storage\WMG6FWY4\10260559.html}
}

@inproceedings{raibleAutomaticPathPlanning2023,
  title = {Automatic {{Path Planning}} for {{Robotic Grinding}} and {{Polishing Tasks}} Based on {{Point Cloud Slicing}}},
  booktitle = {{{ISR Europe}} 2023 - 56th {{International Symposium}} on {{Robotics}}},
  author = {Raible, Julian and Braun, Christopher and Huber, Marco},
  year = {2023},
  month = sep,
  publisher = {VDE Verlag},
  address = {Stuttgart, Germany},
  urldate = {2024-07-27},
  abstract = {The remanufacturing of workpieces suffers from a high manual effort, which emphasises the need for automation of such processes by robots. However, this is associated with some challenges due to the high variance in terms of individual characteristics of possible defects. Shape and surface properties deviating from the ideal virtual 3D model require approaches that allow path planning to be carried out directly on the component to be machined. This paper proposes a novel automatic path planning algorithm for robotic grinding and polishing tasks based on point clouds obtained directly by scanning the workpiece. The adaptive determination of the position and orientation of intersection planes as well as a slicing direction enables obtaining cross-sectional contours by slicing the point cloud. The algorithm generates a continuous, multidirectional path that has a meander shape precisely adapted to the workpiece surface. A qualitative analysis emphasises the robustness of the algorithm with respect to the distribution of the point cloud data and limited amount of noise.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\VXVWZPD7\details.html}
}

@article{rajaniExplainYourselfLeveraging2019,
  title = {Explain {{Yourself}}! {{Leveraging Language Models}} for {{Commonsense Reasoning}}},
  author = {Rajani, Nazneen Fatema and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.02361 [cs]},
  eprint = {1906.02361},
  primaryclass = {cs},
  urldate = {2019-07-10},
  abstract = {Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of worldknowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10\% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language}
}

@misc{rajiConcreteProblemsAI2023,
  title = {Concrete {{Problems}} in {{AI Safety}}, {{Revisited}}},
  author = {Raji, Inioluwa Deborah and Dobbe, Roel},
  year = {2023},
  month = dec,
  number = {arXiv:2401.10899},
  eprint = {2401.10899},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.10899},
  urldate = {2024-08-01},
  abstract = {As AI systems proliferate in society, the AI community is increasingly preoccupied with the concept of AI Safety, namely the prevention of failures due to accidents that arise from an unanticipated departure of a system's behavior from designer intent in AI deployment. We demonstrate through an analysis of real world cases of such incidents that although current vocabulary captures a range of the encountered issues of AI deployment, an expanded socio-technical framing will be required for a more complete understanding of how AI systems and implemented safety mechanisms fail and succeed in real life.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  file = {C:\Users\benja\Zotero\storage\RSIGBE9F\2401.html}
}

@article{rakicevicActiveLearningInformed2019,
  title = {Active Learning via Informed Search in Movement Parameter Space for Efficient Robot Task Learning and Transfer},
  author = {Rakicevic, Nemanja and Kormushev, Petar},
  year = {2019},
  month = dec,
  journal = {Autonomous Robots},
  volume = {43},
  number = {8},
  pages = {1917--1935},
  issn = {1573-7527},
  doi = {10.1007/s10514-019-09842-7},
  urldate = {2020-10-30},
  abstract = {Learning complex physical tasks via trial-and-error is still challenging for high-degree-of-freedom robots. Greatest challenges are devising a suitable objective function that defines the task, and the high sample complexity of learning the task. We propose a novel active learning framework, consisting of decoupled task model and exploration components, which does not require an objective function. The task model is specific to a task and maps the parameter space, defining a trial, to the trial outcome space. The exploration component enables efficient search in the trial-parameter space to generate the subsequent most informative trials, by simultaneously exploiting all the information gained from previous trials and reducing the task model's overall uncertainty. We analyse the performance of our framework in a simulation environment and further validate it on a challenging bimanual-robot puck-passing task. Results show that the robot successfully acquires the necessary skills after only 100 trials without any prior information about the task or target positions. Decoupling the framework's components also enables efficient skill transfer to new environments which is validated experimentally.},
  langid = {english}
}

@misc{rakotosaonaNeRFMeshingDistillingNeural2023,
  title = {{{NeRFMeshing}}: {{Distilling Neural Radiance Fields}} into {{Geometrically-Accurate 3D Meshes}}},
  shorttitle = {{{NeRFMeshing}}},
  author = {Rakotosaona, Marie-Julie and Manhardt, Fabian and Arroyo, Diego Martin and Niemeyer, Michael and Kundu, Abhijit and Tombari, Federico},
  year = {2023},
  month = mar,
  number = {arXiv:2303.09431},
  eprint = {2303.09431},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.09431},
  urldate = {2024-01-11},
  abstract = {With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis has recently made a big leap forward. At the core, NeRF proposes that each 3D point can emit radiance, allowing to conduct view synthesis using differentiable volumetric rendering. While neural radiance fields can accurately represent 3D scenes for computing the image rendering, 3D meshes are still the main scene representation supported by most computer graphics and simulation pipelines, enabling tasks such as real time rendering and physics-based simulations. Obtaining 3D meshes from neural radiance fields still remains an open challenge since NeRFs are optimized for view synthesis, not enforcing an accurate underlying geometry on the radiance field. We thus propose a novel compact and flexible architecture that enables easy 3D surface reconstruction from any NeRF-driven approach. Upon having trained the radiance field, we distill the volumetric 3D representation into a Signed Surface Approximation Network, allowing easy extraction of the 3D mesh and appearance. Our final 3D mesh is physically accurate and can be rendered in real time on an array of devices.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\benja\Zotero\storage\8FQXMQNP\2303.html}
}

@inproceedings{ramanPlanningLargeLanguage2022,
  title = {Planning {{With Large Language Models Via Corrective Re-Prompting}}},
  booktitle = {{{NeurIPS}} 2022 {{Foundation Models}} for {{Decision Making Workshop}}},
  author = {Raman, Shreyas Sundara and Cohen, Vanya and Rosen, Eric and Idrees, Ifrah and Paulius, David and Tellex, Stefanie},
  year = {2022},
  month = nov,
  urldate = {2024-01-11},
  abstract = {Extracting knowledge from Large Language Models (LLM) offers a path to designing intelligent, embodied agents that takes advantage of the common sense knowledge present in large language datasets. Related works have queried LLMs with a wide-range of contextual information, such as goals, sensor observations and scene descriptions, to generate high-level action plans for a specific task. In this work, we propose a prompting-based strategy for extracting executable plans from a LLM that leverages a novel and readily-accessible source of information: precondition errors. Our approach assumes that actions are only afforded execution in certain contexts (i.e. implicit preconditions must be met for an action to execute), and that the embodied agent has the ability to determine if the action is not executable in the current context (e.g: a precondition error is present). When an agent is unable to execute an action in a plan, our approach re-prompts the LLM with precondition error information to extract a useful and executable action to achieve the intended goal in the current context. We evaluate our approach in the VirtualHome simulation environment on 88 different tasks and 7 scenes. We evaluate different prompt templates and compare to methods that naively re-sample actions from the LLM. We find that our approach using precondition errors improves the executability and semantic correctness of plans, while also reducing the number of corrective re-prompts for querying actions.},
  langid = {english}
}

@article{Ramdya2017,
  title = {Climbing {{Favours}} the {{Tripod Gait}} over {{Alternative Faster Insect Gaits}}},
  author = {Ramdya, Pavan and Thandiackal, Robin and Cherney, Raphael and Asselborn, Thibault and Benton, Richard and Ijspeert, Auke Jan and Floreano, Dario},
  year = {2017},
  journal = {Nature Communications},
  volume = {8}
}

@inproceedings{ramirez-amaroAutomaticSegmentationRecognition2014,
  title = {Automatic Segmentation and Recognition of Human Activities from Observation Based on Semantic Reasoning},
  booktitle = {2014 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {{Ramirez-Amaro}, Karinne and Beetz, Michael and Cheng, Gordon},
  year = {2014},
  month = sep,
  pages = {5043--5048},
  issn = {2153-0866},
  doi = {10.1109/IROS.2014.6943279},
  abstract = {Automatically segmenting and recognizing human activities from observations typically requires a very complex and sophisticated perception algorithm. Such systems would be unlikely implemented on-line into a physical system, such as a robot, due to the pre-processing step(s) that those vision systems usually demand. In this work, we present and demonstrate that with an appropriate semantic representation of the activity, and without such complex perception systems, it is sufficient to infer human activities from videos. First, we will present a method to extract the semantic rules based on three simple hand motions, i.e. move, not move and tool use. Additionally, the information of the object properties either ObjectActedOn or ObjectInHand are used. Such properties encapsulate the information of the current context. The above data is used to train a decision tree to obtain the semantic rules employed by a reasoning engine. This means, we extract lower-level information from videos and we reason about the intended human behaviors (high-level). The advantage of the abstract representation is that it allows to obtain more generic models out of human behaviors, even when the information is obtained from different scenarios. The results show that our system correctly segments and recognizes human behaviors with an accuracy of 85\%. Another important aspect of our system is its scalability and adaptability toward new activities, which can be learned on-demand. Our system has been fully implemented on a humanoid robot, the iCub to experimentally validate the performance and the robustness of our system during on-line execution of the robot.},
  keywords = {Accuracy,Cognition,Motion segmentation,Semantics,Training,Videos},
  file = {C:\Users\benja\Zotero\storage\8B335ZB9\6943279.html}
}

@inproceedings{ramirez-amaroBootstrappingHumanoidRobot2014,
  title = {Bootstrapping Humanoid Robot Skills by Extracting Semantic Representations of Human-like Activities from Virtual Reality},
  booktitle = {2014 {{IEEE-RAS International Conference}} on {{Humanoid Robots}}},
  author = {{Ramirez-Amaro}, Karinne and Inamura, Tetsunari and {Dean-Le{\'o}n}, Emmanuel and Beetz, Michael and Cheng, Gordon},
  year = {2014},
  month = nov,
  pages = {438--443},
  issn = {2164-0580},
  doi = {10.1109/HUMANOIDS.2014.7041398},
  abstract = {Advancements in Virtual Reality have enabled well-defined and consistent virtual environments that can capture complex scenarios, such as human everyday activities. Additionally, virtual simulators (such as SIGVerse) are designed to be user-friendly mechanisms between virtual robots/agents and real users allowing a better interaction. We envision such rich scenarios can be used to train robots to learn new behaviors specially in human everyday activities where a diverse variability can be found. In this paper, we present a multi-level framework that is capable to use different input sources such as cameras and virtual environments to understand and execute the demonstrated activities. Our presented framework first obtains the semantic models of human activities from cameras, which are later tested using the SIGVerse virtual simulator to show new complex activities (such as, cleaning the table) using a virtual robot. Our introduced framework is integrated on a real robot, i.e. an iCub, which is capable to process the signals from the virtual environment to then understand the activities performed by the observed robot. This was realized through the use of previous knowledge and experiences that the robot has learned from observing humans activities. Our results show that our framework was able to extract the meaning of the observed motions with 80\% accuracy of recognition by obtaining the objects relationships given the current context via semantic representations to extract high-level understanding of those complex activities even when they represent different behaviors.},
  keywords = {Cameras,Cleaning,Motion segmentation,Robot vision systems,Semantics,Training}
}

@article{ramirez-amaroTransferringSkillsHumanoid2017,
  title = {Transferring Skills to Humanoid Robots by Extracting Semantic Representations from Observations of Human Activities},
  author = {{Ramirez-Amaro}, Karinne and Beetz, Michael and Cheng, Gordon},
  year = {2017},
  month = jun,
  journal = {Artificial Intelligence},
  series = {Special {{Issue}} on {{AI}} and {{Robotics}}},
  volume = {247},
  pages = {95--118},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2015.08.009},
  urldate = {2021-11-12},
  abstract = {In this study, we present a framework that infers human activities from observations using semantic representations. The proposed framework can be utilized to address the difficult and challenging problem of transferring tasks and skills to humanoid robots. We propose a method that allows robots to obtain and determine a higher-level understanding of a demonstrator's behavior via semantic representations. This abstraction from observations captures the ``essence'' of the activity, thereby indicating which aspect of the demonstrator's actions should be executed in order to accomplish the required activity. Thus, a meaningful semantic description is obtained in terms of human motions and object properties. In addition, we validated the semantic rules obtained in different conditions, i.e., three different and complex kitchen activities: 1) making a pancake; 2) making a sandwich; and 3) setting the table. We present quantitative and qualitative results, which demonstrate that without any further training, our system can deal with time restrictions, different execution styles of the same task by several participants, and different labeling strategies. This means, the rules obtained from one scenario are still valid even for new situations, which demonstrates that the inferred representations do not depend on the task performed. The results show that our system correctly recognized human behaviors in real-time in around 87.44\% of cases, which was even better than a random participant recognizing the behaviors of another human (about 76.68\%). In particular, the semantic rules acquired can be used to effectively improve the dynamic growth of the ontology-based knowledge representation. Hence, this method can be used flexibly across different demonstrations and constraints to infer and achieve a similar goal to that observed. Furthermore, the inference capability introduced in this study was integrated into a joint space control loop for a humanoid robot, an iCub, for achieving similar goals to the human demonstrator online.},
  langid = {english},
  keywords = {Activity recognition,Human understanding,Knowledge-based,Semantic representation,Skill transfer}
}

@article{ramirez-amaroUnderstandingIntentionHuman2015,
  title = {Understanding the Intention of Human Activities through Semantic Perception: {{Observation}}, Understanding and Execution on a Humanoid Robot},
  shorttitle = {Understanding the Intention of Human Activities through Semantic Perception},
  author = {{Ram{\'i}rez-Amaro}, Karinne and Beetz, Michael and Cheng, Gordon},
  year = {2015},
  month = mar,
  journal = {Advanced Robotics},
  volume = {29},
  pages = {345--362},
  doi = {10.1080/01691864.2014.1003096},
  abstract = {In this work, we present and demonstrate that with an appropriate semantic representation and even with a very naive perception system, it is sufficient to infer human activities from observations. First, we present a method to extract the semantic rules of human everyday activities. Namely, we extract low-level information from the sensor data and then we infer the high-level by reasoning about the intended human behaviors. The advantage of this abstract representation is that it allows us to obtain more generic models from human behaviors, even when the information is obtained from different scenarios. Another important aspect of our system is its scalability and adaptability toward new activities, which can be learned on-demand. Our system has been fully implemented on a humanoid robot, the iCub, to experimentally validate the performance and the robustness of our system during on-line execution within the control loop of the robot. The results show that the robot is able to make a decision in 0.12 s about the inferred human behaviors with a recognition accuracy of 85\%.}
}

@inproceedings{Ramisa.2012,
  title = {Using Depth and Appearance Features for Informed Robot Grasping of Highly Wrinkled Clothes},
  booktitle = {2012 {{IEEE}} International Conference on Robotics and Automation},
  author = {Ramisa, Arnau and Alenya, Guillem and {Moreno-Noguer}, Francesc and Torras, Carme},
  year = {2012},
  pages = {1703--1708},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2012.6225045},
  bookpagination = {page},
  isbn = {978-1-4673-1405-3}
}

@article{ramshorstInternationalSurveyOpinions2024,
  title = {International Survey on Opinions and Use of Robot-Assisted and Laparoscopic Minimally Invasive Pancreatic Surgery: 5-Year Follow Up},
  shorttitle = {International Survey on Opinions and Use of Robot-Assisted and Laparoscopic Minimally Invasive Pancreatic Surgery},
  author = {van Ramshorst, Tess M. E. and van Hilst, Jony and Bannone, Elisa and Pulvirenti, Alessandra and Asbun, Horacio J. and Boggi, Ugo and Busch, Olivier R. and Dokmak, Safi and Edwin, Bj{\o}rn and Hogg, Melissa and Jang, Jin-Young and Keck, Tobias and Khatkov, Igor and Kohan, Gustavo and Kokudo, Norihiro and Kooby, David A. and Nakamura, Masafumi and Primrose, John N. and Siriwardena, Ajith K. and Toso, Christian and Vollmer, Charles M. and Zeh, Herbert J. and Besselink, Marc G. and Hilal, Mohammad Abu},
  year = {2024},
  month = jan,
  journal = {HPB},
  volume = {26},
  number = {1},
  pages = {63--72},
  publisher = {Elsevier},
  issn = {1365-182X},
  doi = {10.1016/j.hpb.2023.09.004},
  urldate = {2024-04-11},
  langid = {english},
  pmid = {37739876}
}

@article{ranaGatedRecurrentUnit2016,
  title = {Gated {{Recurrent Unit}} ({{GRU}}) for {{Emotion Classification}} from {{Noisy Speech}}},
  author = {Rana, Rajib},
  year = {2016},
  month = dec,
  journal = {arXiv:1612.07778 [cs]},
  eprint = {1612.07778},
  primaryclass = {cs},
  urldate = {2019-08-06},
  abstract = {Despite the enormous interest in emotion classification from speech, the impact of noise on emotion classification is not well understood. This is important because, due to the tremendous advancement of the smartphone technology, it can be a powerful medium for speech emotion recognition in the outside laboratory natural environment, which is likely to incorporate background noise in the speech. We capitalize on the current breakthrough of Recurrent Neural Network (RNN) and seek to investigate its performance for emotion classification from noisy speech. We particularly focus on the recently proposed Gated Recurrent Unit (GRU), which is yet to be explored for emotion recognition from speech. Experiments conducted with speech compounded with eight different types of noises reveal that GRU incurs an 18.16\% smaller run-time while performing quite comparably to the Long Short-Term Memory (LSTM), which is the most popular Recurrent Neural Network proposed to date. This result is promising for any embedded platform in general and will initiate further studies to utilize GRU to its full potential for emotion recognition on smartphones.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction}
}

@inproceedings{ranaSayPlanGroundingLarge2023,
  title = {{{SayPlan}}: {{Grounding Large Language Models}} Using {{3D Scene Graphs}} for {{Scalable Robot Task Planning}}},
  shorttitle = {{{SayPlan}}},
  booktitle = {Proceedings of {{The}} 7th {{Conference}} on {{Robot Learning}}},
  author = {Rana, Krishan and Haviland, Jesse and Garg, Sourav and {Abou-Chakra}, Jad and Reid, Ian and Suenderhauf, Niko},
  year = {2023},
  month = dec,
  pages = {23--72},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-30},
  abstract = {Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a "semantic search" for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an "iterative replanning" pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page https://sayplan.github.io.},
  langid = {english}
}

@inproceedings{rathmairFormalVerificationApproach2021,
  title = {A {{Formal Verification Approach}} for {{Robotic Workflows}}},
  booktitle = {2021 20th {{International Conference}} on {{Advanced Robotics}} ({{ICAR}})},
  author = {Rathmair, Michael and Haspl, Thomas and Komenda, Titanilla and Reiterer, Bernhard and Hofbaur, Michael},
  year = {2021},
  month = dec,
  pages = {670--675},
  doi = {10.1109/ICAR53236.2021.9659366},
  urldate = {2024-04-16},
  abstract = {Formal verification is a powerful tool to show functional correctness of task and workflow descriptions representing robot-based manufacturing processes. A prerequisite is the integration of the process into a well-structured modeling and design architecture. In this paper, we propose a layer-based formal verification scheme that accepts BPMN-based input models. The integrated composition of tools enables state-space comprehensive verification targeting a set of user-defined verification goals. The importance of formal verification scheme is highlighted by our methodology which strictly supports a model refinement and abstraction approach. Since the complexity and size of robot application process models are steadily increasing this feature in combination with a clear verification methodology is a substantial contribution towards industrial acceptance.},
  keywords = {Complexity theory,Formal verification,Manufacturing processes,Service robots,Task analysis},
  file = {C:\Users\benja\Zotero\storage\WGGFHNBT\9659366.html}
}

@inproceedings{ratliffCHOMPGradientOptimization2009,
  title = {{{CHOMP}}: {{Gradient}} Optimization Techniques for Efficient Motion Planning},
  shorttitle = {{{CHOMP}}},
  booktitle = {2009 {{IEEE International Conference}} on {{Robotics}} and Automation ({{ICRA}})},
  author = {Ratliff, Nathan and Zucker, Matt and Bagnell, J. Andrew and Srinivasa, Siddhartha},
  year = {2009},
  month = may,
  pages = {489--494},
  publisher = {IEEE},
  address = {Kobe, Japan},
  doi = {10.1109/ROBOT.2009.5152817},
  urldate = {2020-03-22},
  abstract = {Existing high-dimensional motion planning algorithms are simultaneously overpowered and underpowered. In domains sparsely populated by obstacles, the heuristics used by sampling-based planners to navigate ``narrow passages'' can be needlessly complex; furthermore, additional post-processing is required to remove the jerky or extraneous motions from the paths that such planners generate. In this paper, we present CHOMP, a novel method for continuous path refinement that uses covariant gradient techniques to improve the quality of sampled trajectories. Our optimization technique converges over a wider range of input paths and is able to optimize higherorder dynamics of trajectories than previous path optimization strategies. As a result, CHOMP can be used as a standalone motion planner in many real-world planning queries. The effectiveness of our proposed method is demonstrated in manipulation planning for a 6-DOF robotic arm as well as in trajectory generation for a walking quadruped robot.},
  isbn = {978-1-4244-2788-8},
  langid = {english}
}

@article{ratliffRiemannianMotionPolicies2018,
  title = {Riemannian {{Motion Policies}}},
  author = {Ratliff, Nathan D. and Issac, Jan and Kappler, Daniel and Birchfield, Stan and Fox, Dieter},
  year = {2018},
  month = jul,
  journal = {arXiv:1801.02854 [cs]},
  eprint = {1801.02854},
  primaryclass = {cs},
  urldate = {2020-07-11},
  abstract = {We introduce the Riemannian Motion Policy (RMP), a new mathematical object for modular motion generation. An RMP is a second-order dynamical system (acceleration field or motion policy) coupled with a corresponding Riemannian metric. The motion policy maps positions and velocities to accelerations, while the metric captures the directions in the space important to the policy. We show that RMPs provide a straightforward and convenient method for combining multiple motion policies and transforming such policies from one space (such as the task space) to another (such as the configuration space) in geometrically consistent ways. The operators we derive for these combinations and transformations are provably optimal, have linearity properties making them agnostic to the order of application, and are strongly analogous to the covariant transformations of natural gradients popular in the machine learning literature. The RMP framework enables the fusion of motion policies from different motion generation paradigms, such as dynamical systems, dynamic movement primitives (DMPs), optimal control, operational space control, nonlinear reactive controllers, motion optimization, and model predictive control (MPC), thus unifying these disparate techniques from the literature. RMPs are easy to implement and manipulate, facilitate controller design, simplify handling of joint limits, and clarify a number of open questions regarding the proper fusion of motion generation methods (such as incorporating local reactive policies into long-horizon optimizers). We demonstrate the effectiveness of RMPs on both simulation and real robots, including their ability to naturally and efficiently solve complicated collision avoidance problems previously handled by more complex planners.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics}
}

@article{raviolaHarmonicDriveGear2021,
  title = {Harmonic {{Drive Gear Failures}} in {{Industrial Robots Applications}}: {{An Overview}}},
  shorttitle = {Harmonic {{Drive Gear Failures}} in {{Industrial Robots Applications}}},
  author = {Raviola, Andrea and Martin, Andrea De and Guida, Roberto and Jacazio, Giovanni and Mauro, Stefano and Sorli, Massimo},
  year = {2021},
  month = jun,
  journal = {PHM Society European Conference},
  volume = {6},
  number = {1},
  pages = {11--11},
  issn = {2325-016X},
  doi = {10.36001/phme.2021.v6i1.2849},
  urldate = {2022-08-02},
  abstract = {Industrial robots play a key role in production lines. As a direct consequence, even a slight degradation of their operating conditions could negatively affect the entire manufacturing process. To minimize economical losses, preventive measures such as planned maintenance or stand-by working stations are adopted and, in some cases, additional manipulators are installed to ensure line availability. However, since each robot performs a specific task, it degrades at a different rate from another one, so Planned Preventive Maintenance (PPM) should be replaced with Condition Based Maintenance (CBM) for a more efficient and cost-effective approach. In addition, collaborative robots (cobots) have become more and more popular in the past years increasing the level of automation, in particular in small and medium size companies. Therefore, a failure of an industrial manipulator could not only cause unexpected downtimes, but also jeopardize the safety of the personnel with whom it shares its workspace. A cobot, in fact, is labeled as safe as long as it works in nominal conditions, but this cannot be guaranteed otherwise. Within this framework, Prognostics and Health Management (PHM) techniques could be used for both a customized maintenance on a specific machine and for preventing undesired events like the aforementioned ones. An ongoing research activity at Politecnico di Torino is focused on a model-based approach able to better describe the behavior of an industrial robot in non-nominal operating conditions. In order to properly simulate faults and failures of a manipulator, it has been first necessary to analyze its failure modes and their impact on the entire system. A robot, in fact, is usually equipped with internal sensors and algorithms able to monitor its health status. However, this is only true for software or electrical failures (i.e. power supply, motor or sensors related), but it is not as effective in case of mechanical ones like bearings or gearbox wear. This paper firstly presents an overview of the possible failures related to harmonic drives which are often used in robotics due to their compact and light-weight design and the high reduction ratio. Then, it outlines the work done with the objective of providing an accurate, physics-based description of faults progression of harmonic drives used in industrial robots and the related high-fidelity models in the framework of a PHM system for fault and failure detection, isolation and remaining useful life (RUL) prediction.},
  copyright = {Copyright (c) 2021 Andrea Raviola, Andrea De Martin, Giovanni Jacazio, Stefano Mauro, Massimo Sorli, Roberto Guida},
  langid = {english},
  keywords = {Harmonic Drive}
}

@article{reedGeneralistAgent2022,
  title = {A {{Generalist Agent}}},
  author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio G{\'o}mez and Novikov, Alexander and {Barth-maron}, Gabriel and Gim{\'e}nez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and de Freitas, Nando},
  year = {2022},
  month = aug,
  journal = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  urldate = {2024-01-05},
  abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
  langid = {english}
}

@article{reedNeuralProgrammerInterpreters2015,
  title = {Neural {{Programmer-Interpreters}}},
  author = {Reed, Scott and {de Freitas}, Nando},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.06279 [cs]},
  eprint = {1511.06279},
  primaryclass = {cs},
  urldate = {2019-07-11},
  abstract = {We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-tosequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@patent{reimannVerfahrenUndSystem2011,
  title = {Verfahren Und {{System}} Zum Automatischen {{Schleifen}} Und {{Polieren}} von Lackierten {{Teilen}}},
  author = {Reimann, Markus and Sommer, J{\"o}rg},
  year = {2011},
  month = mar,
  number = {DE102009039093A1},
  urldate = {2021-03-01},
  abstract = {Ein Verfahren zum automatischen Schleifen und/oder Polieren eines lackierten Teils beinhaltet die Schritte: manuelles Erfassen von Fehlstellen des lackierten Teils in einer Fehlstellen-Erfassungsposition des lackierten Teils an einem Pr{\"u}fplatz f{\"u}r das lackierte Teil, Berechnen von Fehlstellen-Erfassungspositionsdaten, welche die Position von Fehlstellen auf dem lackierten Teil angeben, auf der Grundlage der manuell erfassten Fehlstellen des lackierten Teils, Berechnen von Fehlstellen-Bearbeitungspositionsdaten, welche die Position einer Bearbeitung auf dem lackierten Teil angeben, auf der Grundlage der berechneten Fehlstellen-Erfassungspositionsdaten, und automatisches Schleifen und/oder Polieren der manuell erfassten Fehlstellen auf der Grundlage der berechneten Fehlstellen-Bearbeitungspositionsdaten an einem Bearbeitungsplatz f{\"u}r das lackierte Teil. Ein System zur Durchf{\"u}hrung dieses Verfahrens weist hierzu eine Digitalisiereinrichtung, eine erste Berechnungseinrichtung zur Berechnung von Fehlstellen-Erfassungspositionsdaten, eine zweite Berechnungseinrichtung zum Berechnen von Fehlstellen-Bearbeitungspositionsdaten, und eine Schleif- und Poliereinrichtung zum automatischen Schleifen und/oder Polieren der erfassten Fehlstellen auf.},
  assignee = {Bayerische Motoren Werke AG},
  nationality = {DE},
  keywords = {data,defect,defects,painted,polishing}
}

@misc{RemoteDiagnosticServices,
  title = {{Remote Diagnostic Services}},
  journal = {abb.com/de},
  urldate = {2023-04-11},
  howpublished = {https://new.abb.com/de/ueber-uns/geschaeftsbereiche/prozessautomation/service/advanced-services/remote-diagnostic-services},
  langid = {ngerman}
}

@misc{RemoteOperationRobots,
  title = {Remote Operation of Robots},
  urldate = {2023-04-11},
  howpublished = {https://www.universal-robots.com/articles/ur/interface-communication/remote-operation-of-robots/},
  file = {C:\Users\benja\Zotero\storage\2IHSARDP\remote-operation-of-robots.html}
}

@inproceedings{renGeoUDFSurfaceReconstruction2023,
  title = {{{GeoUDF}}: {{Surface Reconstruction}} from {{3D Point Clouds}} via {{Geometry-guided Distance Representation}}},
  shorttitle = {{{GeoUDF}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Ren, Siyu and Hou, Junhui and Chen, Xiaodong and He, Ying and Wang, Wenping},
  year = {2023},
  pages = {14214--14224},
  urldate = {2024-01-11},
  langid = {english}
}

@misc{renGroundedSAMAssembling2024,
  title = {Grounded {{SAM}}: {{Assembling Open-World Models}} for {{Diverse Visual Tasks}}},
  shorttitle = {Grounded {{SAM}}},
  author = {Ren, Tianhe and Liu, Shilong and Zeng, Ailing and Lin, Jing and Li, Kunchang and Cao, He and Chen, Jiayu and Huang, Xinyu and Chen, Yukang and Yan, Feng and Zeng, Zhaoyang and Zhang, Hao and Li, Feng and Yang, Jie and Li, Hongyang and Jiang, Qing and Zhang, Lei},
  year = {2024},
  month = jan,
  number = {arXiv:2401.14159},
  eprint = {2401.14159},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.14159},
  urldate = {2024-08-27},
  abstract = {We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\benja\\Zotero\\storage\\AJSH3AUW\\Ren et al. - 2024 - Grounded SAM Assembling Open-World Models for Diverse Visual Tasks.pdf;C\:\\Users\\benja\\Zotero\\storage\\PERAI8RV\\2401.html}
}

@inproceedings{rettigApplicationConformalGeometric2023,
  title = {Application of {{Conformal Geometric Algebra}} in {{Robotics}}: {{DH-Parameters Extraction}} from {{Joint Axes Poses}}},
  booktitle = {Proceedings of the 18th {{International Conference}} on {{Intelligent Autonomous Systems IAS-18}}},
  author = {Rettig, Oliver and Hinderer, Fabian and Strand, Marcus},
  editor = {{IAS Society}},
  year = {2023}
}

@inproceedings{rettigUnsupervisedHumpDetection2019,
  title = {Unsupervised {{Hump Detection}} for {{Mobile Robots Based On Kinematic Measurements}} and {{Deep-Learning Based Autoencoder}}},
  booktitle = {Intelligent {{Autonomous Systems}} 15},
  author = {Rettig, Oliver and M{\"u}ller, Silvan and Strand, Marcus and Katic, Darko},
  editor = {Strand, Marcus and Dillmann, R{\"u}diger and Menegatti, Emanuele and Ghidoni, Stefano},
  year = {2019},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {99--110},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-01370-7_9},
  abstract = {Small humps on the floor go beyond the detectable scope of laser scanners and are therefore not integrated into SLAM based maps of mobile robots. However, even such small irregularities can have a tremendous effect on the robot's stability and the path quality. As a basis to develop anomaly detection algorithms, example kinematics data is collected for an overrun of a cable channel and a bulb plate. A recurrent neuronal network (RNN), based on the autoencoder principle, could be trained successfully with this data. The described RNN architecture looks promising to be used for realtime anomaly detection and also to quantify path quality.},
  isbn = {978-3-030-01370-7},
  langid = {english},
  keywords = {Anomaly detection,Deep learning,Kinematic measurement,Mobile robotics,Neural networks,Path planning}
}

@inproceedings{rettigWhichDeepArtifical2019,
  title = {Which Deep Artifical Neural Network Architecture to Use for Anomaly Detection in {{Mobile Robots}} Kinematic Data?},
  booktitle = {Machine {{Learning}} for {{Cyber Physical Systems}}},
  author = {Rettig, Oliver and M{\"u}ller, Silvan and Strand, Marcus and Katic, Darko},
  editor = {Beyerer, J{\"u}rgen and K{\"u}hnert, Christian and Niggemann, Oliver},
  year = {2019},
  series = {Technologien F{\"u}r Die Intelligente {{Automation}}},
  pages = {58--65},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-58485-9_7},
  abstract = {Small humps on the floor go beyond the detectable scope of laser scanners and are therefore not integrated into SLAM based maps of mobile robots. However, even such small irregularities can have a tremendous effect on the robot's stability and the path quality. As a basis to develop anomaly detection algorithms, kinematics data is collected exemplarily for an overrun of a cable channel and a bulb plate. A recurrent neuronal network (RNN), based on the autoencoder principle, could be trained successfully with this data. The described RNN architecture looks promising to be used for realtime anomaly detection and also to quantify path quality.},
  isbn = {978-3-662-58485-9},
  langid = {english},
  keywords = {anomaly detection,deep learning,DL4J,inertial sensor data,mobile robotics,neural networks}
}

@book{reuschSafetySecurityAwareConfiguration2022,
  title = {Safety- and {{Security-Aware Configuration Synthesis}} for {{Time-Sensitive Networking}}},
  author = {Reusch, Niklas},
  year = {2022},
  publisher = {Tecnical University of Denmark},
  abstract = {In the past decades, more and more areas of human life have become influenced by networked cyber-physical systems (CPS). Increasingly, we trust these systems to execute critical functions, such as controlling our cars and airplanes and managing dangerous processes in factories and energy systems. Hence, these CPS have stringent safety, real-time, and security requirements. In this thesis, we consider CPS that are using Time-Sensitive Networking (TSN) for communication. The IEEE 802.1 TSN standardization is developing a ``toolbox'' of many standards that extends Ethernet for safety-critical and real-time applications in several areas, e.g., automotive, aerospace, or industrial automation. TSN-based distributed CPS are composed of end-systems interconnected by network switches and duplex physical links; in TSN, communication streams from safety-critical and real-time applications can share the same communication channel with less-critical streams safely. However, the flexibility of TSN comes at the high price of a huge and poorly understood configuration space. TSN has many ``configuration knobs'', that decide, e.g., the real-time transmission of critical traffic via so-called Gate Control List (GCL) schedules, the stream priorities and their assignment to queues, and the routing of streams on disjoint paths to achieve fault-tolerance. Most TSN scheduling mechanisms are designed for homogeneous TSN networks, in which all network devices must have at least the TSN capabilities related to scheduled gates and time synchronization. However, this assumption is often unrealistic since many distributed applications use heterogeneous TSN networks with legacy or off-the shelf end-systems that are unscheduled and/or unsynchronized. In this thesis, we first propose a new scheduling paradigm for heterogeneous TSN networks that intertwines a network calculus worst-case interference analysis within the scheduling step. Thus, we support heterogeneous TSN networks featuring unscheduled and/or unsynchronized end-systems while guaranteeing the real-time properties of critical communication. Security is an important requirement in distributed CPS. We highlight the importance of addressing security at the same time with safety and timing requirements. We consider the Timed Efficient Stream Loss-Tolerant Authentication (TESLA) low-resource multicast authentication protocol to guarantee the security requirements, and redundant disjunct message routes to tolerate link failures. Given a TSN-based distributed CPS, a set of applications with tasks and messages, as well as a set of security and redundancy requirements, in the second part of the thesis we are interested to synthesize a system configuration such that the real-time, safety, and security requirements are upheld. TSN is used within the computing continuum, from interconnecting IoT devices to the networks used in Edge Computing and Cloud Computing data centers. However, as systems become larger and more interconnected, the threat level increases and untrusted devices pose high security risks. Hence, in the final part of the thesis, we consider the use of Remote Attestation (RA) to authenticate the functionality of a remote device, thus, allowing for the provision of strong assurance guarantees. We propose solutions for the automatic management of resources in the IoT to Edge Computing continuum to integrate dynamic Edge applications with safety and security-critical real-time applications. We show that our approach generates dependable configurations that can meet the timing constraints of critical applications, have enough resources to perform RA for security, and can accommodate Edge applications. The configuration synthesis challenges tackled in the thesis form intractable combinatorial optimization problems. We have used a variety of optimization algorithms, from problem-specific heuristics to metaheuristics such as Simulated Annealing, and exact methods such as Constraint Programming, to tackle these problems. These approaches are evaluated on synthetic and realistic test cases of different sizes, and their advantages and disadvantages are discussed and compared to the related work. The approaches proposed in the thesis have been implemented as open-source software prototypes and have been validated via simulations.}
}

@inproceedings{reussGoalConditionedImitationLearning2023,
  title = {Goal-{{Conditioned Imitation Learning}} Using {{Score-based Diffusion Policies}}},
  booktitle = {Robotics: {{Science}} and {{Systems XIX}}},
  author = {Reuss, Moritz and Li, Maximilian and Jia, Xiaogang and Lioutikov, Rudolf},
  year = {2023},
  month = jul,
  volume = {19},
  urldate = {2024-04-28},
  isbn = {978-0-9923747-9-2}
}

@article{Reuter.2009,
  title = {Discrete {{Laplace}}--{{Beltrami}} Operators for Shape Analysis and Segmentation},
  author = {Reuter, Martin and Biasotti, Silvia and Giorgi, Daniela and Patan{\`e}, Giuseppe and Spagnuolo, Michela},
  year = {2009},
  journal = {Computers \& Graphics},
  volume = {33},
  number = {3},
  pages = {381--390},
  issn = {00978493},
  doi = {10.1016/j.cag.2009.03.005},
  pagination = {page}
}

@article{revachKalmanNetNeuralNetwork2022,
  title = {{{KalmanNet}}: {{Neural Network Aided Kalman Filtering}} for {{Partially Known Dynamics}}},
  shorttitle = {{{KalmanNet}}},
  author = {Revach, Guy and Shlezinger, Nir and Ni, Xiaoyong and Escoriza, Adri{\`a} L{\'o}pez and {van Sloun}, Ruud J. G. and Eldar, Yonina C.},
  year = {2022},
  month = jan,
  journal = {IEEE Transactions on Signal Processing},
  volume = {70},
  pages = {1532--1547},
  issn = {1053-587X},
  doi = {10.1109/TSP.2022.3158588},
  urldate = {2024-04-30},
  abstract = {State estimation of dynamical systems in real-time is a fundamental task in signal processing. For systems that are well-represented by a fully known linear Gaussian state space (SS) model, the celebrated Kalman filter (KF) is a low complexity optimal solution. However, both linearity of the underlying SS model and accurate knowledge of it are often not encountered in practice. Here, we present KalmanNet, a real-time state estimator that learns from data to carry out Kalman filtering under non-linear dynamics with partial information. By incorporating the structural SS model with a dedicated recurrent neural network module in the flow of the KF, we retain data efficiency and interpretability of the classic algorithm while implicitly learning complex dynamics from data. We demonstrate numerically that KalmanNet overcomes non-linearities and model mismatch, outperforming classic filtering methods operating with both mismatched and accurate domain knowledge.}
}

@inproceedings{ribeiroWhyShouldTrust2016,
  title = {``{{Why Should I Trust You}}?'': {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {``{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Demonstrations}}},
  author = {Ribeiro, Marco and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = jun,
  pages = {97--101},
  publisher = {Association for Computational Linguistics},
  address = {San Diego, California},
  doi = {10.18653/v1/N16-3020},
  urldate = {2020-10-13}
}

@inproceedings{ridnikImageNet21KPretrainingMasses2021,
  title = {{{ImageNet-21K Pretraining}} for the {{Masses}}},
  booktitle = {Thirty-Fifth {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}} ({{Round}} 1)},
  author = {Ridnik, Tal and {Ben-Baruch}, Emanuel and Noy, Asaf and {Zelnik-Manor}, Lihi},
  year = {2021},
  month = jun,
  urldate = {2024-06-14},
  abstract = {ImageNet-1K serves as the primary dataset for pretraining deep learning models for computer vision tasks. ImageNet-21K dataset, which is bigger and more diverse, is used less frequently for pretraining, mainly due to its complexity, low accessibility, and underestimation of its added value. This paper aims to close this gap, and make high-quality efficient pretraining on ImageNet-21K available for everyone. Via a dedicated preprocessing stage, utilization of WordNet hierarchical structure, and a novel training scheme called semantic softmax, we show that various models significantly benefit from ImageNet-21K pretraining on numerous datasets and tasks, including small mobile-oriented models. We also show that we outperform previous ImageNet-21K pretraining schemes for prominent new models like ViT and Mixer. Our proposed pretraining pipeline is efficient, accessible, and leads to SoTA reproducible results, from a publicly available dataset. The training code and pretrained models are available at: https://github.com/Alibaba-MIIL/ImageNet21K},
  langid = {english}
}

@article{riedmillerLearningPlayingSolving2018,
  title = {Learning by {{Playing}} - {{Solving Sparse Reward Tasks}} from {{Scratch}}},
  author = {Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and {Van de Wiele}, Tom and Mnih, Volodymyr and Heess, Nicolas and Springenberg, Jost Tobias},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.10567 [cs, stat]},
  eprint = {1802.10567},
  primaryclass = {cs, stat},
  urldate = {2019-08-08},
  abstract = {We propose Scheduled Auxiliary Control (SACX), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors -- from scratch -- in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment -- enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach. A video of the rich set of learned behaviours can be found at https://youtu.be/mPKyvocNe M.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning}
}

@article{rishiyurInstantaneouslyTrainedNeural,
  title = {Instantaneously Trained Neural Networks with Complex and Quaternion Inputs},
  author = {Rishiyur, Adityan V},
  pages = {64},
  langid = {english}
}

@article{rishiyurNeuralNetworksComplex,
  title = {Neural {{Networks}} with {{Complex}} and {{Quaternion Inputs}}},
  author = {Rishiyur, Adityan},
  pages = {14},
  langid = {english}
}

@article{ritterACTRCognitiveArchitecture2019,
  title = {{{ACT-R}}: {{A}} Cognitive Architecture for Modeling Cognition},
  shorttitle = {{{ACT-R}}},
  author = {Ritter, Frank E. and Tehranchi, Farnaz and Oury, Jacob D.},
  year = {2019},
  month = may,
  journal = {Wiley Interdisciplinary Reviews. Cognitive Science},
  volume = {10},
  number = {3},
  pages = {e1488},
  issn = {1939-5086},
  doi = {10.1002/wcs.1488},
  abstract = {ACT-R is a hybrid cognitive architecture. It is comprised of a set of programmable information processing mechanisms that can be used to predict and explain human behavior including cognition and interaction with the environment. We start by reviewing its history, which shapes its current form, contrasts and relates it to other architectures, and helps readers to anticipate where it is going. Based on this history, we then describe it as a theory of cognition that is realized as a computer program. After this, we briefly discuss tools for working with ACT-R, and also note several major accomplishments that have been gained by working with ACT-R in both basic and applied science, including summarizing some of the insights about human behavior. We conclude by discussing its future, which we believe will include adding emotions and physiology, increasing usability, and the use of nongenerative models. This article is categorized under: Computer Science {$>$} Artificial Intelligence Psychology {$>$} Reasoning and Decision Making Psychology {$>$} Theory and Methods.},
  langid = {english},
  pmid = {30536740},
  keywords = {ACT-R,Cognition,cognitive architecture,Cognitive Science,Computer Simulation,human memory,Humans,Learning,modeling,Models Psychological,simulation,unified theories of cognition}
}

@inproceedings{Rivers.2007,
  title = {{{FastLSM}}},
  booktitle = {{{ACM SIGGRAPH}} 2007 Papers on - {{SIGGRAPH}} '07},
  author = {Rivers, Alec R. and James, Doug L.},
  editor = {Levoy, Marc},
  year = {2007},
  pages = {82},
  publisher = {ACM Press},
  address = {New York, New York, USA},
  doi = {10.1145/1275808.1276480},
  bookpagination = {page},
  isbn = {978-1-59593-648-6}
}

@inproceedings{robertsPerformanceLifeFluidLubricated2015,
  title = {The {{Performance}} and {{Life}} of {{Fluid-Lubricated Harmonic Drive}}{\textregistered} {{Gears}}},
  booktitle = {16th {{European Space Mechanisms}} and {{Tribology Symposium}}},
  author = {Roberts, E. and Bridgeman, Paul and Jansson, M. and Schulke, M. and Tvaruzka, Adam},
  year = {2015},
  address = {Bilbao},
  urldate = {2022-08-02},
  abstract = {It is known that the performance and life of fluidlubricated Harmonic Drive gears are dependent on a number of parameters. These include speed of rotation, output load, temperature, and type and amount of lubricant. In this paper we present and compare data on the performance and life of Harmonic Drive gears lubricated with PFPE and MAC fluid lubricants. These data include in-situ measurements of efficiency, torsional stiffness and, uniquely, axial force at the Flexspline/Wave-Generator interface made possible by a new test-rig. The latter measurement allows the friction at the WG/FS interface to be monitored which in turn provides an indication of the condition and effectiveness of the lubricant at this interface. The evolution of such parameters during the course of life tests has provided new insights into the behaviour of Harmonic Drive gears and, in particular, the fall-off in performance that defines useful life. 1. HARMONIC DRIVE GEARBOX Harmonic Drive gears are characterised by compactness, high reduction ratios, zero backlash and high positioning accuracy. Additionally they offer high power density (torque-to-weight ratio) and torsional stiffness and these, in particular, make them attractive for use in space applications [1, 2]. The first reported use of Harmonic Drive gears in space was in 1971 during the Apollo 15 mission (in actuators driving the lunar rover wheels) and since then they have found increasing use in space applications, most commonly in compact drive actuators for SADMs and APMs. In the present study testing was carried out on Harmonic Drive HFUC-20-160 component set fitted with hybridceramic Wave Generator bearings (silicon nitride balls, Cronidur{\textregistered} X30 steel races). The gearboxes were supplied fully lubricated and integrated within a testbox by Harmonic Drive AG, Limburg, Germany. Each gearbox comprises: a hybrid Wave Generator (WG) a thin-section radial bearing fitted onto an elliptical plug manufactured from 17-4PH, cond. H1150; a Circular Spline (CS) a rigid gear with internal teeth manufactured from 17-4PH steel, cond. H1150; and a Flexspline (FS) -- manufactured from 15-5PH, cond. H1075 and comprising a non-rigid, thin-walled cylindrical cup with external teeth on a slightly smaller diameter than the Circular Spline. The gear ratio is 160:1. The rated torque of the units is 40Nm. It is noted that the components parts of these gears are all sourced within Europe and that this is the first time this configuration has been subjected to thermal-vacuum testing.},
  isbn = {978-92-9221-302-2},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\L3E4ST3C\bfd697f58f663cd5101715188862aca4ca7f28c3.html}
}

@misc{RobotCellDesign2020,
  title = {Robot Cell Design via Manipulability Calculation},
  year = {2020},
  month = mar,
  publisher = {ArtiMinds Robotics},
  urldate = {2020-10-22},
  abstract = {www.artiminds.com To be able to simulate which points can{\textasciiacute}t be reached by a robot and which points may be difficult to reach, is essential when designing a robot cell. The ArtiMinds RPS feature manipulability calculation supports the user by color-coding the degree of how good an individual point can be reached by the TCP.  More information: www.artiminds.com}
}

@misc{robotichi-techsolutionsKUKA3DPrinting,
  title = {{{KUKA 3D Printing Robots}}},
  author = {{Robotic Hi-Tech Solutions}},
  urldate = {2024-04-02},
  howpublished = {https://www.robotic-hitechsolutions.com/3d-printing-robot/},
  file = {C:\Users\benja\Zotero\storage\5265B7YQ\3d-printing-robot.html}
}

@phdthesis{rocktaschelCombiningRepresentationLearning2018,
  title = {Combining {{Representation Learning}} with {{Logic}} for {{Language Processing}}},
  author = {Rockt{\"a}schel, Tim},
  year = {2018},
  month = mar,
  journal = {Doctoral thesis, UCL (University College London).},
  urldate = {2024-04-03},
  abstract = {The current state-of-the-art in many natural language processing and automated knowledge base completion tasks is held by representation learning methods which learn distributed vector representations of symbols via gradient based optimization. They require little or no hand-crafted features, thus avoiding the need for most preprocessing steps and task-specific assumptions. However, in many cases representation learning requires a large amount of annotated training data to generalize well to unseen data. Such labeled training data is provided by human annotators who often use formal logic as the language for specifying annotations. This thesis investigates different combinations of representation learning methods with logic for reducing the need for annotated training data, and for improving generalization. We introduce a mapping of function-free first-order logic rules to loss functions that we combine with neural link prediction models. Using this method, logical prior knowledge is directly embedded in vector representations of predicates and constants. We find that this method learns accurate predicate representations for which no or little training data is available, while at the same time generalizing to other predicates not explicitly stated in rules. However, this method relies on grounding first-order logic rules, which does not scale to large rule sets. To overcome this limitation, we propose a scalable method for embedding implications in a vector space by only regularizing predicate representations. Subsequently, we explore a tighter integration of representation learning and logical deduction. We introduce an end-to-end differentiable prover -- a neural network that is recursively constructed from Prolog's backward chaining algorithm. The constructed network allows us to calculate the gradient of proofs with respect to symbol representations and to learn these representations from proving facts in a knowledge base. In addition to incorporating complex first-order rules, it induces interpretable logic programs via gradient descent. Lastly, we propose recurrent neural networks with conditional encoding and a neural attention mechanism for determining the logical relationship between two natural language sentences.},
  copyright = {open},
  langid = {english},
  school = {UCL (University College London)}
}

@inproceedings{rodriguezHumanizingNAORobot2014,
  title = {Humanizing {{NAO}} Robot Teleoperation Using {{ROS}}},
  booktitle = {2014 {{IEEE-RAS International Conference}} on {{Humanoid Robots}}},
  author = {Rodriguez, I. and Astigarraga, A. and Jauregi, E. and Ruiz, T. and Lazkano, E.},
  year = {2014},
  month = nov,
  pages = {179--186},
  issn = {2164-0580},
  doi = {10.1109/HUMANOIDS.2014.7041357},
  abstract = {The work presented here proposes two different ROS packages to enrich the teleoperation of the robot NAO: speech-based teleoperation (in Basque) and gesture-based teleoperation together with arm control. These packages have been used and evaluated in a human mimicking experiment. The tools offered can serve as a base for many applications.},
  keywords = {arm control,gesture recognition,gesture-based teleoperation,Google,human mimicking experiment,humanizing NAO robot teleoperation,humanoid robots,Joints,Legged locomotion,Robot kinematics,ROS packages,Speech,speech processing,Speech recognition,speech-based teleoperation,telerobotics},
  file = {C:\Users\benja\Zotero\storage\VNEDLR3L\7041357.html}
}

@inproceedings{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis With Latent Diffusion Models}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  pages = {10684--10695},
  urldate = {2024-04-28},
  langid = {english}
}

@article{romboutsBiologicallyPlausibleReinforcement2013,
  title = {Biologically Plausible Reinforcement Learning of Continuous Actions},
  author = {Rombouts, Jaldert O. and Roelfsema, Pieter R. and Bohte, Sander M.},
  year = {2013},
  month = jul,
  journal = {BMC Neuroscience},
  volume = {14},
  number = {1},
  pages = {P28},
  issn = {1471-2202},
  doi = {10.1186/1471-2202-14-S1-P28},
  urldate = {2024-04-19},
  keywords = {Continuous Action,Discrete Action,Motor Unit,Reinforcement Learning,Sensory Input},
  file = {C:\Users\benja\Zotero\storage\KZCRRI45\1471-2202-14-S1-P28.html}
}

@article{romeroGeneticProgrammingBasedLowLevel2020,
  title = {A {{Genetic Programming-Based Low-Level Instructions Robot}} for {{Realtimebattle}}},
  author = {Romero, Juan and Santos, Antonino and Carballal, Adrian and {Rodriguez-Fernandez}, Nereida and Santos, Iria and {Torrente-Pati{\~n}o}, Alvaro and Tu{\~n}as, Juan and Machado, Penousal},
  year = {2020},
  month = dec,
  journal = {Entropy},
  volume = {22},
  number = {12},
  pages = {1362},
  issn = {1099-4300},
  doi = {10.3390/e22121362},
  urldate = {2022-03-10},
  abstract = {RealTimeBattle is an environment in which robots controlled by programs fight each other. Programs control the simulated robots using low-level messages (e.g., turn radar, accelerate). Unlike other tools like Robocode, each of these robots can be developed using different programming languages. Our purpose is to generate, without human programming or other intervention, a robot that is highly competitive in RealTimeBattle. To that end, we implemented an Evolutionary Computation technique: Genetic Programming. The robot controllers created in the course of the experiments exhibit several different and effective combat strategies such as avoidance, sniping, encircling and shooting. To further improve their performance, we propose a function-set that includes short-term memory mechanisms, which allowed us to evolve a robot that is superior to all of the rivals used for its training. The robot was also tested in a bout with the winner of the previous ``RealTimeBattle Championship'', which it won. Finally, our robot was tested in a multi-robot battle arena, with five simultaneous opponents, and obtained the best results among the contenders.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {artificial intelligence,creative computation,evolutionary game,evolutionary robotics,genetic programming,RealTimeBattle,robots},
  file = {C\:\\Users\\benja\\Zotero\\storage\\GMEN2XRA\\Romero et al. - 2020 - A Genetic Programming-Based Low-Level Instructions.pdf;C\:\\Users\\benja\\Zotero\\storage\\7K4LYP8D\\1362.html}
}

@article{rosadoUsingKinectRobot2014,
  title = {Using {{Kinect}} for {{Robot Gesture Imitation}}},
  author = {Rosado, Jos{\'e} and Silva, Filipe and Santos, V{\'i}tor},
  year = {2014},
  month = jan,
  journal = {Procedia Technology},
  series = {Conference on {{Electronics}}, {{Telecommunications}} and {{Computers}} -- {{CETC}} 2013.},
  volume = {17},
  pages = {423--430},
  issn = {2212-0173},
  doi = {10.1016/j.protcy.2014.10.250},
  urldate = {2021-01-26},
  abstract = {In this paper, we compare two approaches for the reproduction and generalization of human-like movements by an upper-body humanoid robot. The specific purpose is to gain new insights into a key component of imitation learning: the ability to generalize from very few examples. In line with this, we aim to accurately portray the characteristics and generalization performance of those approaches based on the paradigm of motion primitives. Specifically, we focus on two established techniques, the dynamic motion primitives (DMP) and the principal component analysis (PCA). First, we recorded demonstrations from human subjects executing a reaching task in order to create a database of reaching motions. Then, the multiple demonstrations are mapped to an upper-body humanoid robot through an inverse kinematics algorithm that attempts to track the kinematics of the movement of human arms. Finally, the generalization performance is evaluated and the feasibility of the two approaches is discussed for this study of reaching movements.},
  langid = {english},
  keywords = {generalization performance,motion capture,motion primitives,movement reproduction,principal components}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0042519},
  urldate = {2019-07-10},
  langid = {english}
}

@techreport{rosenblattPrinciplesNeurodynamicsPerceptrons1961,
  title = {Principles of {{Neurodynamics}}: {{Perceptrons}} and the {{Theory}} of {{Brain Mechanisms}}},
  author = {Rosenblatt, Frank},
  year = {1961},
  month = mar,
  number = {VG-1196-G-8},
  address = {Buffalo, NY},
  institution = {Cornell Aeronautical Laboratory}
}

@inproceedings{rosenbloomRethinkingPhysicalSymbol2023,
  title = {Rethinking the {{Physical Symbol Systems Hypothesis}}},
  booktitle = {Artificial {{General Intelligence}}},
  author = {Rosenbloom, Paul S.},
  editor = {Hammer, Patrick and Alirezaie, Marjan and Stranneg{\aa}rd, Claes},
  year = {2023},
  pages = {207--216},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-33469-6_21},
  abstract = {It is now more than a half-century since the Physical Symbol Systems Hypothesis (PSSH) was first articulated as an empirical hypothesis. More recent evidence from work with neural networks and cognitive architectures has weakened it, but it has not yet been replaced in any satisfactory manner. Based on a rethinking of the nature of computational symbols -- as atoms or placeholders -- and thus also of the systems in which they participate, a hybrid approach is introduced that responds to these challenges while also helping to bridge the gap between symbolic and neural approaches, resulting in two new hypotheses, one that is to replace the PSSH and the other focused more directly on cognitive architectures.},
  isbn = {978-3-031-33469-6},
  langid = {english},
  keywords = {Cognitive Architectures,Hybrid Symbol Systems,Neural Networks,Physical Symbol Systems}
}

@inproceedings{rosenthalModelingHumansObservation2011,
  title = {Modeling Humans as Observation Providers Using {{POMDPs}}},
  booktitle = {2011 {{RO-MAN}}},
  author = {Rosenthal, Stephanie and Veloso, Manuela},
  year = {2011},
  month = jul,
  pages = {53--58},
  issn = {1944-9437},
  doi = {10.1109/ROMAN.2011.6005272},
  urldate = {2024-08-16},
  abstract = {The ability to obtain accurate observations while navigating in uncertain environments is a difficult challenge in deploying robots. Robots have relied heavily on human supervisors who are always available to provide additional observations to reduce uncertainty. We are instead interested in taking advantage of humans who are already in the environment to receive observations. The challenge is in modeling these humans' availability and higher costs of interruption to determine when to query them during navigation. In this work, we introduce a Human Observation Provider POMDP framework (HOP-POMDP), and contribute new algorithms for planning and executing with HOP-POMDPs that account for the differences between humans and other probabilistic sensors that provide observations. We compare optimal HOP-POMDP policies that plan for needing humans' observations with oracle POMDP policies that do not take human costs and availability into account. We show in benchmark tests and real-world environments that the oracle policies match the optimal HOP-POMDP policy 60\% of the time, and can be used in cases when humans are likely to be available on the shortest paths. However, the HOP-POMDP policies receive higher rewards in general as they take into account the possibility that a human may be unavailable. HOP-POMDP policies only need to be computed once prior to the deployment of the robot, so it is feasible to precompute and use in practice.},
  keywords = {Availability,Benchmark testing,Humans,Navigation,Planning,Robot sensing systems}
}

@article{rossExploitingPotentialUnlabeled2018,
  title = {Exploiting the Potential of Unlabeled Endoscopic Video Data with Self-Supervised Learning},
  author = {Ross, Tobias and Zimmerer, David and Vemuri, Anant and Isensee, Fabian and Wiesenfarth, Manuel and Bodenstedt, Sebastian and Both, Fabian and Kessler, Philip and Wagner, Martin and M{\"u}ller, Beat and Kenngott, Hannes and Speidel, Stefanie and {Kopp-Schneider}, Annette and {Maier-Hein}, Klaus and {Maier-Hein}, Lena},
  year = {2018},
  month = jun,
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  volume = {13},
  number = {6},
  pages = {925--933},
  issn = {1861-6429},
  doi = {10.1007/s11548-018-1772-0},
  abstract = {PURPOSE: Surgical data science is a new research field that aims to observe all aspects of the patient treatment process in order to provide the right assistance at the right time. Due to the breakthrough successes of deep learning-based solutions for automatic image annotation, the availability of reference annotations for algorithm training is becoming a major bottleneck in the field. The purpose of this paper was to investigate the concept of self-supervised learning to address this issue. METHODS: Our approach is guided by the hypothesis that unlabeled video data can be used to learn a representation of the target domain that boosts the performance of state-of-the-art machine learning algorithms when used for pre-training. Core of the method is an auxiliary task based on raw endoscopic video data of the target domain that is used to initialize the convolutional neural network (CNN) for the target task. In this paper, we propose the re-colorization of medical images with a conditional generative adversarial network (cGAN)-based architecture as auxiliary task. A variant of the method involves a second pre-training step based on labeled data for the target task from a related domain. We validate both variants using medical instrument segmentation as target task. RESULTS: The proposed approach can be used to radically reduce the manual annotation effort involved in training CNNs. Compared to the baseline approach of generating annotated data from scratch, our method decreases exploratively the number of labeled images by up to 75\% without sacrificing performance. Our method also outperforms alternative methods for CNN pre-training, such as pre-training on publicly available non-medical (COCO) or medical data (MICCAI EndoVis2017 challenge) using the target task (in this instance: segmentation). CONCLUSION: As it makes efficient use of available (non-)public and (un-)labeled data, the approach has the potential to become a valuable tool for CNN (pre-)training.},
  langid = {english},
  pmid = {29704196},
  keywords = {Algorithms,Computer vision,Endoscopic image processing,Endoscopic instrument segmentation,Endoscopy,Humans,Neural Networks Computer,Self-supervised learning,Supervised Machine Learning,Transfer learning,Video Recording}
}

@article{rougierImplicitExplicitRepresentations2009,
  title = {Implicit and Explicit Representations},
  author = {Rougier, Nicolas P.},
  year = {2009},
  month = mar,
  journal = {Neural Networks},
  series = {What It {{Means}} to {{Communicate}}},
  volume = {22},
  number = {2},
  pages = {155--160},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2009.01.008},
  urldate = {2024-04-17},
  abstract = {During the past decades, the symbol grounding problem, as has been identified by Harnard [Harnard, S. (1990). The symbol grounding problem. Physica D: Nonlinear Phenomena, 42, 335--346], became a prominent problem in the cognitive science society. The idea that a symbol is much more than a mere meaningless token that can be processed through some algorithm, sheds new light on higher brain functions such as language and cognition. We present in this article a computational framework that may help in our understanding of the nature of grounded representations. Two models are briefly introduced that aim at emphasizing the difference we make between implicit and explicit representations.},
  keywords = {Computational neuroscience,Embodied cognition,Representation,Symbol},
  file = {C:\Users\benja\Zotero\storage\8ZGJF29K\S0893608009000112.html}
}

@incollection{rovidaSkiROSSkillBasedRobot2017,
  title = {{{SkiROS}}---{{A Skill-Based Robot Control Platform}} on {{Top}} of {{ROS}}},
  booktitle = {Robot {{Operating System}} ({{ROS}}): {{The Complete Reference}}  ({{Volume}} 2)},
  author = {Rovida, Francesco and Crosby, Matthew and Holz, Dirk and Polydoros, Athanasios S. and Gro{\ss}mann, Bjarne and Petrick, Ronald P. A. and Kr{\"u}ger, Volker},
  editor = {Koubaa, Anis},
  year = {2017},
  series = {Studies in {{Computational Intelligence}}},
  pages = {121--160},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-54927-9_4},
  urldate = {2022-05-06},
  abstract = {The development of cognitive robots in ROS still lacks the support of some key components: a knowledge integration framework and a framework for autonomous mission execution. In this research chapter, we will discuss our skill-based platform SkiROS, that was developed on top of ROS in order to organize robot knowledge and its behavior. We will show how SkiROS offers the possibility to integrate different functionalities in form of skill `apps' and how SkiROS offers services for integrating these skill-apps into a consistent workspace. Furthermore, we will show how these skill-apps can be automatically executed based on autonomous, goal-directed task planning. SkiROS helps the developers to program and port their high-level code over a heterogeneous range of robots, meanwhile the minimal Graphical User Interface (GUI) allows non-expert users to start and supervise the execution. As an application example, we present how SkiROS was used to vertically integrate a robot into the manufacturing system of PSA Peugeot-Citro{\"e}n. We will discuss the characteristics of the SkiROS architecture which makes it not limited to the automotive industry but flexible enough to be used in other application areas as well. SkiROS has been developed on Ubuntu 14.04 LTS and ROS indigo and it can be downloaded at https://github.com/frovida/skiros. A demonstration video is also available at https://youtu.be/mo7UbwXW5W0.},
  isbn = {978-3-319-54927-9},
  langid = {english},
  keywords = {Autonomous robot,Kitting task,Knowledge integration,Planning,Skills,Software engineering}
}

@misc{RSS2021Workshop2021,
  title = {{{RSS}} 2021 {{Workshop}} on {{Declarative}} and {{Neurosymbolic Representations}} in {{Robot Learning}} and {{Control}}},
  year = {2021},
  month = jul,
  urldate = {2022-05-06},
  howpublished = {https://dnr-rob.github.io/},
  file = {C:\Users\benja\Zotero\storage\SJID9XWQ\dnr-rob.github.io.html}
}

@phdthesis{ruderNeuralTransferLearning2019,
  title = {Neural {{Transfer Learning}} for {{Natural Language Processing}}},
  author = {Ruder, Sebastian},
  year = {2019},
  month = feb,
  langid = {english},
  school = {National University of Ireland},
  file = {C:\Users\benja\Zotero\storage\GE77BFVB\Ruder - Neural Transfer Learning for Natural Language Proc.pdf}
}

@article{ruderOverviewGradientDescent2016,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  year = {2016},
  month = sep,
  journal = {arXiv:1609.04747 [cs]},
  eprint = {1609.04747},
  primaryclass = {cs},
  urldate = {2019-08-22},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning}
}

@article{rudinStopExplainingBlack2019,
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  author = {Rudin, Cynthia},
  year = {2019},
  month = may,
  journal = {Nature Machine Intelligence},
  volume = {1},
  number = {5},
  pages = {206--215},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0048-x},
  urldate = {2021-01-04},
  abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
  copyright = {2019 Springer Nature Limited},
  langid = {english}
}

@techreport{ruessAIEngineeringProgressing2021,
  type = {Annual Report},
  title = {{{AI Engineering}}: {{Progressing}} towards {{Robust}} and {{Trustworthy AI Systems}}},
  author = {Rue{\ss}, Harald and Ye, Xin},
  year = {2021},
  month = jun,
  address = {Munich, Germany},
  institution = {fortiss GmbH},
  urldate = {2022-05-05},
  file = {C:\Users\benja\Zotero\storage\8NYZZRU4\fortiss_report_AI_Engineering_en_web.pdf}
}

@techreport{ruessSafeAIHow2022,
  title = {Safe {{AI}} - {{How}} Is This Possible?},
  author = {Rue{\ss}, Harald and Burton, Simon},
  year = {2022},
  institution = {Fraunhofer Institute for Cognitive Systems IKS},
  urldate = {2024-03-01},
  abstract = {How is it possible to develop safe software systems based on artificial intelligence? As simple as the question sounds, a comprehensive answer would be complex. Therefore, the white paper "Safe AI - How is it possible?" focuses on the technical design and the engineering principles of safe AI-based systems. Both of these aspects are critical to the use of cognitive systems in safety-critical applications. An example is an automatic emergency braking system in a vehicle. It can be considered safe if its use prevents accidents at least to a tolerable level in predefined situations. The white paper identifies key challenges in specification, uncertainty, safety, design, analysis, and maintenance that could jeopardize the implementation of this rigorous and precise technical design.},
  langid = {english}
}

@article{ruessSystemsChallengesTrustworthy2022,
  title = {Systems {{Challenges}} for {{Trustworthy Embodied Systems}}},
  author = {Rue{\ss}, Harald},
  year = {2022},
  month = apr,
  journal = {arXiv:2201.03413 [cs]},
  eprint = {2201.03413},
  primaryclass = {cs},
  urldate = {2022-05-05},
  abstract = {A new generation of increasingly autonomous and self-learning embodied systems is about to be developed. When deploying embodied systems into a real-life context we face various engineering challenges, as it is crucial to coordinate the behavior of embodied systems in a beneficial manner, ensure their compatibility with our human-centered social values, and design verifiably safe and reliable human-machine interaction. We are arguing that traditional systems engineering is coming to a climacteric from embedded to embodied systems, and with assuring the trustworthiness of dynamic federations of situationally aware, intent-driven, explorative, ever-evolving, largely non-predictable, and increasingly autonomous embodied systems in uncertain, complex, and unpredictable real-world contexts. We are therefore identifying a number of urgent systems challenges for trustworthy embodied systems, including robust and human-centric AI, cognitive architectures, uncertainty quantification, trustworthy self-integration, and continual analysis and assurance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  file = {C:\Users\benja\Zotero\storage\JPEWE52W\2201.html}
}

@article{ruizUnifyingFunctionalUser2021,
  title = {Unifying {{Functional User Interface Design Principles}}},
  author = {Ruiz, Jenny and Serral, Estefan{\'i}a and Snoeck, Monique},
  year = {2021},
  month = jan,
  journal = {International Journal of Human--Computer Interaction},
  volume = {37},
  number = {1},
  pages = {47--67},
  publisher = {Taylor \& Francis},
  issn = {1044-7318},
  doi = {10.1080/10447318.2020.1805876},
  urldate = {2024-02-01},
  abstract = {Many designers and User Interface (UI) educators discuss principles to be followed when designing the functional aspects of a UI. However, many UI principles have been proposed, scattered in scientific papers and teaching books. Some principles are different, and many are somehow similar or overlapping with others. This makes it very difficult to comprehend where to pay attention to when designing a UI. In this paper, we perform a systematic literature review to first identify the most relevant authors in the domain of functional UI design principles. Focusing on the three most cited works of these authors, we extracted 257 principles. We next analyzed all these principles, unified their variants, and, considering their scientific influence, finally derived a shorter and core selection of 36 principles. This core selection provides educators and UI designers with a clear path to teach, evaluate, learn and improve the UI functional design.}
}

@article{Rume86,
  title = {Learning {{Representations}} by {{Back-Propagating Errors}}},
  author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year = {1986},
  volume = {323},
  pages = {533--536}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  urldate = {2024-04-27},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  copyright = {1986 Springer Nature Limited},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science}
}

@article{ruscelliHorizonTrajectoryOptimization2022,
  title = {Horizon: {{A Trajectory Optimization Framework}} for {{Robotic Systems}}},
  shorttitle = {Horizon},
  author = {Ruscelli, Francesco and Laurenzi, Arturo and Tsagarakis, Nikos G. and Mingo Hoffman, Enrico},
  year = {2022},
  journal = {Frontiers in Robotics and AI},
  volume = {9},
  issn = {2296-9144},
  urldate = {2023-04-23},
  abstract = {This paper presents Horizon, an open-source framework for trajectory optimization tailored to robotic systems that implements a set of tools to simplify the process of dynamic motion generation. Its user-friendly Python-based API allows designing the most complex robot motions using a simple and intuitive syntax. At the same time, the modular structure of Horizon allows for easy customization on many levels, providing several recipes to handle fixed and floating-base systems, contact switching, variable time nodes, multiple transcriptions, integrators and solvers to guarantee flexibility towards diverse tasks. The proposed framework relies on direct simultaneous methods to transcribe the optimal problem into a nonlinear programming problem that can be solved by state-of-the-art solvers. In particular, it provides several off-the-shelf solvers, as well as two custom-implemented solvers, i.e. GN-SQP and Iterative Linear-Quadratic Regulator. Solutions of optimized problems can be stored for warm-starting, and re-sampled at a different frequency while enforcing dynamic feasibility. The proposed framework is validated through a number of use-case scenarios involving several robotic platforms. Finally, an in-depth analysis of a specific case study is carried out, where a highly dynamic motion (i.e., a twisting jump using the quadruped robot Spot{\textregistered} from BostonDynamics1) is generated, in order to highlight the main features of the framework and demonstrate its capabilities.}
}

@book{russellArtificialIntelligenceModern2021,
  title = {{Artificial Intelligence: A Modern Approach}},
  shorttitle = {{Artificial Intelligence}},
  author = {Russell, Stuart and Norvig, Peter},
  year = {2021},
  month = may,
  edition = {4},
  publisher = {Pearson},
  address = {Harlow},
  abstract = {Thelong-anticipated revision of ArtificialIntelligence: A Modern Approach explores the full breadth and depth of the field of artificialintelligence (AI). The 4th Edition brings readers up to date on the latest technologies,presents concepts in a more unified manner, and offers new or expanded coverageof machine learning, deep learning, transfer learning, multi agent systems,robotics, natural language processing, causality, probabilistic programming,privacy, fairness, and safe AI.},
  isbn = {978-1-292-40113-3},
  langid = {Englisch}
}

@inproceedings{Rusu.2008,
  title = {Learning Informative Point Classes for the Acquisition of Object Model Maps},
  booktitle = {2008 10th International Conference on Control, Automation, Robotics and Vision},
  author = {Rusu, Radu Bogdan and Marton, Zoltan Csaba and Blodow, Nico and Beetz, Michael},
  year = {2008},
  pages = {643--650},
  publisher = {IEEE},
  doi = {10.1109/ICARCV.2008.4795593},
  bookpagination = {page},
  isbn = {978-1-4244-2286-9}
}

@inproceedings{Rusu.2009,
  title = {Fast Point Feature Histograms ({{FPFH}}) for {{3D}} Registration},
  booktitle = {2009 {{IEEE}} International Conference on Robotics and Automation},
  author = {Rusu, Radu Bogdan and Blodow, Nico and Beetz, Michael},
  year = {2009},
  pages = {3212--3217},
  publisher = {IEEE},
  doi = {10.1109/ROBOT.2009.5152473},
  bookpagination = {page},
  isbn = {978-1-4244-2788-8}
}

@inproceedings{rusu3DHerePoint2011,
  title = {{{3D}} Is Here: {{Point Cloud Library}} ({{PCL}})},
  shorttitle = {{{3D}} Is Here},
  booktitle = {2011 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Rusu, Radu Bogdan and Cousins, Steve},
  year = {2011},
  month = may,
  pages = {1--4},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2011.5980567},
  urldate = {2024-07-27},
  abstract = {With the advent of new, low-cost 3D sensing hardware such as the Kinect, and continued efforts in advanced point cloud processing, 3D perception gains more and more importance in robotics, as well as other fields. In this paper we present one of our most recent initiatives in the areas of point cloud perception: PCL (Point Cloud Library - http://pointclouds.org). PCL presents an advanced and extensive approach to the subject of 3D perception, and it's meant to provide support for all the common 3D building blocks that applications need. The library contains state-of-the art algorithms for: filtering, feature estimation, surface reconstruction, registration, model fitting and segmentation. PCL is supported by an international community of robotics and perception researchers. We provide a brief walkthrough of PCL including its algorithmic capabilities and implementation strategies.},
  keywords = {Cloud computing,Codes,Data visualization,Libraries,Point cloud compression,Robots,Three-dimensional displays},
  file = {C:\Users\benja\Zotero\storage\2GH8H4KJ\5980567.html}
}

@inproceedings{sacerdotiNonlinearNaturePlans1975,
  title = {The {{Nonlinear Nature}} of {{Plans}}},
  booktitle = {International {{Joint Conference}} on {{Artificial Intelligence}}},
  author = {Sacerdoti, E.},
  year = {1975},
  month = sep,
  urldate = {2024-04-24},
  abstract = {We usually think of plans as linear sequences of actions. This is because plans are usually executed one step at a time. But plans themselves are not constrained by limitations of linearity. This paper describes a new information structure, called the procedural net, that represents a plan as a partial ordering of actions with respec to time. By avoiding premature commitments to a particular order for achieving subgoals, a problem-solving system using this representation can deal easily and directly with problems that are otherwise very difficult to solve.}
}

@article{saeidiAutonomousRoboticLaparoscopic2022,
  title = {Autonomous Robotic Laparoscopic Surgery for Intestinal Anastomosis},
  author = {Saeidi, H. and Opfermann, J. D. and Kam, M. and Wei, S. and Leonard, S. and Hsieh, M. H. and Kang, J. U. and Krieger, A.},
  year = {2022},
  month = jan,
  journal = {Science Robotics},
  volume = {7},
  number = {62},
  pages = {eabj2908},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/scirobotics.abj2908},
  urldate = {2024-04-11},
  abstract = {Autonomous robotic surgery has the potential to provide efficacy, safety, and consistency independent of individual surgeon's skill and experience. Autonomous anastomosis is a challenging soft-tissue surgery task because it requires intricate imaging, tissue tracking, and surgical planning techniques, as well as a precise execution via highly adaptable control strategies often in unstructured and deformable environments. In the laparoscopic setting, such surgeries are even more challenging because of the need for high maneuverability and repeatability under motion and vision constraints. Here we describe an enhanced autonomous strategy for laparoscopic soft tissue surgery and demonstrate robotic laparoscopic small bowel anastomosis in phantom and in vivo intestinal tissues. This enhanced autonomous strategy allows the operator to select among autonomously generated surgical plans and the robot executes a wide range of tasks independently. We then use our enhanced autonomous strategy to perform in vivo autonomous robotic laparoscopic surgery for intestinal anastomosis on porcine models over a 1-week survival period. We compared the anastomosis quality criteria---including needle placement corrections, suture spacing, suture bite size, completion time, lumen patency, and leak pressure---of the developed autonomous system, manual laparoscopic surgery, and robot-assisted surgery (RAS). Data from a phantom model indicate that our system outperforms expert surgeons' manual technique and RAS technique in terms of consistency and accuracy. This was also replicated in the in vivo model. These results demonstrate that surgical robots exhibiting high levels of autonomy have the potential to improve consistency, patient outcomes, and access to a standard surgical technique.}
}

@inproceedings{safronovTaskPlanningBelief2020,
  title = {Task {{Planning}} with {{Belief Behavior Trees}}},
  booktitle = {2020 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Safronov, Evgenii and Colledanchise, Michele and Natale, Lorenzo},
  year = {2020},
  month = oct,
  pages = {6870--6877},
  issn = {2153-0866},
  doi = {10.1109/IROS45743.2020.9341562},
  urldate = {2024-04-25},
  abstract = {In this paper, we propose Belief Behavior Trees (BBTs), an extension to Behavior Trees (BTs) that allows to automatically create a policy that controls a robot in partially observable environments. We extend the semantic of BTs to account for the uncertainty that affects both the conditions and action nodes of the BT. The tree gets synthesized following a planning strategy for BTs proposed recently: from a set of goal conditions we iteratively select a goal and find the action, or in general the subtree, that satisfies it. Such action may have preconditions that do not hold. For those preconditions, we find an action or subtree in the same fashion. We extend this approach by including, in the planner, actions that have the purpose to reduce the uncertainty that affects the value of a condition node in the BT (for example, turning on the lights to have better lighting conditions). We demonstrate that BBTs allows task planning with non-deterministic outcomes for actions. We provide experimental validation of our approach in a real robotic scenario and - for sake of reproducibility - in a simulated one.},
  keywords = {Lighting,Planning,Probabilistic logic,Semantics,Task analysis,Turning,Uncertainty},
  file = {C:\Users\benja\Zotero\storage\B2XCBRIV\9341562.html}
}

@techreport{sagelKnowledgeInvarianceHistory,
  title = {Knowledge as {{Invariance}}: {{History}} and {{Perspectives}} of {{Knowledge-Augmented Machine Learning}}},
  author = {Sagel, Alexander and Sahu, Amit and Matthes, Stefan and Pfeifer, Holger and Qiu, Tianming and Rue{\ss}, Harald and Shen, Hao and W{\"o}rmann, Julian},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\DXEGIPVG\Knowledge as Invariance.pdf}
}

@article{sakib-uz-zamanPolymerBasedAdditiveManufacturing2023,
  title = {Polymer-{{Based Additive Manufacturing}} for {{Orthotic}} and {{Prosthetic Devices}}: {{Industry Outlook}} in {{Canada}}},
  shorttitle = {Polymer-{{Based Additive Manufacturing}} for {{Orthotic}} and {{Prosthetic Devices}}},
  author = {{Sakib-Uz-Zaman}, Chowdhury and Khondoker, Mohammad Abu Hasan},
  year = {2023},
  month = jan,
  journal = {Polymers},
  volume = {15},
  number = {6},
  pages = {1506},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2073-4360},
  doi = {10.3390/polym15061506},
  urldate = {2024-04-03},
  abstract = {The conventional manufacturing methods for fabricating orthotic and prosthetic (O\&P) devices have been in practice for a very long time. Recently, O\&P service providers have started exploring different advanced manufacturing techniques. The objective of this paper is to perform a mini review on recent progress in the use of polymer-based additive manufacturing (AM) for O\&P devices and to gather insights from the O\&P professionals on the current practices and technologies and on the prospect of using AM techniques in this field. In our study, first, scientific articles on AM for O\&P devices were studied. Then, twenty-two (22) interviews were conducted with O\&P professionals from Canada. The primary focus was on five key areas: cost, material, design and fabrication efficiency, structural strength, functionality, and patient satisfaction. The cost of manufacturing the O\&P devices using AM techniques is lower as compared to the conventional methods. O\&P professionals expressed their concern over the materials and structural strength of the 3D-printed prosthetic devices. Published articles report comparable functionality and patient satisfaction for both O\&P devices. AM also greatly improves design and fabrication efficiency. However, due to a lack of qualification standards for 3D printed O\&P devices, 3D printing is being embraced more slowly in the O\&P business than in other industries.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {3D printing,additive manufacturing,orthosis,orthotics,orthotist,prosthesis,prosthetics,prosthetist}
}

@inproceedings{salemiEvaluatingRetrievalQuality2024,
  title = {Evaluating {{Retrieval Quality}} in {{Retrieval-Augmented Generation}}},
  booktitle = {Proceedings of the 47th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Salemi, Alireza and Zamani, Hamed},
  year = {2024},
  month = jul,
  series = {{{SIGIR}} '24},
  pages = {2395--2400},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3626772.3657957},
  urldate = {2024-09-04},
  abstract = {Evaluating retrieval-augmented generation (RAG) presents challenges, particularly for retrieval models within these systems. Traditional end-to-end evaluation methods are computationally expensive. Furthermore, evaluation of the retrieval model's performance based on query-document relevance labels shows a small correlation with the RAG system's downstream performance. We propose a novel evaluation approach, eRAG, where each document in the retrieval list is individually utilized by the large language model within the RAG system. The output generated for each document is then evaluated based on the downstream task ground truth labels. In this manner, the downstream performance for each document serves as its relevance label. We employ various downstream task metrics to obtain document-level annotations and aggregate them using set-based or ranking metrics. Extensive experiments on a wide range of datasets demonstrate that eRAG achieves a higher correlation with downstream RAG performance compared to baseline methods, with improvements in Kendall's tau correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant computational advantages, improving runtime and consuming up to 50 times less GPU memory than end-to-end evaluation.},
  isbn = {9798400704314},
  file = {C:\Users\benja\Zotero\storage\KTZ8G7FQ\Salemi and Zamani - 2024 - Evaluating Retrieval Quality in Retrieval-Augmented Generation.pdf}
}

@article{Salimans2017a,
  title = {Evolution {{Strategies}} as a {{Scalable Alternative}} to {{Reinforcement Learning}}},
  author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  year = {2017},
  month = mar,
  eprint = {1703.03864},
  urldate = {2018-09-01},
  abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
  archiveprefix = {arXiv}
}

@article{salimansEvolutionStrategiesScalable2017,
  title = {Evolution {{Strategies}} as a {{Scalable Alternative}} to {{Reinforcement Learning}}},
  author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  year = {2017},
  month = sep,
  journal = {arXiv:1703.03864 [cs, stat]},
  eprint = {1703.03864},
  primaryclass = {cs, stat},
  urldate = {2020-07-16},
  abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\MBPG5UAD\1703.html}
}

@incollection{salkinConceptualFrameworkIndustry2018,
  title = {A {{Conceptual Framework}} for {{Industry}} 4.0},
  booktitle = {Industry 4.0: {{Managing The Digital Transformation}}},
  author = {Salkin, Ceren and Oner, Mahir and Ustundag, Alp and Cevikcan, Emre},
  editor = {Ustundag, Alp and Cevikcan, Emre},
  year = {2018},
  pages = {3--23},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-57870-5_1},
  urldate = {2024-09-17},
  abstract = {Industrial Revolution emerged many improvements in manufacturingManufacturingand service systems. Because of remarkable and rapid changes appeared in manufacturing and information technology,Information technologysynergy aroused from the integration of the advancements in information technology, services and manufacturing were realized. These advancements conduced to the increasing productivity both in service systems and manufacturing environment. In recent years, manufacturing companies and service systems have been faced substantial challenges due to the necessity in the coordination and connection of disruptive concepts such as communication and networking (Industrial Internet)Industrial internet, embedded systems (Cyber Physical Systems), adaptive roboticsAdaptive robotics, cyber security, data analytics and artificial intelligenceArtificial intelligence, and additive manufacturing. These advancements caused the extension of the developments in manufacturing and information technology, and these coordinated and communicative technologies are constituted to the term, Industry 4.0 which was first announced from German government as one of the key initiatives and highlights a new industrial revolution. As a result, Industry 4.0 indicates more productive systems; companies have been searching the right adaptation of this term. On the other hand, the achievement criteria and performance measurements of the transformation to Industry 4.0 are still uncertain. Additionally, a structured and systematic implementation roadmap is still not clear. Thus, in this study, the fundamental relevance between design principles and technologies is given and conceptual framework for Industry 4.0 is proposed concerning fundamentals of smart products and smart processes development.},
  isbn = {978-3-319-57870-5},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\ZJSE3ZJN\Salkin et al. - 2018 - A Conceptual Framework for Industry 4.0.pdf}
}

@article{salvadorAccurateDynamicTime2007,
  title = {Toward Accurate Dynamic Time Warping in Linear Time and Space},
  author = {Salvador, Stan and Chan, Philip},
  year = {2007},
  month = oct,
  journal = {Intelligent Data Analysis},
  volume = {11},
  number = {5},
  pages = {561--580},
  issn = {1088-467X},
  abstract = {Dynamic Time Warping (DTW) has a quadratic time and space complexity that limits its use to small time series. In this paper we introduce FastDTW, an approximation of DTW that has a linear time and space complexity. FastDTW uses a multilevel approach that recursively projects a solution from a coarser resolution and refines the projected solution. We prove the linear time and space complexity of FastDTW both theoretically and empirically. We also analyze the accuracy of FastDTW by comparing it to two other types of existing approximate DTW algorithms: constraints (such as Sakoe-Chiba Bands) and abstraction. Our results show a large improvement in accuracy over existing methods.}
}

@article{Sanchez.2018,
  title = {Robotic Manipulation and Sensing of Deformable Objects in Domestic and Industrial Applications: A Survey},
  author = {Sanchez, Jose and Corrales, Juan-Antonio and Bouzgarrou, Belhassen-Chedli and Mezouar, Youcef},
  year = {2018},
  journal = {The International Journal of Robotics Research},
  volume = {37},
  number = {7},
  pages = {688--716},
  issn = {0278-3649},
  doi = {10.1177/0278364918779698},
  pagination = {page}
}

@techreport{sannemanStateIndustrialRobotics2020,
  title = {The {{State}} of {{Industrial Robotics}}: {{Emerging Technologies}}, {{Challenges}}, and {{Key Research Directions}}},
  author = {Sanneman, Lindsay and Fourie, Christopher and Shah, Julie},
  year = {2020},
  pages = {33},
  institution = {MIT},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\ZMZTEWT6\Sanneman et al. - The State of Industrial Robotics Emerging Technol.pdf}
}

@book{sannemanStateIndustrialRobotics2021,
  title = {The {{State}} of {{Industrial Robotics}}: {{Emerging Technologies}}, {{Challenges}}, and {{Key Research Directions}}},
  shorttitle = {The {{State}} of {{Industrial Robotics}}},
  author = {Sanneman, Lindsay and Fourie, Christopher and Shah, Julie},
  year = {2021},
  month = jan,
  doi = {10.1561/9781680838015},
  abstract = {Robotics and related technologies are central to the ongoing digitization and advancement of manufacturing. In recent years, a variety of strategic initiatives around the world, including ``Industry 4.0'' introduced in Germany in 2011, have aimed to improve and connect manufacturing technologies in order to optimize production processes. In this work, the authors study the changing technological landscape of robotics and ``internet-of-things'' (IoT)-based connective technologies over the last 8-10 years in the wake of Industry 4.0. The authors interviewed key players within the European robotics ecosystem, including robotics manufacturers and integrators, original equipment manufacturers (OEMs), and applied industrial research institutions and synthesize our findings in this monograph. The author first detail the state-of-the-art robotics and IoT technologies they observed and that the companies discussed during our interviews. They then describe the processes the companies follow when deciding whether and how to integrate new technologies, the challenges they face when integrating these technologies, and some immediate future technological avenues they are exploring in robotics and IoT. Finally, based on their findings, the authors highlight key research directions for the robotics community that can enable improved capabilities in the context of manufacturing.},
  isbn = {978-1-68083-800-8}
}

@book{Santra.2019,
  title = {Einf{\"u}hrung in Die Theoretische Physik},
  author = {Santra, Robin},
  year = {2019},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-58521-4},
  isbn = {978-3-662-58520-7}
}

@article{Sarbolandi.2015,
  title = {Kinect Range Sensing: {{Structured-light}} versus Time-of-Flight Kinect},
  author = {Sarbolandi, Hamed and Lefloch, Damien and Kolb, Andreas},
  year = {2015},
  abstract = {Recently, the new Kinect One has been issued by Microsoft, providing the next generation of real-time range sensing devices based on the Time-of-Flight (ToF) principle. As the first Kinect version was using a structured light approach, one would expect various differences in the characteristics of the range data delivered by both devices. This paper presents a detailed and in-depth comparison between both devices. In order to conduct the comparison, we propose a framework of seven different experimental setups, which is a generic basis for evaluating range cameras such as Kinect. The experiments have been designed with the goal to capture individual effects of the Kinect devices as isolatedly as possible and in a way, that they can also be adopted, in order to apply them to any other range sensing device. The overall goal of this paper is to provide a solid insight into the pros and cons of either device. Thus, scientists that are interested in using Kinect range sensing cameras in their specific application scenario can directly assess the expected, specific benefits and potential problem of either device.}
}

@article{sarkerNeuroSymbolicArtificialIntelligence2022,
  title = {Neuro-{{Symbolic Artificial Intelligence}}: {{Current Trends}}},
  shorttitle = {Neuro-{{Symbolic Artificial Intelligence}}},
  author = {Sarker, Md Kamruzzaman and Zhou, Lu and Eberhart, Aaron and Hitzler, Pascal},
  year = {2022},
  journal = {AI Communications}
}

@article{sathyaWeightedMethodFast2021,
  title = {A {{Weighted Method}} for {{Fast Resolution}} of {{Strictly Hierarchical Robot Task Specifications Using Exact Penalty Functions}}},
  author = {Sathya, Ajay Suresha and Pipeleers, Goele and Decr{\'e}, Wilm and Swevers, Jan},
  year = {2021},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {2},
  pages = {3057--3064},
  issn = {2377-3766},
  doi = {10.1109/LRA.2021.3063026},
  abstract = {Extensive work has been done on efficiently resolving hierarchical robot task specifications that minimize the \${\textbackslash}ell\$-2 norm of linear constraint violations, but not for \${\textbackslash}ell\$-1 norm, in which there has recently been growing interest for sparse control. Both approaches require solving a cascade of quadratic programs (QP) or linear programs (LP). In this letter, we introduce alternate and more efficient approaches to hierarchical \${\textbackslash}ell\$-1 norm minimization by formulating it as a single LP that can be solved by any off-the-shelf solver. The first approach is a recursive method that transforms the lexicographic LP (LLP) into a single objective problem using Lagrangian duality. The second approach, which forms the main focus of this letter, is a weighted method based on the exact penalty method, that is equivalent to the original LLP for a well chosen set of weights. We propose methods to compute and adapt these weights. The algorithms were applied on an interesting dual arm robot task. We discuss and benchmark the computational efficiency of these methods. Simplicity of the weighted method makes it a promising approach for tackling challenging prioritized robot control problems involving a control horizon or nonlinear constraints. Within this letter, we take the first step towards that goal by demonstrating the efficacy of the weighted method on a simpler instantaneous robot control problem with linear constraints.},
  keywords = {Jacobian matrices,Linear programming,Manipulators,Optimal control,Optimization,Optimization and optimal control,redundant robots,Robots,Task analysis,whole-body motion planning and control}
}

@article{saverianoDynamicMovementPrimitives2023,
  title = {Dynamic Movement Primitives in Robotics: {{A}} Tutorial Survey},
  shorttitle = {Dynamic Movement Primitives in Robotics},
  author = {Saveriano, Matteo and {Abu-Dakka}, Fares J and Kramberger, Alja{\v z} and Peternel, Luka},
  year = {2023},
  month = nov,
  journal = {The International Journal of Robotics Research},
  volume = {42},
  number = {13},
  pages = {1133--1184},
  publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  doi = {10.1177/02783649231201196},
  urldate = {2024-03-17},
  abstract = {Biological systems, including human beings, have the innate ability to perform complex tasks in a versatile and agile manner. Researchers in sensorimotor control have aimed to comprehend and formally define this innate characteristic. The idea, supported by several experimental findings, that biological systems are able to combine and adapt basic units of motion into complex tasks finally leads to the formulation of the motor primitives' theory. In this respect, Dynamic Movement Primitives (DMPs) represent an elegant mathematical formulation of the motor primitives as stable dynamical systems and are well suited to generate motor commands for artificial systems like robots. In the last decades, DMPs have inspired researchers in different robotic fields including imitation and reinforcement learning, optimal control, physical interaction, and human--robot co-working, resulting in a considerable amount of published papers. The goal of this tutorial survey is two-fold. On one side, we present the existing DMP formulations in rigorous mathematical terms and discuss the advantages and limitations of each approach as well as practical implementation details. In the tutorial vein, we also search for existing implementations of presented approaches and release several others. On the other side, we provide a systematic and comprehensive review of existing literature and categorize state-of-the-art work on DMP. The paper concludes with a discussion on the limitations of DMPs and an outline of possible research directions.},
  langid = {english}
}

@misc{scaoBLOOM176BParameterOpenAccess2023,
  title = {{{BLOOM}}: {{A 176B-Parameter Open-Access Multilingual Language Model}}},
  shorttitle = {{{BLOOM}}},
  author = {Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c c}ois and Gall{\'e}, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Beno{\^i}t and Muennighoff, Niklas and {del Moral}, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and {McMillan-Major}, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Lauren{\c c}on, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and {van Strien}, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo Gonz{\'a}lez and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, G{\'e}rard and Kruszewski, Germ{\'a}n and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and {Gonzalez-Dios}, Itziar and {de la Rosa}, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, J{\"o}rg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and {allal}, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Mu{\~n}oz, Manuel Romero and Masoud, Maraim and Grandury, Mar{\'i}a and {\v S}a{\v s}ko, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and {de Gibert}, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and L{\'o}pez, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Ta{\c s}ar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M. Saiful and {Al-shaibani}, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and {Ben-David}, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and {von Platen}, Patrick and Cornette, Pierre and Lavall{\'e}e, Pierre Fran{\c c}ois and Lacroix, R{\'e}mi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, St{\'e}phane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and N{\'e}v{\'e}ol, Aur{\'e}lie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and {van der Wal}, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zden{\v e}k and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Mu{\~n}oz and McDuff, Daniel and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and {Miranda-Escalada}, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Cl{\'e}mentine and Peri{\~n}{\'a}n, Daniel Le{\'o}n and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and {de Bykhovetz}, Madeleine Hahn and Takeuchi, Maiko and P{\`a}mies, Marc and Castillo, Maria A. and Nezhurina, Marianna and S{\"a}nger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S. and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and {Sang-aroonsiri}, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Th{\'e}o and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
  year = {2023},
  month = jun,
  number = {arXiv:2211.05100},
  eprint = {2211.05100},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.05100},
  urldate = {2024-03-01},
  abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\benja\Zotero\storage\ZB5JFDKH\2211.html}
}

@incollection{Scha06,
  title = {Dynamic {{Movement Primitives}} - {{A Framework}} for {{Motor Control}} in {{Humans}} and {{Humanoid Robotics}}},
  booktitle = {Adaptive {{Motion}} of {{Animals}} and {{Machines}}},
  author = {Schaal, Stefan},
  year = {2006},
  pages = {261--280}
}

@article{schaalConstructiveIncrementalLearning1998,
  title = {Constructive Incremental Learning from Only Local Information},
  author = {Schaal, S. and Atkeson, C. G.},
  year = {1998},
  month = nov,
  journal = {Neural Computation},
  volume = {10},
  number = {8},
  pages = {2047--2084},
  issn = {1530-888X},
  doi = {10.1162/089976698300016963},
  abstract = {We introduce a constructive, incremental learning system for regression problems that models data by means of spatially localized linear models. In contrast to other approaches, the size and shape of the receptive field of each locally linear model, as well as the parameters of the locally linear model itself, are learned independently, that is, without the need for competition or any other kind of communication. Independent learning is accomplished by incrementally minimizing a weighted local cross-validation error. As a result, we obtain a learning system that can allocate resources as needed while dealing with the bias-variance dilemma in a principled way. The spatial localization of the linear models increases robustness toward negative interference. Our learning system can be interpreted as a nonparametric adaptive bandwidth smoother, as a mixture of experts where the experts are trained in isolation, and as a learning system that profits from combining independent expert knowledge on the same problem. This article illustrates the potential learning capabilities of purely local learning and offers an interesting and powerful approach to learning with receptive fields.},
  langid = {english},
  pmid = {9804671}
}

@incollection{schaalDynamicMovementPrimitives2006,
  title = {Dynamic {{Movement Primitives}} - {{A Framework}} for {{Motor Control}} in {{Humans}} and {{Humanoid Robotics}}},
  booktitle = {Adaptive {{Motion}} of {{Animals}} and {{Machines}}},
  author = {Schaal, Stefan},
  editor = {Kimura, Hiroshi and Tsuchiya, Kazuo and Ishiguro, Akio and Witte, Hartmut},
  year = {2006},
  pages = {261--280},
  publisher = {Springer},
  doi = {10.1007/4-431-31381-8_23},
  urldate = {2019-08-22},
  abstract = {Given the continuous stream of movements that biological systems exhibit in their daily activities, an account for such versatility and creativity has to assume that movement sequences consist of segments, executed either in sequence or with partial or complete overlap. Therefore, a fundamental question that has pervaded research in motor control both in artificial and biological systems revolves around identifying movement primitives (a.k.a. units of actions, basis behaviors, motor schemas, etc.). What are the fundamental building blocks that are strung together, adapted to, and created for ever new behaviors? This paper summarizes results that led to the hypothesis of Dynamic Movement Primitives (DMP). DMPs are units of action that are formalized as stable nonlinear attractor systems. They are useful for autonomous robotics as they are highly flexible in creating complex rhythmic (e.g., locomotion) and discrete (e.g., a tennis swing) behaviors that can quickly be adapted to the inevitable perturbations of a dynamically changing, stochastic environment. Moreover, DMPs provide a formal framework that also lends itself to investigations in computational neuroscience. A recent finding that allows creating DMPs with the help of well-understood statistical learning methods has elevated DMPs from a more heuristic to a principled modeling approach. Theoretical insights, evaluations on a humanoid robot, and behavioral and brain imaging data will serve to outline the framework of DMPs for a general approach to motor control in robotics and biology.},
  isbn = {978-4-431-31381-6},
  langid = {english},
  keywords = {Discrete Movement,Experimental Brain Research,Humanoid Robot,Movement Segmentation,Rhythmic Movement}
}

@inproceedings{scheideBehaviorTreeLearning2021,
  title = {Behavior {{Tree Learning}} for {{Robotic Task Planning}} through {{Monte Carlo DAG Search}} over a {{Formal Grammar}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Scheide, Emily and Best, Graeme and Hollinger, Geoffrey A.},
  year = {2021},
  month = may,
  pages = {4837--4843},
  publisher = {IEEE Press},
  address = {Xi\&apos;an, China},
  doi = {10.1109/ICRA48506.2021.9561027},
  urldate = {2024-04-25},
  abstract = {We present an algorithm for learning behavior trees for robotic task planning, which alleviates the need for time-intensive or infeasible manual design of control architectures. Our method involves representing the search space of behavior trees as a formal grammar and searching over this grammar by means of a new generalization of Monte Carlo tree search (MCTS) for directed acyclic graphs (DAGs), named MCDAGS. Additionally, our method employs simulated annealing to expedite the aggregation of the most functional subtrees. We present simulated experiments for a marine target search and response scenario, and an abstract task selection problem. Our results demonstrate that the learned behavior trees compare favorably with a manually-designed tree, and outperform baseline learning methods. Overall, these results show that our method is a viable technique for the automatic design of behavior trees for robotic task planning.}
}

@article{scheikl2020deep,
  title = {Deep Learning for Semantic Segmentation of Organs and Tissues in Laparoscopic Surgery},
  author = {Scheikl, Paul Maria and Laschewski, Stefan and Kisilenko, Anna and Davitashvili, Tornike and M{\"u}ller, Benjamin and Capek, Manuela and {M{\"u}ller-Stich}, Beat P and Wagner, Martin and {Mathis-Ullrich}, Franziska},
  year = {2020},
  journal = {Current Directions in Biomedical Engineering},
  volume = {6},
  number = {1},
  publisher = {De Gruyter}
}

@book{Schenker.1992,
  title = {Sensor Fusion {{IV}}: {{Control}} Paradigms and Data Structures},
  editor = {Schenker, Paul S.},
  year = {1992},
  series = {{{SPIE}} Proceedings},
  publisher = {SPIE}
}

@article{schepmanGeneralAttitudesArtificial2023,
  title = {The {{General Attitudes}} towards {{Artificial Intelligence Scale}} ({{GAAIS}}): {{Confirmatory Validation}} and {{Associations}} with {{Personality}}, {{Corporate Distrust}}, and {{General Trust}}},
  shorttitle = {The {{General Attitudes}} towards {{Artificial Intelligence Scale}} ({{GAAIS}})},
  author = {Schepman, Astrid and Rodway, Paul},
  year = {2023},
  month = aug,
  journal = {International Journal of Human--Computer Interaction},
  volume = {39},
  number = {13},
  pages = {2724--2741},
  publisher = {Taylor \& Francis},
  issn = {1044-7318},
  doi = {10.1080/10447318.2022.2085400},
  urldate = {2024-03-25},
  abstract = {Acceptance of Artificial Intelligence (AI) may be predicted by individual psychological correlates, examined here. Study 1 reports confirmatory validation of the General Attitudes towards Artificial Intelligence Scale (GAAIS) following initial validation elsewhere. Confirmatory Factor Analysis confirmed the two-factor structure (Positive, Negative) and showed good convergent and divergent validity with a related scale. Study 2 tested whether psychological factors (Big Five personality traits, corporate distrust, and general trust) predicted attitudes towards AI. Introverts had more positive attitudes towards AI overall, likely because of algorithm appreciation. Conscientiousness and agreeableness were associated with forgiving attitudes towards negative aspects of AI. Higher corporate distrust led to negative attitudes towards AI overall, while higher general trust led to positive views of the benefits of AI. The dissociation between general trust and corporate distrust may reflect the public's attributions of the benefits and drawbacks of AI. Results are discussed in relation to theory and prior findings.}
}

@misc{scherlisPolysemanticityCapacityNeural2023,
  title = {Polysemanticity and {{Capacity}} in {{Neural Networks}}},
  author = {Scherlis, Adam and Sachan, Kshitij and Jermyn, Adam S. and Benton, Joe and Shlegeris, Buck},
  year = {2023},
  month = jul,
  number = {arXiv:2210.01892},
  eprint = {2210.01892},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.01892},
  urldate = {2024-05-02},
  abstract = {Individual neurons in neural networks often represent a mixture of unrelated features. This phenomenon, called polysemanticity, can make interpreting neural networks more difficult and so we aim to understand its causes. We propose doing so through the lens of feature {\textbackslash}emph\{capacity\}, which is the fractional dimension each feature consumes in the embedding space. We show that in a toy model the optimal capacity allocation tends to monosemantically represent the most important features, polysemantically represent less important features (in proportion to their impact on the loss), and entirely ignore the least important features. Polysemanticity is more prevalent when the inputs have higher kurtosis or sparsity and more prevalent in some architectures than others. Given an optimal allocation of capacity, we go on to study the geometry of the embedding space. We find a block-semi-orthogonal structure, with differing block sizes in different models, highlighting the impact of model architecture on the interpretability of its neurons.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\benja\Zotero\storage\9E7LQUG2\2210.html}
}

@inproceedings{schlungbaumIndividualUserInterfaces1997,
  title = {Individual {{User Interfaces}} and {{Model-Based User Interface Software Tools}}},
  booktitle = {Proceedings of the 2nd {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Schlungbaum, Egbert},
  year = {1997},
  month = jan,
  series = {{{IUI}} '97},
  pages = {229--232},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/238218.238330},
  urldate = {2024-02-01},
  isbn = {978-0-89791-839-8},
  keywords = {explicit user model,MASTERMIND,model-based user interface development,model-based user interface software tools,TADEUS}
}

@article{schmidPlayerGames2021,
  title = {Player of {{Games}}},
  author = {Schmid, Martin and Moravcik, Matej and Burch, Neil and Kadlec, Rudolf and Davidson, Josh and Waugh, Kevin and Bard, Nolan and Timbers, Finbarr and Lanctot, Marc and Holland, Zach and Davoodi, Elnaz and Christianson, Alden and Bowling, Michael},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.03178 [cs]},
  eprint = {2112.03178},
  primaryclass = {cs},
  urldate = {2021-12-19},
  abstract = {Games have a long history of serving as a benchmark for progress in artificial intelligence. Recently, approaches using search and learning have shown strong performance across a set of perfect information games, and approaches using game-theoretic reasoning and learning have shown strong performance for specific imperfect information poker variants. We introduce Player of Games, a general-purpose algorithm that unifies previous approaches, combining guided search, self-play learning, and game-theoretic reasoning. Player of Games is the first algorithm to achieve strong empirical performance in large perfect and imperfect information games -- an important step towards truly general algorithms for arbitrary environments. We prove that Player of Games is sound, converging to perfect play as available computation time and approximation capacity increases. Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\8IM4YXAI\2112.html}
}

@misc{schmidt-rohrArtiMindsRobotProgramming2013,
  title = {{{ArtiMinds Robot Programming Suite}}},
  author = {{Schmidt-Rohr}, Sven R and J{\"a}kel, Rainer and Dirschl, Gerhard},
  year = {2013},
  urldate = {2017-08-27},
  howpublished = {ArtiMinds Robotics GmbH}
}

@article{Schmidt.2015,
  title = {{{DART}}: Dense Articulated Real-Time Tracking with Consumer Depth Cameras},
  author = {Schmidt, Tanner and Newcombe, Richard and Fox, Dieter},
  year = {2015},
  journal = {Autonomous Robots},
  volume = {39},
  number = {3},
  pages = {239--258},
  issn = {0929-5593},
  doi = {10.1007/s10514-015-9462-z},
  pagination = {page}
}

@article{schmittFutureRampUpManagement2018,
  title = {On the {{Future}} of {{Ramp-Up Management}}},
  author = {Schmitt, Robert and Heine, Ina and Jiang, Ruth and Giedziella, Felix and Basse, Felix and Voet, Hanno and Lu, Stephen},
  year = {2018},
  month = nov,
  journal = {CIRP Journal of Manufacturing Science and Technology},
  volume = {23},
  pages = {217--225},
  issn = {1755-5817},
  doi = {10.1016/j.cirpj.2018.03.001},
  urldate = {2024-02-21},
  abstract = {The aim of this paper is to present research hypotheses and practical implications on developing a novel ramp-up management approach facing recent challenges and trends in the field of agile production. Applying a mixed-method design based on a quantitative pre-study with 67 researchers and qualitative interviews with seven practitioners in ramp-up-relevant fields showed a consensus on the continuing importance of human factors for future ramp-up management. For managing ramp-up phases, real-time data infrastructure is central to support the learning and decision making process and thus to increase ramp-up agility. Along with this requirement, organizations are expected to have flatter hierarchical structures and be able to extend their product portfolio by implementing new technologies such as additive manufacturing.},
  keywords = {Agility,Big data,Industrie 4.0,Internet of things,Paradigm shift,Ramp-up management},
  file = {C:\Users\benja\Zotero\storage\BTKWU8SD\S1755581718300105.html}
}

@inproceedings{schmittPlanningReactiveManipulation2019,
  title = {Planning {{Reactive Manipulation}} in {{Dynamic Environments}}},
  booktitle = {{{IROS}}},
  author = {Schmitt, Philipp S. and Wirnshofer, Florian and Wurm, Kai M. and v. Wichert, Georg and Burgard, Wolfram},
  year = {2019},
  month = nov,
  pages = {136--143},
  issn = {2153-0866},
  doi = {10.1109/IROS40897.2019.8968452},
  abstract = {When robots perform manipulation tasks, they need to determine their own movement, as well as how to make and break contact with objects in their environment. Reasoning about the motions of robots and objects simultaneously leads to a constrained planning problem in a high-dimensional state-space. Additionally, when environments change dynamically motions must be computed in real-time. To this end, we propose a feedback planner for manipulation. We model manipulation as constrained motion and use this model to automatically derive a set of constraint-based controllers. These controllers are used in a switching-control scheme, where the active controller is chosen by a reinforcement learning agent. Our approach is capable of addressing tasks with second-order dynamics, closed kinematic chains, and time-variant environments. We validated our approach in simulation and on a real, dual-arm robot. Extensive simulation of three distinct robots and tasks show a significant increase in robustness compared to a previous approach.},
  keywords = {closed kinematic chains,constrained motion,constrained planning problem,constraint-based controllers,dual-arm robot,dynamic environments,feedback,feedback planner,learning systems,manipulation tasks,manipulator dynamics,manipulator kinematics,manipulators,model manipulation,motion control,path planning,reactive manipulation planning,reinforcement learning agent,second-order dynamics,switching-control scheme,time-variant environments},
  file = {C:\Users\benja\Zotero\storage\DT7AB3P8\8968452.html}
}

@incollection{Scholz.2018,
  title = {Ausgleichsrechnung},
  booktitle = {Optimierung Interaktiv},
  author = {Scholz, Daniel},
  editor = {Scholz, Daniel},
  year = {2018},
  pages = {149--168},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-57953-4_6},
  bookpagination = {page},
  isbn = {978-3-662-57952-7}
}

@book{Scholz.2018b,
  title = {Optimierung Interaktiv},
  editor = {Scholz, Daniel},
  year = {2018},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-57953-4},
  isbn = {978-3-662-57952-7}
}

@article{schoppenthauBuildingDigitalManufacturing2023,
  title = {Building a {{Digital Manufacturing}} as a {{Service Ecosystem}} for {{Catena-X}}},
  author = {Sch{\"o}ppenthau, Felix and Patzer, Florian and Schnebel, Boris and Watson, Kym and Baryschnikov, Nikita and Obst, Birgit and Chauhan, Yashkumar and Kaever, Domenik and Usl{\"a}nder, Thomas and Kulkarni, Piyush},
  year = {2023},
  month = jan,
  journal = {Sensors},
  volume = {23},
  number = {17},
  pages = {7396},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s23177396},
  urldate = {2024-10-23},
  abstract = {Manufacturing as a Service (MaaS) enables a paradigm shift in the current manufacturing landscape, from integrated production and inflexible, fragile supply chains to open production and flexible, robust supply chains. As part of this evolution, new scaling effects for production capacities and customer segments are possible. This article describes how to accomplish this paradigm shift for the automotive industry by building a digital MaaS ecosystem for the large-scale automotive innovation project Catena-X, which aims at a standardized global data exchange based on European values. A digital MaaS ecosystem can not only achieve scaling effects, but also realize new business models and overcome current and future challenges in the areas of legislation, sustainability, and standardization. This article analyzes the state-of-the-art of MaaS ecosystems and describes the development of a digital MaaS ecosystem based on an updated and advanced version of the reference architecture for smart connected factories, called the Smart Factory Web. Furthermore, this article describes a demonstrator for a federated MaaS marketplace for Catena-X which leverages the full technological potential of this digital ecosystem. In conclusion, the evaluation of the implemented digital ecosystem enables the advancement of the reference architecture Smart Factory Web, which can now be used as a blueprint for open, sustainable, and resilient digital manufacturing ecosystems.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Catena-X,digital ecosystem,Gaia-X,Industry 4.0,Manufacturing as a Service (MaaS),Manufacturing-X,production ecosystem,reference architecture,Smart Factory Web,smart manufacturing},
  file = {C:\Users\benja\Zotero\storage\IAP33XLF\Schppenthau et al. - 2023 - Building a Digital Manufacturing as a Service Ecosystem for Catena-X.pdf}
}

@incollection{schrammHowCommunicationWorks1954,
  title = {How {{Communication Works}}},
  booktitle = {Process and {{Effects}} of {{Communication}}},
  author = {Schramm, Wilbur},
  year = {1954},
  pages = {3--26}
}

@inproceedings{Schulman.2013,
  title = {Tracking Deformable Objects with Point Clouds},
  booktitle = {2013 {{IEEE}} International Conference on Robotics and Automation},
  author = {Schulman, John and Lee, Alex and Ho, Jonathan and Abbeel, Pieter},
  year = {2013},
  pages = {1130--1137},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2013.6630714},
  bookpagination = {page},
  isbn = {978-1-4673-5643-5}
}

@incollection{Schulman.2016,
  title = {Learning from Demonstrations through the Use of Non-Rigid Registration},
  booktitle = {Robotics Research},
  author = {Schulman, John and Ho, Jonathan and Lee, Cameron and Abbeel, Pieter},
  editor = {Inaba, Masayuki and Corke, Peter},
  year = {2016},
  series = {Springer Tracts in Advanced Robotics},
  volume = {114},
  pages = {339--354},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-28872-7_20},
  bookpagination = {page},
  isbn = {978-3-319-28870-3}
}

@inproceedings{schulmanFindingLocallyOptimal2013,
  title = {Finding {{Locally Optimal}}, {{Collision-Free Trajectories}} with {{Sequential Convex Optimization}}},
  booktitle = {Robotics: {{Science}} and {{Systems IX}}},
  author = {Schulman, John and Ho, Jonathan and Lee, Alex and Awwal, Ibrahim and Bradlow, Henry and Abbeel, Pieter},
  year = {2013},
  month = jun,
  volume = {09},
  urldate = {2023-04-23},
  isbn = {978-981-07-3937-9}
}

@article{schulmanGradientEstimationUsing2016,
  title = {Gradient {{Estimation Using Stochastic Computation Graphs}}},
  author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
  year = {2016},
  month = jan,
  journal = {arXiv:1506.05254 [cs]},
  eprint = {1506.05254},
  primaryclass = {cs},
  urldate = {2019-12-04},
  abstract = {In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs---directed acyclic graphs that include both deterministic functions and conditional probability distributions---and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{schulmanTrackingDeformableObjects2013,
  title = {Tracking Deformable Objects with Point Clouds},
  booktitle = {2013 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Schulman, John and Lee, Alex and Ho, Jonathan and Abbeel, Pieter},
  year = {2013},
  month = may,
  pages = {1130--1137},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2013.6630714},
  abstract = {We introduce an algorithm for tracking deformable objects from a sequence of point clouds. The proposed tracking algorithm is based on a probabilistic generative model that incorporates observations of the point cloud and the physical properties of the tracked object and its environment. We propose a modified expectation maximization algorithm to perform maximum a posteriori estimation to update the state estimate at each time step. Our modification makes it practical to perform the inference through calls to a physics simulation engine. This is significant because (i) it allows for the use of highly optimized physics simulation engines for the core computations of our tracking algorithm, and (ii) it makes it possible to naturally, and efficiently, account for physical constraints imposed by collisions, grasping actions, and material properties in the observation updates. Even in the presence of the relatively large occlusions that occur during manipulation tasks, our algorithm is able to robustly track a variety of types of deformable objects, including ones that are one-dimensional, such as ropes; two-dimensional, such as cloth; and three-dimensional, such as sponges. Our implementation can track these objects in real time.},
  keywords = {Computational modeling,Inference algorithms,Mathematical model,Noise,Physics,Probabilistic logic,Solid modeling},
  file = {C:\Users\benja\Zotero\storage\ULDW83LM\6630714.html}
}

@article{schulmanTrustRegionPolicy2015,
  title = {Trust {{Region Policy Optimization}}},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  year = {2015},
  month = feb,
  journal = {arXiv:1502.05477 [cs]},
  eprint = {1502.05477},
  primaryclass = {cs},
  urldate = {2019-07-14},
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning}
}

@article{schultheisEASYEnergyEfficientAnalysis2024,
  title = {{{EASY}}: {{Energy-Efficient Analysis}} and {{Control Processes}} in the {{Dynamic Edge-Cloud Continuum}} for {{Industrial Manufacturing}}},
  shorttitle = {{{EASY}}},
  author = {Schultheis, Alexander and Alt, Benjamin and Bast, Sebastian and Guldner, Achim and Jilg, David and Katic, Darko and Mundorf, Johannes and Schlagenhauf, Tobias and Weber, Sebastian and Bergmann, Ralph and Bergweiler, Simon and Creutz, Lars and Dartmann, Guido and Malburg, Lukas and Naumann, Stefan and Rezapour, Mahdi and Ruskowski, Martin},
  year = {2024},
  month = sep,
  journal = {KI - K{\"u}nstliche Intelligenz},
  issn = {1610-1987},
  doi = {10.1007/s13218-024-00868-3},
  urldate = {2024-09-05},
  abstract = {According to the guiding principles of Industry~4.0, edge computing enables the data-sovereign and near-real-time processing of data directly at the point of origin. Using these edge devices in manufacturing organization will drive the use of industrial analysis, control, and Artificial Intelligence (AI) applications close to production. The goal of the EASY project is to make the added value of edge computing available by providing an easily usable Edge-Cloud Continuum with a runtime environment and services for the execution of AI-based Analysis and Control processes. Within this continuum, a dynamic, distributed, and optimized execution of services is automated across the entire spectrum from centralized cloud to decentralized edge instances to increase productivity and resource efficiency.},
  langid = {english},
  keywords = {Analysis and Control Processes,Artificial Intelligence,Edge-Cloud Continuum,Energy- and Resource-Efficiency,my},
  file = {C:\Users\benja\Zotero\storage\FGUCJREM\Schultheis et al. - 2024 - EASY Energy-Efficient Analysis and Control Processes in the Dynamic Edge-Cloud Continuum for Indust.pdf}
}

@misc{SCHUNKCoactEGLC,
  title = {{{SCHUNK Co-act EGL-C Collaborating Long-stroke Gripper}}},
  urldate = {2020-10-07}
}

@misc{seitaVisualModelBasedReinforcement2019,
  title = {Visual {{Model-Based Reinforcement Learning}} as a {{Path}} towards {{Generalist Robots}}},
  author = {Seita, Daniel},
  year = {2019},
  month = may,
  journal = {The Berkeley Artificial Intelligence Research Blog},
  urldate = {2019-05-17},
  abstract = {The BAIR Blog},
  howpublished = {http://bair.berkeley.edu/blog/2018/11/30/visual-rl/},
  file = {C:\Users\benja\Zotero\storage\8V2C2S7J\visual-rl.html}
}

@article{sejnowskiLargeLanguageModels2023,
  title = {Large {{Language Models}} and the {{Reverse Turing Test}}},
  author = {Sejnowski, Terrence J.},
  year = {2023},
  month = feb,
  journal = {Neural Computation},
  volume = {35},
  number = {3},
  pages = {309--342},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01563},
  urldate = {2024-07-31},
  abstract = {Large language models (LLMs) have been transformative. They are pretrained foundational models that are self-supervised and can be adapted with fine-tuning to a wide range of natural language tasks, each of which previously would have required a separate network model. This is one step closer to the extraordinary versatility of human language. GPT-3 and, more recently, LaMDA, both of them LLMs, can carry on dialogs with humans on many topics after minimal priming with a few examples. However, there has been a wide range of reactions and debate on whether these LLMs understand what they are saying or exhibit signs of intelligence. This high variance is exhibited in three interviews with LLMs reaching wildly different conclusions. A new possibility was uncovered that could explain this divergence. What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a reverse Turing test. If so, then by studying interviews, we may be learning more about the intelligence and beliefs of the interviewer than the intelligence of the LLMs. As LLMs become more capable, they may transform the way we interact with machines and how they interact with each other. Increasingly, LLMs are being coupled with sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A road map for achieving artificial general autonomy is outlined with seven major improvements inspired by brain systems and how LLMs could in turn be used to uncover new insights into brain function.},
  file = {C:\Users\benja\Zotero\storage\SQWCNNS8\Large-Language-Models-and-the-Reverse-Turing-Test.html}
}

@inproceedings{sekerConditionalNeuralMovement2019,
  title = {Conditional {{Neural Movement Primitives}}},
  booktitle = {Robotics: {{Science}} and {{Systems}}},
  author = {Seker, Muhammet Yunus and Imre, Mert and Piater, Justus and Ugur, Emre},
  year = {2019},
  month = jun,
  volume = {15},
  urldate = {2020-07-12},
  isbn = {978-0-9923747-5-4},
  file = {C:\Users\benja\Zotero\storage\K5ADWIXG\p71.html}
}

@article{Selvaraju2016,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2016},
  month = oct,
  eprint = {1610.02391},
  urldate = {2018-12-31},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multimodal inputs (e.g. VQA) or reinforcement learning, without any architectural changes or re-training. We combine GradCAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on weakly-supervised localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we conduct human studies to measure if GradCAM explanations help users establish trust in predictions from deep networks and show that GradCAM helps untrained users successfully discern a "stronger" deep network from a "weaker" one. Our code is available at https://github.com/ramprs/grad-cam. A demo and a video of the demo can be found at http://gradcam.cloudcv.org and youtu.be/COjUB9Izk6E.},
  archiveprefix = {arXiv}
}

@article{senftTaskLevelAuthoringRemote2021,
  title = {Task-{{Level Authoring}} for {{Remote Robot Teleoperation}}},
  author = {Senft, Emmanuel and Hagenow, Michael and Welsh, Kevin and Radwin, Robert and Zinn, Michael and Gleicher, Michael and Mutlu, Bilge},
  year = {2021},
  journal = {Frontiers in Robotics and AI},
  volume = {8},
  issn = {2296-9144},
  urldate = {2023-04-06},
  abstract = {Remote teleoperation of robots can broaden the reach of domain specialists across a wide range of industries such as home maintenance, health care, light manufacturing, and construction. However, current direct control methods are impractical, and existing tools for programming robot remotely have focused on users with significant robotic experience. Extending robot remote programming to end users, i.e., users who are experts in a domain but novices in robotics, requires tools that balance the rich features necessary for complex teleoperation tasks with ease of use. The primary challenge to usability is that novice users are unable to specify complete and robust task plans to allow a robot to perform duties autonomously, particularly in highly variable environments. Our solution is to allow operators to specify shorter sequences of high-level commands, which we call task-level authoring, to create periods of variable robot autonomy. This approach allows inexperienced users to create robot behaviors in uncertain environments by interleaving exploration, specification of behaviors, and execution as separate steps. End users are able to break down the specification of tasks and adapt to the current needs of the interaction and environments, combining the reactivity of direct control to asynchronous operation. In this paper, we describe a prototype system contextualized in light manufacturing and its empirical validation in a user study where 18 participants with some programming experience were able to perform a variety of complex telemanipulation tasks with little training. Our results show that our approach allowed users to create flexible periods of autonomy and solve rich manipulation tasks. Furthermore, participants significantly preferred our system over comparative more direct interfaces, demonstrating the potential of our approach for enabling end users to effectively perform remote robot programming.}
}

@article{sfairpalarHumanRobotInterface2020,
  title = {Human--{{Robot Interface}} for {{Embedding Sliding Adjustable Autonomy Methods}}},
  author = {Sfair Palar, Piatan and {de Vargas Terres}, Vin{\'i}cius and {Schneider de Oliveira}, Andr{\'e}},
  year = {2020},
  month = jan,
  journal = {Sensors},
  volume = {20},
  number = {20},
  pages = {5960},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/s20205960},
  urldate = {2021-02-05},
  abstract = {This work discusses a novel human\&ndash;robot interface for a climbing robot for inspecting weld beads in storage tanks in the petrochemical industry. The approach aims to adapt robot autonomy in terms of the operator\&rsquo;s experience, where a remote industrial joystick works in conjunction with an electromyographic armband as inputs. This armband is worn on the forearm and can detect gestures from the operator and rotation angles from the arm. Information from the industrial joystick and the armband are used to control the robot via a Fuzzy controller. The controller works with sliding autonomy (using as inputs data from the angular velocity of the industrial controller, electromyography reading, weld bead position in the storage tank, and rotation angles executed by the operator\&rsquo;s arm) to generate a system capable of recognition of the operator\&rsquo;s skill and correction of mistakes from the operator in operating time. The output from the Fuzzy controller is the level of autonomy to be used by the robot. The levels implemented are Manual (operator controls the angular and linear velocities of the robot); Shared (speeds are shared between the operator and the autonomous system); Supervisory (robot controls the angular velocity to stay in the weld bead, and the operator controls the linear velocity); Autonomous (the operator defines endpoint and the robot controls both linear and angular velocities). These autonomy levels, along with the proposed sliding autonomy, are then analyzed through robot experiments in a simulated environment, showing each of these modes\&rsquo; purposes. The proposed approach is evaluated in virtual industrial scenarios through real distinct operators.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {fuzzy controller,human-robot interface,Myo armband,sliding autonomy}
}

@misc{shahGoalMisgeneralizationWhy2022,
  title = {Goal {{Misgeneralization}}: {{Why Correct Specifications Aren}}'t {{Enough For Correct Goals}}},
  shorttitle = {Goal {{Misgeneralization}}},
  author = {Shah, Rohin and Varma, Vikrant and Kumar, Ramana and Phuong, Mary and Krakovna, Victoria and Uesato, Jonathan and Kenton, Zac},
  year = {2022},
  month = nov,
  number = {arXiv:2210.01790},
  eprint = {2210.01790},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.01790},
  urldate = {2024-03-04},
  abstract = {The field of AI alignment is concerned with AI systems that pursue unintended goals. One commonly studied mechanism by which an unintended goal might arise is specification gaming, in which the designer-provided specification is flawed in a way that the designers did not foresee. However, an AI system may pursue an undesired goal even when the specification is correct, in the case of goal misgeneralization. Goal misgeneralization is a specific form of robustness failure for learning algorithms in which the learned program competently pursues an undesired goal that leads to good performance in training situations but bad performance in novel test situations. We demonstrate that goal misgeneralization can occur in practical systems by providing several examples in deep learning systems across a variety of domains. Extrapolating forward to more capable systems, we provide hypotheticals that illustrate how goal misgeneralization could lead to catastrophic risk. We suggest several research directions that could reduce the risk of goal misgeneralization for future systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\JF98ZTRH\2210.html}
}

@inproceedings{shahLearningDifferentiablePrograms2020,
  title = {Learning Differentiable Programs with Admissible Neural Heuristics},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Shah, Ameesh and Zhan, Eric and Sun, Jennifer J. and Verma, Abhinav and Yue, Yisong and Chaudhuri, Swarat},
  year = {2020},
  month = dec,
  series = {{{NIPS}}'20},
  pages = {4940--4952},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2022-05-01},
  abstract = {We study the problem of learning differentiable functions expressed as programs in a domain-specific language. Such programmatic models can offer benefits such as composability and interpretability; however, learning them requires optimizing over a combinatorial space of program "architectures". We frame this optimization problem as a search in a weighted graph whose paths encode top-down derivations of program syntax. Our key innovation is to view various classes of neural networks as continuous relaxations over the space of programs, which can then be used to complete any partial program. This relaxed program is differentiable and can be trained end-to-end, and the resulting training loss is an approximately admissible heuristic that can guide the combinatorial search. We instantiate our approach on top of the A* algorithm and an iteratively deepened branch-and-bound search, and use these algorithms to learn programmatic classifiers in three sequence classification tasks. Our experiments show that the algorithms outperform state-of-the-art methods for program learning, and that they discover programmatic classifiers that yield natural interpretations and achieve competitive accuracy.},
  isbn = {978-1-71382-954-6}
}

@inproceedings{shahLMNavRoboticNavigation2023,
  title = {{{LM-Nav}}: {{Robotic Navigation}} with {{Large Pre-Trained Models}} of {{Language}}, {{Vision}}, and {{Action}}},
  shorttitle = {{{LM-Nav}}},
  booktitle = {Proceedings of {{The}} 6th {{Conference}} on {{Robot Learning}}},
  author = {Shah, Dhruv and Osi{\'n}ski, B{\l}a{\.z}ej and Ichter, Brian and Levine, Sergey},
  year = {2023},
  month = mar,
  pages = {492--504},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-16},
  abstract = {Goal-conditioned policies for robotic navigation can be trained on large, unannotated datasets, providing for good generalization to real-world settings. However, particularly in vision-based settings where specifying goals requires an image, this makes for an unnatural interface. Language provides a more convenient modality for communication with robots, but contemporary methods typically require expensive supervision, in the form of trajectories annotated with language descriptions. We present a system, LM-Nav, for robotic navigation that enjoys the benefits of training on unannotated large datasets of trajectories, while still providing a high-level interface to the user. Instead of utilizing a labeled instruction following dataset, we show that such a system can be constructed entirely out of pre-trained models for navigation (ViNG), image-language association (CLIP), and language modeling (GPT-3), without requiring any fine-tuning or language-annotated robot data. LM-Nav extracts landmarks names from an instruction, grounds them in the world via the image-language model, and then reaches them via the (vision-only) navigation model. We instantiate LM-Nav on a real-world  mobile robot and demonstrate long-horizon navigation through complex, outdoor environments from natural language instructions.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\YD8S3CJ2\Shah et al. - 2023 - LM-Nav Robotic Navigation with Large Pre-Trained .pdf}
}

@article{shahriariDeepNeuralNetwork2020,
  title = {A {{Deep Neural Network}} as {{Surrogate Model}} for {{Forward Simulation}} of {{Borehole Resistivity Measurements}}},
  author = {Shahriari, M. and Pardo, D. and Moser, B. and Sobieczky, F.},
  year = {2020},
  month = jan,
  journal = {Procedia Manufacturing},
  series = {International {{Conference}} on {{Industry}} 4.0 and {{Smart Manufacturing}} ({{ISM}} 2019)},
  volume = {42},
  pages = {235--238},
  issn = {2351-9789},
  doi = {10.1016/j.promfg.2020.02.075},
  urldate = {2022-05-04},
  abstract = {Inverse problems appear in multiple industrial applications. Solving such inverse problems require the repeated solution of the forward problem. This is the most time-consuming stage when employing inversion techniques, and it constitutes a severe limitation when the inversion needs to be performed in real-time. In here, we focus on the real-time inversion of resistivity measurements for geosteering. We investigate the use of a deep neural network (DNN) to approximate the forward function arising from Maxwell's equations, which govern the electromagnetic wave propagation through a media. By doing so, the evaluation of the forward problems is performed offline, allowing for the online real-time evaluation (inversion) of the DNN.},
  langid = {english},
  keywords = {Deep Learning,forward problem,geosteering,inverse problem,resistivity measurement}
}

@article{shahValueFunctionSpaces2022,
  title = {Value {{Function Spaces}}: {{Skill-Centric State Abstractions}} for {{Long-Horizon Reasoning}}},
  shorttitle = {Value {{Function Spaces}}},
  author = {Shah, Dhruv and Xu, Peng and Lu, Yao and Xiao, Ted and Toshev, Alexander and Levine, Sergey and Ichter, Brian},
  year = {2022},
  month = mar,
  journal = {arXiv:2111.03189 [cs]},
  eprint = {2111.03189},
  primaryclass = {cs},
  urldate = {2022-05-07},
  abstract = {Reinforcement learning can train policies that effectively perform complex tasks. However for long-horizon tasks, the performance of these methods degrades with horizon, often necessitating reasoning over and chaining lower-level skills. Hierarchical reinforcement learning aims to enable this by providing a bank of low-level skills as action abstractions. Hierarchies can further improve on this by abstracting the space states as well. We posit that a suitable state abstraction should depend on the capabilities of the available lower-level policies. We propose Value Function Spaces: a simple approach that produces such a representation by using the value functions corresponding to each lower-level skill. These value functions capture the affordances of the scene, thus forming a representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations for maze-solving and robotic manipulation tasks demonstrate that our approach improves long-horizon performance and enables better zero-shot generalization than alternative model-free and model-based methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\5C7Y3K87\2111.html}
}

@incollection{shapeevActiveLearningUncertainty2020,
  title = {Active {{Learning}} and {{Uncertainty Estimation}}},
  booktitle = {Machine {{Learning Meets Quantum Physics}}},
  author = {Shapeev, Alexander and Gubaev, Konstantin and Tsymbalov, Evgenii and Podryabinkin, Evgeny},
  editor = {Sch{\"u}tt, Kristof T. and Chmiela, Stefan and {von Lilienfeld}, O. Anatole and Tkatchenko, Alexandre and Tsuda, Koji and M{\"u}ller, Klaus-Robert},
  year = {2020},
  series = {Lecture {{Notes}} in {{Physics}}},
  pages = {309--329},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-40245-7_15},
  urldate = {2020-12-19},
  abstract = {Active learning refers to collections of algorithms of systematically constructing the training dataset. It is closely related to uncertainty estimation---we, generally, do not need to train our model on samples on which our prediction already has low uncertainty. This chapter reviews active learning algorithms in the context of molecular modeling and illustrates their applications on practical problems.},
  isbn = {978-3-030-40245-7},
  langid = {english}
}

@inproceedings{sharmaSelfImprovingRobotsEndtoEnd2023,
  title = {Self-{{Improving Robots}}: {{End-to-End Autonomous Visuomotor Reinforcement Learning}}},
  shorttitle = {Self-{{Improving Robots}}},
  booktitle = {7th {{Annual Conference}} on {{Robot Learning}}},
  author = {Sharma, Archit and Ahmed, Ahmed M. and Ahmad, Rehaan and Finn, Chelsea},
  year = {2023},
  month = aug,
  urldate = {2024-04-28},
  abstract = {In imitation and reinforcement learning (RL), the cost of human supervision limits the amount of data that the robots can be trained on. While RL offers a framework for building self-improving robots that can learn via trial-and-error autonomously, practical realizations end up requiring extensive human supervision for reward function design and repeated resetting of the environment between episodes of interactions. In this work, we propose MEDAL++, a novel design for self-improving robotic systems: given a small set of expert demonstrations at the start, the robot autonomously practices the task by learning to both do and undo the task, simultaneously inferring the reward function from the demonstrations. The policy and reward function are learned end-to-end from high-dimensional visual inputs, bypassing the need for explicit state estimation or task-specific pre-training for visual encoders used in prior work. We first evaluate our proposed system on a simulated non-episodic benchmark EARL, finding that MEDAL++ is both more data efficient and gets up to 30\% better final performance compared to state-of-the-art vision-based methods. Our real-robot experiments show that MEDAL++ can be applied to manipulation problems in larger environments than those considered in prior work, and autonomous self-improvement can improve the success rate by 30\% to 70\% over behavioral cloning on just the expert data.},
  langid = {english}
}

@article{shawProspectsEngineeringDiscipline1990,
  title = {Prospects for an Engineering Discipline of Software},
  author = {Shaw, M.},
  year = {1990},
  month = nov,
  journal = {IEEE Software},
  volume = {7},
  number = {6},
  pages = {15--24},
  issn = {1937-4194},
  doi = {10.1109/52.60586},
  urldate = {2024-09-20},
  abstract = {Although software engineering is not yet a true engineering discipline, it has the potential to become one. Older engineering fields are examined to ascertain the character that software engineering might have. The current state of software technology is discussed, covering information processing as an economic force, the growing role of software in critical applications, the maturity of development techniques, and the scientific basis for software engineering practice. Five basic steps that the software engineering profession must take to become a true engineering discipline are described. They are: understanding the nature of expertise, recognizing different ways to get information, encouraging routine practice, expecting professional specializations, and improving the coupling between science and commercial practice.{$<>$}},
  keywords = {Buildings,Design engineering,Knowledge engineering,Mathematics,Production,Programming,Reliability engineering,Software design,Software development management,Software engineering},
  file = {C\:\\Users\\benja\\Zotero\\storage\\PUKJ64BA\\Shaw - 1990 - Prospects for an engineering discipline of software.pdf;C\:\\Users\\benja\\Zotero\\storage\\S227QIXA\\60586.html}
}

@article{sheetzTrendsAdoptionRobotic2020,
  title = {Trends in the {{Adoption}} of {{Robotic Surgery}} for {{Common Surgical Procedures}}},
  author = {Sheetz, Kyle H. and Claflin, Jake and Dimick, Justin B.},
  year = {2020},
  month = jan,
  journal = {JAMA Network Open},
  volume = {3},
  number = {1},
  pages = {e1918911},
  issn = {2574-3805},
  doi = {10.1001/jamanetworkopen.2019.18911},
  urldate = {2024-04-12},
  abstract = {Increasing use of robotic surgery for common surgical procedures with limited evidence and unclear clinical benefit is raising concern. Analyses of population-based trends in practice and how hospitals' acquisition of robotic surgical technologies is associated with their use are limited.To characterize trends in the use of robotic surgery for common surgical procedures.This cohort study used clinical registry data from Michigan from January 1, 2012, through June 30, 2018. Trends were characterized in the use of robotic surgery for common procedures for which traditional laparoscopic minimally invasive surgery was already considered a safe and effective approach for most surgeons when clinically feasible. A multigroup interrupted time series analysis was performed to determine how procedural approaches (open, laparoscopic, and robotic) change after hospitals launch a robotic surgery program. Data were analyzed from March 1 through April 19, 2019.Initiation of robotic surgery.Procedure approach (ie, robotic, open, or laparoscopic).The study cohort included 169\,404 patients (mean [SD] age, 55.4 [16.9] years; 90\,595 women [53.5\%]) at 73 hospitals. The use of robotic surgery increased from 1.8\% in 2012 to 15.1\% in 2018 (8.4-fold increase; slope, 2.1\% per year; 95\% CI, 1.9\%-2.3\%). For certain procedures, the magnitude of the increase was greater; for example, for inguinal hernia repair, the use of robotic surgery increased from 0.7\% to 28.8\% (41.1-fold change; slope, 5.4\% per year; 95\% CI, 5.1\%-5.7\%). The use of robotic surgery increased 8.8\% in the first 4 years after hospitals began performing robotic surgery (2.8\% per year; 95\% CI, 2.7\%-2.9\%). This trend was associated with a decrease in laparoscopic surgery from 53.2\% to 51.3\% (difference, -1.9\%; 95\% CI, -2.2\% to -1.6\%). Before adopting robotic surgery, hospitals' use of laparoscopic surgery increased 1.3\% per year. After adopting robotic surgery, the use of laparoscopic surgery declined 0.3\% (difference in trends, -1.6\%; 95\% CI, -1.7\% to -1.5\%).These results suggest that robotic surgery has continued to diffuse across a broad range of common surgical procedures. Hospitals that launched robotic surgery programs had a broad and immediate increase in the use of robotic surgery, which was associated with a decrease in traditional laparoscopic minimally invasive surgery.},
  file = {C:\Users\benja\Zotero\storage\V7JLJ83W\2758472.html}
}

@phdthesis{Shewchuk.1994,
  title = {An Introduction to the Conjugate Gradient Method without the Agonizing Pain},
  author = {Shewchuk, {\relax JR}},
  year = {1994},
  address = {Pittsburgh},
  school = {Carnegie Mellon University}
}

@article{shimaDifferentiableSparseOptimal2023,
  title = {Differentiable {{Sparse Optimal Control}}},
  author = {Shima, Ryotaro and Moriyasu, Ryuta and Kawaguchi, Sho and Kashima, Kenji},
  year = {2023},
  journal = {IEEE Control Systems Letters},
  volume = {7},
  pages = {3126--3131},
  issn = {2475-1456},
  doi = {10.1109/LCSYS.2023.3289269},
  urldate = {2024-06-24},
  abstract = {This letter develops a framework for differentiating sparse optimal control inputs with respect to cost parameters. The difficulty lies in the non-smoothness induced by a sparsity-enhancing regularizer. To avoid this, we identify the optimal inputs as a unique zero point of a function using the proximal technique. This enables us to characterize the differentiability and employ the implicit function theorem. We also demonstrate the effectiveness of our approach using a numerical example of inverse optimal control.},
  keywords = {computer-aided control design,Jacobian matrices,machine learning,Mathematical models,Optimal control,Qualifications,Sensitivity,Sparse matrices,Sufficient conditions},
  file = {C:\Users\benja\Zotero\storage\LBJ8TRQX\10163487.html}
}

@inproceedings{shimanoVALVersatileRobot1979,
  title = {{{VAL}}: A Versatile Robot Programming and Control System},
  shorttitle = {{{VAL}}},
  booktitle = {{{COMPSAC}} 79. {{Proceedings}}. {{Computer Software}} and {{The IEEE Computer Society}}'s {{Third International Applications Conference}}, 1979.},
  author = {Shimano, B.},
  year = {1979},
  month = nov,
  pages = {878--883},
  doi = {10.1109/CMPSAC.1979.762620},
  urldate = {2024-04-15},
  abstract = {VAL is a computer based control system and programming language which has been designed specifically for use with industrial robots. In this paper, VAL is described as it is implemented as an integral part of the Unimation PUMA robot. First, the general capabilities of VAL are presented. These include the ability to interactively edit, interpret, debug, execute, and store user programs in an environment that supports concurrent console communication and program execution. Then, the system hardware is described. Next, the various methods of tool point trajectory control which are available in VAL are described. This is followed by a brief description of the VAL language. Special instructions for dynamically modifying position data are described along with instructions for automatically adapting programs based upon sensory information.},
  keywords = {Computer industry,Computer languages,Control systems,Electrical equipment industry,Hardware,Industrial control,Laboratories,Robot control,Robot programming,Service robots},
  file = {C:\Users\benja\Zotero\storage\RNDCGRIC\762620.html}
}

@book{shimbunPokaYokeImprovingProduct1988,
  title = {{Poka-Yoke: Improving Product Quality by Preventing Defects}},
  shorttitle = {{Poka-Yoke}},
  author = {Shimbun, Nikkan Kogyo},
  year = {1988},
  month = dec,
  publisher = {Taylor \& Francis Inc},
  address = {Cambridge, Massachusetts},
  abstract = {If your goal is 100\% zero defects, here is the book for you  a completely illustrated guide to poka-yoke (mistake-proofing) for supervisors and shop-floor workers. Many poka-yoke ideas come from line workers and are implemented with the help of engineering staff or tooling or machine specialists. The result is better product quality and greater participation by workers in efforts to improve your processes, your products, and your company as a whole.The first section of the book uses a simple, illustrated format to summarize many of the concepts and main features of poka-yoke. The second section shows 240 examples of poka-yoke improvements implemented in Japanese plants.The book:Organizes examples according to the broad issue or problem they address.Pinpoints how poka-yoke applies to specific devices, parts and products, categories of improvement methods, and processes.Provides sample improvement forms for you to sketch out your own ideas. Use Poka-yoke in study groups as a model for your improvement efforts. It may be your single most important step toward eliminating defects completely. (For an industrial engineering perspective on how source inspection and poka-yoke can work together to reduce defects to zero, see Shigeo Shingo's Zero Quality Control.)},
  isbn = {978-0-915299-31-7},
  langid = {Englisch}
}

@misc{Shimkin2009,
  title = {Kinematic {{Models}} for {{Target Tracking}}},
  author = {Shimkin, Nahum},
  year = {2009},
  publisher = {Technion -- Israel Institute of Technology},
  address = {Haifa, Israel}
}

@inproceedings{shinImprovingNeuralProgram2018,
  title = {Improving {{Neural Program Synthesis}} with {{Inferred Execution Traces}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Shin, Richard and Polosukhin, Illia and Song, Dawn},
  year = {2018},
  pages = {10},
  address = {Montreal, Canada},
  abstract = {The task of program synthesis, or automatically generating programs that are consistent with a provided specification, remains a challenging task in artificial intelligence. As in other fields of AI, deep learning-based end-to-end approaches have made great advances in program synthesis. However, compared to other fields such as computer vision, program synthesis provides greater opportunities to explicitly exploit structured information such as execution traces. While execution traces can provide highly detailed guidance for a program synthesis method, they are more difficult to obtain than more basic forms of specification such as input/output pairs. Therefore, we use the insight that we can split the process into two parts: infer traces from input/output examples, then infer programs from traces. Our application of this idea leads to state-of-the-art results in program synthesis in the Karel domain, improving accuracy to 81.3\% from the 77.12\% of prior work.},
  langid = {english}
}

@inproceedings{shinSyntheticDatasetsNeural2018,
  title = {Synthetic {{Datasets}} for {{Neural Program Synthesis}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Shin, Richard and Kant, Neel and Gupta, Kavi and Bender, Chris and Trabucco, Brandon and Singh, Rishabh and Song, Dawn},
  year = {2018},
  month = sep,
  urldate = {2022-04-02},
  abstract = {The goal of program synthesis is to automatically generate programs in a particular language from corresponding specifications, e.g. input-output behavior. Many current approaches achieve...},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\72EAPFP9\forum.html}
}

@inproceedings{shiProvableGuaranteesNeural2023,
  title = {Provable {{Guarantees}} for {{Neural Networks}} via {{Gradient Feature Learning}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Shi, Zhenmei and Wei, Junyi and Liang, Yingyu},
  year = {2023},
  month = nov,
  urldate = {2024-04-19},
  abstract = {Neural networks have achieved remarkable empirical performance, while the current theoretical analysis is not adequate for understanding their success, e.g., the Neural Tangent Kernel approach fails to capture their key feature learning ability, while recent analyses on feature learning are typically problem-specific. This work proposes a unified analysis framework for two-layer networks trained by gradient descent. The framework is centered around the principle of feature learning from gradients, and its effectiveness is demonstrated by applications in several prototypical problems, such as mixtures of Gaussians and parity functions. The framework also sheds light on interesting network learning phenomena such as feature learning beyond kernels and the lottery ticket hypothesis.},
  langid = {english}
}

@incollection{shoemakeUniformRandomRotations1992,
  title = {Uniform {{Random Rotations}}},
  booktitle = {Graphics {{Gems III}} ({{IBM Version}})},
  author = {Shoemake, Ken},
  editor = {Kirk, David},
  year = {1992},
  month = jan,
  pages = {124--132},
  publisher = {Morgan Kaufmann},
  address = {San Francisco},
  doi = {10.1016/B978-0-08-050755-2.50036-1},
  urldate = {2019-08-23},
  abstract = {A planar rotation can be represented in several ways---for example, as an angle between 0 and 2{$\pi$} or as a unit complex number x + iy = cos {\texttheta} + i sin {\texttheta}. Planar rotations combine by summing their angles modulo 2{$\pi$} ; so one way to generate a uniform planar rotation is to generate a uniform angle. This chapter describes a uniformly distributed spatial rotation as one not having a uniformly distributed angle. For a unit quaternion, the {$\omega$} component is the cosine of half the angle of rotation. When the angle is uniformly distributed between 0 and 2{$\pi$}, the average magnitude of {$\omega$} will be 2/{$\pi$} 0.6366, which exceeds the correct value for a uniform rotation by a factor of 3/2. It is easy to generate random unit quaternions and, hence, rotations with the correct distribution. Pairs of independent variables with Gaussian distribution can easily be generated using the polar or Box--Muller method, which transforms a point uniformly distributed within the unit disk. The Gaussian generation can be folded into the unit quaternion generation to give an efficient algorithm.},
  isbn = {978-0-12-409673-8}
}

@article{shridharUncertaintyEstimationsSoftplus2019,
  title = {Uncertainty {{Estimations}} by {{Softplus}} Normalization in {{Bayesian Convolutional Neural Networks}} with {{Variational Inference}}},
  author = {Shridhar, Kumar and Laumann, Felix and Liwicki, Marcus},
  year = {2019},
  month = may,
  journal = {arXiv:1806.05978 [cs, stat]},
  eprint = {1806.05978},
  primaryclass = {cs, stat},
  urldate = {2020-12-19},
  abstract = {We introduce a novel uncertainty estimation for classification tasks for Bayesian convolutional neural networks with variational inference. By normalizing the output of a Softplus function in the final layer, we estimate aleatoric and epistemic uncertainty in a coherent manner. The intractable posterior probability distributions over weights are inferred by Bayes by Backprop. Firstly, we demonstrate how this reliable variational inference method can serve as a fundamental construct for various network architectures. On multiple datasets in supervised learning settings (MNIST, CIFAR-10, CIFAR-100), this variational inference method achieves performances equivalent to frequentist inference in identical architectures, while the two desiderata, a measure for uncertainty and regularization are incorporated naturally. Secondly, we examine how our proposed measure for aleatoric and epistemic uncertainties is derived and validate it on the aforementioned datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\HWQLGCGV\1806.html}
}

@article{shuNeuralProgrammingExample2017,
  title = {Neural {{Programming}} by {{Example}}},
  author = {Shu, Chengxun and Zhang, Hongyu},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.04990 [cs]},
  eprint = {1703.04990},
  primaryclass = {cs},
  urldate = {2019-05-17},
  abstract = {Programming by Example (PBE) targets at automatically inferring a computer program for accomplishing a certain task from sample input and output. In this paper, we propose a deep neural networks (DNN) based PBE model called Neural Programming by Example (NPBE), which can learn from input-output strings and induce programs that solve the string manipulation problems. Our NPBE model has four neural network based components: a string encoder, an input-output analyzer, a program generator, and a symbol selector. We demonstrate the effectiveness of NPBE by training it end-to-end to solve some common string manipulation problems in spreadsheet systems. The results show that our model can induce string manipulation programs effectively. Our work is one step towards teaching DNN to generate computer programs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Computer Science - Software Engineering},
  file = {C:\Users\benja\Zotero\storage\MXDGQNIY\1703.html}
}

@inproceedings{shysheyaFiTParameterEfficient2022,
  title = {{{FiT}}: {{Parameter Efficient Few-shot Transfer Learning}} for {{Personalized}} and {{Federated Image Classification}}},
  shorttitle = {{{FiT}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Shysheya, Aliaksandra and Bronskill, John F. and Patacchiola, Massimiliano and Nowozin, Sebastian and Turner, Richard E.},
  year = {2022},
  month = sep,
  urldate = {2024-06-15},
  abstract = {Modern deep learning systems are increasingly deployed in situations such as personalization and federated learning where it is necessary to support i) learning on small amounts of data, and ii) communication efficient distributed training protocols. In this work, we develop FiLM Transfer (FiT) which fulfills these requirements in the image classification setting by combining ideas from transfer learning (fixed pretrained backbones and fine-tuned FiLM adapter layers) and meta-learning (automatically configured Naive Bayes classifiers and episodic training) to yield parameter efficient models with superior classification accuracy at low-shot. The resulting parameter efficiency is key for enabling few-shot learning, inexpensive model updates for personalization, and communication efficient federated learning. We experiment with FiT on a wide range of downstream datasets and show that it achieves better classification accuracy than the leading Big Transfer (BiT) algorithm at low-shot and achieves state-of-the art accuracy on the challenging VTAB-1k benchmark, with fewer than 1\% of the updateable parameters. Finally, we demonstrate the parameter efficiency and superior accuracy of FiT in distributed low-shot applications including model personalization and federated learning where model update size is an important performance metric.},
  langid = {english}
}

@inproceedings{siaterlisAdoptionAIEU2022,
  title = {Adoption of {{AI}} in {{EU Manufacturing}}. {{Gaps}} and {{Challenges}}},
  booktitle = {Proceedings of the 33rd {{International DAAAM Symposium}} 2022},
  author = {Siaterlis, George and Nikolakis, Nikolaos and Alexopoulos, Kosmas and Makris, S.},
  year = {2022},
  month = jan,
  pages = {0547--0550},
  doi = {10.2507/33rd.daaam.proceedings.077},
  isbn = {978-3-902734-36-5}
}

@book{Siciliano.2016,
  title = {Springer {{Handbook}} of {{Robotics}}},
  editor = {Siciliano, Bruno and Khatib, Oussama},
  year = {2016},
  edition = {2nd ed.},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-32552-1},
  isbn = {978-3-319-32550-7}
}

@article{sidiropoulosHumanrobotCollaborativeObject2021,
  title = {Human-Robot Collaborative Object Transfer Using Human Motion Prediction Based on {{Cartesian}} Pose {{Dynamic Movement Primitives}}},
  author = {Sidiropoulos, Antonis and Karayiannidis, Yiannis and Doulgeri, Zoe},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.03155 [cs]},
  eprint = {2104.03155},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {In this work, the problem of human-robot collaborative object transfer to unknown target poses is addressed. The desired pattern of the end-effector pose trajectory to a known target pose is encoded using DMPs (Dynamic Movement Primitives). During transportation of the object to new unknown targets, a DMP-based reference model and an EKF (Extended Kalman Filter) for estimating the target pose and time duration of the human's intended motion is proposed. A stability analysis of the overall scheme is provided. Experiments using a Kuka LWR4+ robot equipped with an ATI sensor at its end-effector validate its efficacy with respect to the required human effort and compare it with an admittance control scheme.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\IY4ZN5L9\2104.html}
}

@article{siegelmannComputationalPowerNeural1995,
  title = {On the {{Computational Power}} of {{Neural Nets}}},
  author = {Siegelmann, H. T. and Sontag, E. D.},
  year = {1995},
  month = feb,
  journal = {Journal of Computer and System Sciences},
  volume = {50},
  number = {1},
  pages = {132--150},
  issn = {0022-0000},
  doi = {10.1006/jcss.1995.1013},
  urldate = {2019-08-23},
  abstract = {This paper deals with finite size networks which consist of interconnections of synchronously evolving processors. Each processor updates its state by applying a "sigmoidal" function to a linear combination of the previous states of all units. We prove that one may simulate all Turing machines by such nets. In particular, one can simulate any multi-stack Turing machine in real time, and there is a net made up of 886 processors which computes a universal partial-recursive function. Products (high order nets) are not required, contrary to what had been stated in the literature. Non-deterministic Turing machines can be simulated by non-deterministic rational nets, also in real time. The simulation result has many consequences regarding the decidability, or more generally the complexity, of questions about recursive nets.}
}

@misc{SiemensBautMit2020,
  title = {{Siemens baut mit Volkswagen digitalisierte Elektroauto-Produktion}},
  year = {2020},
  month = mar,
  journal = {Automationspraxis},
  urldate = {2024-04-16},
  abstract = {Siemens unterst{\"u}tzt Volkswagen dabei, seine Fertigung im Fahrzeugwerk Zwickau auf die Produktion von Elektroautos umzustellen.},
  howpublished = {https://automationspraxis.industrie.de/industrie-4-0/siemens-baut-mit-volkswagen-digitalisierte-elektroauto-produktion/},
  langid = {ngerman},
  file = {C:\Users\benja\Zotero\storage\HVU5B429\siemens-baut-mit-volkswagen-digitalisierte-elektroauto-produktion.html}
}

@misc{SiemensMindSphere2020,
  title = {Siemens {{MindSphere}}},
  year = {2020},
  month = may,
  urldate = {2020-05-15},
  abstract = {Develop apps for MindSphere, the IoT platform from Siemens. Make machines smart, make production lines fast, and prevent equipment failures before they happen. Build, scale, and deploy fast using open APIs.},
  howpublished = {https://siemens.mindsphere.io/en},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\87X3UNZI\en.html}
}

@article{silverLearningSymbolicOperators2021,
  title = {Learning {{Symbolic Operators}} for {{Task}} and {{Motion Planning}}},
  author = {Silver, Tom and Chitnis, Rohan and Tenenbaum, Joshua and Kaelbling, Leslie Pack and {Lozano-Perez}, Tomas},
  year = {2021},
  month = jul,
  journal = {arXiv:2103.00589 [cs]},
  eprint = {2103.00589},
  primaryclass = {cs},
  urldate = {2021-10-19},
  abstract = {Robotic planning problems in hybrid state and action spaces can be solved by integrated task and motion planners (TAMP) that handle the complex interaction between motion-level decisions and task-level plan feasibility. TAMP approaches rely on domain-specific symbolic operators to guide the task-level search, making planning efficient. In this work, we formalize and study the problem of operator learning for TAMP. Central to this study is the view that operators define a lossy abstraction of the transition model of a domain. We then propose a bottom-up relational learning method for operator learning and show how the learned operators can be used for planning in a TAMP system. Experimentally, we provide results in three domains, including long-horizon robotic planning tasks. We find our approach to substantially outperform several baselines, including three graph neural network-based model-free approaches from the recent literature. Video: https://youtu.be/iVfpX9BpBRo Code: https://git.io/JCT0g},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\benja\\Zotero\\storage\\5H6HTHSK\\Silver et al. - 2021 - Learning Symbolic Operators for Task and Motion Pl.pdf;C\:\\Users\\benja\\Zotero\\storage\\FZJZQ6NJ\\2103.html}
}

@inproceedings{silverLearningSymbolicOperators2021a,
  title = {Learning {{Symbolic Operators}} for {{Task}} and {{Motion Planning}}},
  booktitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Silver, Tom and Chitnis, Rohan and Tenenbaum, Joshua and Kaelbling, Leslie Pack and {Lozano-P{\'e}rez}, Tom{\'a}s},
  year = {2021},
  month = sep,
  pages = {3182--3189},
  issn = {2153-0866},
  doi = {10.1109/IROS51168.2021.9635941},
  urldate = {2024-03-08},
  abstract = {Robotic planning problems in hybrid state and action spaces can be solved by integrated task and motion planners (TAMP) that handle the complex interaction between motion-level decisions and task-level plan feasibility. TAMP approaches rely on domain-specific symbolic operators to guide the task-level search, making planning efficient. In this work, we formalize and study the problem of operator learning for TAMP. Central to this study is the view that operators define a lossy abstraction of the transition model of a domain. We then propose a bottom-up relational learning method for operator learning and show how the learned operators can be used for planning in a TAMP system. Experimentally, we provide results in three domains, including long-horizon robotic planning tasks. We find our approach to substantially outperform several baselines, including three graph neural network-based model-free approaches from the recent literature. Video: https://youtu.be/iVfpX9BpBRo. Code: https://git.io/JCT0g},
  keywords = {Intelligent robots,Learning systems,Planning,Reinforcement learning,Standards,Task analysis},
  file = {C:\Users\benja\Zotero\storage\CN266JHC\9635941.html}
}

@article{silverPredictronEndToEndLearning2017,
  title = {The {{Predictron}}: {{End-To-End Learning}} and {{Planning}}},
  shorttitle = {The {{Predictron}}},
  author = {Silver, David and {van Hasselt}, Hado and Hessel, Matteo and Schaul, Tom and Guez, Arthur and Harley, Tim and {Dulac-Arnold}, Gabriel and Reichert, David and Rabinowitz, Neil and Barreto, Andre and Degris, Thomas},
  year = {2017},
  month = jul,
  journal = {arXiv:1612.08810 [cs]},
  eprint = {1612.08810},
  primaryclass = {cs},
  urldate = {2020-04-03},
  abstract = {One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple ``imagined'' planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-toend so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{silverRewardEnough2021,
  title = {Reward Is Enough},
  author = {Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
  year = {2021},
  month = oct,
  journal = {Artificial Intelligence},
  volume = {299},
  pages = {103535},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2021.103535},
  urldate = {2021-12-05},
  abstract = {In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.},
  langid = {english},
  keywords = {Artificial general intelligence,Artificial intelligence,Reinforcement learning,Reward}
}

@inproceedings{simonSelftuningRobotProgram1990,
  title = {Self-Tuning of Robot Program Primitives},
  booktitle = {, {{IEEE International Conference}} on {{Robotics}} and {{Automation Proceedings}}},
  author = {Simon, D.A. and Weiss, L.E. and Sanderson, A.C.},
  year = {1990},
  month = may,
  pages = {708-713 vol.1},
  doi = {10.1109/ROBOT.1990.126068},
  abstract = {Strategies used and parameter selection problems encountered in developing robot programs are addressed by describing an approach to self-tuning of robot program parameters. In this approach, the robot program incorporates control primitives with adjustable parameters and an associated cost function. A hybrid gradient-based and direct-search algorithm uses experimentally measured performance data to adjust the parameters to seek optimal performance and track system variations. Alternative control strategies which have first been optimized with the same cost function are then assessed in terms of their optimized behavior. It is demonstrated that the optimal control strategy for a particular task is a function not only of task geometry, but also of the desired performance.{$<>$}},
  keywords = {Automatic control,Control system synthesis,Cost function,Feedback,Force sensors,Motion control,Motion planning,Robot control,Robot sensing systems,Robotics and automation}
}

@misc{SimulatorIndustrialRobots2020,
  title = {Simulator for Industrial Robots and Offline Programming - {{RoboDK}}},
  year = {2020},
  month = may,
  urldate = {2020-05-12},
  howpublished = {https://robodk.com/},
  file = {C:\Users\benja\Zotero\storage\G9XBWJY9\robodk.com.html}
}

@article{singhEndtoEndRoboticReinforcement2019,
  title = {End-to-{{End Robotic Reinforcement Learning}} without {{Reward Engineering}}},
  author = {Singh, Avi and Yang, Larry and Hartikainen, Kristian and Finn, Chelsea and Levine, Sergey},
  year = {2019},
  month = may,
  journal = {arXiv:1904.07854 [cs, stat]},
  eprint = {1904.07854},
  primaryclass = {cs, stat},
  urldate = {2020-07-01},
  abstract = {The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, real-world applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward specifications by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efficient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot's camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually specified reward functions, and with only 1-4 hours of interaction with the real world.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\P28EGXIK\1904.html}
}

@inproceedings{singhProgPromptGeneratingSituated2023,
  title = {{{ProgPrompt}}: {{Generating Situated Robot Task Plans}} Using {{Large Language Models}}},
  shorttitle = {{{ProgPrompt}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  year = {2023},
  month = may,
  pages = {11523--11530},
  doi = {10.1109/ICRA48891.2023.10161317},
  urldate = {2024-06-16},
  abstract = {Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io},
  keywords = {Automation,Manipulators,Natural languages,Planning,Task analysis},
  file = {C:\Users\benja\Zotero\storage\4BSXISBW\10161317.html}
}

@article{sirohiEfficientLPSEfficientLiDAR2021,
  title = {{{EfficientLPS}}: {{Efficient LiDAR Panoptic Segmentation}}},
  shorttitle = {{{EfficientLPS}}},
  author = {Sirohi, Kshitij and Mohan, Rohit and B{\"u}scher, Daniel and Burgard, Wolfram and Valada, Abhinav},
  year = {2021},
  month = feb,
  urldate = {2021-11-10},
  abstract = {Panoptic segmentation of point clouds is a crucial task that enables autonomous vehicles to comprehend their vicinity using their highly accurate and reliable LiDAR sensors. Existing top-down approaches tackle this problem by either combining independent task-specific networks or translating methods from the image domain ignoring the intricacies of LiDAR data and thus often resulting in sub-optimal performance. In this paper, we present the novel top-down Efficient LiDAR Panoptic Segmentation (EfficientLPS) architecture that addresses multiple challenges in segmenting LiDAR point clouds including distance-dependent sparsity, severe occlusions, large scale-variations, and re-projection errors. EfficientLPS comprises of a novel shared backbone that encodes with strengthened geometric transformation modeling capacity and aggregates semantically rich range-aware multi-scale features. It incorporates new scale-invariant semantic and instance segmentation heads along with the panoptic fusion module which is supervised by our proposed panoptic periphery loss function. Additionally, we formulate a regularized pseudo labeling framework to further improve the performance of EfficientLPS by training on unlabelled data. We benchmark our proposed model on two large-scale LiDAR datasets: nuScenes, for which we also provide ground truth annotations, and SemanticKITTI. Notably, EfficientLPS sets the new state-of-the-art on both these datasets.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\YDXAZ4YN\2102.html}
}

@inproceedings{skalseDefiningCharacterizingReward2024,
  title = {Defining and Characterizing Reward Hacking},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Skalse, Joar and Howe, Nikolaus H. R. and Krasheninnikov, Dmitrii and Krueger, David},
  year = {2024},
  month = apr,
  series = {{{NIPS}} '22},
  pages = {9460--9471},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2024-05-02},
  abstract = {We provide the first formal definition of \textbf{reward hacking}, a phenomenon where optimizing an imperfect proxy reward function, \${\textbackslash}mathcal\{{\textbackslash}tilde\{R\}\}\$, leads to poor performance according to the true reward function, \${\textbackslash}mathcal\{R\}\$. We say that a proxy is \textbf{unhackable} if increasing the expected proxy return can never decrease the expected true return. Intuitively, it might be possible to create an unhackable proxy by leaving some terms out of the reward function (making it "narrower") or overlooking fine-grained distinctions between roughly equivalent outcomes, but we show this is usually not the case. A key insight is that the linearity of reward (in state-action visit counts) makes unhackability a very strong condition. In particular, for the set of all stochastic policies, two reward functions can only be unhackable if one of them is constant. We thus turn our attention to deterministic policies and finite sets of stochastic policies, where non-trivial unhackable pairs always exist, and establish necessary and sufficient conditions for the existence of simplifications, an important special case of unhackability. Our results reveal a tension between using reward functions to specify narrow tasks and aligning AI systems with human values.},
  isbn = {978-1-71387-108-8}
}

@inproceedings{Skelly.2007,
  title = {Improved Feature Descriptors for {{3D}} Surface Matching},
  booktitle = {Two- and Three-Dimensional Methods for Inspection and Metrology v},
  author = {Skelly, Luke J. and Sclaroff, Stan},
  editor = {Huang, Peisen S.},
  year = {2007},
  series = {{{SPIE}} Proceedings},
  pages = {67620A},
  publisher = {SPIE},
  doi = {10.1117/12.753263},
  bookpagination = {page}
}

@article{skubicSpatialLanguageHumanrobot2004,
  title = {Spatial Language for Human-Robot Dialogs},
  author = {Skubic, M. and Perzanowski, D. and Blisard, S. and Schultz, A. and Adams, W. and Bugajska, M. and Brock, D.},
  year = {2004},
  month = may,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume = {34},
  number = {2},
  pages = {154--167},
  issn = {1558-2442},
  doi = {10.1109/TSMCC.2004.826273},
  urldate = {2024-08-15},
  abstract = {In conversation, people often use spatial relationships to describe their environment, e.g., "There is a desk in front of me and a doorway behind it," and to issue directives, e.g., "go around the desk and through the doorway." In our research, we have been investigating the use of spatial relationships to establish a natural communication mechanism between people and robots, in particular, for novice users. In this paper, the work on robot spatial relationships is combined with a multimodal robot interface. We show how linguistic spatial descriptions and other spatial information can be extracted from an evidence grid map and how this information can be used in a natural, human-robot dialog. Examples using spatial language are included for both robot-to-human feedback and also human-to-robot commands. We also discuss some linguistic consequences in the semantic representations of spatial and locative information based on this work.},
  keywords = {Cognition,Cognitive robotics,Data mining,Feedback,Humans,Laboratories,Layout,Military computing,Navigation,Robot sensing systems}
}

@inproceedings{Slavcheva.2017,
  title = {{{KillingFusion}}: {{Non-rigid 3D}} Reconstruction without Correspondences},
  booktitle = {2017 {{IEEE}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
  author = {Slavcheva, Miroslava and Baust, Maximilian and Cremers, Daniel and Ilic, Slobodan},
  year = {2017},
  pages = {5474--5483},
  publisher = {IEEE},
  doi = {10.1109/CVPR.2017.581},
  bookpagination = {page},
  isbn = {978-1-5386-0457-1}
}

@incollection{Smisek.2013,
  title = {{{3D}} with Kinect},
  booktitle = {Consumer Depth Cameras for Computer Vision},
  author = {Smisek, Jan and Jancosek, Michal and Pajdla, Tomas},
  editor = {Fossati, Andrea and Gall, Juergen and Grabner, Helmut and Ren, Xiaofeng and Konolige, Kurt},
  year = {2013},
  pages = {3--25},
  publisher = {Springer London},
  address = {London},
  doi = {10.1007/978-1-4471-4640-7_1},
  bookpagination = {page},
  isbn = {978-1-4471-4639-1}
}

@article{smithTopdownSynthesisDivideandconquer1985,
  title = {Top-down Synthesis of Divide-and-Conquer Algorithms},
  author = {Smith, Douglas R.},
  year = {1985},
  month = sep,
  journal = {Artificial Intelligence},
  volume = {27},
  number = {1},
  pages = {43--96},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(85)90083-9},
  urldate = {2024-04-18},
  abstract = {A top-down method is presented for the derivation of algorithms from a formal specification of a problem. This method has been implemented in a system called cypress. The synthesis process involves the top-down decomposition of the initial specification into a hierarchy of specifications for subproblems. Synthesizing programs for each of these subproblems results in the composition of a hierarchically structured program. The initial specification is allowed to be partial in that some or all of the input conditions may be missing. cypress completes the specification and produces a totally correct applicative program. Much of cypress' knowledge comes in the form of `design strategies' for various classes of algorithms. The structure of a class of divide-and-conquer algorithms is explored and provides the basis for several design strategies. Detailed derivations of mergesort and quicksort algorithms are presented.},
  file = {C:\Users\benja\Zotero\storage\5Z2WURI5\0004370285900839.html}
}

@inproceedings{smitsITASCToolMultisensor2008,
  title = {{{iTASC}}: A Tool for Multi-Sensor Integration in Robot Manipulation},
  shorttitle = {{{iTASC}}},
  booktitle = {2008 {{IEEE International Conference}} on {{Multisensor Fusion}} and {{Integration}} for {{Intelligent Systems}}},
  author = {Smits, Ruben and De Laet, Tinne and Claes, Kasper and Bruyninckx, Herman and De Schutter, Joris},
  year = {2008},
  month = aug,
  pages = {426--433},
  doi = {10.1109/MFI.2008.4648032},
  abstract = {iTASC (acronym for dasiainstantaneous task specification and controlpsila) by J. De Schutter (2007) is a systematic constraint-based approach to specify complex tasks of general sensor-based robot systems. iTASC integrates both instantaneous task specification and estimation of geometric uncertainty in a unified framework. Automatic derivation of controller and estimator equations follows from a geometric task model that is obtained using a systematic task modeling procedure. The approach applies to a large variety of robot systems (mobile robots, multiple robot systems, dynamic human-robot interaction, etc.), various sensor systems, and different robot tasks. Using an example task, this paper shows that iTASC is a powerful tool for multi-sensor integration in robot manipulation. The example task includes multiple sensors: encoders, a force sensor, cameras, a laser distance sensor and a laser scanner. The paper details the systematic modeling procedure for the example task and elaborates on the task specific choice of two types of task coordinates: feature coordinates, defined with respect to object and feature frames, which facilitate the task specification, and uncertainty coordinates to model geometric uncertainty. Experimental results for the example task are presented.},
  keywords = {Cameras,Distance measurement,Probes,Robot kinematics,Robot sensing systems,Robots,Uncertainty},
  file = {C:\Users\benja\Zotero\storage\ABM7BVN5\4648032.html}
}

@misc{smitsOrocosKinematicsDynamics2020,
  title = {Orocos {{Kinematics}} and {{Dynamics Library}}},
  author = {Smits, Ruben},
  year = {2020},
  month = oct,
  urldate = {2020-10-26},
  abstract = {Orocos Kinematics and Dynamics C++ library. Contribute to orocos/orocos\_kinematics\_dynamics development by creating an account on GitHub.},
  howpublished = {Open Robot Control Software}
}

@article{smolenskyProperTreatmentConnectionism1988,
  title = {On the Proper Treatment of Connectionism},
  author = {Smolensky, Paul},
  year = {1988},
  month = mar,
  journal = {Behavioral and Brain Sciences},
  volume = {11},
  number = {1},
  pages = {1--23},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X00052432},
  urldate = {2024-04-17},
  abstract = {A set of hypotheses is formulated for a connectionist approach to cognitive modeling. These hypotheses are shown to be incompatible with the hypotheses underlying traditional cognitive models. The connectionist models considered are massively parallel numerical computational systems that are a kind of continuous dynamical system. The numerical variables in the system correspond semantically to fine-grained features below the level of the concepts consciously used to describe the task domain. The level of analysis is intermediate between those of symbolic cognitive models and neural models. The explanations of behavior provided are like those traditional in the physical sciences, unlike the explanations provided by symbolic models.Higher-level analyses of these connectionist models reveal subtle relations to symbolic models. Parallel connectionist memory and linguistic processes are hypothesized to give rise to processes that are describable at a higher level as sequential rule application. At the lower level, computation has the character of massively parallel satisfaction of soft numerical constraints; at the higher level, this can lead to competence characterizable by hard rules. Performance will typically deviate from this competence since behavior is achieved not by interpreting hard rules but by satisfying soft constraints. The result is a picture in which traditional and connectionist theoretical constructs collaborate intimately to provide an understanding of cognition.},
  langid = {english},
  keywords = {cognition,computation,connectionism,dynamical systems,networks,neural models,parallel distributed processing,symbolic models}
}

@article{Smolka2013,
  title = {A New Galloping Gait in an Insect},
  author = {Smolka, Jochen and Byrne, Marcus J and Scholtz, Clarke H and Dacke, Marie},
  year = {2013},
  journal = {Current Biology},
  volume = {23},
  number = {20},
  pages = {R913 - R915},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2013.09.031}
}

@article{snellPrototypicalNetworksFewshot2017,
  title = {Prototypical {{Networks}} for {{Few-shot Learning}}},
  author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
  year = {2017},
  month = jun,
  journal = {arXiv:1703.05175 [cs, stat]},
  eprint = {1703.05175},
  primaryclass = {cs, stat},
  urldate = {2021-06-21},
  abstract = {We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\DPXAT95Z\1703.html}
}

@inproceedings{snoekInputWarpingBayesian2014,
  title = {Input {{Warping}} for {{Bayesian Optimization}} of {{Non-Stationary Functions}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Snoek, Jasper and Swersky, Kevin and Zemel, Rich and Adams, Ryan},
  year = {2014},
  month = jun,
  pages = {1674--1682},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2021-06-21},
  abstract = {Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions.  The ability to accurately model distributions over...},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\ACEKQFQG\snoek14.html}
}

@article{sobaniaComprehensiveSurveyProgram2023,
  title = {A {{Comprehensive Survey}} on {{Program Synthesis With Evolutionary Algorithms}}},
  author = {Sobania, Dominik and Schweim, Dirk and Rothlauf, Franz},
  year = {2023},
  month = feb,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {27},
  number = {1},
  pages = {82--97},
  issn = {1941-0026},
  doi = {10.1109/TEVC.2022.3162324},
  urldate = {2024-04-18},
  abstract = {The automatic generation of computer programs is one of the main applications with practical relevance in the field of evolutionary computation. With program synthesis techniques not only software developers could be supported in their everyday work but even users without any programming knowledge could be empowered to automate repetitive tasks and implement their own new functionality. In recent years, many novel program synthesis approaches based on evolutionary algorithms have been proposed and evaluated on common benchmark problems. Therefore, we identify and discuss in this survey the relevant evolutionary program synthesis approaches in the literature and provide an in-depth analysis of their performance. The most influential approaches we identify are stack-based, grammar-guided, as well as linear genetic programming (GP). For the stack-based approaches, we identify 37 in-scope papers, and for the grammar-guided and linear GP approaches, we identify 12 and 5 papers, respectively. Furthermore, we find that these approaches perform well on benchmark problems if there is a simple mapping from the given input to the correct output. On problems where this mapping is complex, e.g., if the problem consists of several subproblems or requires iteration/recursion for a correct solution, results tend to be worse. Consequently, for future work, we encourage researchers not only to use a program's output for assessing the quality of a solution but also the way toward a solution (e.g., correctly solved subproblems).},
  keywords = {Benchmark testing,Benchmarks,Codes,evolutionary algorithms,Evolutionary computation,genetic programming (GP),Natural languages,program synthesis,Programming,Python,Task analysis},
  file = {C:\Users\benja\Zotero\storage\VJQK9ZU2\9743417.html}
}

@inproceedings{soetensRealtimeHybridTaskBased2005,
  title = {Realtime {{Hybrid Task-Based Control}} for {{Robots}} and {{Machine Tools}}},
  booktitle = {Proceedings of the 2005 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Soetens, P. and Bruyninckx, H.},
  year = {2005},
  month = apr,
  pages = {259--264},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2005.1570129},
  abstract = {This paper presents work in the field of hard realtime robotics and machine control. We analyse the requirements of a hybrid realtime control task specification allowing the integration of discrete and continuous control tasks. We propose an application independent task structure providing data flow consistency under simulataneous access by different control layers. We provide an execution flow mechanism to guarantee execution time determinism, yet allowing flexibility to react to a changing environment. We use state machines for process monitoring and a thread-safe realtime event system to communicate changes. The tasks can be distributed over a network and communicate using interfaces or manipulate streams of data in the loop. The presented task structure is applied to a real world example.},
  keywords = {architecture,Condition monitoring,Control systems,Delay,Distributed control,distribution,Jitter,Machine tools,Mechanical engineering,Mechatronics,monitoring,realtime control,Robot control,Safety}
}

@misc{softbankroboticsNAOqiDeveloperGuide,
  title = {{{NAOqi Developer Guide}}},
  author = {Softbank Robotics},
  urldate = {2021-01-30},
  howpublished = {http://doc.aldebaran.com/2-5/index\_dev\_guide.html},
  file = {C:\Users\benja\Zotero\storage\VXVTJIYV\index_dev_guide.html}
}

@inproceedings{sohnAmortisedDeepParameter2016,
  title = {Amortised {{Deep Parameter Optimisation}} of {{GPGPU Work Group Size}} for {{OpenCV}}},
  booktitle = {Search {{Based Software Engineering}}},
  author = {Sohn, Jeongju and Lee, Seongmin and Yoo, Shin},
  editor = {Sarro, Federica and Deb, Kalyanmoy},
  year = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {211--217},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-47106-8_14},
  abstract = {GPGPU (General Purpose computing on Graphics Processing Units) enables massive parallelism by taking advantage of the Single Instruction Multiple Data (SIMD) architecture of the large number of cores found on modern graphics cards. A parameter called local work group size controls how many work items are concurrently executed on a single compute unit. Though critical to the performance, there is no deterministic way to tune it, leaving developers to manual trial and error. This paper applies amortised optimisation to determine the best local work group size for GPGPU implementations of OpenCV template matching feature. The empirical evaluation shows that optimised local work group size can outperform the default value with large effect sizes.},
  isbn = {978-3-319-47106-8},
  langid = {english},
  keywords = {Execution Time,Global Memory,Graphic Processing Unit,Single Instruction Multiple Data,Template Match}
}

@article{sohnTechnologyAcceptanceTheories2020,
  title = {Technology Acceptance Theories and Factors Influencing Artificial {{Intelligence-based}} Intelligent Products},
  author = {Sohn, Kwonsang and Kwon, Ohbyung},
  year = {2020},
  month = apr,
  journal = {Telematics and Informatics},
  volume = {47},
  pages = {101324},
  issn = {0736-5853},
  doi = {10.1016/j.tele.2019.101324},
  urldate = {2024-09-18},
  abstract = {The rapid growth of artificial intelligence (AI) technology has prompted the development of AI-based intelligent products. Accordingly, various technology acceptance theories have been used to explain acceptance of these products. This comparative study determines which models best explain consumer acceptance of AI-based intelligent products and which factors have the greatest impact in terms of purchase intention. We assessed the utility of the Technology Acceptance Model (TAM), the Theory of Planned Behavior (TPB), the Unified Theory of Acceptance and Use of Technology (UTAUT), and the Value-based Adoption Model (VAM) using data collected from a survey sample of 378 respondents, modeling user acceptance in terms of behavioral intention to use AI-based intelligent products. In addition, we employed decomposition analysis to compare each factor included in these models in terms of influence on purchase intention. We found that the VAM performed best in modeling user acceptance. Among the various factors, enjoyment was found to influence user purchase intention the most, followed by subjective norms. The findings of this study confirm that acceptance of highly innovative products with minimal practical value, such as AI-based intelligent products, is more influenced by interest in technology than in utilitarian aspects.},
  keywords = {AI-based intelligent products,Decomposition analysis,Purchase intention,Technology acceptance theory,Technology adoption},
  file = {C:\Users\benja\Zotero\storage\RLMDH7B7\S0736585319308160.html}
}

@inproceedings{songLearningSlideUnknown2020,
  title = {Learning to {{Slide Unknown Objects}} with {{Differentiable Physics Simulations}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVI}}},
  author = {Song, Changkyu and Boularias, Abdeslam},
  year = {2020},
  month = jul,
  volume = {16},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-6-1},
  file = {C:\Users\benja\Zotero\storage\QVCZIB7R\p099.html}
}

@inproceedings{songLearningSlideUnknown2020a,
  title = {Learning to {{Slide Unknown Objects}} with {{Differentiable Physics Simulations}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVI}}},
  author = {Song, Changkyu and Boularias, Abdeslam},
  year = {2020},
  month = jul,
  volume = {16},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-6-1},
  file = {C:\Users\benja\Zotero\storage\XAET57ED\p099.html}
}

@article{songRapidlyAdaptableLegged2020,
  title = {Rapidly {{Adaptable Legged Robots}} via {{Evolutionary Meta-Learning}}},
  author = {Song, Xingyou and Yang, Yuxiang and Choromanski, Krzysztof and Caluwaerts, Ken and Gao, Wenbo and Finn, Chelsea and Tan, Jie},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.01239 [cs]},
  eprint = {2003.01239},
  primaryclass = {cs},
  urldate = {2020-07-12},
  abstract = {Learning adaptable policies is crucial for robots to operate autonomously in our complex and quickly changing world. In this work, we present a new meta-learning method that allows robots to quickly adapt to changes in dynamics. In contrast to gradient-based meta-learning algorithms that rely on second-order gradient estimation, we introduce a more noise-tolerant Batch Hill-Climbing adaptation operator and combine it with meta-learning based on evolutionary strategies. Our method significantly improves adaptation to changes in dynamics in high noise settings, which are common in robotics applications. We validate our approach on a quadruped robot that learns to walk while subject to changes in dynamics. We observe that our method significantly outperforms prior gradient-based approaches, enabling the robot to adapt its policy to changes based on less than 3 minutes of real data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics}
}

@article{sooriArtificialIntelligenceMachine2023,
  title = {Artificial Intelligence, Machine Learning and Deep Learning in Advanced Robotics, a Review},
  author = {Soori, Mohsen and Arezoo, Behrooz and Dastres, Roza},
  year = {2023},
  month = jan,
  journal = {Cognitive Robotics},
  volume = {3},
  pages = {54--70},
  issn = {2667-2413},
  doi = {10.1016/j.cogr.2023.04.001},
  urldate = {2024-03-21},
  abstract = {Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) have revolutionized the field of advanced robotics in recent years. AI, ML, and DL are transforming the field of advanced robotics, making robots more intelligent, efficient, and adaptable to complex tasks and environments. Some of the applications of AI, ML, and DL in advanced robotics include autonomous navigation, object recognition and manipulation, natural language processing, and predictive maintenance. These technologies are also being used in the development of collaborative robots (cobots) that can work alongside humans and adapt to changing environments and tasks. The AI, ML, and DL can be used in advanced transportation systems in order to provide safety, efficiency, and convenience to the passengers and transportation companies . Also, the AI, ML, and DL are playing a critical role in the advancement of manufacturing assembly robots, enabling them to work more efficiently, safely, and intelligently. Furthermore, they have a wide range of applications in aviation management, helping airlines to improve efficiency, reduce costs, and improve customer satisfaction. Moreover, the AI, ML, and DL can help taxi companies in order to provide better, more efficient, and safer services to customers. The research presents an overview of current developments in AI, ML, and DL in advanced robotics systems and discusses various applications of the systems in robot modification. Further research works regarding the applications of AI, ML, and DL in advanced robotics systems are also suggested in order to fill the gaps between the existing studies and published papers. By reviewing the applications of AI, ML, and DL in advanced robotics systems, it is possible to investigate and modify the performances of advanced robots in various applications in order to enhance productivity in advanced robotic industries.},
  keywords = {Advanced robotics,Artificial intelligence,Deep learning,Machine learning},
  file = {C:\Users\benja\Zotero\storage\9RSBRQ4T\S2667241323000113.html}
}

@article{Sorkine.2005,
  title = {Laplacian Mesh Processing},
  author = {Sorkine, Olga},
  year = {2005},
  journal = {EUROGRAPHICS '05},
  abstract = {Vol 0, No 0}
}

@inproceedings{Sorkine.2007,
  title = {As-Rigid-as-Possible Surface Modeling},
  booktitle = {Proceedings of the Fifth Eurographics Symposium on Geometry Processing},
  author = {Sorkine, Olga and Alexa, Marc},
  year = {2007},
  series = {{{SGP}} '07},
  pages = {109--116},
  publisher = {Eurographics Association},
  address = {Aire-la-Ville, Switzerland, Switzerland},
  bookpagination = {page},
  isbn = {978-3-905673-46-3}
}

@misc{Sorkine.2017,
  title = {Least-Squares Rigid Motion Using {{SVD}}},
  author = {Sorkine, Olga and Alexa, Marc},
  year = {2017},
  publisher = {Department of Computer Science, ETH Zurich},
  address = {Z{\"u}rich}
}

@article{Sorkine.b,
  title = {As-Rigid-as-Possible Surface Modeling (Talk Slides)},
  author = {Sorkine, Olga and Alexa, Marc}
}

@misc{SpeechtoTextAutomaticSpeech,
  title = {Speech-to-{{Text}}: {{Automatic Speech Recognition}}},
  shorttitle = {Speech-to-{{Text}}},
  journal = {Google Cloud},
  urldate = {2023-08-16},
  abstract = {Accurately convert voice to text in over 125 languages and variants by applying Google's powerful machine learning models with an easy-to-use API.},
  howpublished = {https://cloud.google.com/speech-to-text},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\EEGZJJHX\speech-to-text.html}
}

@incollection{spitzerTacitRepresentationsArtificial2016,
  title = {Tacit {{Representations}} and {{Artificial Intelligence}}: {{Hidden Lessons}} from an {{Embodied Perspective}} on {{Cognition}}},
  shorttitle = {Tacit {{Representations}} and {{Artificial Intelligence}}},
  booktitle = {Fundamental {{Issues}} of {{Artificial Intelligence}}},
  author = {Spitzer, Elena},
  editor = {M{\"u}ller, Vincent C.},
  year = {2016},
  series = {Synthese {{Library}}},
  pages = {425--441},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-26485-1_25},
  urldate = {2024-02-06},
  abstract = {In this paper, I explore how an embodied perspective on cognition might inform research on artificial intelligence. Many embodied cognition theorists object to the central role that representations play on the traditional view of cognition. Based on these objections, it may seem that the lesson from embodied cognition is that AI should abandon representation as a central component of intelligence. However, I argue that the lesson from embodied cognition is actually that AI research should shift its focus from how to utilize explicit representations to how to create and use tacit representations. To develop this suggestion, I provide an overview of the commitments of the classical view and distinguish three critiques of the role that representations play in that view. I provide further exploration and defense of Daniel Dennett's distinction between explicit and tacit representations. I argue that we should understand the embodied cognition approach using a framework that includes tacit representations. Given this perspective, I will explore some AI research areas that may be recommended by an embodied perspective on cognition.},
  isbn = {978-3-319-26485-1},
  langid = {english},
  keywords = {Artificial intelligence,Embodied cognition,Representation,Tacit representations},
  file = {C:\Users\benja\Zotero\storage\QF9UJYBM\Spitzer - 2016 - Tacit Representations and Artificial Intelligence.pdf}
}

@incollection{Spura.2019,
  title = {Belastungs- Und Spannungsarten},
  booktitle = {Technische Mechanik 2. {{Elastostatik}}},
  author = {Spura, Christian},
  editor = {Spura, Christian},
  year = {2019},
  pages = {11--20},
  publisher = {Springer Fachmedien Wiesbaden},
  address = {Wiesbaden},
  doi = {10.1007/978-3-658-19979-1_2},
  bookpagination = {page},
  isbn = {978-3-658-19978-4}
}

@book{Spura.2019b,
  title = {Technische Mechanik 2. {{Elastostatik}}},
  editor = {Spura, Christian},
  year = {2019},
  publisher = {Springer Fachmedien Wiesbaden},
  address = {Wiesbaden},
  doi = {10.1007/978-3-658-19979-1},
  isbn = {978-3-658-19978-4}
}

@incollection{Spura.2019c,
  title = {Einf{\"u}hrung in Die Elastostatik},
  booktitle = {Technische Mechanik 2. {{Elastostatik}}},
  author = {Spura, Christian},
  editor = {Spura, Christian},
  year = {2019},
  pages = {1--10},
  publisher = {Springer Fachmedien Wiesbaden},
  address = {Wiesbaden},
  doi = {10.1007/978-3-658-19979-1_1},
  bookpagination = {page},
  isbn = {978-3-658-19978-4}
}

@article{sridharanTheoryExplanationsHuman2019,
  title = {Towards a {{Theory}} of {{Explanations}} for {{Human}}--{{Robot Collaboration}}},
  author = {Sridharan, Mohan and Meadows, Ben},
  year = {2019},
  month = dec,
  journal = {KI - K{\"u}nstliche Intelligenz},
  volume = {33},
  number = {4},
  pages = {331--342},
  issn = {1610-1987},
  doi = {10.1007/s13218-019-00616-y},
  urldate = {2020-10-08},
  abstract = {This paper makes two contributions towards enabling a robot to provide explanatory descriptions of its decisions, the underlying knowledge and beliefs, and the experiences that informed these beliefs. First, we present a theory of explanations comprising (i) claims about representing, reasoning with, and learning domain knowledge to support the construction of explanations; (ii) three fundamental axes to characterize explanations; and (iii) a methodology for constructing these explanations. Second, we describe an architecture for robots that implements this theory and supports scalability to complex domains and explanations. We demonstrate the architecture's capabilities in the context of a simulated robot (a) moving target objects to desired locations or people; or (b) following recipes to bake biscuits.},
  langid = {english}
}

@inproceedings{srinivasGaussianProcessOptimization2010,
  title = {Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design},
  shorttitle = {Gaussian Process Optimization in the Bandit Setting},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham and Seeger, Matthias},
  year = {2010},
  month = jun,
  series = {{{ICML}}'10},
  pages = {1015--1022},
  publisher = {Omnipress},
  address = {Madison, WI, USA},
  urldate = {2024-05-23},
  abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
  isbn = {978-1-60558-907-7}
}

@inproceedings{srinivasUniversalPlanningNetworks2018,
  title = {Universal {{Planning Networks}}: {{Learning Generalizable Representations}} for {{Visuomotor Control}}},
  shorttitle = {Universal {{Planning Networks}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Srinivas, Aravind and Jabri, Allan and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
  year = {2018},
  month = jul,
  pages = {4732--4741},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-25},
  abstract = {A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities. Visit https://sites.google. com/view/upn-public/home for video highlights.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\5QXH5XD6\Srinivas et al. - 2018 - Universal Planning Networks Learning Generalizabl.pdf}
}

@article{Sriv14,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  pages = {1929--1958}
}

@inproceedings{srivastavaCombinedTaskMotion2014,
  title = {Combined Task and Motion Planning through an Extensible Planner-Independent Interface Layer},
  booktitle = {{{ICRA}}},
  author = {Srivastava, Siddharth and Fang, Eugene and Riano, Lorenzo and Chitnis, Rohan and Russell, Stuart and Abbeel, Pieter},
  year = {2014},
  month = may,
  pages = {639--646},
  doi = {10.1109/ICRA.2014.6906922},
  urldate = {2020-07-11},
  isbn = {978-1-4799-3685-4},
  langid = {english}
}

@article{staffellHowDoesWind2014,
  title = {How Does Wind Farm Performance Decline with Age?},
  author = {Staffell, Iain and Green, Richard},
  year = {2014},
  month = jun,
  journal = {Renewable Energy},
  volume = {66},
  pages = {775--786},
  issn = {0960-1481},
  doi = {10.1016/j.renene.2013.10.041},
  urldate = {2024-07-27},
  abstract = {Ageing is a fact of life. Just as with conventional forms of power generation, the energy produced by a wind farm gradually decreases over its lifetime, perhaps due to falling availability, aerodynamic performance or conversion efficiency. Understanding these factors is however complicated by the highly variable availability of the wind. This paper reveals the rate of ageing of a national fleet of wind turbines using free public data for the actual and theoretical ideal load factors from the UK's 282 wind farms. Actual load factors are recorded monthly for the period of 2002--2012, covering 1686 farm-years of operation. Ideal load factors are derived from a high resolution wind resource assessment made using NASA data to estimate the hourly wind speed at the location and hub height of each wind farm, accounting for the particular models of turbine installed. By accounting for individual site conditions we confirm that load factors do decline with age, at a similar rate to other rotating machinery. Wind turbines are found to lose 1.6~{\textpm}~0.2\% of their output per year, with average load factors declining from 28.5\% when new to 21\% at age 19. This trend is consistent for different generations of turbine design and individual wind farms. This level of degradation reduces a wind farm's output by 12\% over a twenty year lifetime, increasing the levelised cost of electricity by 9\%.},
  keywords = {Ageing,Degradation,Levelised cost,Load factor,Reanalysis,Wind farm},
  file = {C:\Users\benja\Zotero\storage\249U9CSG\S0960148113005727.html}
}

@article{staronBringingSoftwareEngineering2024,
  title = {Bringing {{Software Engineering Discipline}} to the {{Development}} of {{AI-Enabled Systems}}},
  author = {Staron, Miroslaw and Abrah{\~a}o, Silvia and Lewis, Grace and Muccini, Henry and Honnenahalli, Chetan},
  year = {2024},
  month = sep,
  journal = {IEEE Software},
  volume = {41},
  number = {5},
  pages = {79--82},
  issn = {1937-4194},
  doi = {10.1109/MS.2024.3408388},
  urldate = {2024-09-17},
  abstract = {Engineering AI Software systems is starting to evolve from the pure development of machine learning (ML) models to a more structured discipline that treats ML components as part of much larger software systems. As such, more structured principles are required for their development, such as established design principles, established development models, and safeguards for deployed ML models. This column focuses on papers presented at the Third International Conference on AI Engineering---Software Engineering for AI (CAIN 2024). The selected papers reflect the current development of the field of AI systems engineering and AI software development, taking it to the next level of maturity. Feedback or suggestions are welcome. In addition, if you try or adopt any of the practices included in the column, please send us and the authors of the paper(s) a note about your experiences.},
  keywords = {Artificial intelligence,Machine learning,Modeling,Software development management,Software engineering,Software systems},
  file = {C\:\\Users\\benja\\Zotero\\storage\\GVLFZKGF\\Staron et al. - 2024 - Bringing Software Engineering Discipline to the Development of AI-Enabled Systems.pdf;C\:\\Users\\benja\\Zotero\\storage\\H2MYDU54\\10629215.html}
}

@inproceedings{Steder.2011,
  title = {Point Feature Extraction on {{3D}} Range Scans Taking into Account Object Boundaries},
  booktitle = {{{3D}} Is Here: {{Point}} Cloud Library ({{PCL}})},
  author = {Steder, Bastian and Rusu, Radu Bogdan and Konolige, Kurt and Burgard, Wolfram},
  editor = {Rusu, Radu Bogdan and Cousins, Steve},
  year = {2011},
  pages = {2601--2608},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2011.5980187},
  bookpagination = {page},
  isbn = {978-1-61284-386-5}
}

@inproceedings{steffenHoLLiECAMultifunctional2024,
  title = {{{HoLLiE C}}---{{A Multifunctional Bimanual Mobile Robot Supporting Versatile Care Applications}}},
  booktitle = {Intelligent {{Autonomous Systems}} 18},
  author = {Steffen, Lea and Schulze, Martin and Eichmann, Christian and Koch, Robin and Hermann, Andreas and Frietsch Mussulin, Rosa and Graaf, Friedrich and Wilbrandt, Robert and Gro{\ss}e Besselmann, Marvin and Roennau, Arne and Dillmann, R{\"u}diger},
  editor = {Lee, Soon-Geul and An, Jinung and Chong, Nak Young and Strand, Marcus and Kim, Joo H.},
  year = {2024},
  pages = {127--140},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-44981-9_11},
  abstract = {Care robotics as a research field has developed a lot in recent years, driven by the rapidly increasing need for it. However, these technologies are mostly limited to a very concrete and usually relatively simple use case. The bimanual robot House of Living Labs intelligent Escort (HoLLiE) includes an omnidirectional mobile platform. This paper presents how HoLLiE is adapted, by flexible software and hardware modules, for different care applications. The design goal of HoLLiE was to be human-like but abstract enough to ensure a high level of acceptance, which is very advantageous for its use in hospitals. After a short retrospect of previous generations of HoLLiE, it is highlighted how the current version is equipped with a variety of additional sensors and actuators to allow a wide range of possible applications. Then, the software stack of HoLLiE is depicted, with a focus on navigation and force-sensitive intention recognition.},
  isbn = {978-3-031-44981-9},
  langid = {english},
  keywords = {Bimanual,Care/nursing,Multi-purpose,Omnidirectional,Service robot},
  file = {C:\Users\benja\Zotero\storage\34EJVWV3\Steffen et al. - 2024 - HoLLiE CA Multifunctional Bimanual Mobile Robot Supporting Versatile Care Applications.pdf}
}

@article{steinmetzRAZERHRIVisual2018,
  title = {{{RAZER}}---{{A HRI}} for {{Visual Task-Level Programming}} and {{Intuitive Skill Parameterization}}},
  author = {Steinmetz, Franz and Wollschl{\"a}ger, Annika and Weitschat, Roman},
  year = {2018},
  month = jul,
  journal = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {3},
  pages = {1362--1369},
  issn = {2377-3766},
  doi = {10.1109/LRA.2018.2798300},
  abstract = {Maintaining competitiveness and mitigating health issues test caused by unergonomic working conditions are two main reasons for automating production processes. But such automation is expensive, also because test experts are required to program the robots. One approach to lowering these costs is to enable shop-floor workers to program robots by providing task-level programming tools. Task-level programming is an established approach, yet appropriate workflows for experts and shop-floor workers remain to be defined. The objective of this letter is to evaluate RAZER, a framework for robot task-level programming, in which skill programming and parameter interface definitions are integrated. The framework provides workflows for both experts-creating skills and providing their parameter interfaces-and for shop-floor workers-using these skills to create executable robot tasks in an intuitive human-robot interface (HRI). The HRI is a graphical user interface that runs in a browser, and provides access to other man-machine interface, such as programming by demonstration. Two pilot and two user studies proof that RAZER fulfills the demands of both experts and novice users.},
  keywords = {automating production processes,control engineering computing,Education,factory automation,graphical user interface,graphical user interfaces,Graphical user interfaces,HRI,human factors and human-in-the-loop,human-centered automation,human-robot interaction,human-robot interface,intelligent and flexible manufacturing,intuitive skill parameterization,Libraries,man-machine interface,middleware and programming environments,mobile robots,motion control,parameter interface definitions,RAZER,robot programming,Robot programming,robot task-level programming tools,shop-floor workers,Software,Task analysis,unergonomic working conditions,visual programming,visual task-level programming},
  file = {C:\Users\benja\Zotero\storage\4PI8T3DF\8269311.html}
}

@article{stenmarkDemonstrationsSkillsHighLevel2016,
  title = {From {{Demonstrations}} to {{Skills}} for {{High-Level Programming}} of {{Industrial Robots}}},
  author = {Stenmark, Maj and Topp, Elin Anna},
  year = {2016},
  series = {{{AAAI Fall Symposium Series}}},
  pages = {4},
  abstract = {In this paper we describe our approach to robotic skill representation and a prototypical implementation of a programming-by-demonstration approach that allows users to generate skills and robot program primitives for later refinement and re-use. We intend to evaluate the applicability of this approach to high-level programming in a user study, which we also explain.},
  langid = {english}
}

@article{stepputtisImitationLearningRobot2019,
  title = {Imitation {{Learning}} of {{Robot Policies}} by {{Combining Language}}, {{Vision}} and {{Demonstration}}},
  author = {Stepputtis, Simon and Campbell, Joseph and Phielipp, Mariano and Baral, Chitta and Amor, Heni Ben},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.11744 [cs]},
  eprint = {1911.11744},
  primaryclass = {cs},
  urldate = {2020-06-30},
  abstract = {In this work we propose a novel end-to-end imitation learning approach which combines natural language, vision, and motion information to produce an abstract representation of a task, which in turn is used to synthesize specific motion controllers at run-time. This multimodal approach enables generalization to a wide variety of environmental conditions and allows an end-user to direct a robot policy through verbal communication. We empirically validate our approach with an extensive set of simulations and show that it achieves a high task success rate over a variety of conditions while remaining amenable to probabilistic interpretability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\PJC48EKQ\1911.html}
}

@misc{stereolabs_zed_nodate,
  title = {{{ZED}} Mini Camera},
  author = {Inc., Stereolabs}
}

@misc{stereolabsZEDCameraSDK,
  title = {{{ZED Camera}} and {{SDK Overview}}},
  author = {Stereolabs},
  file = {C:\Users\benja\Zotero\storage\A3AFJ48M\zed-camera-datasheet.pdf}
}

@inproceedings{stocklAutonomousSurfaceGrinding2023,
  title = {Autonomous {{Surface Grinding}} of {{Wind Turbine Blades}}},
  booktitle = {Intelligent {{Autonomous Systems}} 18},
  author = {St{\"o}ckl, Florian and Strand, Marcus and M{\"u}ller, Silvan and Huber, Marco and Raible, Julian and Braun, Christopher and Katic, Darko and Alt, Benjamin and Merkt, Holger},
  editor = {Lee, Soon-Geul and An, Jinung and Chong, Nak Young and Strand, Marcus and Kim, Joo H.},
  year = {2023},
  month = jul,
  pages = {451--457},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-44981-9_38},
  abstract = {Discarded wind turbine blades generate a considerable amount of waste that could be reduced by remanufacturing. Manual remanufacturing is too costly, which is why research is being conducted into automation techniques. The main problem is the individuality of work pieces due to damages. This work presents a workflow that includes damage analysis based on scans of the blade, subsequent path planning, control engineering with an AI controller for grinding and automatic review of the grinding process. Current problems are the inaccuracy of the robot used for scanning and the colour sensitivity of the used laser scanner. Our next steps besides solving the mentioned problems are to train a supervised machine learning algorithm with damage examples and to implement a specific and multi-step path planning algorithm.},
  copyright = {All rights reserved},
  isbn = {978-3-031-44981-9},
  langid = {english},
  keywords = {Autonomous grinding,Hybrid AI,my,Remanufacturing}
}

@inproceedings{stoneOpenWorldObjectManipulation2023,
  title = {Open-{{World Object Manipulation}} Using {{Pre-Trained Vision-Language Models}}},
  booktitle = {7th {{Annual Conference}} on {{Robot Learning}}},
  author = {Stone, Austin and Xiao, Ted and Lu, Yao and Gopalakrishnan, Keerthana and Lee, Kuang-Huei and Vuong, Quan and Wohlhart, Paul and Kirmani, Sean and Zitkovich, Brianna and Xia, Fei and Finn, Chelsea and Hausman, Karol},
  year = {2023},
  month = aug,
  urldate = {2024-04-29},
  abstract = {For robots to follow instructions from people, they must be able to connect the rich semantic information in human vocabulary, e.g. ``can you get me the pink stuffed whale?'' to their sensory observations and actions. This brings up a notably difficult challenge for robots: while robot learning approaches allow robots to learn many different behaviors from first-hand experience, it is impractical for robots to have first-hand experiences that span all of this semantic information. We would like a robot's policy to be able to perceive and pick up the pink stuffed whale, even if it has never seen any data interacting with a stuffed whale before. Fortunately, static data on the internet has vast semantic information, and this information is captured in pre-trained vision-language models. In this paper, we study whether we can interface robot policies with these pre-trained models, with the aim of allowing robots to complete instructions involving object categories that the robot has never seen first-hand. We develop a simple approach, which we call Manipulation of Open-World Objects (MOO), which leverages a pre-trained vision-language model to extract object-identifying information from the language command and image, and conditions the robot policy on the current image, the instruction, and the extracted object information. In a variety of experiments on a real mobile manipulator, we find that MOO generalizes zero-shot to a wide range of novel object categories and environments. In addition, we show how MOO generalizes to other, non-language-based input modalities to specify the object of interest such as finger pointing, and how it can be further extended to enable open-world navigation and manipulation. The project's website and evaluation videos can be found at https://robot-moo.github.io/.},
  langid = {english}
}

@article{strazdasRobotsWizardsInvestigation2020,
  title = {Robots and {{Wizards}}: {{An Investigation Into Natural Human}}--{{Robot Interaction}}},
  shorttitle = {Robots and {{Wizards}}},
  author = {Strazdas, Dominykas and Hintz, Jan and Fel{\ss}berg, Anna-Maria and {Al-Hamadi}, Ayoub},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {207635--207642},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3037724},
  urldate = {2024-07-22},
  abstract = {The goal of the study was to research different communication modalities needed for intuitive Human-Robot Interaction. This study utilizes a Wizard of Oz prototyping method to enable a restriction-free, intuitive interaction with an industrial robot. The data from 36 test subjects suggests a high preference for speech input, automatic path planning and pointing gestures. The catalogue developed during this experiment contains intrinsic gestures suggesting that the two most popular gestures per action can be sufficient to cover the majority of users. The system scored an average of 74\% in different user interface experience questionnaires, while containing forced flaws. These findings allow a future development of an intuitive Human-Robot interaction system with high user acceptance.},
  keywords = {Activity recognition,Cameras,cooperative systems,Correlation,Education,gesture recognition,human-robot interaction,intelligent robots,interactive systems,robot control,Robot kinematics,robot learning,Robot vision systems,Service robots,Task analysis,telerobotics},
  file = {C:\Users\benja\Zotero\storage\6FUYRYLE\9264629.html}
}

@article{strobelOptimizingOutcomesPancreatic2019,
  title = {Optimizing the Outcomes of Pancreatic Cancer Surgery},
  author = {Strobel, Oliver and Neoptolemos, John and J{\"a}ger, Dirk and B{\"u}chler, Markus W.},
  year = {2019},
  month = jan,
  journal = {Nature Reviews Clinical Oncology},
  volume = {16},
  number = {1},
  pages = {11--26},
  publisher = {Nature Publishing Group},
  issn = {1759-4782},
  doi = {10.1038/s41571-018-0112-1},
  urldate = {2024-04-12},
  abstract = {Pancreatic cancer is likely to become the second most frequent cause of cancer-associated mortality within the next decade. Surgical resection with adjuvant systemic chemotherapy currently provides the only chance of long-term survival. However, only 10--20\% of patients with pancreatic cancer are diagnosed with localized, surgically resectable disease. The majority of patients present with metastatic disease and are not candidates for surgery, while surgery remains underused even in those with resectable disease owing to historical concerns regarding safety and efficacy. However, advances made over the past decade in the safety and efficacy of surgery have resulted in perioperative mortality of around 3\% and 5-year survival approaching 30\% after resection and adjuvant chemotherapy. Furthermore, owing to advances in both surgical techniques and systemic chemotherapy, the indications for resection have been extended to include locally advanced tumours. Many aspects of pancreatic cancer surgery, such as the management of postoperative morbidities, sequencing of resection and systemic therapy, and use of neoadjuvant therapy followed by resection for tumours previously considered unresectable, are rapidly evolving. In this Review, we summarize the current status of and new developments in pancreatic cancer surgery, while highlighting the most important research questions for attempts to further optimize outcomes.},
  copyright = {2018 Springer Nature Limited},
  langid = {english},
  keywords = {Chemotherapy,Combination drug therapy,Pancreatic cancer,Surgical oncology}
}

@inproceedings{strudelSegmenterTransformerSemantic2021,
  title = {Segmenter: {{Transformer}} for {{Semantic Segmentation}}},
  shorttitle = {Segmenter},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  year = {2021},
  pages = {7262--7272},
  urldate = {2024-06-14},
  langid = {english}
}

@book{Strutz.2011,
  title = {Data Fitting and Uncertainty: {{A}} Practical Introduction to Weighted Least Squares and Beyond},
  author = {Strutz, Tilo},
  year = {2011},
  publisher = {Vieweg + Teubner},
  address = {Wiesbaden},
  isbn = {978-3-8348-1022-9},
  price = {ca. EUR 44.90}
}

@inproceedings{stulpHierarchicalReinforcementLearning2011,
  title = {Hierarchical Reinforcement Learning with Movement Primitives},
  booktitle = {2011 11th {{IEEE-RAS International Conference}} on {{Humanoid Robots}}},
  author = {Stulp, Freek and Schaal, Stefan},
  year = {2011},
  month = oct,
  pages = {231--238},
  issn = {2164-0580},
  doi = {10.1109/Humanoids.2011.6100841},
  urldate = {2024-04-21},
  abstract = {Temporal abstraction and task decomposition drastically reduce the search space for planning and control, and are fundamental to making complex tasks amenable to learning. In the context of reinforcement learning, temporal abstractions are studied within the paradigm of hierarchical reinforcement learning. We propose a hierarchical reinforcement learning approach by applying our algorithm PI2 to sequences of Dynamic Movement Primitives. For robots, this representation has some important advantages over discrete representations in terms of scalability and convergence speed. The parameters of the Dynamic Movement Primitives are learned simultaneously at different levels of temporal abstraction. The shape of a movement primitive is optimized w.r.t. the costs up to the next primitive in the sequence, and the subgoals between two movement primitives w.r.t. the costs up to the end of the entire movement primitive sequence. We implement our approach on an 11-DOF arm and hand, and evaluate it in a pick-and-place task in which the robot transports an object between different shelves in a cupboard.},
  keywords = {Aerospace electronics,Cost function,Learning,Noise,Robots,Shape,Trajectory},
  file = {C:\Users\benja\Zotero\storage\UFTG3NQ5\6100841.html}
}

@inproceedings{stulpLearningCompactParameterized2013,
  title = {Learning Compact Parameterized Skills with a Single Regression},
  booktitle = {2013 13th {{IEEE-RAS International Conference}} on {{Humanoid Robots}} ({{Humanoids}})},
  author = {Stulp, Freek and Raiola, Gennaro and Hoarau, Antoine and Ivaldi, Serena and Sigaud, Olivier},
  year = {2013},
  month = oct,
  pages = {417--422},
  issn = {2164-0580},
  doi = {10.1109/HUMANOIDS.2013.7030008},
  urldate = {2024-04-22},
  abstract = {One of the long-term challenges of programming by demonstration is achieving generality, i.e. automatically adapting the reproduced behavior to novel situations. A common approach for achieving generality is to learn parameterizable skills from multiple demonstrations for different situations. In this paper, we generalize recent approaches on learning parameterizable skills based on dynamical movement primitives (DMPs), such that task parameters are also passed as inputs to the function approximator of the DMP. This leads to a more general, flexible, and compact representation of parameterizable skills, as demonstrated by our empirical evaluation on the iCub and Meka humanoid robots.},
  keywords = {Kernel,Robots,Shape,Training,Trajectory,Vectors,Visualization},
  file = {C:\Users\benja\Zotero\storage\4KLZ2L58\7030008.html}
}

@article{sugimotoMobileMixedRealityEnvironment2011,
  title = {A {{Mobile Mixed-Reality Environment}} for {{Children}}'s {{Storytelling Using}} a {{Handheld Projector}} and a {{Robot}}},
  author = {Sugimoto, M.},
  year = {2011},
  month = jul,
  journal = {IEEE Transactions on Learning Technologies},
  volume = {4},
  number = {03},
  pages = {249--260},
  publisher = {IEEE Computer Society},
  issn = {1939-1382},
  doi = {10.1109/TLT.2011.13},
  urldate = {2021-01-26},
  abstract = {This paper describes a system called GENTORO that uses a robot and a handheld projector for supporting children's storytelling activities. GENTORO differs from many existing systems in that children can make a robot play their own story in a physical space augmented by mixed-reality technologies. Pilot studies have been conducted to clarify the design requirements of GENTORO from both technological and practical viewpoints. A user study indicates that GENTORO's ability to enable manipulation of a robot using a handheld projector in a physical space can enhance children's embodied participation in, and their level of engagement with, their storytelling activities, and can support children in designing and expressing creative and original stories.},
  langid = {english}
}

@inproceedings{suleimanHumanMotionImitation2008,
  title = {On Human Motion Imitation by Humanoid Robot},
  booktitle = {2008 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Suleiman, W. and Yoshida, E. and Kanehiro, F. and Laumond, J. and Monin, A.},
  year = {2008},
  month = may,
  pages = {2697--2704},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2008.4543619},
  abstract = {In this paper, the imitation of human captured motions by a humanoid robot is considered. The main objective is to reproduce an imitated motion which should be as close as possible to the original human captured motion. To achieve this goal, the imitation problem is formulated as an optimization problem and the physical limits of the humanoid robot are considered as constraints. The optimization problem is then solved recursively by using an efficient dynamics algorithm, which allows the calculation of the gradient function with respect to the control parameters analytically. The simulation results using OpenHRP platform, which is a dynamical simulator for humanoid robot motions, have pointed out that the imitated motions preserve the salient characteristics of the original human captured motion. Moreover the optimization procedure converges well thanks to the analytical calculation of the gradient function.},
  keywords = {dynamics algorithm,gradient function,gradient methods,human captured motion,human motion imitation,humanoid robot,humanoid robots,Humanoid robots,Humans,Intelligent robots,Intelligent systems,Joints,motion control,optimisation,optimization problem,Robot control,Robotics and automation,Service robots,Stability,Torque}
}

@article{summersMethodologyLISPProgram1977,
  title = {A {{Methodology}} for {{LISP Program Construction}} from {{Examples}}},
  author = {Summers, Phillip D.},
  year = {1977},
  month = jan,
  journal = {Journal of the ACM},
  volume = {24},
  number = {1},
  pages = {161--175},
  issn = {0004-5411, 1557-735X},
  doi = {10.1145/321992.322002},
  urldate = {2024-04-18},
  abstract = {An automatic programming system, THESYS, for constructing recursive LISP programs from examples of what they do is described. The construction methodology is illustrated as a series of transformations from the set of examples to a program satisfying the examples. The transformations consist of (1) deriving the specific computation associated with a specific example, (2) deriving control flow predicates, and (3) deriving an equivalent program specification in the form of recurrence relations. Equivalence between certain recurrence relations and various program schemata is proved. A detailed description of the construction of four programs is presented to illustrate the application of the methodology.},
  langid = {english}
}

@article{sunApproximationMethodStiffness2018,
  title = {An Approximation Method for Stiffness Calculation of Robotic Arms with Hybrid Open- and Closed-Loop Kinematic Chains},
  author = {Sun, Longfei and Fang, Lijin},
  year = {2018},
  month = feb,
  journal = {Advances in Mechanical Engineering},
  volume = {10},
  number = {2},
  pages = {1687814018761297},
  publisher = {SAGE Publications},
  issn = {1687-8140},
  doi = {10.1177/1687814018761297},
  urldate = {2021-02-06},
  abstract = {Industrial robots have advantages of large workspace, compact structure, and good flexibility, but the stiffness of the robot is relatively weak due to the compliance of reducers and its series structure. In this article, a five-degree-of-freedom robot with non-backlash driving is presented. A parallelogram structure with diagonal driven is used for robotic arms which is useful to improve the overall stiffness of the robot. First, the detailed structure of the robot is introduced, and the kinematic characteristics of the robot are analyzed. Second, a stiffness approximation method is proposed to evaluate the stiffness of the robot in the global workspace. The overall deformations under certain external loads which are composed of deflection deformations and stretching deformations are calculated based on the strain energy method and the properties of the components. The effectiveness of the approximation method used for evaluating the stiffness of the robot which has hybrid open- and closed-loop kinematic chains is verified through the finite element analysis results and the experimental results. Finally, the stiffness evaluation results show that the stiffness of the proposed robot is better than that of the industrial robot, which makes it more suitable for most of the industrial applications, such as handling, palletizing, and drilling.},
  langid = {english},
  keywords = {anti-backlash,industrial applications,parallelogram,Robot,stiffness}
}

@incollection{sunCLARIONCognitiveArchitecture2017,
  title = {The {{CLARION}} Cognitive Architecture: {{Toward}} a Comprehensive Theory of the Mind},
  shorttitle = {The {{CLARION}} Cognitive Architecture},
  booktitle = {The {{Oxford}} Handbook of Cognitive Science},
  author = {Sun, Ron},
  year = {2017},
  pages = {117--133},
  publisher = {Oxford University Press},
  address = {New York, NY, US},
  abstract = {This chapter presents a hybrid cognitive architecture CLARION, which is significantly different from most existing cognitive architectures in several important respects. CLARION is hybrid in that it (a) combines connectionist and symbolic representations, (b) combines implicit and explicit psychological processes, and (c) combines cognition (in the narrow sense) and other psychological processes. Overall, CLARION is a modularly structured cognitive architecture consisting of a number of functional subsystems. It also has a dual representational structure, with both implicit and explicit representations. CLARION has been successful in capturing a variety of psychological processes in a variety of task domains based on Its structuring of functional modules. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  isbn = {978-0-19-984219-3 978-0-19-984417-3},
  keywords = {Cognitive Processes,Cognitive Psychology,Metacognition,Motivation,Simulation,Task,Theory of Mind},
  file = {C:\Users\benja\Zotero\storage\26Q5S9CG\2016-61176-006.html}
}

@book{sunConnectionistsymbolicIntegrationUnified1997,
  title = {Connectionist-Symbolic Integration: {{From}} Unified to Hybrid Approaches},
  shorttitle = {Connectionist-Symbolic Integration},
  editor = {Sun, Ron and Alexandre, Frederic},
  year = {1997},
  series = {Connectionist-Symbolic Integration: {{From}} Unified to Hybrid Approaches},
  pages = {xii, 378},
  publisher = {Lawrence Erlbaum Associates Publishers},
  address = {Mahwah, NJ, US},
  abstract = {This book tries to bring to light many new ideas, controversies, and syntheses in this broad area. The focus is on learning and architectures that feature hybrid representations and support hybrid learning. With this book, we hope to provide an information clearinghouse for various proposed approaches and models that share the common belief that connectionist and symbolic models can be usefully combined and integrated, and such integration may lead to significant advances in our understanding of intelligence. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  isbn = {978-0-8058-2348-6 978-0-8058-2349-3},
  keywords = {Artificial Intelligence,Cognitive Psychology,Connectionism,Learning,Models},
  file = {C:\Users\benja\Zotero\storage\NGTZVT5V\1997-36530-000.html}
}

@article{sunderhaufLimitsPotentialsDeep2018,
  title = {The Limits and Potentials of Deep Learning for Robotics},
  author = {S{\"u}nderhauf, Niko and Brock, Oliver and Scheirer, Walter and Hadsell, Raia and Fox, Dieter and Leitner, J{\"u}rgen and Upcroft, Ben and Abbeel, Pieter and Burgard, Wolfram and Milford, Michael and Corke, Peter},
  year = {2018},
  month = apr,
  journal = {The International Journal of Robotics Research},
  volume = {37},
  number = {4-5},
  pages = {405--420},
  publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  doi = {10.1177/0278364918770733},
  urldate = {2022-04-30},
  abstract = {The application of deep learning in robotics leads to very specific problems and research questions that are typically not addressed by the computer vision and machine learning communities. In this paper we discuss a number of robotics-specific learning, reasoning, and embodiment challenges for deep learning. We explain the need for better evaluation metrics, highlight the importance and unique challenges for deep robotic learning in simulation, and explore the spectrum between purely data-driven and model-driven approaches. We hope this paper provides a motivating overview of important research directions to overcome the current limitations, and helps to fulfill the promising potentials of deep learning in robotics.},
  langid = {english},
  keywords = {deep learning,machine learning,robotic vision,Robotics}
}

@article{sundermeyerContactGraspNetEfficient6DoF2021,
  title = {Contact-{{GraspNet}}: {{Efficient}} 6-{{DoF Grasp Generation}} in {{Cluttered Scenes}}},
  shorttitle = {Contact-{{GraspNet}}},
  author = {Sundermeyer, Martin and Mousavian, Arsalan and Triebel, Rudolph and Fox, Dieter},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.14127 [cs]},
  eprint = {2103.14127},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {Grasping unseen objects in unconstrained, cluttered environments is an essential skill for autonomous robotic manipulation. Despite recent progress in full 6-DoF grasp learning, existing approaches often consist of complex sequential pipelines that possess several potential failure points and run-times unsuitable for closed-loop grasping. Therefore, we propose an end-to-end network that efficiently generates a distribution of 6-DoF parallel-jaw grasps directly from a depth recording of a scene. Our novel grasp representation treats 3D points of the recorded point cloud as potential grasp contacts. By rooting the full 6-DoF grasp pose and width in the observed point cloud, we can reduce the dimensionality of our grasp representation to 4-DoF which greatly facilitates the learning process. Our class-agnostic approach is trained on 17 million simulated grasps and generalizes well to real world sensor data. In a robotic grasping study of unseen objects in structured clutter we achieve over 90\% success rate, cutting the failure rate in half compared to a recent state-of-the-art method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\S74FFLYS\2103.html}
}

@inproceedings{sunHowFineTuneBERT2019,
  title = {How to {{Fine-Tune BERT}} for {{Text Classification}}?},
  booktitle = {Chinese {{Computational Linguistics}}},
  author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  editor = {Sun, Maosong and Huang, Xuanjing and Ji, Heng and Liu, Zhiyuan and Liu, Yang},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {194--206},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-32381-3_16},
  abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.},
  isbn = {978-3-030-32381-3},
  langid = {english},
  keywords = {BERT,Text classification,Transfer learning}
}

@article{sunHumanActionRecognition2022,
  title = {Human {{Action Recognition}} from {{Various Data Modalities}}: {{A Review}}},
  shorttitle = {Human {{Action Recognition}} from {{Various Data Modalities}}},
  author = {Sun, Zehua and Ke, Qiuhong and Rahmani, Hossein and Bennamoun, Mohammed and Wang, Gang and Liu, Jun},
  year = {2022},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  eprint = {2012.11866},
  primaryclass = {cs},
  pages = {1--20},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2022.3183112},
  urldate = {2022-06-24},
  abstract = {Human Action Recognition (HAR) aims to understand human behavior and assign a label to each action. It has a wide range of applications, and therefore has been attracting increasing attention in the field of computer vision. Human actions can be represented using various data modalities, such as RGB, skeleton, depth, infrared, point cloud, event stream, audio, acceleration, radar, and WiFi signal, which encode different sources of useful yet distinct information and have various advantages depending on the application scenarios. Consequently, lots of existing works have attempted to investigate different types of approaches for HAR using various modalities. In this paper, we present a comprehensive survey of recent progress in deep learning methods for HAR based on the type of input data modality. Specifically, we review the current mainstream deep learning methods for single data modalities and multiple data modalities, including the fusion-based and the co-learning-based frameworks. We also present comparative results on several benchmark datasets for HAR, together with insightful observations and inspiring future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\benja\Zotero\storage\DMJQVTHA\2012.html}
}

@article{sunLearningDynamicsTimeRecursive2008,
  title = {Learning the {{Dynamics}} and {{Time-Recursive Boundary Detection}} of {{Deformable Objects}}},
  author = {Sun, Walter and Cetin, {\relax M{\"u}}jdat and Chan, Raymond and Willsky, Alan S.},
  year = {2008},
  month = nov,
  journal = {IEEE Transactions on Image Processing},
  volume = {17},
  number = {11},
  pages = {2186--2200},
  issn = {1941-0042},
  doi = {10.1109/TIP.2008.2004638},
  abstract = {We propose a principled framework for recursively segmenting deformable objects across a sequence of frames. We demonstrate the usefulness of this method on left ventricular segmentation across a cardiac cycle. The approach involves a technique for learning the system dynamics together with methods of particle-based smoothing as well as nonparametric belief propagation on a loopy graphical model capturing the temporal periodicity of the heart. The dynamic system state is a low-dimensional representation of the boundary, and the boundary estimation involves incorporating curve evolution into recursive state estimation. By formulating the problem as one of state estimation, the segmentation at each particular time is based not only on the data observed at that instant, but also on predictions based on past and future boundary estimates. Although this paper focuses on left ventricle segmentation, the method generalizes to temporally segmenting any deformable object.},
  keywords = {Blood,Cardiac imaging,curve evolution,graphical models,Graphical models,Heart,Humans,image segmentation,Image segmentation,learning,left ventricle (LV),level sets,magnetic resonance imaging,Object detection,particle filtering,recursive estimation,Recursive estimation,smoothing,Smoothing methods,State estimation,Sun},
  file = {C:\Users\benja\Zotero\storage\EFINQCD9\4648486.html}
}

@inproceedings{sunMetaTransferLearningFewShot2019,
  title = {Meta-{{Transfer Learning}} for {{Few-Shot Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Sun, Qianru and Liu, Yaoyao and Chua, Tat-Seng and Schiele, Bernt},
  year = {2019},
  pages = {403--412},
  urldate = {2024-06-13}
}

@inproceedings{sunNeuralProgramSynthesis2018,
  title = {Neural {{Program Synthesis}} from {{Diverse Demonstration Videos}}},
  booktitle = {{{ICML}}},
  author = {Sun, Shao-Hua and Noh, Hyeonwoo and Somasundaram, Sriram and Lim, Joseph},
  year = {2018},
  month = jul,
  pages = {4790--4799},
  urldate = {2019-08-02},
  abstract = {Interpreting decision making logic in demonstration videos is key to collaborating with and mimicking humans. To empower machines with this ability, we propose a neural program synthesizer that is ...},
  langid = {english}
}

@article{sunUnsupervisedDomainAdaptation2019,
  title = {Unsupervised {{Domain Adaptation}} through {{Self-Supervision}}},
  author = {Sun, Yu and Tzeng, Eric and Darrell, Trevor and Efros, Alexei A.},
  year = {2019},
  month = sep,
  journal = {arXiv:1909.11825 [cs, stat]},
  eprint = {1909.11825},
  primaryclass = {cs, stat},
  urldate = {2019-12-09},
  abstract = {This paper addresses unsupervised domain adaptation, the setting where labeled training data is available on a source domain, but the goal is to have good performance on a target domain with only unlabeled data. Like much of previous work, we seek to align the learned representations of the source and target domains while preserving discriminability. The way we accomplish alignment is by learning to perform auxiliary self-supervised task(s) on both domains simultaneously. Each self-supervised task brings the two domains closer together along the direction relevant to that task. Training this jointly with the main task classifier on the source domain is shown to successfully generalize to the unlabeled target domain. The presented objective is straightforward to implement and easy to optimize. We achieve state-of-the-art results on four out of seven standard benchmarks, and competitive results on segmentation adaptation. We also demonstrate that our method composes well with another popular pixel-level adaptation method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{susskindNeuroSymbolicAIEmerging2021,
  title = {Neuro-{{Symbolic AI}}: {{An Emerging Class}} of {{AI Workloads}} and Their {{Characterization}}},
  shorttitle = {Neuro-{{Symbolic AI}}},
  author = {Susskind, Zachary and Arden, Bryce and John, Lizy K. and Stockton, Patrick and John, Eugene B.},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.06133 [cs]},
  eprint = {2109.06133},
  primaryclass = {cs},
  urldate = {2022-03-18},
  abstract = {Neuro-symbolic artificial intelligence is a novel area of AI research which seeks to combine traditional rules-based AI approaches with modern deep learning techniques. Neuro-symbolic models have already demonstrated the capability to outperform state-of-the-art deep learning models in domains such as image and video reasoning. They have also been shown to obtain high accuracy with significantly less training data than traditional models. Due to the recency of the field's emergence and relative sparsity of published results, the performance characteristics of these models are not well understood. In this paper, we describe and analyze the performance characteristics of three recent neuro-symbolic models. We find that symbolic models have less potential parallelism than traditional neural models due to complex control flow and low-operational-intensity operations, such as scalar multiplication and tensor addition. However, the neural aspect of computation dominates the symbolic part in cases where they are clearly separable. We also find that data movement poses a potential bottleneck, as it does in many ML workloads.},
  archiveprefix = {arXiv},
  keywords = {C.4,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Performance,I.2.m},
  file = {C:\Users\benja\Zotero\storage\EZRHM588\2109.html}
}

@inproceedings{sussmanWhyProgrammingGood2005,
  title = {Why Programming Is a Good Medium for Expressing Poorly Understood and Sloppily Formulated Ideas},
  booktitle = {Companion to the 20th Annual {{ACM SIGPLAN}} Conference on {{Object-oriented}} Programming, Systems, Languages, and Applications},
  author = {Sussman, Gerald Jay},
  year = {2005},
  month = oct,
  series = {{{OOPSLA}} '05},
  pages = {6},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1094855.1094860},
  urldate = {2024-03-07},
  abstract = {I have stolen my title from the title of a paper given by Marvin Minsky in the 1960s, because it most effectively expresses what I will try to convey in this talk.We have been programming universal computers for about 50 years. Programming provides us with new tools to express ourselves. We now have intellectual tools to describe "how to" as well as "what is." This is a profound transformation: it is a revolution in the way we think and in the way we express what we think.For example, one often hears a student or teacher complain that the student knows the "theory" of some subject but cannot effectively solve problems. We should not be surprised: the student has no formal way to learn technique. We expect the student to learn to solve problems by an inefficient process: the student watches the teacher solve a few problems, hoping to abstract the general procedures from the teacher's behavior on particular examples. The student is never given any instructions on how to abstract from examples, nor is the student given any language for expressing what has been learned. It is hard to learn what one cannot express. But now we can express it!Expressing methodology in a computer language forces it to be unambiguous and computationally effective. The task of formulating a method as a computer-executable program and debugging that program is a powerful exercise in the learning process. The programmer expresses his/her poorly understood or sloppily formulated idea in a precise way, so that it becomes clear what is poorly understood or sloppily formulated. Also, once formalized procedurally, a mathematical idea becomes a tool that can be used directly to compute results.I will defend this viewpoint with examples and demonstrations from electrical engineering and from classical mechanics.},
  isbn = {978-1-59593-193-1}
}

@misc{suSystematicReviewTransformerbased2023,
  title = {A {{Systematic Review}} for {{Transformer-based Long-term Series Forecasting}}},
  author = {Su, Liyilei and Zuo, Xumin and Li, Rui and Wang, Xin and Zhao, Heng and Huang, Bingding},
  year = {2023},
  month = oct,
  number = {arXiv:2310.20218},
  eprint = {2310.20218},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.20218},
  urldate = {2024-04-29},
  abstract = {The emergence of deep learning has yielded noteworthy advancements in time series forecasting (TSF). Transformer architectures, in particular, have witnessed broad utilization and adoption in TSF tasks. Transformers have proven to be the most successful solution to extract the semantic correlations among the elements within a long sequence. Various variants have enabled transformer architecture to effectively handle long-term time series forecasting (LTSF) tasks. In this article, we first present a comprehensive overview of transformer architectures and their subsequent enhancements developed to address various LTSF tasks. Then, we summarize the publicly available LTSF datasets and relevant evaluation metrics. Furthermore, we provide valuable insights into the best practices and techniques for effectively training transformers in the context of time-series analysis. Lastly, we propose potential research directions in this rapidly evolving field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\S283DIK6\2310.html}
}

@inproceedings{sutantoEncodingPhysicalConstraints2020,
  title = {Encoding {{Physical Constraints}} in {{Differentiable Newton-Euler Algorithm}}},
  booktitle = {Proceedings of the 2nd {{Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {Sutanto, Giovanni and Wang, Austin and Lin, Yixin and Mukadam, Mustafa and Sukhatme, Gaurav and Rai, Akshara and Meier, Franziska},
  year = {2020},
  month = jul,
  pages = {804--813},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-10-20},
  abstract = {The recursive Newton-Euler Algorithm (RNEA) is a popular technique in robotics for computing the dynamics of robots. The computed dynamics can then be used for torque control with inverse dynamics, or for forward dynamics computations. RNEA can be framed as a differentiable computational graph, enabling the dynamics parameters of the robot to be learned from data. However, the dynamics parameters learned in this manner can be physically implausible. In this work, we incorporate physical constraints in the learning by adding structure to the learned parameters. This results in a framework that can learn physically plausible dynamics, improving the training speed as well as generalization of the learned dynamics models. We evaluate our method on real-time inverse dynamics predictions of a 7 degree of freedom robot arm, both in simulation and on the real robot. Our experiments study a spectrum of structure added to learned dynamics, and compare their performance and generalization.},
  langid = {english}
}

@misc{suttonBitterLesson2019,
  title = {The {{Bitter Lesson}}},
  author = {Sutton, Richard S.},
  year = {2019},
  month = mar,
  journal = {Incomplete Ideas},
  urldate = {2024-01-24},
  file = {C:\Users\benja\Zotero\storage\GX9SUY5E\BitterLesson.html}
}

@article{suttonMDPsSemiMDPsFramework1999,
  title = {Between {{MDPs}} and Semi-{{MDPs}}: {{A}} Framework for Temporal Abstraction in Reinforcement Learning},
  shorttitle = {Between {{MDPs}} and Semi-{{MDPs}}},
  author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
  year = {1999},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {112},
  number = {1},
  pages = {181--211},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(99)00052-1},
  urldate = {2024-04-29},
  abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options---closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.},
  keywords = {Hierarchical planning,Intra-option learning,Macroactions,Macros,Markov decision processes,Options,Reinforcement learning,Semi-Markov decision processes,Subgoals,Temporal abstraction},
  file = {C:\Users\benja\Zotero\storage\2EU66BE7\S0004370299000521.html}
}

@article{swanMetaheuristicsLarge2022,
  title = {Metaheuristics ``{{In}} the {{Large}}''},
  author = {Swan, Jerry and Adriaensen, Steven and Brownlee, Alexander E. I. and Hammond, Kevin and Johnson, Colin G. and Kheiri, Ahmed and Krawiec, Faustyna and Merelo, J. J. and Minku, Leandro L. and {\"O}zcan, Ender and Pappa, Gisele L. and {Garc{\'i}a-S{\'a}nchez}, Pablo and S{\"o}rensen, Kenneth and Vo{\ss}, Stefan and Wagner, Markus and White, David R.},
  year = {2022},
  month = mar,
  journal = {European Journal of Operational Research},
  volume = {297},
  number = {2},
  pages = {393--406},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2021.05.042},
  urldate = {2024-09-07},
  abstract = {Following decades of sustained improvement, metaheuristics are one of the great success stories of optimization research. However, in order for research in metaheuristics to avoid fragmentation and a lack of reproducibility, there is a pressing need for stronger scientific and computational infrastructure to support the development, analysis and comparison of new approaches. To this end, we present the vision and progress of the Metaheuristics ``In the Large'' project. The conceptual underpinnings of the project are: truly extensible algorithm templates that support reuse without modification, white box problem descriptions that provide generic support for the injection of domain specific knowledge, and remotely accessible frameworks, components and problems that will enhance reproducibility and accelerate the field's progress. We argue that, via such principled choice of infrastructure support, the field can pursue a higher level of scientific enquiry. We describe our vision and report on progress, showing how the adoption of common protocols for all metaheuristics can help liberate the potential of the field, easing the exploration of the design space of metaheuristics.},
  keywords = {Architecture,Evolutionary Computation,Frameworks,Heuristic design,Heuristic methods,Interoperability,Operational Research},
  file = {C\:\\Users\\benja\\Zotero\\storage\\SGBEE9E3\\Swan et al. - 2022 - Metaheuristics In the Large.pdf;C\:\\Users\\benja\\Zotero\\storage\\8EBW8KCL\\S0377221721004707.html}
}

@article{swansonPatternsPatientsAdvances2023,
  title = {From Patterns to Patients: {{Advances}} in Clinical Machine Learning for Cancer Diagnosis, Prognosis, and Treatment},
  shorttitle = {From Patterns to Patients},
  author = {Swanson, Kyle and Wu, Eric and Zhang, Angela and Alizadeh, Ash A. and Zou, James},
  year = {2023},
  month = apr,
  journal = {Cell},
  volume = {186},
  number = {8},
  pages = {1772--1791},
  publisher = {Elsevier},
  issn = {0092-8674, 1097-4172},
  doi = {10.1016/j.cell.2023.01.035},
  urldate = {2024-03-01},
  langid = {english},
  pmid = {36905928}
}

@article{SzymonRusinkiewicz.2001,
  title = {Efficient Variants of the {{ICP}} Algorithm},
  author = {Rusinkiewicz, S. and Levoy, M.},
  year = {2001},
  journal = {Proc. 3DIM},
  pages = {145--152},
  doi = {10.1109/IM.2001.924423},
  pagination = {page}
}

@inproceedings{tabrezImprovingHumanRobotInteraction2019,
  title = {Improving {{Human-Robot Interaction Through Explainable Reinforcement Learning}}},
  booktitle = {2019 14th {{ACM}}/{{IEEE International Conference}} on {{Human-Robot Interaction}} ({{HRI}})},
  author = {Tabrez, Aaquib and Hayes, Bradley},
  year = {2019},
  month = mar,
  pages = {751--753},
  issn = {2167-2148},
  doi = {10.1109/HRI.2019.8673198},
  abstract = {Gathering the most informative data from humans without overloading them remains an active research area in AI, and is closely coupled with the problems of determining how and when information should be communicated to others [12]. Current decision support systems (DSS) are still overly simple and static, and cannot adapt to changing environments we expect to deploy in modern systems [3], [4], [9], [11]. They are intrinsically limited in their ability to explain rationale versus merely listing their future behaviors, limiting a human's understanding of the system [2], [7]. Most probabilistic assessments of a task are conveyed after the task/skill is attempted rather than before [10], [14], [16]. This limits failure recovery and danger avoidance mechanisms. Existing work on predicting failures relies on sensors to accurately detect explicitly annotated and learned failure modes [13]. As such, important non-obvious pieces of information for assessing appropriate trust and/or course-of-action (COA) evaluation in collaborative scenarios can go overlooked, while irrelevant information may instead be provided that increases clutter and mental workload. Understanding how AI models arrive at specific decisions is a key principle of trust [8]. Therefore, it is critically important to develop new strategies for anticipating, communicating, and explaining justifications and rationale for AI driven behaviors via contextually appropriate semantics.},
  keywords = {AI driven behaviors,AI models,annotated failure modes,Artificial intelligence,changing environments,COA evaluation,Collaboration,collaborative scenarios,communicating,course-of-action evaluation,danger avoidance mechanisms,decision support systems,DSS,explainable reinforcement learning,explaining justifications,failure recovery,future behaviors,Hidden Markov models,human-robot interaction,irrelevant information,learned failure modes,learning (artificial intelligence),Maintenance engineering,mental workload,modern systems,nonobvious pieces,Planning,probabilistic assessments,rationale versus,Robots,skill,specific decisions,task,Task analysis},
  file = {C:\Users\benja\Zotero\storage\IAHVLZCG\8673198.html}
}

@inproceedings{tanEfficientNetRethinkingModel2019,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Tan, Mingxing and Le, Quoc},
  year = {2019},
  month = may,
  pages = {6105--6114},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-02-27},
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flower (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.},
  langid = {english}
}

@article{Tang.2013,
  title = {A {{GPU-based}} Streaming Algorithm for High-Resolution Cloth Simulation},
  author = {Tang, Min and Tong, Ruofeng and Narain, Rahul and Meng, Chang and Manocha, Dinesh},
  year = {2013},
  journal = {Computer Graphics Forum},
  volume = {32},
  number = {7},
  pages = {21--30},
  issn = {01677055},
  doi = {10.1111/cgf.12208},
  pagination = {page}
}

@inproceedings{Tang.2016,
  title = {Robotic Manipulation of Deformable Objects by Tangent Space Mapping and Non-Rigid Registration},
  booktitle = {2016 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems ({{IROS}})},
  author = {Tang, Te and Liu, Changliu and Chen, Wenjie and Tomizuka, Masayoshi},
  year = {2016},
  pages = {2689--2696},
  publisher = {IEEE},
  doi = {10.1109/IROS.2016.7759418},
  bookpagination = {page},
  isbn = {978-1-5090-3762-9}
}

@article{Tang.2018,
  title = {A Framework for Manipulating Deformable Linear Objects by Coherent Point Drift},
  author = {Tang, Te and Wang, Changhao and Tomizuka, Masayoshi},
  year = {2018},
  journal = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {4},
  pages = {3426--3433},
  doi = {10.1109/LRA.2018.2852770},
  pagination = {page}
}

@article{taniokaAutonomicNervousActivity2020,
  title = {{Autonomic nervous activity of patient with schizophrenia during Pepper CPGE-led upper limb range of motion exercises}},
  author = {Tanioka, Ryuichi and Yasuhara, Yuko and Osaka, Kyoko and Kai, Yoshihiro and Zhao, Yueren and Tanioka, Tetsuya and Takase, Kensaku and Dino, Michael Joseph S. and Locsin, Rozzano C.},
  year = {2020},
  month = feb,
  journal = {Enfermeria Clinica},
  volume = {30 Suppl 1},
  pages = {48--53},
  issn = {1579-2013},
  doi = {10.1016/j.enfcli.2019.09.023},
  abstract = {Healthcare for the increasing senior population is a significant challenge. To address this problem, the use of healthcare robot is increasingly being recognized to have strong potential in addressing older adult and psychiatric patients' healthcare and welfare needs. The purpose of this preliminary study is to evaluate the changes in the autonomic nervous activity of an elderly patient with schizophrenia during upper limb range of motion (ROM) exercises led by Pepper (SoftBank Robotics) with the application program of Care Prevention Gymnastics Exercises (Pepper-CPGE) made by Xing Company in Japan. A Pepper-CPGE-led three-minute radio gymnastics program (Radio Exercises \#1, consisted of three types of exercises) and instructions were developed. The normal aging 69-year old schizophrenic subject followed instructions from Pepper throughout the intervention. Electrocardiography (ECG) records, heart rate, acceleration score, and ROM exercises of upper limb were collected and analyzed using the GSM's Bonaly Light instrument and Image-J analysis. (1) The high-frequency (HF) (indicative of parasympathetic nervous activity), and low-frequency (LF)/HF ratio (indicative of sympathetic nerve activity) were increased in reference from the baseline data before the exercise stretching the chest. (2) The momentum decreased as the ROM of shoulder joint flexion declined. Declines in both parasympathetic and sympathetic nervous activity were observed. Also, (3) when the exercise "bend the body forward" was performed, the HF and LF/HF also decreased. Evaluation of robot-based rehabilitation exercise program effectiveness by the ROM exercises image analysis and autonomic nervous activity is essential for the futurist programming of ROM exercise among patients with schizophrenia.},
  langid = {eng, spa},
  pmid = {32115166},
  keywords = {Autonomic nervous activity,Humanoid robot,Older adult patient with schizophrenia,Range of motion exercises}
}

@article{taniokaNursingRehabilitativeCare2019,
  title = {Nursing and {{Rehabilitative Care}} of the {{Elderly Using Humanoid Robots}}},
  author = {Tanioka, Tetsuya},
  year = {2019},
  journal = {The journal of medical investigation: JMI},
  volume = {66},
  number = {1.2},
  pages = {19--23},
  issn = {1349-6867},
  doi = {10.2152/jmi.66.19},
  abstract = {Japan's declining birth rate and increasing aging population prompted intercessory efforts towards robot technologies in nursing practice for theelderly. Today, technological companies are developing robots that meet universal health care technology demands. While human caring focus on human-to-human relationships,but between humans and nonhumans, e.g. Humanoid Nursing Robot (HNRs)-to-human relationships, caring practices have not been forthcoming. When HNRs can support patients independently, capabilities much like being human will be required, including intelligence and skill competencies. Currently, Tanioka's research group is conducting clinical trials of humanoid robots equipped with applications using Pepper (manufactured by SOFTBANK CORPORATION), towards elderly care and rehabilitation at the Mifune Hospital, Kagawa prefecture. Care Prevention Gymnastics Exercises (Pepper-CPGE) was madeby Xing Company, Japan. Therefore, this paper aims to describe the clinical trial outcomes based on the Transactive Relationship Theory of Nursing (TRETON) (Tanioka, 2017) emphasizing nursing engagement processes between HNRs and human persons. Observable effects include positive changes in relationships of patients, humanoid robots and healthcare providers. Emphasizing ethical concerns and human person safety as critical factors of care, and fears for divergent robot use are observed. Cooperative undertakings with various interdisciplinary activities mark the visioning of Japanese human caring ideas for an aging society. J. Med. Invest. 66 : 19-23, February, 2019.},
  langid = {english},
  pmid = {31064938},
  keywords = {Biomedical Technology,Clinical Trials as Topic,Empathy,Health Services for the Aged,Humanoid Nursing Robots (HNRs),Humans,Nursing,Nursing Care,Nursing Theory,Rehabilitative Care,Robotics,the Elderly,Transactive relationship},
  file = {C:\Users\benja\Zotero\storage\LLS2HLDE\Tanioka - 2019 - Nursing and Rehabilitative Care of the Elderly Usi.pdf}
}

@article{taniokaPotentialLegalIssues2018,
  title = {Potential {{Legal Issues}} and {{Care Implications}} during {{Care-Prevention Gymnastic Exercises}} for the {{Elderly Using Pepper}} in {{Long Term Health Care Facilities}}},
  author = {Tanioka, Ryuichi and Locsin, Rozzano and Yasuhara, Yuko and Tanioka, Tetsuya},
  year = {2018},
  month = aug,
  journal = {Intelligent Control and Automation},
  volume = {9},
  number = {3},
  pages = {85--93},
  publisher = {Scientific Research Publishing},
  doi = {10.4236/ica.2018.93007},
  urldate = {2021-01-30},
  abstract = {In Japan, the shortage of personnel is a problem in long-term care nursing and rehabilitative care prevention. Nevertheless, Japan has taken measures to compensate for these shortages by promoting medical and nursing care activities using robotic technologies, and employing human resources from overseas. The purpose of this study was to determine potential legal issues and subsequent implications for care during prevention gymnastic exercises for the elderly using Pepper in long-term health facilities. The application program of Care-Prevention Gymnastics Exercises for Pepper (Pepper with CPGE) was made by the Xing Company Japan. Currently, care workers become intermediaries for the safe use of Pepper with CPGE. However, it was realized that some legal issues may arise if Pepper with CPGE alone will carry out these preventive care programs for the elderly without the presence of care workers as intermediaries. In this situation, using Pepper with CPGE alone to conduct care prevention gymnastics will require safety measures to prevent these possible practice issues and anticipate implications for care. In this regard, determining detailed target levels of rehabilitation exercise demands and environmental setting safety become essential factors. The use of humanoid robots in healthcare is expected to influence more practice protocols in contemporary and futurist rehabilitative human care. The identification of possible safety issues in performance and environmental situations, and implications for care are critical to ensure safe and valuable rehabilitative health care practices for the elderly population.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\ILJJALWQ\Tanioka et al. - 2018 - Potential Legal Issues and Care Implications durin.pdf}
}

@inproceedings{tanSurveyDeepTransfer2018,
  title = {A {{Survey}} on {{Deep Transfer Learning}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} -- {{ICANN}} 2018},
  author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
  editor = {K{\r u}rkov{\'a}, V{\v e}ra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {270--279},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-01424-7_27},
  abstract = {As a new classification platform, deep learning has recently received increasing attention from researchers and has been successfully applied to many domains. In some domains, like bioinformatics and robotics, it is very difficult to construct a large-scale well-annotated dataset due to the expense of data acquisition and costly annotation, which limits its development. Transfer learning relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data. This survey focuses on reviewing the current researches of transfer learning by using deep neural network and its applications. We defined deep transfer learning, category and review the recent research works based on the techniques used in deep transfer learning.},
  isbn = {978-3-030-01424-7},
  langid = {english},
  keywords = {Deep transfer learning,Survey,Transfer learning}
}

@misc{taoriAlpacaStrongReplicable2023,
  title = {Alpaca: {{A Strong}}, {{Replicable Instruction-Following Model}}},
  author = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.},
  year = {2023},
  month = mar,
  journal = {Stanford CRFM},
  urldate = {2023-10-11},
  file = {C:\Users\benja\Zotero\storage\7SN5ZUU8\alpaca.html}
}

@inproceedings{tateGeneratingProjectNetworks1977,
  title = {Generating {{Project Networks}}},
  booktitle = {Proceedings of the 5th {{International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Tate, Austin},
  year = {1977},
  volume = {2},
  pages = {888--893},
  publisher = {Morgan Kaufmann},
  address = {Cambridge, USA},
  urldate = {2019-07-06},
  abstract = {Procedures for optimization and resource allocation in Operations Research first require a project network for the task to be specified. The specification of a project network is at present done in an intuitive way. AI work in plan formation has developed formalisms for specifying primitive activities, and recent work by Sacerdoti (1975a) has developed a planner able to generate a plan as a partially ordered network of actions. The "planning: a joint AI/OR approach" project at Edinburgh has extended such work and provided a hierarchic planner which can aid in the generation of project networks. This paper describes the planner (NONLIN) and the Task Formalism (TF) used to hierarchically specify a domain.}
}

@book{Taylor.,
  title = {Procedings of the Alvey Vision Conference 1988},
  editor = {Taylor, C. J.},
  year = {1988},
  publisher = {Alvey Vision Club},
  doi = {10.5244/C.2}
}

@incollection{Taylor.2016,
  title = {Medical Robotics and Computer-Integrated Surgery},
  booktitle = {Springer Handbook of Robotics},
  author = {Taylor, Russell H. and Menciassi, Arianna and Fichtinger, Gabor and Fiorini, Paolo and Dario, Paolo},
  editor = {Siciliano, Bruno and Khatib, Oussama},
  year = {2016},
  volume = {44},
  pages = {1657--1684},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-32552-1_63},
  bookpagination = {page},
  isbn = {978-3-319-32550-7}
}

@article{taylorAMLManufacturingLanguage1982,
  title = {{{AML}}: {{A Manufacturing Language}}},
  shorttitle = {{{AML}}},
  author = {Taylor, R.H. and Summers, P.D. and Meyer, J.M.},
  year = {1982},
  month = sep,
  journal = {The International Journal of Robotics Research},
  volume = {1},
  number = {3},
  pages = {19--41},
  publisher = {SAGE Publications Ltd STM},
  issn = {0278-3649},
  doi = {10.1177/027836498200100302},
  urldate = {2024-04-15},
  abstract = {AML, A Maufacturing Language, was designed to be a well-structured, semantically powerful interactive language for robot programming. In this paper, we identify the de sign objectives for such a language and give a technical description of AML. Important features are described and illustrated through representative examples of robot appli cations programming.},
  langid = {english}
}

@inproceedings{tchapmi2017segcloud,
  title = {Segcloud: {{Semantic}} Segmentation of 3d Point Clouds},
  booktitle = {2017 International Conference on {{3D}} Vision ({{3DV}})},
  author = {Tchapmi, Lyne and Choy, Christopher and Armeni, Iro and Gwak, JunYoung and Savarese, Silvio},
  year = {2017},
  pages = {537--547},
  organization = {IEEE}
}

@misc{TecnomatixSiemensSoftware,
  title = {Tecnomatix {\textbar} {{Siemens Software}}},
  journal = {Siemens Digital Industries Software},
  urldate = {2021-09-16},
  abstract = {Learn how Tecnomatix, Siemens plant simulation software, can help you transform innovative ideas and raw materials into real products.},
  howpublished = {https://www.plm.automation.siemens.com/global/en/products/tecnomatix/},
  file = {C:\Users\benja\Zotero\storage\V2K5VCYM\tecnomatix.html}
}

@article{Tedeschi2014,
  title = {Design {{Issues}} for {{Hexapod Walking Robots}}},
  author = {Tedeschi, Franco and Carbone, Giuseppe},
  year = {2014},
  journal = {Robotics},
  volume = {3},
  number = {2},
  pages = {181--206},
  issn = {2218-6581},
  doi = {10.3390/robotics3020181}
}

@inproceedings{teichmannMultiNetRealtimeJoint2018,
  title = {{{MultiNet}}: {{Real-time Joint Semantic Reasoning}} for {{Autonomous Driving}}},
  shorttitle = {{{MultiNet}}},
  booktitle = {2018 {{IEEE Intelligent Vehicles Symposium}} ({{IV}})},
  author = {Teichmann, M. and Weber, M. and Z{\"o}llner, M. and Cipolla, R. and Urtasun, R.},
  year = {2018},
  month = jun,
  pages = {1013--1020},
  doi = {10.1109/IVS.2018.8500504},
  abstract = {While most approaches to semantic reasoning have focused on improving performance, in this paper we argue that computational times are very important in order to enable real time applications such as autonomous driving. Towards this goal, we present an approach to joint classification, detection and semantic segmentation using a unified architecture where the encoder is shared amongst the three tasks. Our approach is very simple, can be trained end-to-end and performs extremely well in the challenging KITTI dataset. Our approach is also very efficient, allowing us to perform inference at more then 23 frames per second. Training scripts and trained weights to reproduce our results can be found here: https://github.com/MarvinTeichmann/MultiNet.},
  keywords = {autonomous driving,Computer architecture,Decoding,driver information systems,Feature extraction,image classification,image segmentation,inference mechanisms,joint classification,KITTI dataset,Microprocessors,MultiNet,object detection,Proposals,real-time joint semantic reasoning,semantic segmentation,Semantics,Task analysis,unified architecture}
}

@article{tellexApproachingSymbolGrounding2011,
  title = {Approaching the {{Symbol Grounding Problem}} with {{Probabilistic Graphical Models}}},
  author = {Tellex, Stefanie A. and Kollar, Thomas Fleming and Dickerson, Steven R. and Walter, Matthew R. and Banerjee, Ashis and Teller, Seth and Roy, Nicholas},
  year = {2011},
  journal = {AI Magazine},
  volume = {32},
  number = {4},
  pages = {64--76},
  publisher = {Association for the Advancement of Artificial Intelligence},
  issn = {0738-4602},
  doi = {10.1609/aimag.v32i4.2384},
  urldate = {2024-08-16},
  abstract = {In order for robots to engage in dialog with human teammates, they must have the ability to map between words in the language and aspects of the external world. A solution to this symbol grounding problem (Harnad, 1990) would enable a robot to interpret commands such as ``Drive over to receiving and pick up the tire pallet.'' In this article we describe several of our results that use probabilistic inference to address the symbol grounding problem. Our specific approach is to develop models that factor according to the linguistic structure of a command. We first describe an early result, a generative model that factors according to the sequential structure of language, and then discuss our new framework, generalized grounding graphs (G3). The G3 framework dynamically instantiates a probabilistic graphical model for a natural language input, enabling a mapping between words in language and concrete objects, places, paths and events in the external world. We report on corpus-based experiments where the robot is able to learn and use word meanings in three real-world tasks: indoor navigation, spatial language video retrieval, and mobile manipulation.},
  copyright = {Creative Commons Attribution-Noncommercial-Share Alike 3.0},
  langid = {american},
  annotation = {Accepted: 2012-10-02T14:51:12Z}
}

@article{tellexRobotsThatUse2020,
  title = {Robots {{That Use Language}}},
  author = {Tellex, Stefanie and Gopalan, Nakul and {Kress-Gazit}, Hadas and Matuszek, Cynthia},
  year = {2020},
  month = may,
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  volume = {3},
  number = {1},
  pages = {25--55},
  issn = {2573-5144, 2573-5144},
  doi = {10.1146/annurev-control-101119-071628},
  urldate = {2024-04-15},
  abstract = {This article surveys the use of natural language in robotics from a robotics point of view. To use human language, robots must map words to aspects of the physical world, mediated by the robot's sensors and actuators. This problem differs from other natural language processing domains due to the need to ground the language to noisy percepts and physical actions. Here, we describe central aspects of language use by robots, including understanding natural language requests, using language to drive learning about the physical world, and engaging in collaborative dialogue with a human partner. We describe common approaches, roughly divided into learning methods, logic-based methods, and methods that focus on questions of human--robot interaction. Finally, we describe several application domains for language-using robots.},
  langid = {english}
}

@article{tenorthKnowledgebasedSpecificationRobot,
  title = {Knowledge-Based {{Specification}} of {{Robot Motions}}},
  author = {Tenorth, Moritz and Bartels, Georg and Beetz, Michael},
  pages = {6},
  abstract = {In many cases, the success of a manipulation action performed by a robot is determined by how it is executed and by how the robot moves during the action. Examples are tasks such as unscrewing a bolt, pouring liquids and flipping a pancake. This aspect is often abstracted away in AI planning and action languages that assume that an action is successful as long as all preconditions are fulfilled. In this paper we investigate how constraint-based motion representations used in robot control can be combined with a semantic knowledge base in order to let a robot reason about movements and to automatically generate executable motion descriptions that can be adapted to different robots, objects and tools.},
  langid = {english}
}

@inproceedings{tenorthKnowledgebasedSpecificationRobot2014,
  title = {Knowledge-Based {{Specification}} of {{Robot Motions}}},
  booktitle = {{{ECAI}}},
  author = {Tenorth, M. and Bartels, Georg and Beetz, M.},
  year = {2014},
  doi = {10.3233/978-1-61499-419-0-873},
  abstract = {This paper investigates how constraint-based motion representations used in robot control can be combined with a semantic knowledge base in order to let a robot reason about movements and to automatically generate executable motion descriptions that can be adapted to different robots, objects and tools. In many cases, the success of a manipulation action performed by a robot is determined by how it is executed and by how the robot moves during the action. Examples are tasks such as unscrewing a bolt, pouring liquids and flipping a pancake. This aspect is often abstracted away in AI planning and action languages that assume that an action is successful as long as all preconditions are fulfilled. In this paper we investigate how constraint-based motion representations used in robot control can be combined with a semantic knowledge base in order to let a robot reason about movements and to automatically generate executable motion descriptions that can be adapted to different robots, objects and tools.}
}

@article{tenorthKnowRobKnowledgeProcessing2013,
  title = {{{KnowRob}}: {{A}} Knowledge Processing Infrastructure for Cognition-Enabled Robots},
  shorttitle = {{{KnowRob}}},
  author = {Tenorth, Moritz and Beetz, Michael},
  year = {2013},
  month = apr,
  journal = {The International Journal of Robotics Research},
  volume = {32},
  number = {5},
  pages = {566--590},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364913481635},
  urldate = {2019-08-05},
  abstract = {Autonomous service robots will have to understand vaguely described tasks, such as ``set the table'' or ``clean up''. Performing such tasks as intended requires robots to fully, precisely, and appropriately parameterize their low-level control programs. We propose knowledge processing as a computational resource for enabling robots to bridge the gap between vague task descriptions and the detailed information needed to actually perform those tasks in the intended way. In this article, we introduce the KNOWROB knowledge processing system that is specifically designed to provide autonomous robots with the knowledge needed for performing everyday manipulation tasks. The system allows the realization of ``virtual knowledge bases'': collections of knowledge pieces that are not explicitly represented but computed on demand from the robot's internal data structures, its perception system, or external sources of information. This article gives an overview of the different kinds of knowledge, the different inference mechanisms, and interfaces for acquiring knowledge from external sources, such as the robot's perception system, observations of human activities, Web sites on the Internet, as well as Web-based knowledge bases for information exchange between robots. We evaluate the system's scalability and present different integrated experiments that show its versatility and comprehensiveness.},
  langid = {english}
}

@article{tenorthRepresentationsRobotKnowledge2017,
  title = {Representations for Robot Knowledge in the {{KnowRob}} Framework},
  author = {Tenorth, Moritz and Beetz, Michael},
  year = {2017},
  month = jun,
  journal = {Artificial Intelligence},
  series = {Special {{Issue}} on {{AI}} and {{Robotics}}},
  volume = {247},
  pages = {151--169},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2015.05.010},
  urldate = {2024-07-07},
  abstract = {In order to robustly perform tasks based on abstract instructions, robots need sophisticated knowledge processing methods. These methods have to supply the difference between the (often shallow and symbolic) information in the instructions and the (detailed, grounded and often real-valued) information needed for execution. For filling these information gaps, a robot first has to identify them in the instructions, reason about suitable information sources, and combine pieces of information from different sources and of different structure into a coherent knowledge base. To this end we propose the KnowRob knowledge processing system for robots. In this article, we discuss why the requirements of a robot knowledge processing system differ from what is commonly investigated in AI research, and propose to re-consider a KR system as a semantically annotated view on information and algorithms that are often already available as part of the robot's control system. We then introduce representational structures and a common vocabulary for representing knowledge about robot actions, events, objects, environments, and the robot's hardware as well as inference procedures that operate on this common representation. The KnowRob system has been released as open-source software and is being used on several robots performing complex object manipulation tasks. We evaluate it through prototypical queries that demonstrate the expressive power and its impact on the robot's performance.},
  keywords = {Autonomous robots,Knowledge representation,Knowledge-enabled robotics},
  file = {C:\Users\benja\Zotero\storage\S22HXSXD\S0004370215000843.html}
}

@misc{tensorflow2015-whitepaper,
  title = {{{TensorFlow}}: {{Large-scale}} Machine Learning on Heterogeneous Systems},
  author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'e}, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015}
}

@article{terwieschLearningProcessImprovement2001,
  title = {Learning and Process Improvement during Production Ramp-Up},
  author = {Terwiesch, Christian and E. Bohn, Roger},
  year = {2001},
  month = mar,
  journal = {International Journal of Production Economics},
  volume = {70},
  number = {1},
  pages = {1--19},
  issn = {0925-5273},
  doi = {10.1016/S0925-5273(00)00045-1},
  urldate = {2024-02-21},
  abstract = {Rapid product lifecycles and high development costs pressure manufacturing firms to cut not only their development times (time-to-market), but also the time to reach full capacity utilization (time-to-volume). The period between completion of development and full capacity utilization is known as production ramp-up. During that time, the new production process is ill understood, which causes low yields and low production rates. This paper analyzes the interactions among capacity utilization, yields, and process improvement (learning). We model learning in the form of deliberate experiments, which reduce capacity in the short run. This creates a trade-off between experiments and production. High selling prices during ramp-up raise the opportunity cost of experiments, yet early learning is more valuable than later learning. We formalize the resulting intertemporal trade-off between the short-term opportunity cost of capacity and the long term value of learning as a dynamic program. The paper also examines the tradeoff between production speed and yield/quality, where faster production rates lead to more defects. Finally, we show what happens if managers misunderstand the sources of learning.},
  keywords = {Experimentation,Learning curve,Ramp-up,Start-up,Yield},
  file = {C\:\\Users\\benja\\Zotero\\storage\\ESHCMEZ4\\Terwiesch and E. Bohn - 2001 - Learning and process improvement during production.pdf;C\:\\Users\\benja\\Zotero\\storage\\IMX2I4PL\\S0925527300000451.html}
}

@article{testaExperimentalStiffnessIdentification2017,
  title = {Experimental Stiffness Identification in the Joints of a Lightweight Robot},
  author = {Testa, Giuseppe},
  year = {2017},
  journal = {undefined},
  urldate = {2022-08-01},
  abstract = {The aim of this thesis is to develop a simple experimental methodology for the joint stiffness identification and its application to any robot and also to prove a methods already present in bibliography. The estimation of stiffness parameters of a robot is of paramount importance for the precision of movement, in fact in the last year the application of robots is justified by many factors, one of these factors is the high precision of movement. The aim of this thesis is to develop a simple experimental methodology for the joint stiffness identification and its application to any robot and also to prove a methods already present in bibliography.  The case study in this work is a lightweight robot, namely the UR5 manipulators. Usually these robots are slender and low-weight, and provided with flexible joints, these characteristics shall make a modal analysis of fundamental importance, because the robots that present a high flexibility may generate vibratory phenomenon, that would affect its performance. Therefore the stiffness analysis is the basis and preliminary of modal analysis. The stiffness analysis was made necessary by the lack of knowledge of some UR5 robot parameters as the stiffness, as this thesis' case, or even damping.  In the first three chapters there are, respectively, an introduction on the factors that influence the stiffness in the robotic structures; then are provided all the physical and mathematical knowledges, which underlie the study of stiffness, and finally is illustrated the state of art in stiffness evaluation methods.  In the last three chapters there are, respectively, an overview of all components that have been used in laboratory for the evaluation of stiffness, subsequently the value of stiffness are presented respectively through the adopted methodology and the developed methodology, as final step an analysis of results was conducted to compare and to analyze the two methods.  Some fundamental conclusions of this work are, respectively, that for conduct an analysis of stiffness is necessary a measurement system with high accuracy, subsequently both the adopted method and developed method may be applied in particular configurations, where the Jacobian matrix don't change following the load application.},
  langid = {english},
  file = {C\:\\Users\\benja\\Zotero\\storage\\7DSU4XC8\\Master_Thesis_-_Giuseppe_Testa.pdf;C\:\\Users\\benja\\Zotero\\storage\\GJE89PSL\\6e8982eb2af84b2531baad3a1df0c0b2b045bc38.html}
}

@inproceedings{theisRequirementsExplainabilityAcceptance2023,
  title = {Requirements for~{{Explainability}} and~{{Acceptance}} of~{{Artificial Intelligence}} in~{{Collaborative Work}}},
  booktitle = {Artificial {{Intelligence}} in {{HCI}}},
  author = {Theis, Sabine and Jentzsch, Sophie and Deligiannaki, Fotini and Berro, Charles and Raulf, Arne Peter and Bruder, Carmen},
  editor = {Degen, Helmut and Ntoa, Stavroula},
  year = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {355--380},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-35891-3_22},
  abstract = {The increasing prevalence of Artificial Intelligence (AI) in safety-critical contexts such as air-traffic control leads to systems that are practical and efficient, and to some extent explainable to humans to be trusted and accepted. The present structured literature analysis examines \$\$n = 236\$\$articles on the requirements for the explainability and acceptance of AI. Results include a comprehensive review of \$\$n = 48\$\$articles on information people need to perceive an AI as explainable, the information needed to accept an AI, and representation and interaction methods promoting trust in an AI. Results indicate that the two main groups of users are developers who require information about the internal operations of the model and end users who require information about AI results or behavior. Users' information needs vary in specificity, complexity, and urgency and must consider context, domain knowledge, and the user's cognitive resources. The acceptance of AI systems depends on information about the system's functions and performance, privacy and ethical considerations, as well as goal-supporting information tailored to individual preferences and information to establish trust in the system. Information about the system's limitations and potential failures can increase acceptance and trust. Trusted interaction methods are human-like, including natural language, speech, text, and visual representations such as graphs, charts, and animations. Our results have significant implications for future human-centric AI systems being developed. Thus, they are suitable as input for further application-specific investigations of user needs.},
  isbn = {978-3-031-35891-3},
  langid = {english},
  keywords = {Acceptance,Air-traffic control,Artificial intelligence,Explainability,Information needs,Safety-critical contexts,Structured literature analysis,User requirement analysis}
}

@inproceedings{thermosDeepLearningApproach2020,
  title = {A {{Deep Learning Approach}} to {{Object Affordance Segmentation}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Thermos, Spyridon and Daras, Petros and Potamianos, Gerasimos},
  year = {2020},
  month = may,
  pages = {2358--2362},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9054167},
  abstract = {Learning to understand and infer object functionalities is an important step towards robust visual intelligence. Significant research efforts have recently focused on segmenting the object parts that enable specific types of human-object interaction, the so-called "object affordances". However, most works treat it as a static semantic segmentation problem, focusing solely on object appearance and relying on strong supervision and object detection. In this paper, we propose a novel approach that exploits the spatio-temporal nature of human-object interaction for affordance segmentation. In particular, we design an autoencoder that is trained using ground-truth labels of only the last frame of the sequence, and is able to infer pixel-wise affordance labels in both videos and static images. Our model surpasses the need for object labels and bounding boxes by using a soft-attention mechanism that enables the implicit localization of the interaction hotspot. For evaluation purposes, we introduce the SOR3D-AFF corpus, which consists of human-object interaction sequences and supports 9 types of affordances in terms of pixel-wise annotation, covering typical manipulations of tool-like objects. We show that our model achieves competitive results compared to strongly supervised methods on SOR3D-AFF, while being able to predict affordances for similar unseen objects in two affordance image-only datasets.},
  keywords = {affordance,Affordances,Annotations,deep neural networks,human-object interaction,Image segmentation,Predictive models,segmentation,soft attention,Task analysis,Videos,Visualization},
  file = {C:\Users\benja\Zotero\storage\5NTR6NP5\9054167.html}
}

@inproceedings{thomasNewSkillBased2013,
  title = {A New Skill Based Robot Programming Language Using {{UML}}/{{P Statecharts}}},
  booktitle = {2013 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Thomas, Ulrike and Hirzinger, Gerd and Rumpe, Bernhard and Schulze, Christoph and Wortmann, Andreas},
  year = {2013},
  month = may,
  pages = {461--466},
  publisher = {IEEE},
  address = {Karlsruhe, Germany},
  doi = {10.1109/ICRA.2013.6630615},
  urldate = {2020-09-15},
  abstract = {This paper introduces the new robot programming language LightRocks(Light Weight Robot Coding for Skills), a domain specific language (DSL) for robot programming. The language offers three different level of abstraction for robot programming. On lowest level skills are coded by domain experts. On a more abstract level these skills are supposed to be combined by shop floor workers or technicians to define tasks. The language is designed to allow as much flexibility as necessary on the lowest level of abstraction and is kept as simple as possible with the more abstract layers. A Statechart like model is used to describe the different levels of detail. For this we apply the UML/P and the language workbench MontiCore. To this end we are able to generate code while hiding controller specific implementation details. In addition the development in LightRocks is supported by a generic graphical editor implemented as an Eclipse plugin.},
  isbn = {978-1-4673-5643-5 978-1-4673-5641-1},
  langid = {english}
}

@inproceedings{thomasonLearningMultiModalGrounded2016,
  title = {Learning {{Multi-Modal Grounded Linguistic Semantics}} by {{Playing}} "{{I Spy}}"},
  booktitle = {International {{Joint Conference}} on {{Artificial Intelligence}}},
  author = {Thomason, Jesse and Sinapov, Jivko and Svetlik, Maxwell and Stone, P. and Mooney, R.},
  year = {2016},
  month = jul,
  urldate = {2024-08-16},
  abstract = {Grounded language learning bridges words like 'red' and 'square' with robot perception. The vast majority of existing work in this space limits robot perception to vision. In this paper, we build perceptual models that use haptic, auditory, and proprioceptive data acquired through robot exploratory behaviors to go beyond vision. Our system learns to ground natural language words describing objects using supervision from an interactive humanrobot "I Spy" game. In this game, the human and robot take turns describing one object among several, then trying to guess which object the other has described. All supervision labels were gathered from human participants physically present to play this game with a robot. We demonstrate that our multi-modal system for grounding natural language outperforms a traditional, vision-only grounding framework by comparing the two on the "I Spy" task. We also provide a qualitative analysis of the groundings learned in the game, visualizing what words are understood better with multimodal sensory information as well as identifying learned word meanings that correlate with physical object properties (e.g. 'small' negatively correlates with object weight).}
}

@inproceedings{thompsonComputationalLimitsDeep2022,
  title = {The {{Computational Limits}} of {{Deep Learning}}},
  booktitle = {Ninth {{Workshop}} on {{Computing}} within {{Limits}}},
  author = {Thompson, Neil C. and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel F.},
  year = {2022},
  month = jul,
  eprint = {2007.05558},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.05558},
  urldate = {2024-01-24},
  abstract = {Deep learning's recent history has been one of achievement: from triumphing over humans in the game of Go to world-leading performance in image classification, voice recognition, translation, and other tasks. But this progress has come with a voracious appetite for computing power. This article catalogs the extent of this dependency, showing that progress across a wide variety of applications is strongly reliant on increases in computing power. Extrapolating forward this reliance reveals that progress along current lines is rapidly becoming economically, technically, and environmentally unsustainable. Thus, continued progress in these applications will require dramatically more computationally-efficient methods, which will either have to come from changes to deep learning or from moving to other machine learning methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\DHK7TXSY\2007.html}
}

@article{thompsonDeepLearningsDiminishing2021,
  title = {Deep {{Learning}}'s {{Diminishing Returns}}: {{The Cost}} of {{Improvement}} Is {{Becoming Unsustainable}}},
  shorttitle = {Deep {{Learning}}'s {{Diminishing Returns}}},
  author = {Thompson, Neil C. and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel F.},
  year = {2021},
  month = oct,
  journal = {IEEE Spectrum},
  volume = {58},
  number = {10},
  pages = {50--55},
  issn = {1939-9340},
  doi = {10.1109/MSPEC.2021.9563954},
  urldate = {2024-01-25},
  abstract = {Deep learning is now being used to translate between languages, predict how proteins fold, analyze medical scans, and play games as complex as Go, to name just a few applications of a technique that is now becoming pervasive. Success in those and other realms has brought this machine-learning technique from obscurity in the early 2000s to dominance today. {$\bullet$} Although deep learning's rise to fame is relatively recent, its origins are not. In 1958, back when mainframe computers filled rooms and ran on vacuum tubes, knowledge of the interconnections between neurons in the brain inspired Frank Rosenblatt at Cornell to design the first artificial neural network, which he presciently described as a ``pattern-recognizing device.'' But Rosenblatt's ambitions outpaced the capabilities of his era---and he knew it. Even his inaugural paper was forced to acknowledge the voracious appetite of neural networks for computational power, bemoaning that ``as the number of connections in the network increases{\dots}the burden on a conventional digital computer soon becomes excessive.'' {$\bullet$} Fortunately for such artificial neural networks---later rechristened ``deep learning'' when they included extra layers of neurons---decades of Moore's Law and other improvements in computer hardware yielded a roughly 10-million-fold increase in the number of computations that a computer could do in a second. So when researchers returned to deep learning in the late 2000s, they wielded tools equal to the challenge.},
  keywords = {Deep learning,Knowledge engineering,Moore's Law,Neurons,Proteins,Semiconductor device modeling,Tools},
  file = {C:\Users\benja\Zotero\storage\FK3RG6ZE\9563954.html}
}

@article{Thorpe2001,
  title = {Spike-Based Strategies for Rapid Processing},
  author = {Thorpe, Simon and Delorme, Arnaud and Van Rullen, Rufin},
  year = {2001},
  journal = {Neural networks},
  volume = {14},
  number = {6-7},
  pages = {715--725},
  publisher = {Elsevier}
}

@misc{ThreeWaysRemotely,
  title = {Three {{Ways}} to {{Remotely Program}} a {{Robot}}},
  journal = {BirdBrain Technologies},
  urldate = {2023-04-06},
  abstract = {join us for a webinar on the topic on August 7th. NetsBlox Snap! that is optimized for passing messages between student-built projects over the internet. NetsBlox allows anyone to connect {\dots}},
  howpublished = {https://learn.birdbraintechnologies.com/remote-robots/three-ways-remote-robots/},
  langid = {american},
  file = {C:\Users\benja\Zotero\storage\W37G9U8Y\three-ways-remote-robots.html}
}

@incollection{thrunLearningLearnIntroduction1998,
  title = {Learning to {{Learn}}: {{Introduction}} and {{Overview}}},
  shorttitle = {Learning to {{Learn}}},
  booktitle = {Learning to {{Learn}}},
  author = {Thrun, Sebastian and Pratt, Lorien},
  editor = {Thrun, Sebastian and Pratt, Lorien},
  year = {1998},
  pages = {3--17},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4615-5529-2_1},
  urldate = {2024-06-14},
  abstract = {Over the past three decades or so, research on machine learning and data mining has led to a wide variety of algorithms that learn general functions from experience. As machine learning is maturing, it has begun to make the successful transition from academic research to various practical applications. Generic techniques such as decision trees and artificial neural networks, for example, are now being used in various commercial and industrial applications (see e.g., [Langley, 1992; Widrow et al., 1994]).},
  isbn = {978-1-4615-5529-2},
  langid = {english}
}

@article{Tian.2010,
  title = {Modeling Deformations of General Parametric Shells Grasped by a Robot Hand},
  author = {Tian, Jiang and Jia, Yan-Bin},
  year = {2010},
  journal = {IEEE Transactions on Robotics},
  volume = {26},
  number = {5},
  pages = {837--852},
  issn = {1552-3098},
  doi = {10.1109/TRO.2010.2050350},
  pagination = {page}
}

@article{tianCloudBasedRobustSemaphore2018,
  title = {A {{Cloud-Based Robust Semaphore Mirroring System}} for {{Social Robots}}},
  author = {Tian, N. and Kuo, B. and Ren, X. and Yu, M. and Zhang, Robert and Huang, Bill and Goldberg, Ken and Sojoudi, S.},
  year = {2018},
  journal = {2018 IEEE 14th International Conference on Automation Science and Engineering (CASE)},
  doi = {10.1109/COASE.2018.8560553},
  abstract = {We present a cloud-based human-robot interaction system that automatically controls a humanoid robot to mirror a human demonstrator performing flag semaphores. We use a cloud-based framework called Human Augmented Robotic Intelligence (HARI) to perform gesture recognition of the human demonstrator and gesture control of a local humanoid robot, named Pepper. To ensure that the system is real-time, we design a system to maximize cloud computation contribution to the deep-neural-network-based gesture recognition system, OpenPose, and to minimize communication costs between the cloud and the robot. A hybrid control system is used to hide latency caused by either routing or physical distances. We conducted real-time semaphore mirroring experiments in which both the robots and the demonstrator were located in Tokyo, Japan, whereas the cloud server was deployed in the United States. The total latency was 400ms for the video streaming to the cloud and 108ms for the robot commanding from the cloud. Further, we measured the reliability of our gesture-based semaphore recognition system with two human subjects, and were able to achieve 90\% and 76.7\% recognition accuracy, respectively, for the two subjects with open-loop when the subjects were not allowed to see the recognition results. We could achieve 100\% recognition accuracy when both subjects were allowed to adapt to the recognition system under a closed-loop setting. Lastly, we showed that we can support two humanoid robots with a single server at the same time. With this real-time cloud-based HRI system, we illustrate that we can deploy gesture-based human-robot globally and at scale.}
}

@article{tieckLearningContinuousMuscle2018,
  title = {Learning Continuous Muscle Control for a Multi-Joint Arm by Extending Proximal Policy Optimization with a Liquid State Machine},
  author = {Tieck, J Camilo Vasquez and Pogan, Marin Vlastelica and Kaiser, Jacques and Arne, Roenau and Gewaltig, Marc-oliver and Dillmann, R{\"u}diger},
  year = {2018},
  journal = {ICANN},
  keywords = {ing,muscle control,neurorobotics,reinforcement learning,reservoir comput-,spiking networks}
}

@article{Tiwari2007,
  title = {Energy-{{Efficient Wireless Sensor Network Design}} and {{Implementation}} for {{Condition-Based Maintenance}}},
  author = {Tiwari, Ankit and Ballal, Prasanna and Lewis, Frank L.},
  year = {2007},
  month = mar,
  journal = {ACM Transactions on Sensor Networks},
  volume = {3},
  number = {1},
  pages = {1-es},
  publisher = {ACM},
  issn = {15504859},
  doi = {10.1145/1210669.1210670},
  urldate = {2018-05-12},
  keywords = {Condition-based maintenance,medium access control (MAC),TDMA}
}

@article{tobinDomainRandomizationTransferring2017,
  title = {Domain {{Randomization}} for {{Transferring Deep Neural Networks}} from {{Simulation}} to the {{Real World}}},
  author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.06907 [cs]},
  eprint = {1703.06907},
  primaryclass = {cs},
  urldate = {2019-05-17},
  abstract = {Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to \$1.5\$cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\NCA3JYNZ\1703.html}
}

@incollection{Tombari.2010,
  title = {Unique Signatures of Histograms for Local Surface Description},
  booktitle = {Computer Vision -- {{ECCV}} 2010},
  author = {Tombari, Federico and Salti, Samuele and Di Stefano, Luigi},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
  year = {2010},
  series = {Lecture Notes in Computer Science},
  volume = {6313},
  pages = {356--369},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-15558-1_26},
  bookpagination = {page},
  isbn = {978-3-642-15557-4}
}

@inproceedings{Tombari.2011,
  title = {A Combined Texture-Shape Descriptor for Enhanced {{3D}} Feature Matching},
  booktitle = {2011 18th {{IEEE}} International Conference on Image Processing},
  author = {Tombari, Federico and Salti, Samuele and Di Stefano, Luigi},
  year = {2011},
  pages = {809--812},
  publisher = {IEEE},
  doi = {10.1109/ICIP.2011.6116679},
  bookpagination = {page},
  isbn = {978-1-4577-1303-3}
}

@article{tongSystematicAnalysisFunctional2017,
  title = {A {{Systematic Analysis}} of {{Functional Safety Certification Practices}} in {{Industrial Robot Software Development}}},
  author = {Tong, Xie and Lei, Wu},
  year = {2017},
  month = jan,
  journal = {MATEC Web of Conferences},
  volume = {100},
  pages = {02011},
  doi = {10.1051/matecconf/201710002011},
  abstract = {For decades, industry robotics have delivered on the promise of speed, efficiency and productivity. The last several years have seen a sharp resurgence in the orders of industrial robots in China, and the areas addressed within industrial robotics has extended into safety-critical domains. However, safety standards have not yet been implemented widely in academia and engineering applications, particularly in robot software development. This paper presents a systematic analysis of functional safety certification practices in software development for the safety-critical software of industrial robots, to identify the safety certification practices used for the development of industrial robots in China and how these practices comply with the safety standard requirements. Reviewing from Chinese academic papers, our research shows that safety standards are barely used in software development of industrial robot. The majority of the papers propose various solutions to achieve safety, but only about two thirds of the papers refer to non-standardized approaches that mainly address the systematic level rather than the software development level. In addition, our research shows that with the development of artificial intelligent, an emerging field is still on the quest for standardized and suitable approaches to develop safety-critical software.}
}

@phdthesis{tonnesenDynamicallyCoupledParticle1998,
  type = {Thesis},
  title = {Dynamically Coupled Particle Systems for Geometric Modeling, Reconstruction, and Animation},
  author = {Tonnesen, David Love},
  year = {1998},
  doi = {10/PQDD_0001/NQ41520.pdf},
  urldate = {2022-08-18},
  abstract = {This dissertation presents a new technique, based on dynamically coupled particle systems, for creating and manipulating complex three dimensional shapes in a fluid like manner. The most novel feature of this approach to shape representation is the use of self organizing primitive elements. In the simplest case, these primitive elements, or particles, each posses state variables of position and mass, and the system of elements interact through pairwise potential energy functions. More complex systems include additional state variables combined with simple heuristics to create application specific behavior. The ability of these systems to self organize provides a representation technique which exhibits dynamically changing structure, an attribute not found in popular spline and polygonally based representations. To illustrate the usefulness of this approach it is applied to the following problems: free form shape modeling, computer assisted animation, and surface reconstruction. For free-form modeling the approach supports smoothness constraints similar to those inherent in the deformation energies of popular, elastic surface models. Unlike spline patches or parameterized surface models, the model does not attempt to enforce analytical continuity conditions, such as tangent or curvature continuity over the surface. Applied to computer assisted animation the approach computes the movement and deformation of models mimicking, at a rudimentary level, the physical behavior of flexible solids and fluids. Applied to surface reconstruction, these systems can infer surface structure from sparse data sets, without a prior knowledge of the surface structure or the topological genus. In summary, dynamically coupled particle systems provide an useful alternative to traditional shape representation and manipulation techniques.},
  langid = {american},
  annotation = {Accepted: 2008-08-08T14:06:02Z},
  file = {C:\Users\benja\Zotero\storage\MBKQJU8K\13355.html}
}

@inproceedings{tosattoContextualLatentMovementsOffPolicy2021,
  title = {Contextual {{Latent-Movements Off-Policy Optimization}} for {{Robotic Manipulation Skills}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Tosatto, Samuele and Chalvatzaki, Georgia and Peters, Jan},
  year = {2021},
  month = may,
  pages = {10815--10821},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9561870},
  urldate = {2024-09-29},
  abstract = {Parameterized movement primitives have been extensively used for imitation learning of robotic tasks. However, the high-dimensionality of the parameter space hinders the improvement of such primitives in the reinforcement learning (RL) setting, especially for learning with physical robots. In this paper we propose a novel view on handling the demonstrated trajectories for acquiring low-dimensional, non-linear latent dynamics, using mixtures of probabilistic principal component analyzers (MPPCA) on the movements' parameter space. Moreover, we introduce a new contextual off-policy RL algorithm, named LAtent-Movements Policy Optimization (LAMPO). LAMPO can provide gradient estimates from previous experience using self-normalized importance sampling, hence, making full use of samples collected in previous learning iterations. These advantages combined provide a complete framework for sample-efficient off-policy optimization of movement primitives for robot learning of high-dimensional manipulation skills. Our experimental results conducted both in simulation and on a real robot show that LAMPO provides sample-efficient policies against common approaches in literature. Code available at https://github.com/SamuelePolimi/lampo.},
  keywords = {Codes,Conferences,Heuristic algorithms,Monte Carlo methods,Probabilistic logic,Reinforcement learning,Robot learning},
  file = {C\:\\Users\\benja\\Zotero\\storage\\5JJP7HBB\\Tosatto et al. - 2021 - Contextual Latent-Movements Off-Policy Optimization for Robotic Manipulation Skills.pdf;C\:\\Users\\benja\\Zotero\\storage\\489TRXMV\\9561870.html}
}

@inproceedings{toussaintDifferentiablePhysicsStable2018,
  title = {Differentiable {{Physics}} and {{Stable Modes}} for {{Tool-Use}} and {{Manipulation Planning}}},
  booktitle = {Robotics: {{Science}} and {{Systems XIV}}},
  author = {Toussaint, Marc and Allen, Kelsey and Smith, Kevin and Tenenbaum, Joshua},
  year = {2018},
  month = jun,
  volume = {14},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-4-7},
  file = {C:\Users\benja\Zotero\storage\RMVD6M8V\p44.html}
}

@inproceedings{toussaintLogicgeometricProgrammingOptimizationbased2015,
  title = {Logic-Geometric Programming: An Optimization-Based Approach to Combined Task and Motion Planning},
  shorttitle = {Logic-Geometric Programming},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Artificial Intelligence}}},
  author = {Toussaint, Marc},
  year = {2015},
  month = jul,
  series = {{{IJCAI}}'15},
  pages = {1930--1936},
  publisher = {AAAI Press},
  address = {Buenos Aires, Argentina},
  urldate = {2024-06-25},
  abstract = {We consider problems of sequential robot manipulation (aka. combined task and motion planning) where the objective is primarily given in terms of a cost function over the final geometric state, rather than a symbolic goal description. In this case we should leverage optimization methods to inform search over potential action sequences. We propose to formulate the problem holistically as a 1st- order logic extension of a mathematical program: a non-linear constrained program over the full world trajectory where the symbolic state-action sequence defines the (in-)equality constraints. We tackle the challenge of solving such programs by proposing three levels of approximation: The coarsest level introduces the concept of the effective end state kinematics, parametrically describing all possible end state configurations conditional to a given symbolic action sequence. Optimization on this level is fast and can inform symbolic search. The other two levels optimize over interaction keyframes and eventually over the full world trajectory across interactions. We demonstrate the approach on a problem of maximizing the height of a physically stable construction from an assortment of boards, cylinders and blocks.},
  isbn = {978-1-57735-738-4}
}

@inproceedings{toussaintMultiboundTreeSearch2017,
  title = {Multi-Bound Tree Search for Logic-Geometric Programming in Cooperative Manipulation Domains},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Toussaint, Marc and Lopes, Manuel},
  year = {2017},
  month = may,
  pages = {4044--4051},
  doi = {10.1109/ICRA.2017.7989464},
  urldate = {2024-06-25},
  abstract = {Joint symbolic and geometric planning is one of the core challenges in robotics. We address the problem of multi-agent cooperative manipulation, where we aim for jointly optimal paths for all agents and over the full manipulation sequence. This joint optimization problem can be framed as a logic-geometric program. Existing solvers lack several features (such as consistently handling kinematic switches) and efficiency to handle the cooperative manipulation domain. We propose a new approximate solver scheme, combining ideas from branch-and-bound and MCTS and exploiting multiple levels of bounds to better direct the search. We demonstrate the method in a scenario where a Baxter robot needs to help a human to reach for objects.},
  keywords = {Aerospace electronics,Kinematics,Optimization,Planning,Programming,Robots,Search problems},
  file = {C:\Users\benja\Zotero\storage\EGT6XLBR\7989464.html}
}

@misc{touvronLlama2Open2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.09288},
  urldate = {2024-06-15},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\benja\Zotero\storage\3E3X8I7Y\2307.html}
}

@misc{touvronLLaMAOpenEfficient2023,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13971},
  eprint = {2302.13971},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.13971},
  urldate = {2023-10-30},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\benja\Zotero\storage\VJJVYMGG\2302.html}
}

@article{traftonACTREmbodiedCognitive2013,
  title = {{{ACT-R}}/{{E}}: An Embodied Cognitive Architecture for Human-Robot Interaction},
  shorttitle = {{{ACT-R}}/{{E}}},
  author = {Trafton, J. Gregory and Hiatt, Laura M. and Harrison, Anthony M. and Tamborello, Franklin P. and Khemlani, Sangeet S. and Schultz, Alan C.},
  year = {2013},
  month = feb,
  journal = {Journal of Human-Robot Interaction},
  volume = {2},
  number = {1},
  pages = {30--55},
  doi = {10.5898/JHRI.2.1.Trafton},
  urldate = {2024-09-17},
  abstract = {We present ACT-R/E (Adaptive Character of Thought-Rational / Embodied), a cognitive architecture for human-robot interaction. Our reason for using ACT-R/E is two-fold. First, ACT-R/E enables researchers to build good embodied models of people to understand how and why people think the way they do. Then, we leverage that knowledge of people by using it to predict what a person will do in different situations; e.g., that a person may forget something and may need to be reminded or that a person cannot see everything the robot sees. We also discuss methods of how to evaluate a cognitive architecture and show numerous empirically validated examples of ACT-R/E models.},
  file = {C:\Users\benja\Zotero\storage\MKDZCD9Z\Trafton et al. - 2013 - ACT-RE an embodied cognitive architecture for human-robot interaction.pdf}
}

@inproceedings{traftonChildrenRobotsLearning2006,
  title = {Children and Robots Learning to Play Hide and Seek},
  booktitle = {Proceedings of the 1st {{ACM SIGCHI}}/{{SIGART}} Conference on {{Human-robot}} Interaction},
  author = {Trafton, J. Gregory and Schultz, Alan C. and Perznowski, Dennis and Bugajska, Magdalena D. and Adams, William and Cassimatis, Nicholas L. and Brock, Derek P.},
  year = {2006},
  month = mar,
  series = {{{HRI}} '06},
  pages = {242--249},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1121241.1121283},
  urldate = {2024-08-15},
  abstract = {How do children learn how to play hide and seek? At age 3-4, children do not typically have perspective taking ability, so their hiding ability should be extremely limited. We show through a case study that a 3 1/2 year old child can, in fact, play a credible game of hide and seek, even though she does not seem to have perspective taking ability. We propose that children are able to learn how to play hide and seek by learning the features and relations of objects (e.g., containment, under) and use that information to play a credible game of hide and seek. We model this hypothesis within the ACT-R cognitive architecture and put the model on a robot, which is able to mimic the child's hiding behavior. We also take the "hiding" model and use it as the basis for a "seeking" model. We suggest that using the same representations and procedures that a person uses allows better interaction between the human and robotic system.},
  isbn = {978-1-59593-294-5}
}

@article{tranBayesianGenerativeActive2019,
  title = {Bayesian {{Generative Active Deep Learning}}},
  author = {Tran, Toan and Do, Thanh-Toan and Reid, Ian and Carneiro, Gustavo},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.11643 [cs, stat]},
  eprint = {1904.11643},
  primaryclass = {cs, stat},
  urldate = {2020-12-19},
  abstract = {Deep learning models have demonstrated outstanding performance in several problems, but their training process tends to require immense amounts of computational and human resources for training and labeling, constraining the types of problems that can be tackled. Therefore, the design of effective training methods that require small labeled training sets is an important research direction that will allow a more effective use of resources.Among current approaches designed to address this issue, two are particularly interesting: data augmentation and active learning. Data augmentation achieves this goal by artificially generating new training points, while active learning relies on the selection of the "most informative" subset of unlabeled training samples to be labelled by an oracle. Although successful in practice, data augmentation can waste computational resources because it indiscriminately generates samples that are not guaranteed to be informative, and active learning selects a small subset of informative samples (from a large un-annotated set) that may be insufficient for the training process. In this paper, we propose a Bayesian generative active deep learning approach that combines active learning with data augmentation -- we provide theoretical and empirical evidence (MNIST, CIFAR-\${\textbackslash}\{10,100{\textbackslash}\}\$, and SVHN) that our approach has more efficient training and better classification results than data augmentation and active learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\I56QYFBS\1904.html}
}

@inproceedings{trapaniTaskModelingTaskoriented2017,
  title = {Task Modeling for Task-Oriented Robot Programming},
  booktitle = {2017 22nd {{IEEE International Conference}} on {{Emerging Technologies}} and {{Factory Automation}} ({{ETFA}})},
  author = {Trapani, S. and Indri, M.},
  year = {2017},
  month = sep,
  pages = {1--8},
  doi = {10.1109/ETFA.2017.8247650},
  abstract = {A joint research between Politecnico di Torino and COMAU is going on, aimed at defining a task oriented programming approach to allow soft skilled programmers to develop optimized programs for complex robotic cells. Within this paradigm, the user is simply asked to specify the features of the robotic cell and a structured set of tasks defining the required process. The paper is focused on the first step necessary to achieve the overall goal, i.e., the definition of a generic task model. The model is built starting from the analysis of the most important industrial applications, and can include physical and synchronization constraints, as well as various possible requirements and situations relative to the characteristics of the robots in the cell and of the involved work-pieces. A welding process is considered as typical industrial case study, to show how to build the corresponding task model according to the proposed approach.},
  keywords = {Collision avoidance,complex robotic cells,control engineering computing,generic task model,industrial applications,industrial robots,object-oriented programming,optimisation,optimized program development,Politecnico di Torino,Programming,robot programming,Robots,soft skilled programmers,Solid modeling,synchronization constraints,task analysis,task modeling,task-oriented robot programming,Tools,Welding,welding process},
  file = {C:\Users\benja\Zotero\storage\RZMDWM63\8247650.html}
}

@inproceedings{trapaniTaskModelingTaskoriented2017a,
  title = {Task Modeling for Task-Oriented Robot Programming},
  booktitle = {2017 22nd {{IEEE International Conference}} on {{Emerging Technologies}} and {{Factory Automation}} ({{ETFA}})},
  author = {Trapani, S. and Indri, M.},
  year = {2017},
  month = sep,
  pages = {1--8},
  doi = {10.1109/ETFA.2017.8247650},
  abstract = {A joint research between Politecnico di Torino and COMAU is going on, aimed at defining a task oriented programming approach to allow soft skilled programmers to develop optimized programs for complex robotic cells. Within this paradigm, the user is simply asked to specify the features of the robotic cell and a structured set of tasks defining the required process. The paper is focused on the first step necessary to achieve the overall goal, i.e., the definition of a generic task model. The model is built starting from the analysis of the most important industrial applications, and can include physical and synchronization constraints, as well as various possible requirements and situations relative to the characteristics of the robots in the cell and of the involved work-pieces. A welding process is considered as typical industrial case study, to show how to build the corresponding task model according to the proposed approach.},
  keywords = {Collision avoidance,complex robotic cells,control engineering computing,generic task model,industrial applications,industrial robots,object-oriented programming,optimisation,optimized program development,Politecnico di Torino,Programming,robot programming,Robots,soft skilled programmers,Solid modeling,synchronization constraints,task analysis,task modeling,task-oriented robot programming,Tools,Welding,welding process},
  file = {C:\Users\benja\Zotero\storage\J369L54L\8247650.html}
}

@article{tremblayDeepObjectPose2018,
  title = {Deep {{Object Pose Estimation}} for {{Semantic Robotic Grasping}} of {{Household Objects}}},
  author = {Tremblay, Jonathan and To, Thang and Sundaralingam, Balakumar and Xiang, Yu and Fox, Dieter and Birchfield, Stan},
  year = {2018},
  month = sep,
  journal = {Conference on Robot Learning (CoRL)},
  eprint = {1809.10790},
  urldate = {2021-02-02},
  abstract = {Using synthetic data for training deep neural networks for robotic manipulation holds the promise of an almost unlimited amount of pre-labeled training data, generated safely out of harm's way. One of the key challenges of synthetic data, to date, has been to bridge the so-called reality gap, so that networks trained on synthetic data operate correctly when exposed to real-world data. We explore the reality gap in the context of 6-DoF pose estimation of known objects from a single RGB image. We show that for this problem the reality gap can be successfully spanned by a simple combination of domain randomized and photorealistic data. Using synthetic data generated in this manner, we introduce a one-shot deep neural network that is able to perform competitively against a state-of-the-art network trained on a combination of real and synthetic data. To our knowledge, this is the first deep network trained only on synthetic data that is able to achieve state-of-the-art performance on 6-DoF object pose estimation. Our network also generalizes better to novel environments including extreme lighting conditions, for which we show qualitative results. Using this network we demonstrate a real-time system estimating object poses with sufficient accuracy for real-world semantic grasping of known household objects in clutter by a real robot.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C\:\\Users\\benja\\Zotero\\storage\\T9H42IPM\\Tremblay et al. - 2018 - Deep Object Pose Estimation for Semantic Robotic G.pdf;C\:\\Users\\benja\\Zotero\\storage\\87CTM44Q\\1809.html}
}

@inproceedings{trochimczukModelingProgrammingSimulation2019,
  title = {Modeling, {{Programming}} and Simulation of Robotized Workcells Created for Industrial and Service Needs},
  booktitle = {18th {{International Scientific Conference Engineering}} for {{Rural Development}}},
  author = {Trochimczuk, Roman and Lukaszewicz, Andrzej and Szczebiot, Ryszard and Kirillov, Alexey G. and Mircheski, Ile},
  year = {2019},
  month = may,
  doi = {10.22616/ERDev2019.18.N455},
  urldate = {2020-05-12},
  abstract = {The paper shows and compares the functional features of off-line software environments of selected manufacturers of industrial robots. These softwares used for modelling, programming and simulation of robotic workcells are created for industrial and service needs (e.g. agricultural services), or simulation use of robots adapted to cooperate with humans. The use of such solutions greatly improves the assessment of the safety and exploitation conditions of robotic systems in various areas and aspects of life. It gives the investor an opportunity to make an opinion on the economic and social aspects of using robots in the manufacturing and service sector, too. Such systems (in particular Festo COSIMIR{\textregistered} and ROS programs described in the paper) are becoming an important element of the so-called Industry 4.0, so you should have knowledge about them.},
  langid = {english}
}

@article{truhnLargeLanguageModels2023,
  title = {Large Language Models Should Be Used as Scientific Reasoning Engines, Not Knowledge Databases},
  author = {Truhn, Daniel and {Reis-Filho}, Jorge S. and Kather, Jakob Nikolas},
  year = {2023},
  month = dec,
  journal = {Nature Medicine},
  volume = {29},
  number = {12},
  pages = {2983--2984},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-023-02594-z},
  urldate = {2024-03-01},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Biomedical engineering,Diseases,Scientific community}
}

@inproceedings{Trumble.2017,
  title = {Total Capture: {{3D}} Human Pose Estimation Fusing Video and Inertial Sensors},
  booktitle = {Procedings of the British Machine Vision Conference 2017},
  author = {Trumble, Matthew and Gilbert, Andrew and Malleson, Charles and Hilton, Adrian and Collomosse, John},
  editor = {Kim, Tae-Kyun and Zafeiriou, Stefanos and Brostow, Gabriel and Mikolajczyk, Krystian},
  year = {2017},
  publisher = {British Machine Vision Association},
  doi = {10.5244/C.31.14},
  isbn = {1-901725-60-X}
}

@book{Tsai.1986,
  title = {An Efficient and Accurate Camera Calibration Technique for {{3D}} Machine Vision.},
  editor = {Tsai, R. Y.},
  year = {1986}
}

@article{turrentineMorbidityMortalityCost2015,
  title = {Morbidity, {{Mortality}}, {{Cost}}, and {{Survival Estimates}} of {{Gastrointestinal Anastomotic~Leaks}}},
  author = {Turrentine, Florence E. and Denlinger, Chaderick E. and Simpson, Virginia B. and Garwood, Robert A. and Guerlain, Stephanie and Agrawal, Abhinav and Friel, Charles M. and LaPar, Damien J. and Stukenborg, George J. and Jones, R. Scott},
  year = {2015},
  month = feb,
  journal = {Journal of the American College of Surgeons},
  volume = {220},
  number = {2},
  pages = {195--206},
  issn = {1072-7515},
  doi = {10.1016/j.jamcollsurg.2014.11.002},
  urldate = {2024-04-12},
  abstract = {Background Anastomotic leak, a potentially deadly postoperative occurrence, particularly interests surgeons performing gastrointestinal procedures. We investigated incidence, cost, and impact on survival of anastomotic leak in gastrointestinal surgical procedures at an academic center. Study Design We conducted a chart review of American College of Surgeons NSQIP operative procedures with gastrointestinal anastomosis from January 1, 2003 through April 30, 2006. Each case with an American College of Surgeons NSQIP 30-day postoperative complication was systematically reviewed for evidence of anastomotic leak for 12 months after the operative date. We tracked patients for up to 10 years to determine survival. Morbidity, mortality, and cost for patients with gastrointestinal anastomotic leaks were compared with patients with anastomoses that remained intact. Results Unadjusted analyses revealed significant differences between patients who had anastomotic leaks develop and those who did not: morbidity (98.0\% vs 28.4\%; p {$<$} 0.0001), length of stay (13 vs 5 days; p {$\leq$} 0.0001), 30-day mortality (8.4\% vs 2.5\%; p {$<$} 0.0001), long-term mortality (36.4\% vs 20.0\%; p {$\leq$} 0.0001), and hospital costs (chi-square [2]~= 359.8; p {$<$} 0.0001). Multivariable regression demonstrated that anastomotic leak was associated with congestive heart failure (odds ratio [OR]~= 31.5; 95\% CI, 2.6--381.4; p~= 0.007), peripheral vascular disease (OR~= 4.6; 95\% CI, 1.0--20.5; p~= 0.048), alcohol abuse (OR~= 3.7; 95\% CI, 1.6--8.3; p~= 0.002), steroid use~(OR~= 2.3; 95\% CI: 1.1--5.0; p~= 0.027), abnormal sodium (OR~= 0.4; 95\% CI, 0.2--0.7; p~= 0.002), weight loss (OR~= 0.2; 95\% CI, 0.06--0.7; p~= 0.011), and location of anastomosis: rectum (OR~= 14.0; 95\% CI, 2.6--75.5; p~= 0.002), esophagus (OR~= 13.0; 95\% CI, 3.6--46.2; p {$<$} 0.0001), pancreas (OR~= 12.4; 95\% CI, 3.3--46.2; p {$<$} 0.0001), small intestine (OR~= 6.9; 95\% CI, 1.8--26.4; p~= 0.005), and colon (OR~= 5.2; 95\% CI, 1.5--17.7; p~=~0.009). Conclusions Significant morbidity, mortality, and cost accompany gastrointestinal anastomotic leaks. Patients who experience an anastomotic leak have lower rates of survival at 30 days and long term.}
}

@article{tuttleUnderstandingModelingBehavior1992,
  title = {Understanding and {{Modeling}} the {{Behavior}} of a {{Harmonic Drive Gear Transmission}}},
  author = {Tuttle, Timothy D.},
  year = {1992},
  month = may,
  urldate = {2022-08-01},
  abstract = {In my research, I have performed an extensive  experimental investigation of harmonic-drive  properties such as stiffness, friction, and  kinematic error. From my experimental  results, I have found that these properties can  be sharply non-linear and highly dependent  on operating conditions. Due to the complex  interaction of these poorly behaved  transmission properties, dynamic response  measurements showed surprisingly agitated  behavior, especially around system  resonance. Theoretical models developed to  mimic the observed response illustrated that  non-linear frictional effects cannot be ignored  in any accurate harmonic-drive  representation. Additionally, if behavior  around system resonance must be replicated,  kinematic error and transmission compliance  as well as frictional dissipation from gear-tooth rubbing must all be incorporated into the  model.},
  langid = {american},
  annotation = {Accepted: 2004-10-20T19:55:38Z},
  file = {C:\Users\benja\Zotero\storage\VHQM4RQA\6803.html}
}

@article{tuxfamily.,
  title = {Basic Linear Solving by {{Eigen}}},
  author = {{tuxfamily}}
}

@inproceedings{udeOrientationCartesianSpace2014,
  title = {Orientation in {{Cartesian}} Space Dynamic Movement Primitives},
  booktitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Ude, Ale{\v s} and Nemec, Bojan and Petri{\'c}, Tadej and Morimoto, Jun},
  year = {2014},
  month = may,
  pages = {2997--3004},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2014.6907291},
  urldate = {2024-04-22},
  abstract = {Dynamic movement primitives (DMPs) were proposed as an efficient way for learning and control of complex robot behaviors. They can be used to represent point-to-point and periodic movements and can be applied in Cartesian or in joint space. One problem that arises when DMPs are used to define control policies in Cartesian space is that there exists no minimal, singularity-free representation of orientation. In this paper we show how dynamic movement primitives can be defined for non minimal, singularity free representations of orientation, such as rotation matrices and quaternions. All of the advantages of DMPs, including ease of learning, the ability to include coupling terms, and scale and temporal invariance, can be adopted in our formulation. We have also proposed a new phase stopping mechanism to ensure full movement reproduction in case of perturbations.},
  keywords = {Angular velocity,Differential equations,Equations,Mathematical model,Quaternions,Robots,Trajectory},
  file = {C:\Users\benja\Zotero\storage\XJNCL447\6907291.html}
}

@article{Uhlmann2003,
  title = {Covariance {{Consistency Methods}} for {{Fault-Tolerant Distributed Data Fusion}}},
  author = {Uhlmann, Jeffrey K.},
  year = {2003},
  month = sep,
  journal = {Information Fusion},
  volume = {4},
  number = {3},
  pages = {201--215},
  publisher = {Elsevier},
  doi = {10.1016/S1566-2535(03)00036-8},
  urldate = {2018-05-02},
  abstract = {This paper presents a general, rigorous, and fault-tolerant framework for maintaining consistent mean and covariance estimates in an arbitrary, dynamic, distributed network of information processing nodes. In particular, a solution is provided that addresses the information deconfliction problem that arises when estimates from two or more different nodes are determined to be inconsistent with each other, e.g., when two high precision (small covariance) estimates place the position of a particular object at very different locations. The challenge is to be able to resolve such inconsistencies without having to access and exploit global information to determine which of the estimates is spurious. The solution proposed in this paper is called Covariance Union.}
}

@article{ujikeEncounterPepperCPGEElderly2019,
  title = {Encounter of {{Pepper-CPGE}} for the Elderly and Patients with Schizophrenia: An Innovative Strategy to Improve Patient's Recreation, Rehabilitation, and Communication},
  shorttitle = {Encounter of {{Pepper-CPGE}} for the Elderly and Patients with Schizophrenia},
  author = {Ujike, Shoko and Yasuhara, Yuko and Osaka, Kyoko and Sato, Miki and Catangui, Elmer and Edo, Shoko and Takigawa, Eiji and Mifune, Yoshihiro and Tanioka, Tetsuya and Mifune, Kazushi},
  year = {2019},
  journal = {The journal of medical investigation: JMI},
  volume = {66},
  number = {1.2},
  pages = {50--53},
  issn = {1349-6867},
  doi = {10.2152/jmi.66.50},
  abstract = {In Japan, humanoid robots has been introduced in the medical and elderly care environment. The application program of Care Prevention Gymnastics Exercises for Pepper (Pepper-CPGE) made by Xing Company, Japan is a body-brain gymnastics recreation program for 40 minutes tailored to the elderly's functional level. It consists of moving, watching/healing, and playing. "Move the body" exercise and other active range of motion activities are done according to the music. Pepper-CPGE was introduced as a clinical trial at the Mifune hospital, beginning in May 2018. At the units where clinical trials are done, 80\% of the patients are with mental illness diagnosis with decreased physical functions often moving by wheelchair only. When Pepper-CPGE was introduced, the following changes were observed : (1) communication between patients and nurses during rehabilitation care using Pepper-CPGE was increased ; (2) patients wereinteractive, engaged, and actively participated in the Care Prevention Gymnastic Exercises using Pepper-CPGE ; (3) patients had fun and enjoyed talking to Pepper-CPGE. Interventions using Pepper-CPGE appear to be an effective rehabilitation strategy to increase engagement and participation of elderly patients who require long-term care and rehabilitation. J. Med. Invest. 66 : 50-53, February, 2019.},
  langid = {english},
  pmid = {31064954},
  keywords = {Communication,Exercise Therapy,Gymnastics,Humanoid robot,Humans,Long-term care unit,Psychiatric hospital,Recreation,Rehabilitation,Schizophrenia},
  file = {C:\Users\benja\Zotero\storage\T98QLR4B\Ujike et al. - 2019 - Encounter of Pepper-CPGE for the elderly and patie.pdf}
}

@article{ullahAnalysisDeepNeural2021,
  title = {Analysis of {{Deep Neural Networks}} for {{Human Activity Recognition}} in {{Videos}}---{{A Systematic Literature Review}}},
  author = {Ullah, Hadiqa Aman and Letchmunan, Sukumar and Zia, M. Sultan and Butt, Umair Muneer and Hassan, Fadratul Hafinaz},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {126366--126387},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3110610},
  urldate = {2024-07-19},
  abstract = {From the past few decades, Human activity recognition (HAR) is one of the vital research areas in computer vision in which much research is ongoing. The researcher's focus is shifting towards this area due to its vast range of real-life applications to assist in daily living. Therefore, it is necessary to validate its performance on standard benchmark datasets and state-of-the-art systems before applying it in real-life applications. The primary objective of this Systematic Literature Review (SLR) is to collect existing research on video-based human activity recognition, summarize, and analyze the state-of-the-art deep learning architectures regarding various methodologies, challenges, and issues. The top five scientific databases (such as ACM, IEEE, ScienceDirect, SpringerLink, and Taylor \& Francis) are accessed to accompany this systematic study by summarizing 70 different research articles on human activity recognition after critical review. Human activity recognition in videos is a challenging problem due to its diverse and complex nature. For accurate video classification, extraction of both spatial and temporal features from video sequences is essential. Therefore, this SLR focuses on reviewing the recent advancements in stratified self-deriving feature-based deep learning architectures. Furthermore, it explores various deep learning techniques available for HAR, challenges researchers to face to build a robust model, and state-of-the-art datasets used for evaluation. This SLR intends to provide a baseline for video-based human activity recognition research while emphasizing several challenges regarding human activity recognition accuracy in video sequences using deep neural architectures.},
  keywords = {Activity recognition,Computer architecture,computer vision,deep learning,Deep learning,Feature extraction,Magnetic sensors,Sensors,SLR,Spatio-temporal features,Videos},
  file = {C:\Users\benja\Zotero\storage\44YLGX5K\9530410.html}
}

@article{ulyanovDeepImagePrior2020,
  title = {Deep {{Image Prior}}},
  author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  year = {2020},
  month = jul,
  journal = {International Journal of Computer Vision},
  volume = {128},
  number = {7},
  pages = {1867--1888},
  issn = {1573-1405},
  doi = {10.1007/s11263-020-01303-4},
  urldate = {2020-07-03},
  abstract = {Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity (Code and supplementary material are available at https://dmitryulyanov.github.io/deep\_image\_prior).},
  langid = {english}
}

@article{Umeyama.1991,
  title = {Least-Squares Estimation of Transformation Parameters between Two Point Patterns},
  author = {Umeyama, S.},
  year = {1991},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {13},
  number = {4},
  pages = {376--380},
  issn = {01628828},
  doi = {10.1109/34.88573},
  pagination = {page}
}

@misc{UniversalRobots,
  title = {Universal {{Robots}}},
  urldate = {2020-10-07},
  abstract = {Collaborative robots from Universal Robots are enabling companies of all sizes to use robotic automation in their production environments. Cobots are easy to program, flexible to deploy and collaborative and safe to work alongside},
  howpublished = {https://www.universal-robots.com/},
  file = {C:\Users\benja\Zotero\storage\K7C2TNPT\www.universal-robots.com.html}
}

@misc{universalrobotsPolyScopeManual2018,
  title = {{{PolyScope Manual}}},
  author = {{Universal Robots}},
  year = {2018},
  urldate = {2021-02-25},
  file = {C:\Users\benja\Zotero\storage\B57WNHJ2\Software_Manual_en_Global.pdf}
}

@misc{UniversalRobotsSimulator,
  title = {Universal {{Robots Simulator}} ({{URSim}})},
  howpublished = {Universal Robots AS}
}

@misc{universalrobotsUR5UserManual2017,
  title = {{{UR5 User Manual}}},
  author = {Universal Robots},
  year = {2017},
  urldate = {2020-10-22}
}

@techreport{universalrobotsURScriptProgrammingLanguage2018,
  title = {The {{URScript Programming Language}}},
  author = {{Universal Robots}},
  year = {2018},
  month = dec,
  institution = {Universal Robots},
  urldate = {2020-10-26}
}

@misc{universityofbremenDonbot2023,
  title = {Donbot},
  author = {{University of Bremen}},
  year = {2023},
  journal = {Institute for Artificial Intelligence},
  urldate = {2023-02-15},
  howpublished = {https://ai.uni-bremen.de/research/robots/donbot},
  file = {C:\Users\benja\Zotero\storage\FAK6ZUGA\donbot.html}
}

@misc{unrealengine,
  title = {Unreal {{Engine}}},
  author = {{Epic Games}},
  year = {2019},
  month = apr
}

@misc{ur14,
  title = {Universal {{Robots User Manual}}},
  year = {2014},
  publisher = {Universal Robots},
  urldate = {2017-08-27}
}

@article{urenTechnologyReadinessOrganizational2023,
  title = {Technology Readiness and the Organizational Journey towards {{AI}} Adoption: {{An}} Empirical Study},
  shorttitle = {Technology Readiness and the Organizational Journey towards {{AI}} Adoption},
  author = {Uren, Victoria and Edwards, John S.},
  year = {2023},
  month = feb,
  journal = {International Journal of Information Management},
  volume = {68},
  pages = {102588},
  issn = {0268-4012},
  doi = {10.1016/j.ijinfomgt.2022.102588},
  urldate = {2024-09-10},
  abstract = {Artificial Intelligence (AI) is viewed as having potential for significant economic and social impact. However, its history of boom and bust cycles can make potential adopters wary. A cross-sectional, qualitative study was carried out, with a purposive sample of AI experts from research, development and business functions, to gain a deeper understanding of the adoption process. Technology Readiness Levels were used as a benchmark against which the experts could align their experiences. A model of AI adoption is proposed which embeds an extended version of the People, Processes, Technology lens, incorporating Data. The model suggests that people, process and data readiness are required in addition to technology readiness to achieve long term operational success with AI. The findings further indicate that innovative organizations should build bridges between technical and business functions.},
  keywords = {Artificial Intelligence,Data,IT-business alignment,System development,Technology adoption},
  file = {C\:\\Users\\benja\\Zotero\\storage\\AXUR2G8T\\Uren und Edwards - 2023 - Technology readiness and the organizational journey towards AI adoption An empirical study.pdf;C\:\\Users\\benja\\Zotero\\storage\\66KVZIT7\\S0268401222001220.html}
}

@inproceedings{urieliOptimizingInterdependentSkills2011,
  title = {On Optimizing Interdependent Skills: A Case Study in Simulated {{3D}} Humanoid Robot Soccer},
  shorttitle = {On Optimizing Interdependent Skills},
  booktitle = {10th {{International Conference}} on {{Autonomous Agents}} and {{Multiagent Systems}} ({{AAMAS}} 2011)},
  author = {Urieli, Daniel and MacAlpine, Patrick and Kalyanakrishnan, Shivaram and Bentor, Yinon and Stone, P.},
  year = {2011},
  address = {Taipei, Taiwan},
  abstract = {In several realistic domains an agent's behavior is composed of multiple interdependent skills. For example, consider a humanoid robot that must play soccer, as is the focus of this paper. In order to succeed, it is clear that the robot needs to walk quickly, turn sharply, and kick the ball far. However, these individual skills are ineffective if the robot falls down when switching from walking to turning, or if it cannot position itself behind the ball for a kick.    This paper presents a learning architecture for a humanoid robot soccer agent that has been fully deployed and tested within the RoboCup 3D simulation environment. First, we demonstrate that individual skills such as walking and turning can be parameterized and optimized to match the best performance statistics reported in the literature. These results are achieved through effective use of the CMA-ES optimization algorithm. Next, we describe a framework for optimizing skills in conjunction with one another, a little-understood problem with substantial practical significance. Over several phases of learning, a total of roughly 100--150 parameters are optimized. Detailed experiments show that an agent thus optimized performs comparably with the top teams from the RoboCup 2010 competitions, while taking relatively few man-hours for development.}
}

@article{Urze01,
  title = {Evolution of {{Adaptive Synapses}}: {{Robots}} with {{Fast Adaptive Behavior}} in {{New Environments}}},
  author = {Urzelai, Joseba and Floreano, Dario},
  year = {2001},
  journal = {Evolutionary Computation},
  volume = {9},
  number = {4},
  pages = {495--524}
}

@article{vahrenkampRobotSoftwareFramework2015,
  title = {The Robot Software Framework {{ArmarX}}},
  author = {Vahrenkamp, Nikolaus and W{\"a}chter, Mirko and Kr{\"o}hnert, Manfred and Welke, Kai and Asfour, Tamim},
  year = {2015},
  month = apr,
  journal = {it - Information Technology},
  volume = {57},
  number = {2},
  pages = {99--111},
  publisher = {De Gruyter Oldenbourg},
  issn = {2196-7032},
  doi = {10.1515/itit-2014-1066},
  urldate = {2022-06-24},
  abstract = {With ArmarX we introduce a robot programming environment that has been developed in order to ease the realization of higher level capabilities needed by complex robotic systems such as humanoid robots. ArmarX is built upon the idea that consistent disclosure of the system state strongly facilitates the development process of distributed robot applications. We show the applicability of ArmarX by introducing a robot architecture for a humanoid system and discuss essential aspects based on an exemplary pick and place task. With several tools that are provided by the ArmarX framework, such as graphical user interfaces (GUI) or statechart editors, the programmer is enabled to efficiently build and inspect component based robotics software systems.},
  langid = {english},
  keywords = {component-based software development,robot development environment,Robotics software framework,software for humanoid robots}
}

@inproceedings{vakilinejadIdentificationCompensationPeriodic2019,
  title = {Identification and {{Compensation}} of Periodic Gear Transmission Errors in {{Robot Manipulators}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Industrial Technology}} ({{ICIT}})},
  author = {Vakilinejad, Mohammad and Olabi, Adel and Gibaru, Olivier},
  year = {2019},
  month = feb,
  pages = {126--132},
  issn = {2643-2978},
  doi = {10.1109/ICIT.2019.8754980},
  abstract = {Position and path accuracy are among the obstacles preventing robot manipulators to be more and more used for machining applications. Nonlinear joint errors are among different sources of error in robot positioning causing flactuational imperfections to a desired trajectory of TCP. In this paper we propose a novel method to identify and reduce the fluctuations in path of robot TCP resulting from the nonlinear behavior of joint transmission systems. This method is more time efficient compared to the rest of methods present in the literature. A practical example is presented in which we have reduced the fluctuation domain from 0.15(mm) to 0.09(mm) in an industrial robot arm.},
  keywords = {Gears,Joints,Manipulators,Robot kinematics,Service robots,Trajectory},
  file = {C:\Users\benja\Zotero\storage\KGDGX48Z\8754980.html}
}

@inproceedings{valkovHOUDINILifelongLearning2018,
  title = {{{HOUDINI}}: {{Lifelong Learning}} as {{Program Synthesis}}},
  shorttitle = {{{HOUDINI}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Valkov, Lazar and Chaudhari, Dipak and Srivastava, Akash and Sutton, Charles and Chaudhuri, Swarat},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-04-02}
}

@misc{vallboehmerNehmenWirJetzt2018,
  title = {{,,Nehmen wir jetzt jeden?`` -- Eine Umfrage in deutschen chirurgischen Kliniken - BDC{\textbar}Online}},
  shorttitle = {{,,Nehmen wir jetzt jeden?}},
  author = {Vallb{\"o}hmer, Daniel},
  year = {2018},
  month = oct,
  journal = {BDC},
  urldate = {2024-04-12},
  abstract = {F{\"u}r junge {\"A}rzte f{\"a}llt die aktuelle Stellensituation in Deutschland vor allem in den operativen F{\"a}chern sehr g{\"u}nstig aus. Vor allem in weniger beliebten Regionen hat sich in der Chirurgie ein echter Bewerbermangel eingestellt. In manchen Kliniken bleiben vakante Stellen sogar {\"u}ber Monate unbesetzt.},
  langid = {ngerman},
  file = {C:\Users\benja\Zotero\storage\J34ESND4\nehmen-wir-jetzt-jeden-eine-umfrage-in-deutschen-chirurgischen-kliniken.html}
}

@article{vanbaarSimtoRealTransferLearning2018,
  title = {Sim-to-{{Real Transfer Learning}} Using {{Robustified Controllers}} in {{Robotic Tasks}} Involving {{Complex Dynamics}}},
  author = {{van Baar}, Jeroen and Sullivan, Alan and Cordorel, Radu and Jha, Devesh and Romeres, Diego and Nikovski, Daniel},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.04720 [cs, stat]},
  eprint = {1809.04720},
  primaryclass = {cs, stat},
  urldate = {2019-07-14},
  abstract = {Learning robot tasks or controllers using deep reinforcement learning has been proven effective in simulations. Learning in simulation has several advantages. For example, one can fully control the simulated environment, including halting motions while performing computations. Another advantage when robots are involved, is that the amount of time a robot is occupied learning a task---rather than being productive---can be reduced by transferring the learned task to the real robot. Transfer learning requires some amount of fine-tuning on the real robot. For tasks which involve complex (non-linear) dynamics, the fine-tuning itself may take a substantial amount of time. In order to reduce the amount of fine-tuning we propose to learn robustified controllers in simulation. Robustified controllers are learned by exploiting the ability to change simulation parameters (both appearance and dynamics) for successive training episodes. An additional benefit for this approach is that it alleviates the precise determination of physics parameters for the simulator, which is a non-trivial task. We demonstrate our proposed approach on a real setup in which a robot aims to solve a maze game, which involves complex dynamics due to static friction and potentially large accelerations. We show that the amount of fine-tuning in transfer learning for a robustified controller is substantially reduced compared to a non-robustified controller.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@mastersthesis{vandegarDifferentiableSurrogateModels2020,
  title = {Differentiable {{Surrogate Models}} to {{Solve Nonlinear Inverse Problems}}},
  author = {Vandegar, Maxime},
  year = {2020},
  month = jun,
  address = {Li{\`e}ge, Belgium},
  urldate = {2024-03-31},
  abstract = {Doing inference on a model defining an implicit likelihood that is not known in closed form is called likelihood-free inference. This occurs frequently in engineering and science domains where a simulator is used as a generative model of data, but the likelihood of the generated data is not known and is intractable. Given observed data, we combine the idea of hierarchical Bayesian modeling, empirical Bayes, and neural density estimation with normalizing flow to first learn a surrogate approximation of the model likelihood and then, to learn a prior distribution over the model parameters. The learned prior and the surrogate likelihood further allow to learn a posterior distribution for each observation. This is a general approach to likelihood-free inference, and is especially useful in settings where the simulator is too costly to run at inference time. We show the applicability of our methods on a real physical problem from high energy physics (HEP).},
  chapter = {Universit{\'e} de Li{\`e}ge},
  langid = {english},
  school = {Universit{\'e} de Li{\`e}ge},
  annotation = {Accepted: 2020-07-02T02:10:43Z}
}

@article{vankriekenAnalyzingDifferentiableFuzzy2022,
  title = {Analyzing {{Differentiable Fuzzy Logic Operators}}},
  author = {{van Krieken}, Emile and Acar, Erman and {van Harmelen}, Frank},
  year = {2022},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {302},
  pages = {103602},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2021.103602},
  urldate = {2024-04-03},
  abstract = {The AI community is increasingly putting its attention towards combining symbolic and neural approaches, as it is often argued that the strengths and weaknesses of these approaches are complementary. One recent trend in the literature is weakly supervised learning techniques that employ operators from fuzzy logics. In particular, these use prior background knowledge described in such logics to help the training of a neural network from unlabeled and noisy data. By interpreting logical symbols using neural networks, this background knowledge can be added to regular loss functions, hence making reasoning a part of learning. We study, both formally and empirically, how a large collection of logical operators from the fuzzy logic literature behave in a differentiable learning setting. We find that many of these operators, including some of the most well-known, are highly unsuitable in this setting. A further finding concerns the treatment of implication in these fuzzy logics, and shows a strong imbalance between gradients driven by the antecedent and the consequent of the implication. Furthermore, we introduce a new family of fuzzy implications (called sigmoidal implications) to tackle this phenomenon. Finally, we empirically show that it is possible to use Differentiable Fuzzy Logics for semi-supervised learning, and compare how different operators behave in practice. We find that, to achieve the largest performance improvement over a supervised baseline, we have to resort to non-standard combinations of logical operators which perform well in learning, but no longer satisfy the usual logical laws.},
  keywords = {Fuzzy logic,Learning with constraints,Neural-symbolic AI},
  file = {C:\Users\benja\Zotero\storage\CDN5IMXY\S0004370221001533.html}
}

@inproceedings{varadarajanObjectPartSegmentation2011,
  title = {Object Part Segmentation and Classification in Range Images for Grasping},
  booktitle = {2011 15th {{International Conference}} on {{Advanced Robotics}} ({{ICAR}})},
  author = {Varadarajan, Karthik Mahesh and Vincze, Markus},
  year = {2011},
  month = jun,
  pages = {21--27},
  doi = {10.1109/ICAR.2011.6088647},
  abstract = {Recognition by Components (RBC) has been one of the most conceptually significant frameworks for modeling human visual object recognition. Extension of the model to practical robotic applications have been traditionally limited by the lack of good response in textureless areas in the case of conventional inexpensive stereo cameras as well as by the need for expensive laser based sensor systems to compensate for this deficiency. The recent availability of RGB-D sensors such as the PrimeSense sensor has opened new avenues for practical usage of these sensors for robotic applications such as grasping. In this paper, we present novel algorithms for segmentation of objects and parts from range images with extensions based on semantic cues to yield robust part detection. The detected parts are then parameterized using a superquadric based fitting framework and classified into one of different generic shapes. The categorization of the parts enables rules for grasping the object. This Grasping by Components (GBC) scheme is a natural extension of the RBC framework and provides a scalable framework for grasping of objects. This scheme also permits the grasping of novel objects in the scene, with at least one known grasp affordance.},
  keywords = {Affordance,Cavity resonators,Cognitive Object Recognition,Fitting,Grasping,Grasping by Components,Image edge detection,Image segmentation,Recognition by Components,Robot sensing systems},
  file = {C:\Users\benja\Zotero\storage\QNJHSMV9\6088647.html}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is All You Need},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  series = {{{NIPS}}'17},
  pages = {6000--6010},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2024-09-29},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  isbn = {978-1-5108-6096-4},
  file = {C:\Users\benja\Zotero\storage\DY42DMAD\Vaswani et al. - 2017 - Attention is all you need.pdf}
}

@article{velickovicGraphAttentionNetworks2018,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  month = feb,
  urldate = {2019-07-15},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior...}
}

@phdthesis{velickovicResurgenceStructureDeep2019,
  title = {The Resurgence of Structure in Deep Neural Networks},
  author = {Veli{\v c}kovi{\'c}, Petar},
  year = {2019},
  month = may,
  doi = {10.17863/CAM.39380},
  urldate = {2019-07-10},
  abstract = {Machine learning with deep neural networks ("deep learning") allows for learning complex features directly from raw input data, completely eliminating hand-crafted, "hard-coded" feature extraction from the learning pipeline. This has lead to state-of-the-art performance being achieved across several---previously disconnected---problem domains, including computer vision, natural language processing, reinforcement learning and generative modelling. These success stories nearly universally go hand-in-hand with availability of immense quantities of labelled training examples ("big data") exhibiting simple grid-like structure (e.g. text or images), exploitable through convolutional or recurrent layers. This is due to the extremely large number of degrees-of-freedom in neural networks, leaving their generalisation ability vulnerable to effects such as overfitting. However, there remain many domains where extensive data gathering is not always appropriate, affordable, or even feasible. Furthermore, data is generally organised in more complicated kinds of structure---which most existing approaches would simply discard. Examples of such tasks are abundant in the biomedical space; with e.g. small numbers of subjects available for any given clinical study, or relationships between proteins specified via interaction networks. I hypothesise that, if deep learning is to reach its full potential in such environments, we need to reconsider "hard-coded" approaches---integrating assumptions about inherent structure in the input data directly into our architectures and learning algorithms, through structural inductive biases. In this dissertation, I directly validate this hypothesis by developing three structure-infused neural network architectures (operating on sparse multimodal and graph-structured data), and a structure-informed learning algorithm for graph neural networks, demonstrating significant outperformance of conventional baseline models and algorithms.},
  copyright = {All rights reserved},
  langid = {english},
  school = {University of Cambridge}
}

@inproceedings{velikNeurosymbolicNetworksIntroduction2008,
  title = {Neuro-Symbolic Networks: Introduction to a New Information Processing Principle},
  shorttitle = {Neuro-Symbolic Networks},
  booktitle = {2008 6th {{IEEE International Conference}} on {{Industrial Informatics}}},
  author = {Velik, Rosemarie and Bruckner, Dietmar},
  year = {2008},
  month = jul,
  pages = {1042--1047},
  publisher = {IEEE},
  address = {Daejeon, South Korea},
  doi = {10.1109/INDIN.2008.4618256},
  urldate = {2019-08-02},
  abstract = {Neural networks and symbolic systems are two different approaches to model cognitive functions. Both methods have certain strengths but also suffer from a number of weak points, which are however disparate. By combining characteristics of neural and symbolic processing, these weaknesses could be overcome. This paper introduces a new information processing principle based on so-called neuro-symbolic networks, which incorporates these two approaches. The utility of the suggested principle is outlined for applications in machine perception.},
  isbn = {978-1-4244-2170-1},
  langid = {english}
}

@misc{vempralaChatGPTRoboticsDesign2023,
  title = {{{ChatGPT}} for {{Robotics}}: {{Design Principles}} and {{Model Abilities}}},
  shorttitle = {{{ChatGPT}} for {{Robotics}}},
  author = {Vemprala, Sai and Bonatti, Rogerio and Bucker, Arthur and Kapoor, Ashish},
  year = {2023},
  month = jul,
  number = {arXiv:2306.17582},
  eprint = {2306.17582},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.17582},
  urldate = {2023-08-16},
  abstract = {This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing users to interact with it primarily via natural language instructions. In addition to these studies, we introduce an open-sourced research tool called PromptCraft, which contains a platform where researchers can collaboratively upload and vote on examples of good prompting schemes for robotics applications, as well as a sample robotics simulator with ChatGPT integration, making it easier for users to get started with using ChatGPT for robotics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\VLMHZDJI\2306.html}
}

@inproceedings{vermaImitationProjectedProgrammaticReinforcement2019,
  title = {Imitation-{{Projected Programmatic Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Verma, Abhinav and Le, Hoang and Yue, Yisong and Chaudhuri, Swarat},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-05-01},
  abstract = {We study the problem of programmatic reinforcement learning, in which policies are represented as short programs in a symbolic language. Programmatic policies can be more interpretable, generalizable, and amenable to formal verification than neural policies; however, designing rigorous learning approaches for such policies remains a challenge. Our approach to this challenge - a meta-algorithm called PROPEL - is based on three insights. First, we view our learning task as optimization in policy space, modulo the constraint that the desired policy has a programmatic representation, and solve this optimization problem using a form of mirror descent that takes a gradient step into the unconstrained policy space and then projects back onto the constrained space.  Second, we view the unconstrained policy space as mixing neural and programmatic representations, which enables employing state-of-the-art deep policy gradient approaches.  Third, we cast the projection step as program synthesis via imitation learning, and exploit contemporary combinatorial methods for this task. We present theoretical convergence results for PROPEL and empirically evaluate the approach in three continuous control domains. The experiments show that PROPEL can significantly outperform state-of-the-art approaches for learning programmatic policies.}
}

@article{vermaInterpolationConsistencyTraining2019,
  title = {Interpolation {{Consistency Training}} for {{Semi-Supervised Learning}}},
  author = {Verma, Vikas and Lamb, Alex and Kannala, Juho and Bengio, Yoshua and {Lopez-Paz}, David},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.03825 [cs, stat]},
  eprint = {1903.03825},
  primaryclass = {cs, stat},
  urldate = {2019-05-21},
  abstract = {We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm for training Deep Neural Networks in the semi-supervised learning paradigm. ICT encourages the prediction at an interpolation of unlabeled points to be consistent with the interpolation of the predictions at those points. In classification problems, ICT moves the decision boundary to low-density regions of the data distribution. Our experiments show that ICT achieves state-of-theart performance when applied to standard neural network architectures on the CIFAR-10 and SVHN benchmark datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{vermaNeuralNetworkApproaches2024,
  title = {Neural {{Network Approaches}} for {{Parameterized Optimal Control}}},
  author = {Verma, Deepanshu and Winovich, Nick and Ruthotto, Lars and Waanders, Bart van Bloemen},
  year = {2024},
  month = feb,
  number = {arXiv:2402.10033},
  eprint = {2402.10033},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.10033},
  urldate = {2024-03-08},
  abstract = {We consider numerical approaches for deterministic, finite-dimensional optimal control problems whose dynamics depend on unknown or uncertain parameters. We seek to amortize the solution over a set of relevant parameters in an offline stage to enable rapid decision-making and be able to react to changes in the parameter in the online stage. To tackle the curse of dimensionality arising when the state and/or parameter are high-dimensional, we represent the policy using neural networks. We compare two training paradigms: First, our model-based approach leverages the dynamics and definition of the objective function to learn the value function of the parameterized optimal control problem and obtain the policy using a feedback form. Second, we use actor-critic reinforcement learning to approximate the policy in a data-driven way. Using an example involving a two-dimensional convection-diffusion equation, which features high-dimensional state and parameter spaces, we investigate the accuracy and efficiency of both training paradigms. While both paradigms lead to a reasonable approximation of the policy, the model-based approach is more accurate and considerably reduces the number of PDE solves.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control},
  file = {C:\Users\benja\Zotero\storage\GEZZEE9Z\2402.html}
}

@incollection{vernonCognitiveArchitectures2022,
  title = {Cognitive {{Architectures}}},
  booktitle = {Cognitive {{Robotics}}},
  author = {Vernon, David},
  editor = {Cangelosi, Angelo and Asada, Minoru},
  year = {2022},
  month = may,
  series = {Intelligent {{Robotics}} and {{Autonomous Agents}}},
  publisher = {MIT Press},
  address = {Cambridge, Massachusetts},
  urldate = {2024-01-24},
  isbn = {978-0-262-36932-9},
  langid = {english}
}

@incollection{vernonICubCognitiveArchitecture2011,
  title = {The {{iCub Cognitive Architecture}}},
  booktitle = {A {{Roadmap}} for {{Cognitive Development}} in {{Humanoid Robots}}},
  author = {Vernon, David and {von Hofsten}, Claes and Fadiga, Luciano},
  editor = {Vernon, David and {von Hofsten}, Claes and Fadiga, Luciano},
  year = {2011},
  series = {Cognitive {{Systems Monographs}}},
  pages = {121--153},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-16904-5_7},
  urldate = {2022-06-27},
  abstract = {In this chapter, we examine how the roadmap guidelines set out in Chap. 6 have influenced the design of a cognitive architecture for the iCub humanoid robot. We begin with a overview of the iCub and we discuss briefly the iCub mechatronics and software infrastructure. We then describe the iCub cognitive architecture, focussing the selection of a minimal set of phylogenetic capabilities derived from the seven groups of roadmap guidelines. Since the iCub cognitive architecture is a work-in-progress, it represents only a partial implementation of the roadmap guidelines. Consequently, we close the chapter by examining the exact extent to which each guideline has been followed. The next chapter, which concludes the book, addresses some of the research challenges posed by a complete implementation of the roadmap guidelines.},
  isbn = {978-3-642-16904-5},
  langid = {english},
  keywords = {Attention System,Episodic Memory,Humanoid Robot,Interaural Time Difference,Smooth Pursuit}
}

@article{vernonProspectionCognitionCase2015,
  title = {Prospection in {{Cognition}}: {{The Case}} for {{Joint Episodic-Procedural Memory}} in {{Cognitive Robotics}}},
  shorttitle = {Prospection in {{Cognition}}},
  author = {Vernon, David and Beetz, Michael and Sandini, Giulio},
  year = {2015},
  month = jul,
  journal = {Frontiers in Robotics and AI},
  volume = {2},
  publisher = {Frontiers},
  issn = {2296-9144},
  doi = {10.3389/frobt.2015.00019},
  urldate = {2024-09-21},
  abstract = {{$<$}p{$>$}Prospection lies at the core of cognition: it is the means by which an agent -- a person or a cognitive robot -- shifts its perspective from immediate sensory experience to anticipate future events, be they the actions of other agents or the outcome of its own actions. Prospection, accomplished by internal simulation, requires mechanisms for both perceptual imagery and motor imagery. While it is known that these two forms of imagery are tightly entwined in the mirror neuron system, we do not yet have an effective model of the mentalizing network which would provide a framework to integrate declarative episodic and procedural memory systems and to combine experiential knowledge with skillful know-how. Such a framework would be founded on joint perceptuo-motor representations. In this paper, we examine the case for this form of representation, contrasting sensory-motor theory with ideo-motor theory, and we discuss how such a framework could be realized by joint episodic-procedural memory. We argue that such a representation framework has several advantages for cognitive robots. Since episodic memory operates by recombining imperfectly recalled past experience, this allows it to simulate new or unexpected events. Furthermore, by virtue of its associative nature, joint episodic-procedural memory allows the internal simulation to be conditioned by current context, semantic memory, and the agent's value system. Context and semantics constrain the combinatorial explosion of potential perception-action associations and allow effective action selection in the pursuit of goals, while the value system provides the motives that underpin the agent's autonomy and cognitive development. This joint episodic-procedural memory framework is neutral regarding the final implementation of these episodic and procedural memories, which can be configured sub-symbolically as associative networks or symbolically as content-addressable image databases and databases of motor-control scripts.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Attention,autonomy,cognitive system,development,episodic memory,ideo-motor theory,internal simulation,perceptuo-motor representation,procedural memory,prospection,value system},
  file = {C:\Users\benja\Zotero\storage\YN2VHDIB\Vernon et al. - 2015 - Prospection in Cognition The Case for Joint Episodic-Procedural Memory in Cognitive Robotics.pdf}
}

@article{villaniSurveyHumanRobotInteraction2018,
  title = {Survey on {{Human-Robot Interaction}} for {{Robot Programming}} in {{Industrial Applications}}},
  author = {Villani, Valeria and Pini, Fabio and Leali, Francesco and Secchi, Cristian and Fantuzzi, Cesare},
  year = {2018},
  journal = {IFAC-PapersOnLine},
  volume = {51},
  number = {11},
  pages = {66--71},
  issn = {24058963},
  doi = {10.1016/j.ifacol.2018.08.236},
  urldate = {2019-05-17},
  langid = {english}
}

@inproceedings{vincalekItsJourneyNot2021,
  title = {It's the Journey Not the Destination: Building Genetic Algorithms Practitioners Can Trust},
  shorttitle = {It's the Journey Not the Destination},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {Vincalek, Jakub and Walton, Sean and Evans, Ben},
  year = {2021},
  month = jul,
  series = {{{GECCO}} '21},
  pages = {231--232},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3449726.3459483},
  urldate = {2023-06-01},
  abstract = {This poster paper presents 2 recommendations for algorithm developers as best practices in the context of engineering design. Genetic algorithms are well suited for multi-objective optimisation problems which are common in engineering. However, the use of genetic algorithms in industry remains low. To understand why, 23 participants (N = 23) with varying degrees of expertise in the domain of engineering design took part in a 3-part mixed methods survey. The open-ended questions in the survey were analysed using reflexive thematic analysis. A common theme among participants was a lack of trust towards the results of genetic algorithms, as well as the process which the algorithms take to reach a result. Based on this, the following recommendations are made: better communication between developers and engineers, and visualising algorithm behaviour.},
  isbn = {978-1-4503-8351-6},
  keywords = {design optimisation,human in the loop,optimisation algorithms,reflexive thematic analysis}
}

@book{VittorioScarano.2008,
  title = {Eurographics Italian Chapter Conference},
  editor = {{Vittorio Scarano} and {Rosario De Chiara} and {Ugo Erra}},
  year = {2008},
  publisher = {The Eurographics Association},
  isbn = {978-3-905673-68-5}
}

@article{vocetkaInfluenceDriftRobot2021,
  title = {Influence of {{Drift}} on {{Robot Repeatability}} and {{Its Compensation}}},
  author = {Vocetka, Michal and Bobovsk{\'y}, Zdenko and Babjak, Jan and Suder, Ji{\v r}{\'i} and Grushko, Stefan and Mlotek, Jakub and Krys, V{\'a}clav and Hagara, Martin},
  year = {2021},
  month = jan,
  journal = {Applied Sciences},
  volume = {11},
  number = {22},
  pages = {10813},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app112210813},
  urldate = {2022-08-01},
  abstract = {This paper presents an approach to compensate for the effect of thermal expansion on the structure of an industrial robot and thus to reduce the repeatability difference of the robot in cold and warm conditions. In contrast to previous research in this area that deals with absolute accuracy, this article is focused on determining achievable repeatability. To unify and to increase the robot repeatability, the measurements with highly accurate sensors were performed under different conditions on an industrial robot ABB IRB1200, which was equipped with thermal sensors, mounted on a pre-defined position around joints. The performed measurements allowed to implement a temperature-based prediction model of the end effector positioning error. Subsequent tests have shown that the implemented model used for the error compensation proved to be highly effective. Using the methodology presented in this article, the impact of drift can be reduced by up to 89.9\%. A robot upgraded with a compensation principle described in this article does not have to be warmed up as it works with the same low repeatability error in the entire range of the achievable temperatures.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {robot drift,robot precision,robot repeatability,robot warm-up}
}

@article{vollmerUserStudyRobot2018,
  title = {A {{User Study}} on {{Robot Skill Learning Without}} a {{Cost Function}}: {{Optimization}} of {{Dynamic Movement Primitives}} via {{Naive User Feedback}}},
  shorttitle = {A {{User Study}} on {{Robot Skill Learning Without}} a {{Cost Function}}},
  author = {Vollmer, Anna-Lisa and Hemion, Nikolas J.},
  year = {2018},
  journal = {Frontiers in Robotics and AI},
  volume = {5},
  issn = {2296-9144},
  urldate = {2020-10-30},
  abstract = {Enabling users to teach their robots new tasks at home is a major challenge for research in personal robotics. This work presents a user study in which participants were asked to teach the robot Pepper a game of skill. The robot was equipped with a state-of-the-art skill learning method, based on dynamic movement primitives (DMPs). The only feedback participants could give was a discrete rating after each of Pepper's movement executions (``very good,'' ``good,'' ``average,'' ``not so good,'' ``not good at all''). We compare the learning performance of the robot when applying user-provided feedback with a version of the learning where an objectively determined cost via hand-coded cost function and external tracking system is applied. Our findings suggest that (a) an intuitive graphical user interface for providing discrete feedback can be used for robot learning of complex movement skills when using DMP-based optimization, making the tedious definition of a cost function obsolete; and (b) un-experienced users with no knowledge about the learning algorithm naturally tend to apply a working rating strategy, leading to similar learning performance as when using the objectively determined cost. We discuss insights about difficulties when learning from user provided feedback, and make suggestions how learning continuous movement skills from non-expert humans could be improved.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\KYG9V2Y8\2921123.html}
}

@article{vonruedenInformedMachineLearning2021,
  title = {Informed {{Machine Learning}} - {{A Taxonomy}} and {{Survey}} of {{Integrating Prior Knowledge}} into {{Learning Systems}}},
  author = {{von Rueden}, Laura and Mayer, Sebastian and Beckh, Katharina and Georgiev, Bogdan and Giesselbach, Sven and Heese, Raoul and Kirsch, Birgit and Walczak, Michal and Pfrommer, Julius and Pick, Annika and Ramamurthy, Rajkumar and Garcke, Jochen and Bauckhage, Christian and Schuecker, Jannis},
  year = {2021},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--1},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2021.3079836},
  abstract = {Despite its great success, machine learning can have its limits when dealing with insufficient training data. A potential solution is the additional integration of prior knowledge into the training process which leads to the notion of informed machine learning. In this paper, we present a structured overview of various approaches in this field. We provide a definition and propose a concept for informed machine learning which illustrates its building blocks and distinguishes it from conventional machine learning. We introduce a taxonomy that serves as a classification framework for informed machine learning approaches. It considers the source of knowledge, its representation, and its integration into the machine learning pipeline. Based on this taxonomy, we survey related research and describe how different knowledge representations such as algebraic equations, logic rules, or simulation results can be used in learning systems. This evaluation of numerous papers on the basis of our taxonomy uncovers key methods in the field of informed machine learning.},
  keywords = {Expert Knowledge,Hybrid,Informed,Machine learning,Machine Learning,Mathematical model,Neuro-Symbolic,Pipelines,Prior Knowledge,Survey,Systematics,Taxonomy,Training,Training data},
  file = {C:\Users\benja\Zotero\storage\9Z5SXIV8\9429985.html}
}

@misc{Vreeken2003,
  title = {Spiking Neural Networks, an Introduction},
  author = {Vreeken, Jilles},
  year = {2003},
  publisher = {{Utrecht University: Information and Computing Sciences}}
}

@article{Vreekena,
  title = {Spiking Neural Networks, an Introduction},
  author = {Vreeken, Jilles},
  urldate = {2018-05-10},
  abstract = {Biological neurons use short and sudden increases in voltage to send information. These signals are more commonly known as action potentials, spikes or pulses. Recent neurological research has shown that neurons encode information in the timing of single spikes, and not only just in their average firing frequency. This paper gives an introduction to spiking neural networks, some biological background, and will present two models of spiking neurons that employ pulse coding. Networks of spiking neurons are more powerful than their non-spiking predecessors as they can encode temporal information in their signals, but therefore do also need different and biologically more plausible rules for synaptic plasticity. You constantly receive sensory input from your environ-ment. You process this information, recognizing food or dan-ger, and take appropriate actions. Not only you; anything that interacts with its environment needs to do so. Mimicking such a seemingly simple mechanism in a robot proofs to be insanely difficult. Nature must laugh at our feeble attempts; animals perform this behaviour with apparent ease. The reason for this mind-boggling performance lies in their neural structure or 'brain'. Millions and millions of neurons are interconnected with each other and cooperate to efficiently process incoming signals and decide on actions. A typical neuron sends its signals out to over 10.000 other neu-rons, making it clear to even to inexpert reader that the signal flow is rather complicated. To put it mildly: we do not under-stand the brain that well yet. In fact, we do not even com-pletely understand the functioning of a single neuron. The chemical activity of the synapse already proves to be infi-nitely more complex than firstly assumed. However, the rough concept of how neurons work is un-derstood: neurons send out short pulses of electrical energy as signals, if they have received enough of these themselves. This basically simple mechanism has been moulded into a mathematical model for computer use. Artificial as these computerised neurons are, we refer to them as networks of artificial neurons, or artificial neural networks. We will sketch a short history of these now; the biological back-ground of the real neuron will be drawn in the next chapter. Generations of artificial neurons}
}

@misc{vuCOASTConstraintsStreams2024,
  title = {{{COAST}}: {{Constraints}} and {{Streams}} for {{Task}} and {{Motion Planning}}},
  shorttitle = {{{COAST}}},
  author = {Vu, Brandon and Migimatsu, Toki and Bohg, Jeannette},
  year = {2024},
  month = may,
  number = {arXiv:2405.08572},
  eprint = {2405.08572},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.08572},
  urldate = {2024-07-21},
  abstract = {Task and Motion Planning (TAMP) algorithms solve long-horizon robotics tasks by integrating task planning with motion planning; the task planner proposes a sequence of actions towards a goal state and the motion planner verifies whether this action sequence is geometrically feasible for the robot. However, state-of-the-art TAMP algorithms do not scale well with the difficulty of the task and require an impractical amount of time to solve relatively small problems. We propose Constraints and Streams for Task and Motion Planning (COAST), a probabilistically-complete, sampling-based TAMP algorithm that combines stream-based motion planning with an efficient, constrained task planning strategy. We validate COAST on three challenging TAMP domains and demonstrate that our method outperforms baselines in terms of cumulative task planning time by an order of magnitude. You can find more supplementary materials on our project {\textbackslash}href\{https://branvu.github.io/coast.github.io\}\{website\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\LTJEJFR4\2405.html}
}

@techreport{w3cRDF11Turtle2014,
  title = {{{RDF}} 1.1 {{Turtle}}: {{Terse RDF Triple Language}}},
  author = {{W3C}},
  year = {2014},
  month = feb,
  institution = {World Wide Web Consortium},
  urldate = {2024-07-06},
  file = {C:\Users\benja\Zotero\storage\IHUQRL5G\turtle.html}
}

@article{wachterArmarXStatechartConcept2016,
  title = {The {{ArmarX Statechart Concept}}: {{Graphical Programing}} of {{Robot Behavior}}},
  shorttitle = {The {{ArmarX Statechart Concept}}},
  author = {W{\"a}chter, Mirko and Ottenhaus, Simon and Kr{\"o}hnert, M. and Vahrenkamp, N. and Asfour, T.},
  year = {2016},
  journal = {Front. Robot. AI},
  doi = {10.3389/frobt.2016.00033},
  abstract = {This work introduces the ArmarX statechart concept, which is used for describing control and data flow of robot programs, and shows that using hierarchical and distributed statecharts increases reusability, allows skill transfer between robots, and hides complexity in robot programming. Programming sophisticated robots such as service robots or humanoids is still a complex endeavor. Although programming robotic applications requires specialist knowledge, a robot software environment should support convenient development while maintaining full flexibility needed when realizing challenging robotics tasks. In addition, several desirable properties should be fulfilled, such as robustness, reusability of existing programs, and skill transfer between robots. In this work, we introduce the ArmarX statechart concept, which is used for describing control and data flow of robot programs. This event-driven statechart approach of ArmarX helps realizing important features such as increased robustness through distributed program execution, convenient programming through graphical user interfaces, and versatility by interweaving dynamic statechart structure with custom user-code. We show that using hierarchical and distributed statecharts increases reusability, allows skill transfer between robots, and hides complexity in robot programming by splitting robot behavior into control flow and functionality.}
}

@inproceedings{wagnerHoLLiECaresMultifunktionalerRoboter2020,
  title = {{{HoLLiECares}}. {{Ein}} Multifunktionaler {{Roboter}} F{\"u}r Die Professionelle {{Pflege}}},
  booktitle = {Kann {{Digital Pflege}}?},
  author = {Wagner, Felix and Schuh, Svea and Gebert, Anne and Hermann, Andreas and Lengenfelder, Christian and R{\"o}nnau, Arne and Steffen, Lea and Greff, Tobias and Werth, Dirk},
  year = {2020},
  month = sep,
  pages = {94--97},
  publisher = {Pflegepraxiszentrum N{\"u}rnberg},
  address = {N{\"u}rnberg},
  abstract = {Der Einsatz robotischer Assistenzsysteme im Bereich der Pflege ist auch in Deutschland in der Diskussion. Die Entwicklung erster, zumeist prototypischer Systeme f{\"u}r den Praxiseinsatz hat bereits begonnen. Hintergrund f{\"u}r diese Entwicklungen sind neben den technologischen M{\"o}glichkeiten der demografische Wandel, der Fachkr{\"a}ftemangel und die hohen physischen und psychischen Belastungen des Pflegepersonals. Die Annahme ist, dass es im Umfeld der professionellen Pflege T{\"a}tigkeiten gibt, die durch den Einsatzrobotischer Assistenzsysteme {\"u}bernommen, beschleunigt oder vereinfacht werden k{\"o}nnen. Im Rahmen des Projektes HoLLiECares sollen Einsatzbereiche f{\"u}r Robotik im Krankenhaus identifiziert werden und passgenaue, prototypische L{\"o}sungen f{\"u}r den Einsatz eines multifunktionalen Roboters entwickelt werden.}
}

@article{wagnerLearningRobotCognitive2021,
  title = {A Learning Robot for Cognitive Camera Control in Minimally Invasive Surgery},
  author = {Wagner, Martin and Bihlmaier, Andreas and Kenngott, Hannes G{\"o}tz and Mietkowski, Patrick and Scheikl, Paul Maria and Bodenstedt, Sebastian and {Schiepe-Tiska}, Anja and Vetter, Josephin and Nickel, Felix and Speidel, S. and W{\"o}rn, H. and {Mathis-Ullrich}, F. and {M{\"u}ller-Stich}, B. P.},
  year = {2021},
  month = sep,
  journal = {Surgical Endoscopy},
  volume = {35},
  number = {9},
  pages = {5365--5374},
  issn = {1432-2218},
  doi = {10.1007/s00464-021-08509-8},
  abstract = {BACKGROUND: We demonstrate the first self-learning, context-sensitive, autonomous camera-guiding robot applicable to minimally invasive surgery. The majority of surgical robots nowadays are telemanipulators without autonomous capabilities. Autonomous systems have been developed for laparoscopic camera guidance, however following simple rules and not adapting their behavior to specific tasks, procedures, or surgeons. METHODS: The herein presented methodology allows different robot kinematics to perceive their environment, interpret it according to a knowledge base and perform context-aware actions. For training, twenty operations were conducted with human camera guidance by a single surgeon. Subsequently, we experimentally evaluated the cognitive robotic camera control. A VIKY~EP system and a KUKA LWR 4 robot were trained on data from manual camera guidance after completion of the surgeon's learning curve. Second, only data from VIKY EP were used to train the LWR and finally data from training with the LWR were used to re-train the LWR. RESULTS: The duration of each operation decreased with the robot's increasing experience from 1704~s\,{\textpm}\,244~s to 1406~s\,{\textpm}\,112~s, and 1197~s. Camera guidance quality (good/neutral/poor) improved from 38.6/53.4/7.9 to 49.4/46.3/4.1\% and 56.2/41.0/2.8\%. CONCLUSIONS: The cognitive camera robot improved its performance with experience, laying the foundation for a new generation of cognitive surgical robots that adapt to a surgeon's needs.},
  langid = {english},
  pmcid = {PMC8346448},
  pmid = {33904989},
  keywords = {Artificial intelligence,Cognition,Cognitive surgical robotics,Colorectal surgery,Humans,Laparoscopy,Learning Curve,Machine learning,Minimally Invasive Surgical Procedures,Robotics,Surgical data science}
}

@article{wagnerSurgomicsPersonalizedPrediction2022,
  title = {Surgomics: Personalized Prediction of Morbidity, Mortality and Long-Term Outcome in Surgery Using Machine Learning on Multimodal Data},
  shorttitle = {Surgomics},
  author = {Wagner, Martin and Brandenburg, Johanna M. and Bodenstedt, Sebastian and Schulze, Andr{\'e} and Jenke, Alexander C. and Stern, Antonia and Daum, Marie T. J. and M{\"u}ndermann, Lars and Kolbinger, Fiona R. and Bhasker, Nithya and Schneider, Gerd and {Krause-J{\"u}ttler}, Grit and Alwanni, Hisham and {Fritz-Kebede}, Fleur and Burgert, Oliver and Wilhelm, Dirk and Fallert, Johannes and Nickel, Felix and {Maier-Hein}, Lena and Dugas, Martin and Distler, Marius and Weitz, J{\"u}rgen and {M{\"u}ller-Stich}, Beat-Peter and Speidel, Stefanie},
  year = {2022},
  month = nov,
  journal = {Surgical Endoscopy},
  volume = {36},
  number = {11},
  pages = {8568--8591},
  issn = {1432-2218},
  doi = {10.1007/s00464-022-09611-1},
  abstract = {BACKGROUND: Personalized medicine requires the integration and analysis of vast amounts of patient data to realize individualized care. With Surgomics, we aim to facilitate personalized therapy recommendations in surgery by integration of intraoperative surgical data and their analysis with machine learning methods to leverage the potential of this data in analogy to Radiomics and Genomics. METHODS: We defined Surgomics as the entirety of surgomic features that are process characteristics of a surgical procedure automatically derived from multimodal intraoperative data to quantify processes in the operating room. In a multidisciplinary team we discussed potential data sources like endoscopic videos, vital sign monitoring, medical devices and instruments and respective surgomic features. Subsequently, an online questionnaire was sent to experts from surgery and (computer) science at multiple centers for rating the features' clinical relevance and technical feasibility. RESULTS: In total, 52 surgomic features were identified and assigned to eight feature categories. Based on the expert survey (n\,=\,66 participants) the feature category with the highest clinical relevance as rated by surgeons was "surgical skill and quality of performance" for morbidity and mortality (9.0\,{\textpm}\,1.3 on a numerical rating scale from 1 to 10) as well as for long-term (oncological) outcome (8.2\,{\textpm}\,1.8). The feature category with the highest feasibility to be automatically extracted as rated by (computer) scientists was "Instrument" (8.5\,{\textpm}\,1.7). Among the surgomic features ranked as most relevant in their respective category were "intraoperative adverse events", "action performed with instruments", "vital sign monitoring", and "difficulty of surgery". CONCLUSION: Surgomics is a promising concept for the analysis of intraoperative data. Surgomics may be used together with preoperative features from clinical data and Radiomics to predict postoperative morbidity, mortality and long-term outcome, as well as to provide tailored feedback for surgeons.},
  langid = {english},
  pmcid = {PMC9613751},
  pmid = {36171451},
  keywords = {Artificial intelligence,Humans,Machine Learning,Minimally invasive surgery,Morbidity,Precision medicine,Prediction model,Radiomics,Surgeons,Surgical data science}
}

@article{wakeChatGPTEmpoweredLongStep2023,
  title = {{{ChatGPT Empowered Long-Step Robot Control}} in {{Various Environments}}: {{A Case Application}}},
  shorttitle = {{{ChatGPT Empowered Long-Step Robot Control}} in {{Various Environments}}},
  author = {Wake, Naoki and Kanehira, Atsushi and Sasabuchi, Kazuhiro and Takamatsu, Jun and Ikeuchi, Katsushi},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  eprint = {2304.03893},
  primaryclass = {cs},
  pages = {95060--95078},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3310935},
  urldate = {2024-04-30},
  abstract = {This paper demonstrates how OpenAI's ChatGPT can be used in a few-shot setting to convert natural language instructions into a sequence of executable robot actions. The paper proposes easy-to-customize input prompts for ChatGPT that meet common requirements in practical applications, such as easy integration with robot execution systems and applicability to various environments while minimizing the impact of ChatGPT's token limit. The prompts encourage ChatGPT to output a sequence of predefined robot actions, represent the operating environment in a formalized style, and infer the updated state of the operating environment. Experiments confirmed that the proposed prompts enable ChatGPT to act according to requirements in various environments, and users can adjust ChatGPT's output with natural language feedback for safe and robust operation. The proposed prompts and source code are open-source and publicly available at https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\VKECUR9B\2304.html}
}

@incollection{waldonKinematics2008,
  title = {Kinematics},
  booktitle = {Springer {{Handbook}} of {{Robotics}}},
  author = {Waldon, Kenneth J. and Schmiedeler, James},
  editor = {Siciliano, Bruno and Khatib, Oussama},
  year = {2008},
  pages = {11--35},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  langid = {english}
}

@article{wanDifferentiableTrajectoryOptimization2023,
  title = {Differentiable {{Trajectory Optimization}} as a {{Policy Class}} for {{Reinforcement}} and {{Imitation Learning}}},
  author = {Wan, Weikang and Wang, Yufei and Erickson, Zackory and Held, David},
  year = {2023},
  month = oct,
  urldate = {2024-09-07},
  abstract = {This paper introduces DiffTOP, a new policy class for reinforcement learning and imitation learning that utilizes differentiable trajectory optimization to generate the policy actions. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end, e.g., using the policy gradient loss in reinforcement learning, or using the imitation loss in imitation learning. When applied to model-based reinforcement learning, DiffTOP addresses the ``objective mismatch'' issue of prior algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. When applied to imitation learning, DiffTOP performs test-time trajectory optimization to compute the actions with a learned cost function, outperforming prior methods that only perform forward passes of the policy network to generate actions. We benchmark DiffTOP on 15 model-based RL tasks, and 13 imitation learning tasks with high-dimensional image and point cloud inputs, and show that it outperforms prior state-of-the-art methods in both domains.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\G7FPVKBW\Wan et al. - 2023 - Differentiable Trajectory Optimization as a Policy Class for Reinforcement and Imitation Learning.pdf}
}

@inproceedings{Wang.2018,
  title = {A Unified Controller for Region-Reaching and Deforming of Soft Objects},
  booktitle = {2018 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems ({{IROS}})},
  author = {Wang, Zerui and Li, Xiang and {Navarro-Alarcon}, David and Liu, Yun-hui},
  year = {2018},
  pages = {472--478},
  publisher = {IEEE},
  doi = {10.1109/IROS.2018.8593543},
  bookpagination = {page},
  isbn = {978-1-5386-8094-0}
}

@incollection{wangBackpropagationCallbacksFoundations2018,
  title = {Backpropagation with {{Callbacks}}: {{Foundations}} for {{Efficient}} and {{Expressive Differentiable Programming}}},
  shorttitle = {Backpropagation with {{Callbacks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Wang, Fei and Decker, James and Wu, Xilun and Essertel, Gregory and Rompf, Tiark},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {10180--10191},
  publisher = {Curran Associates, Inc.},
  urldate = {2020-10-19},
  file = {C\:\\Users\\benja\\Zotero\\storage\\P4SHQLTP\\8221-backpropagation-with-callbacks-foundations-for-efficient-and-expressive-differentiable-pro.html;C\:\\Users\\benja\\Zotero\\storage\\U2XJAV5T\\8221-backpropagation-with-callbacks-foundations-for-efficient-and-expressive-differentiable-pro.html}
}

@article{wangDynamicGraphCNN2018,
  title = {Dynamic {{Graph CNN}} for {{Learning}} on {{Point Clouds}}},
  author = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},
  year = {2018},
  month = jan,
  journal = {arXiv:1801.07829 [cs]},
  eprint = {1801.07829},
  primaryclass = {cs},
  urldate = {2019-10-10},
  abstract = {Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks including ModelNet40, ShapeNetPart, and S3DIS.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{wangExecutableCodeActions2024,
  title = {Executable {{Code Actions Elicit Better LLM Agents}}},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Machine Learning}}},
  author = {Wang, Xingyao and Chen, Yangyi and Yuan, Lifan and Zhang, Yizhe and Li, Yunzhu and Peng, Hao and Ji, Heng},
  year = {2024},
  month = jul,
  pages = {50208--50232},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-08-28},
  abstract = {Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20\% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\L8R9U26L\Wang et al. - 2024 - Executable Code Actions Elicit Better LLM Agents.pdf}
}

@article{wangGeneralizingFewExamples2020,
  title = {Generalizing from a {{Few Examples}}: {{A Survey}} on {{Few-shot Learning}}},
  shorttitle = {Generalizing from a {{Few Examples}}},
  author = {Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
  year = {2020},
  month = jun,
  journal = {ACM Computing Surveys},
  volume = {53},
  number = {3},
  pages = {63:1--63:34},
  issn = {0360-0300},
  doi = {10.1145/3386252},
  urldate = {2021-05-12},
  abstract = {Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research.1},
  keywords = {Few-shot learning,low-shot learning,meta-learning,one-shot learning,prior knowledge,small sample learning}
}

@article{wangGeneralizingFewExamples2020a,
  title = {Generalizing from a {{Few Examples}}: {{A Survey}} on {{Few-shot Learning}}},
  shorttitle = {Generalizing from a {{Few Examples}}},
  author = {Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
  year = {2020},
  month = jun,
  journal = {ACM Computing Surveys},
  volume = {53},
  number = {3},
  pages = {63:1--63:34},
  issn = {0360-0300},
  doi = {10.1145/3386252},
  urldate = {2024-06-13},
  abstract = {Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research.1},
  keywords = {Few-shot learning,low-shot learning,meta-learning,one-shot learning,prior knowledge,small sample learning}
}

@article{wangGeneralizingUnseenDomains2023,
  title = {Generalizing to {{Unseen Domains}}: {{A Survey}} on {{Domain Generalization}}},
  shorttitle = {Generalizing to {{Unseen Domains}}},
  author = {Wang, Jindong and Lan, Cuiling and Liu, Chang and Ouyang, Yidong and Qin, Tao and Lu, Wang and Chen, Yiqiang and Zeng, Wenjun and Yu, Philip S.},
  year = {2023},
  month = aug,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {35},
  number = {8},
  pages = {8052--8072},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2022.3178128},
  urldate = {2024-03-30},
  abstract = {Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets, applications, and our open-sourced codebase for fair evaluation. Finally, we summarize existing literature and present some potential research topics for the future.},
  keywords = {Adaptation models,Computational modeling,Data models,domain adaptation,Domain generalization,Multitasking,out-of-distribution generalization,Predictive models,Task analysis,Training,transfer learning},
  file = {C:\Users\benja\Zotero\storage\C9LKI3HE\9782500.html}
}

@inproceedings{wangGenSimGeneratingRobotic2023,
  title = {{{GenSim}}: {{Generating Robotic Simulation Tasks}} via {{Large Language Models}}},
  shorttitle = {{{GenSim}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Wang, Lirui and Ling, Yiyang and Yuan, Zhecheng and Shridhar, Mohit and Bao, Chen and Qin, Yuzhe and Wang, Bailin and Xu, Huazhe and Wang, Xiaolong},
  year = {2023},
  month = oct,
  urldate = {2024-04-30},
  abstract = {Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scene-level diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language models' (LLM) grounding and coding ability. Our approach, dubbed GenSim, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM bootstraps from previous tasks and iteratively proposes novel tasks that would be helpful in solving more complex tasks. We use GPT4 to expand the existing benchmark by ten times to over 100 tasks, on which we conduct supervised finetuning and evaluate several LLMs including finetuned GPTs and Code Llama on code generation for robotic simulation tasks. Furthermore, we observe that LLMs-generated simulation programs can enhance task-level generalization significantly when used for multitask policy training. We further find that with minimal sim-to-real adaptation, the multitask policies pretrained on GPT4-generated simulation tasks exhibit stronger transfer to unseen long-horizon tasks in the real world and outperform baselines by 25\%. See our project website (https://gen-sim.github.io) and demo (https://huggingface.co/spaces/Gen-Sim/Gen-Sim) for visualizations and open-source models and datasets.},
  langid = {english}
}

@misc{wangImprovementHeavyLoad2020,
  type = {Research {{Article}}},
  title = {Improvement of {{Heavy Load Robot Positioning Accuracy}} by {{Combining}} a {{Model-Based Identification}} for {{Geometric Parameters}} and an {{Optimized Neural Network}} for the {{Compensation}} of {{Nongeometric Errors}}},
  author = {Wang, Yuxiang and Chen, Zhangwei and Zu, Hongfei and Zhang, Xiang and Mao, Chentao and Wang, Zhirong},
  year = {2020},
  month = jan,
  journal = {Complexity},
  volume = {2020},
  pages = {e5896813},
  publisher = {Hindawi},
  issn = {1076-2787},
  doi = {10.1155/2020/5896813},
  urldate = {2021-02-06},
  abstract = {The positioning accuracy of a robot is of great significance in advanced robotic manufacturing systems. This paper proposes a novel calibration method for improving robot positioning accuracy. First of all, geometric parameters are identified on the basis of the product of exponentials (POE) formula. The errors of the reduction ratio and the coupling ratio are identified at the same time. Then, joint stiffness identification is carried out by adding a load to the end-effector. Finally, residual errors caused by nongeometric parameters are compensated by a multilayer perceptron neural network (MLPNN) based on beetle swarm optimization algorithm. The calibration is implemented on a SIASUN SR210D robot manipulator. Results show that the proposed method possesses better performance in terms of faster convergence and higher precision.},
  howpublished = {https://www.hindawi.com/journals/complexity/2020/5896813/},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\QM98XYGJ\5896813.html}
}

@article{wangImprovementHeavyLoad2020a,
  title = {Improvement of {{Heavy Load Robot Positioning Accuracy}} by {{Combining}} a {{Model-Based Identification}} for {{Geometric Parameters}} and an {{Optimized Neural Network}} for the {{Compensation}} of {{Nongeometric Errors}}},
  author = {Wang, Yuxiang and Chen, Zhangwei and Zu, Hongfei and Zhang, Xiang and Mao, Chentao and Wang, Zhirong},
  year = {2020},
  month = jan,
  journal = {Complexity},
  volume = {2020},
  pages = {e5896813},
  publisher = {Hindawi},
  issn = {1076-2787},
  doi = {10.1155/2020/5896813},
  urldate = {2022-08-01},
  abstract = {The positioning accuracy of a robot is of great significance in advanced robotic manufacturing systems. This paper proposes a novel calibration method for improving robot positioning accuracy. First of all, geometric parameters are identified on the basis of the product of exponentials (POE) formula. The errors of the reduction ratio and the coupling ratio are identified at the same time. Then, joint stiffness identification is carried out by adding a load to the end-effector. Finally, residual errors caused by nongeometric parameters are compensated by a multilayer perceptron neural network (MLPNN) based on beetle swarm optimization algorithm. The calibration is implemented on a SIASUN SR210D robot manipulator. Results show that the proposed method possesses better performance in terms of faster convergence and higher precision.},
  langid = {english}
}

@article{wangInsufficientKnowledgeResources,
  title = {Insufficient {{Knowledge}} and {{Resources}} --- {{A Biological Constraint}} and {{Its Functional Implications}}},
  author = {Wang, Pei},
  pages = {6},
  abstract = {Insufficient knowledge and resources is not only a biological constraint on human and animal intelligence, but also has important functional implications for artificial intelligence (AI) systems. Traditional theories dominating AI research typically assume some kind of sufficiency of knowledge and resources, so cannot solve many problems in the field. AI needs new theories obeying this constraint, which cannot be obtained by minor revisions or extensions of the traditional theories. The practice of NARS, an AI project, shows that such new theories are feasible and promising in providing a new theoretical foundation for AI.},
  langid = {english}
}

@article{wangNARSThinkingMachine,
  title = {From {{NARS}} to a {{Thinking Machine}}},
  author = {Wang, Pei},
  pages = {18},
  abstract = {NARS is an AGI project developed in the framework of reasoning system, and it adapts to its environment with insufficient knowledge and resources. The development of NARS takes an incremental approach, by extending the formal model stage by stage. The system, when finished, can be further augmented in several directions.},
  langid = {english}
}

@article{wangOfflineOnlineLearningDeformation2022,
  title = {Offline-{{Online Learning}} of {{Deformation Model}} for {{Cable Manipulation}} with {{Graph Neural Networks}}},
  author = {Wang, Changhao and Zhang, Yuyou and Zhang, Xiang and Wu, Zheng and Zhu, Xinghao and Jin, Shiyu and Tang, Te and Tomizuka, Masayoshi},
  year = {2022},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {2},
  eprint = {2203.15004},
  primaryclass = {cs},
  pages = {5544--5551},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2022.3158376},
  urldate = {2022-08-18},
  abstract = {Manipulating deformable linear objects by robots has a wide range of applications, e.g., manufacturing and medical surgery. To complete such tasks, an accurate dynamics model for predicting the deformation is critical for robust control. In this work, we deal with this challenge by proposing a hybrid offline-online method to learn the dynamics of cables in a robust and data-efficient manner. In the offline phase, we adopt Graph Neural Network (GNN) to learn the deformation dynamics purely from the simulation data. Then a linear residual model is learned in real-time to bridge the sim-to-real gap. The learned model is then utilized as the dynamics constraint of a trust region based Model Predictive Controller (MPC) to calculate the optimal robot movements. The online learning and MPC run in a closed-loop manner to robustly accomplish the task. Finally, comparative results with existing methods are provided to quantitatively show the effectiveness and robustness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\8X6BQ7N3\2203.html}
}

@article{wangPretrainNotPretrain2020,
  title = {To {{Pretrain}} or {{Not}} to {{Pretrain}}: {{Examining}} the {{Benefits}} of {{Pretraining}} on {{Resource Rich Tasks}}},
  shorttitle = {To {{Pretrain}} or {{Not}} to {{Pretrain}}},
  author = {Wang, Sinong and Khabsa, Madian and Ma, Hao},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.08671 [cs, stat]},
  eprint = {2006.08671},
  primaryclass = {cs, stat},
  urldate = {2021-06-21},
  abstract = {Pretraining NLP models with variants of Masked Language Model (MLM) objectives has recently led to a significant improvements on many tasks. This paper examines the benefits of pretrained models as a function of the number of training samples used in the downstream task. On several text classification tasks, we show that as the number of training examples grow into the millions, the accuracy gap between finetuning BERT-based model and training vanilla LSTM from scratch narrows to within 1\%. Our findings indicate that MLM-based models might reach a diminishing return point as the supervised data size increases significantly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\JHMSE68E\2006.html}
}

@article{wangReviewApplicationAdditive2020,
  title = {A {{Review}} of the {{Application}} of {{Additive Manufacturing}} in {{Prosthetic}} and {{Orthotic Clinics}} from a {{Biomechanical Perspective}}},
  author = {Wang, Yan and Tan, Qitao and Pu, Fang and Boone, David and Zhang, Ming},
  year = {2020},
  month = nov,
  journal = {Engineering},
  volume = {6},
  number = {11},
  pages = {1258--1266},
  issn = {2095-8099},
  doi = {10.1016/j.eng.2020.07.019},
  urldate = {2024-04-03},
  abstract = {Prostheses and orthoses are common assistive devices to meet the biomechanical needs of people with physical disabilities. The traditional fabrication approach for prostheses or orthoses is a material-wasting, time-consuming, and labor-intensive process. Additive manufacturing (AM) technology has advantages that can solve these problems. Many trials have been conducted in fabricating prostheses and orthoses. However, there is still a gap between the hype and the expected realities of AM in prosthetic and orthotic clinics. One of the key challenges is the lack of a systematic framework of integrated technologies with the AM procedure; another challenge is the need to design a prosthetic or orthotic product that can meet the requirements of both comfort and function. This study reviews the current state of application of AM technologies in prosthesis and orthosis fabrication, and discusses optimal design using computational methods and biomechanical evaluations of product performance. A systematic framework of the AM procedure is proposed, which covers the scanning of affected body parts through to the final designed adaptable product. A cycle of optimal design and biomechanical evaluation of products using finite-element analysis is included in the framework. A mature framework of the AM procedure and sufficient evidence that the resulting products show satisfactory biomechanical performance will promote the application of AM in prosthetic and orthotic clinics.},
  keywords = {3D printing,Additive manufacturing,Biomechanics of the musculoskeletal system,Computational model,Prostheses and orthoses}
}

@inproceedings{wangReviewCodeGeneration2023,
  title = {A {{Review}} on {{Code Generation}} with {{LLMs}}: {{Application}} and {{Evaluation}}},
  shorttitle = {A {{Review}} on {{Code Generation}} with {{LLMs}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Medical Artificial Intelligence}} ({{MedAI}})},
  author = {Wang, Jianxun and Chen, Yixiang},
  year = {2023},
  month = nov,
  pages = {284--289},
  doi = {10.1109/MedAI59581.2023.00044},
  urldate = {2024-09-25},
  abstract = {Code generation is a longstanding subject in the field of computer science and software engineering, which aims at realizing an agent capable of writing code automatically aligning with human desire. With the booming development of large language models (LLMs) in recent years, code generation techniques powered by LLMs with strong coding ability have caught many researchers' interest. In this study, we conduct a review of recent studies about code generation with LLMs, from the application of LLM-based code generation to the evaluation of LLM-generated code. We find, with the powerful code understanding and writing ability LLMs provide, these novel techniques can be applied to manage various software engineering tasks, and indeed boost the productivity of developers to a great extent. But we also find, as an equally important subject, the evaluation receives less attention from researchers than the application. We conclude some limitations in existing studies about the evaluation of code generated by LLMs, like inadequate quality characteristics considered. And we think more effort is needed to narrow the gap between research on the evaluation and the application.},
  keywords = {automatic program repair,code completion,code generation,code quality evaluation,Codes,Computer science,Encoding,large language models (LLMs),Productivity,Software engineering,Task analysis,Writing}
}

@article{wangSupervisedMetaReinforcementLearning2024,
  title = {Supervised {{Meta-Reinforcement Learning With Trajectory Optimization}} for {{Manipulation Tasks}}},
  author = {Wang, Lei and Zhang, Yunzhou and Zhu, Delong and Coleman, Sonya and Kerr, Dermot},
  year = {2024},
  month = apr,
  journal = {IEEE Transactions on Cognitive and Developmental Systems},
  volume = {16},
  number = {2},
  pages = {681--691},
  issn = {2379-8939},
  doi = {10.1109/TCDS.2023.3286465},
  urldate = {2024-09-07},
  abstract = {Learning from small amounts of samples with reinforcement learning (RL) is challenging in many tasks, especially, in real-world applications, such as robotics. Meta-RL (meta-RL) has been proposed as an approach to address this problem by generalizing to new tasks through experience from previous similar tasks. However, these approaches generally perform meta-optimization by focusing direct policy search methods on validation samples from adapted policies, thus, requiring large amounts of on-policy samples during meta-training. To this end, we propose a novel algorithm called supervised meta-RL with trajectory optimization (SMRL-TO) by integrating model-agnostic meta-learning (MAML) and iterative LQR (iLQR)-based trajectory optimization. Our approach is designed to provide online supervision for validation samples through iLQR-based trajectory optimization and embed simple imitation learning into the meta-optimization rather than policy gradient steps. This is actually a bi-level optimization that needs to calculate several gradient updates in each meta-iteration, consisting of off-policy RL in the inner loop and online imitation learning in the outer loop. SMRL-TO can achieve significant improvements in sample efficiency without human-provided demonstrations, due to the effective supervision from iLQR-based trajectory optimization. In this article, we describe how to use iLQR-based trajectory optimization to obtain labeled data and then how leverage them to assist the training of meta-learner. Through a series of robotic manipulation tasks, we further show that compared with the previous methods, the proposed approach can substantially improve sample efficiency and achieve better asymptotic performance.},
  keywords = {Complexity theory,Dynamical systems,Heuristic algorithms,Iterative LQR (iLQR),meta learning,reinforcement learning (RL),robotic manipulation,Robots,Task analysis,Training,trajectory optimization,Trajectory optimization},
  file = {C:\Users\benja\Zotero\storage\GF2Z6T3D\Wang et al. - 2024 - Supervised Meta-Reinforcement Learning With Trajectory Optimization for Manipulation Tasks.pdf}
}

@inproceedings{wangTwoStageParameterIdentification2019,
  title = {A {{Two-Stage Parameter Identification Method}} and {{Compensation Verification}} for {{Heavy Load Robot}}},
  booktitle = {2019 4th {{International Conference}} on {{Control}}, {{Robotics}} and {{Cybernetics}} ({{CRC}})},
  author = {Wang, Z. and Mao, C. and Chen, Z. and Wang, Y. and Zhou, J. and Lu, Z.},
  year = {2019},
  month = sep,
  pages = {38--41},
  doi = {10.1109/CRC.2019.00017},
  abstract = {Kinematic calibration plays a significant role in improving the robot positioning accuracy. Due to the mechanical factors such as joint deformation and heavy load, some errors are non-geometrical and nonlinear. In this case, a single kinematic calibration method is difficult to achieve good results especially for robots with heavy load. In this paper, a two-stage parameter identification method is proposed, which also deals with joint deformation and heavy load dependent errors to achieve a higher positioning accuracy. In the first stage, the method builds the kinematic model and identifies the geometric errors using the DH model and the distance model. In the second stage, the stiffness of the joint is analyzed and the positioning accuracy is improved on the robot dynamics. Finally, the experiments are carried out on the 6R robot with heavy load. The experimental results demonstrate the effectiveness and correctness of the method.},
  keywords = {calibration,Calibration,compensation verification,DH,DH-HEMTs,distance model,geometric errors,heavy load dependent errors,heavy load robot,higher positioning accuracy,joint deformation,joint stiffness,kinematic model,Load modeling,manipulator kinematics,parameter estimation,Parameter estimation,parameter identification,position control,robot dynamics,Robot kinematics,robot positioning accuracy,Service robots,single kinematic calibration method,two-stage,two-stage parameter identification method},
  file = {C:\Users\benja\Zotero\storage\BYG32PRG\9058834.html}
}

@article{wangUnifiedArtificialIntelligence,
  title = {Toward a {{Unified Artificial Intelligence}}},
  author = {Wang, Pei},
  pages = {8},
  abstract = {To integrate existing AI techniques into a consistent system, an intelligent core is needed, which is general and flexible, and can use the other techniques as tools to solve concrete problems. Such a system, NARS, is introduced. It is a general-purpose reasoning system developed to be adaptive and capable of working with insufficient knowledge and resources. Compared to traditional reasoning system, NARS is different in all major components (language, semantics, inference rules, memory structure, and control mechanism).},
  langid = {english}
}

@misc{wangWeaverFoundationModels2024,
  title = {Weaver: {{Foundation Models}} for {{Creative Writing}}},
  shorttitle = {Weaver},
  author = {Wang, Tiannan and Chen, Jiamin and Jia, Qingrui and Wang, Shuai and Fang, Ruoyu and Wang, Huilin and Gao, Zhaowei and Xie, Chunzhao and Xu, Chuou and Dai, Jihong and Liu, Yibin and Wu, Jialong and Ding, Shengwei and Li, Long and Huang, Zhiwei and Deng, Xinle and Yu, Teng and Ma, Gangan and Xiao, Han and Chen, Zixin and Xiang, Danjun and Wang, Yunxia and Zhu, Yuanyuan and Xiao, Yi and Wang, Jing and Wang, Yiru and Ding, Siran and Huang, Jiayang and Xu, Jiayi and Tayier, Yilihamu and Hu, Zhenyu and Gao, Yuan and Zheng, Chengfeng and Ye, Yueshu and Li, Yihang and Wan, Lei and Jiang, Xinyue and Wang, Yujie and Cheng, Siyu and Song, Zhule and Tang, Xiangru and Xu, Xiaohua and Zhang, Ningyu and Chen, Huajun and Jiang, Yuchen Eleanor and Zhou, Wangchunshu},
  year = {2024},
  month = jan,
  number = {arXiv:2401.17268},
  eprint = {2401.17268},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.17268},
  urldate = {2024-03-01},
  abstract = {This work introduces Weaver, our first family of large language models (LLMs) dedicated to content creation. Weaver is pre-trained on a carefully selected corpus that focuses on improving the writing capabilities of large language models. We then fine-tune Weaver for creative and professional writing purposes and align it to the preference of professional writers using a suit of novel methods for instruction data synthesis and LLM alignment, making it able to produce more human-like texts and follow more diverse instructions for content creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for different applications and can be dynamically dispatched by a routing agent according to query complexity to balance response quality and computation cost. Evaluation on a carefully curated benchmark for assessing the writing capabilities of LLMs shows Weaver models of all sizes outperform generalist LLMs several times larger than them. Notably, our most-capable Weaver Ultra model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing scenarios, demonstrating the advantage of training specialized LLMs for writing purposes. Moreover, Weaver natively supports retrieval-augmented generation (RAG) and function calling (tool usage). We present various use cases of these abilities for improving AI-assisted writing systems, including integration of external knowledge bases, tools, or APIs, and providing personalized writing assistance. Furthermore, we discuss and summarize a guideline and best practices for pre-training and fine-tuning domain-specific LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\DIM9L4US\2401.html}
}

@article{wanNonGeometricErrorCompensation2021,
  title = {Non-{{Geometric Error Compensation}} for {{Long-Stroke Cartesian Robot With Semi-Analytical Beam Deformation}} and {{Gaussian Process Regression Model}}},
  author = {Wan, Hongyu and Chen, Silu and Liu, Yisha and Zhang, Chi and Jin, Chaochao and Wang, Jin and Yang, Guilin},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {51910--51924},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3069873},
  abstract = {Till now, most calibration methods only compensate geometric error caused by inaccurate kinematic parameters, while the desired accuracy may still not be achieved when the robot is performing long-stroke, heavy-duty loading and unloading tasks. In this paper, a generalized semi-analytical beam deformation model is firstly proposed for the Cartesian robot to compensate the non-geometric error due to structural deformation under both distributed and concentrated loads. The adjustment factors are introduced in this model to deal with the over-constrained boundary conditions for both intermediate and side modules of the long-stroke Cartesian robot. This improves the fitness of the coming geometric error model, which assumes the beam to be rigid and straight. In addition, as the major error components which do not conform to the Gaussian distribution have been extracted in earlier steps, the Gaussian process regression model is imported to predict the residual error more accurately. In this way, comprehensive geometric and non-geometric error modeling and compensation procedures are formed for the multi-module, long-stroke Cartesian robot. Simulations and real-time experiments are conducted to validate the effectiveness of the proposed method.},
  keywords = {beam,Calibration,Cartesian robot,Deformable models,deformation,error compensation,Gaussian process regression,kinematics,Kinematics,Load modeling,Robots,Solid modeling,Strain}
}

@inproceedings{weberDirect3DDetection2019,
  title = {Direct {{3D Detection}} of {{Vehicles}} in {{Monocular Images}} with a {{CNN}} Based {{3D Decoder}}},
  booktitle = {2019 {{IEEE Intelligent Vehicles Symposium}} ({{IV}})},
  author = {Weber, Michael and F{\"u}rst, Michael and Z{\"o}llner, J. Marius},
  year = {2019},
  month = jun,
  pages = {417--423},
  issn = {2642-7214},
  doi = {10.1109/IVS.2019.8814198},
  abstract = {In autonomous driving, the detection of objects like surrounding vehicles based on monocular RGB images is usually performed by 2D bounding box detectors. The resulting 2D objects can be used for a first coarse 3D position estimate but for a precise location, additional sensor data has to be taken into account. For further use in sensor fusion systems and environment maps it is preferable to detect objects, their orientation and dimensions directly in 3D coordinates. To address this 3D object detection task, we propose a direct 3D bounding box estimator which is realized as CNN decoder module and can be connected to most 2D object detectors like SSD[1], OverFeat[2], YOLO[3] and RetinaNet[4] or directly to CNN feature extractors like VGG [2] and ResNet [5]. The 3D parameters of the objects such as dimension and orientation are directly predicted by the CNN module. To successfully train this complex MultiNet architecture, a combination and modification of current loss functions is proposed. The fastest of the proposed network module combinations is capable of detecting objects in 3D camera coordinates at a frame rate of 28 fps.},
  keywords = {Cameras,Decoding,Detectors,Feature extraction,Three-dimensional displays,Training,Two dimensional displays},
  file = {C:\Users\benja\Zotero\storage\VECULRKK\8814198.html}
}

@misc{weberWhyLSTMsStop2017,
  title = {Why {{LSTMs Stop Your Gradients From Vanishing}}: {{A View}} from the {{Backwards Pass}}},
  shorttitle = {Why {{LSTMs Stop Your Gradients From Vanishing}}},
  author = {Weber, Noah},
  year = {2017},
  month = nov,
  journal = {weberna's blog},
  urldate = {2019-08-03},
  abstract = {LSTMs: The Gentle Giants On their surface, LSTMs (and related architectures such as GRUs) seems like wonky, overly complex contraptions. Indeed, at first it seems almost sacrilegious to add these bulky accessories to our beautifully elegant Elman-style recurrent neural networks (RNNs)! However, unlike bloated software (such as Skype), this extra complexity is warranted in the case of LSTMs (also unlike Skype is the fact that LSTM/GRUs usually work pretty well). If you have read any paper that appeared around 2015-2016 that uses LSTMs you probably know that LSTMS solve the vanishing gradient problem that had plagued vanilla RNNs before hand.},
  howpublished = {https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html},
  langid = {english}
}

@article{websterReliableAutonomousRobotic2016,
  title = {Toward {{Reliable Autonomous Robotic Assistants Through Formal Verification}}: {{A Case Study}}},
  shorttitle = {Toward {{Reliable Autonomous Robotic Assistants Through Formal Verification}}},
  author = {Webster, Matt and Dixon, Clare and Fisher, Michael and Salem, Maha and Saunders, Joe and Koay, Kheng Lee and Dautenhahn, Kerstin and {Saez-Pons}, Joan},
  year = {2016},
  month = apr,
  journal = {IEEE Transactions on Human-Machine Systems},
  volume = {46},
  number = {2},
  pages = {186--196},
  issn = {2168-2305},
  doi = {10.1109/THMS.2015.2425139},
  urldate = {2024-04-16},
  abstract = {It is essential for robots working in close proximity to people to be both safe and trustworthy. We present a case study on formal verification for a high-level planner/scheduler for the Care-O-bot, an autonomous personal robotic assistant. We describe how a model of the Care-O-bot and its environment was developed using Brahms, a multiagent workflow language. Formal verification was then carried out by automatically translating this model to the input language of an existing model checker. Four sample properties based on system requirements were verified. We then refined the environment model three times to increase its accuracy and the persuasiveness of the formal verification results. The first refinement uses a user activity log based on real-life experiments, but is deterministic. The second refinement uses the activities from the user activity log nondeterministically. The third refinement uses ``conjoined activities'' based on an observation that many user activities can overlap. The four samples properties were verified for each refinement of the environment model. Finally, we discuss the approach of environment model refinement with respect to this case study.},
  keywords = {Autonomous systems,formal verification,human-robot teams,model checking,Navigation,Robot sensing systems,robotics,Software,TV,Watches},
  file = {C:\Users\benja\Zotero\storage\B95DIN55\7106471.html}
}

@inproceedings{weiAgentBasedCognitiveRobot2013,
  title = {An {{Agent-Based Cognitive Robot Architecture}}},
  booktitle = {Programming {{Multi-Agent Systems}}},
  author = {Wei, Changyun and Hindriks, Koen V.},
  editor = {Dastani, Mehdi and H{\"u}bner, Jomi F. and Logan, Brian},
  year = {2013},
  pages = {54--71},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-38700-5_4},
  abstract = {We propose a new~cognitive robot control architecture in which the cognitive layer can be~programmed by means of the agent programming language~Goal. The architecture exploits the support that agent-oriented programming offers for creating~cognitive robotic agents, including symbolic knowledge representation, deliberation via modular, high-level action selection, and support for multiple, declarative goals. The benefits of the architecture are that it provides a flexible approach to develop cognitive robots and support for a clean and clear separation of concerns about symbolic reasoning and sub-symbolic processing. We discuss the design of our architecture and discuss the issue of translating sub-symbolic information and behavior control into symbolic representations needed at the cognitive layer. An interactive navigation task is presented as a proof of concept.},
  isbn = {978-3-642-38700-5},
  langid = {english},
  keywords = {Global Memory,Humanoid Robot,Interface Layer,Robot Control,Symbolic Representation},
  file = {C:\Users\benja\Zotero\storage\RCT72FE7\Wei und Hindriks - 2013 - An Agent-Based Cognitive Robot Architecture.pdf}
}

@inproceedings{Weik.2000,
  title = {A Passive Full Body Scanner Using Shape from Silhouettes},
  booktitle = {Proceedings 15th International Conference on Pattern Recognition. {{ICPR-2000}}},
  author = {Weik, S.},
  year = {2000},
  pages = {750--753},
  publisher = {IEEE Comput. Soc},
  doi = {10.1109/ICPR.2000.905495},
  bookpagination = {page},
  isbn = {0-7695-0750-6}
}

@inproceedings{weintropEvaluatingCoBloxComparative2018,
  title = {Evaluating {{CoBlox}}: {{A Comparative Study}} of {{Robotics Programming Environments}} for {{Adult Novices}}},
  shorttitle = {Evaluating {{CoBlox}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}  - {{CHI}} '18},
  author = {Weintrop, David and Afzal, Afsoon and Salac, Jean and Francis, Patrick and Li, Boyang and Shepherd, David C. and Franklin, Diana},
  year = {2018},
  pages = {1--12},
  publisher = {ACM Press},
  address = {Montreal QC, Canada},
  doi = {10.1145/3173574.3173940},
  urldate = {2019-07-09},
  abstract = {A new wave of collaborative robots designed to work alongside humans is bringing the automation historically seen in large-scale industrial settings to new, diverse contexts. However, the ability to program these machines often requires years of training, making them inaccessible or impractical for many. This paper rethinks what robot programming interfaces could be in order to make them accessible and intuitive for adult novice programmers. We created a block-based interface for programming a onearmed industrial robot and conducted a study with 67 adult novices comparing it to two programming approaches in widespread use in industry. The results show participants using the block-based interface successfully implemented robot programs faster with no loss in accuracy while reporting higher scores for usability, learnability, and overall satisfaction. The contribution of this work is showing the potential for using block-based programming to make powerful technologies accessible to a wider audience.},
  isbn = {978-1-4503-5620-6},
  langid = {english}
}

@inproceedings{weissmannModelCheckingIndustrial2011,
  title = {Model {{Checking Industrial Robot Systems}}},
  booktitle = {Model {{Checking Software}}},
  author = {Wei{\ss}mann, Markus and Bedenk, Stefan and Buckl, Christian and Knoll, Alois},
  editor = {Groce, Alex and Musuvathi, Madanlal},
  year = {2011},
  pages = {161--176},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-22306-8_11},
  abstract = {Modern production plants are highly automated complex systems consisting of several robots and other working machines. Errors leading to damage and stop of production are extremely expensive and must be avoided by all means. Hence, the state of practice is to test control programs in advance which implies high effort and comes with high costs. To increase the confidence into the control systems and to reduce the necessary effort, this paper proposes to use model checking to verify certain properties. It presents a compiler that can transform industrial robot programs into PROMELA models. Since the statements of the robot programming language can not be mapped directly into PROMELA statements, we apply compiler optimization techniques to close the semantic gap. In case of a specification violation the trace is mapped to the original context so that the robot programmer can reconstruct the problem. As a case study we applied the tool to verify the absence of collisions and deadlocks. We were able to detect one deadlock in a car-body welding station with 9 robots, correct the program and verify the correctness of the resulting system.},
  isbn = {978-3-642-22306-8},
  langid = {english},
  keywords = {abstract interpretation,distributed systems,industrial robots,model checking}
}

@techreport{Welc01,
  title = {An {{Introduction}} to the {{Kalman Filter}}},
  author = {{Welch Greg} and Bishop, Gary},
  year = {2001}
}

@article{weldChallengeCraftingIntelligible2019,
  title = {The Challenge of Crafting Intelligible Intelligence},
  author = {Weld, Daniel S. and Bansal, Gagan},
  year = {2019},
  month = may,
  journal = {Communications of the ACM},
  volume = {62},
  number = {6},
  pages = {70--79},
  issn = {0001-0782},
  doi = {10.1145/3282486},
  urldate = {2022-04-29},
  abstract = {To trust the behavior of complex AI algorithms, especially in mission-critical settings, they must be made intelligible.}
}

@misc{WeltmarktFuerMedizintechnik,
  title = {{Der Weltmarkt f{\"u}r Medizintechnik w{\"a}chst weiter}},
  shorttitle = {{Wirtschaft}},
  journal = {https://www.kma-online.de},
  urldate = {2022-01-10},
  abstract = {Seit 2010 verzeichnen Medizintechnikhersteller ein Umsatzwachstum von durchschnittlich f{\"u}nf Prozent pro Jahr. Damit scheint nun Schluss zu sein. W{\"a}hrend die Wachstumszahlen seit 2018 im Inland deutlich zur{\"u}ckgehen, w{\"a}chst das Auslandsgesch{\"a}ft weiterhin. Marcus Kuhlmann, Leiter Medizintechnik bei Spectaris, im Interview.},
  howpublished = {https://www.kma-online.de/aktuelles/medizintechnik/detail/der-weltmarkt-fuer-medizintechnik-waechst-weiter-a-42663},
  langid = {ngerman},
  file = {C:\Users\benja\Zotero\storage\6XBP8WL2\der-weltmarkt-fuer-medizintechnik-waechst-weiter-a-42663.html}
}

@inproceedings{wenEndtoEndSemisupervisedLearning2021,
  title = {End-to-{{End Semi-supervised Learning}} for {{Differentiable Particle Filters}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Wen, Hao and Chen, Xiongjie and Papagiannis, Georgios and Hu, Conghui and Li, Yunpeng},
  year = {2021},
  month = may,
  pages = {5825--5831},
  issn = {2577-087X},
  doi = {10.1109/ICRA48506.2021.9561889},
  urldate = {2024-09-29},
  abstract = {Recent advances in incorporating neural networks into particle filters provide the desired flexibility to apply particle filters in large-scale real-world applications. The dynamic and measurement models in this framework are learnable through the differentiable implementation of particle filters. Past efforts in optimising such models often require the knowledge of true states which can be expensive to obtain or even unavailable in practice. In this paper, in order to reduce the demand for annotated data, we present an end-to-end learning objective based upon the maximisation of a pseudo-likelihood function which can improve the estimation of states when large portion of true states are unknown. We assess performance of the proposed method in state estimation tasks in robotics with simulated and real-world datasets.},
  keywords = {Atmospheric measurements,History,Neural networks,Particle filters,Particle measurements,Semisupervised learning,Task analysis},
  file = {C\:\\Users\\benja\\Zotero\\storage\\TK3BTD4H\\Wen et al. - 2021 - End-to-End Semi-supervised Learning for Differentiable Particle Filters.pdf;C\:\\Users\\benja\\Zotero\\storage\\R7GAIKBS\\9561889.html}
}

@inproceedings{werlingFastFeatureCompleteDifferentiable2021,
  title = {Fast and {{Feature-Complete Differentiable Physics Engine}} for {{Articulated Rigid Bodies}} with {{Contact Constraints}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVII}}},
  author = {Werling, Keenon and Omens, Dalton and Lee, Jeongseok and Exarchos, Ioannis and Liu, C. Karen},
  year = {2021},
  month = jul,
  volume = {17},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-7-8},
  file = {C:\Users\benja\Zotero\storage\TVDENRFU\p034.html}
}

@misc{whiteIntroducingIntrinsicFlowstate2023,
  title = {Introducing {{Intrinsic Flowstate}}},
  author = {White, Wendy Tan},
  year = {2023},
  month = may,
  journal = {Intrinsic},
  urldate = {2024-02-27},
  abstract = {Welcoming applications from solution builders to a private beta program.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\9GW5SD46\introducing-intrinsic-flowstate.html}
}

@article{wielemaker:2011:tplp,
  title = {{{SWI-prolog}}},
  author = {Wielemaker, Jan and Schrijvers, Tom and Triska, Markus and Lager, Torbj{\"o}rn},
  year = {2012},
  journal = {Theory and Practice of Logic Programming},
  volume = {12},
  number = {1-2},
  pages = {67--96},
  issn = {1471-0684},
  abstract = {SWI-Prolog is neither a commercial Prolog system nor a purely academic enterprise, but increasingly a community project. The core system has been shaped to its current form while being used as a tool for building research prototypes, primarily for knowledge-intensive and interactive systems. Community contributions have added several interfaces and the constraint (CLP) libraries. Commercial involvement has created the initial garbage collector, added several interfaces and two development tools: PlDoc (a literate programming documentation system) and PlUnit (a unit testing environment). In this article we present SWI-Prolog as an integrating tool, supporting a wide range of ideas developed in the Prolog community and acting as glue between foreign resources. This article itself is the glue between technical articles on SWI-Prolog, providing context and experience in applying them over a longer period.}
}

@inproceedings{wieserRoboticHomeAssistant2016,
  title = {A {{Robotic Home Assistant}} with {{Memory Aid Functionality}}},
  booktitle = {{{KI}}},
  author = {Wieser, Iris and Toprak, Sibel and Grenzing, Andreas and Hinz, Tobias and Auddy, S. and Karaoguz, Ethem Can and Chandran, A. and Remmels, Melanie and Shinawi, A. E. and Josifovski, J. and Vankadara, Leena Chennuru and Wahab, Faiz Ul and Bahnemiri, Alireza M. and Sahu, D. and Heinrich, S. and Navarro, N. and Strahl, E. and Twiefel, Johannes and Wermter, S.},
  year = {2016},
  doi = {10.1007/978-3-319-46073-4_8},
  abstract = {We present the robotic system IRMA Interactive Robotic Memory Aid that assists humans in their search for misplaced belongings within a natural home-like environment. Our stand-alone system integrates state-of-the-art approaches in a novel manner to achieve a seamless and intuitive human-robot interaction. IRMA directs its gaze toward the speaker and understands the person's verbal instructions independent of specific grammatical constructions. It determines the positions of relevant objects and navigates collision-free within the environment. In addition, IRMA produces natural language descriptions for the objects' positions by using furniture as reference points. To evaluate IRMA's usefulness, a user study with 20 participants has been conducted. IRMA achieves an overall user satisfaction score of 4.05 and a perceived accuracy rating of 4.15 on a scale from 1---5 with 5 being the best.}
}

@article{wiethofHybridIntelligenceCombining2021,
  title = {Hybrid {{Intelligence}} -- {{Combining}} the {{Human}} in the {{Loop}} with the {{Computer}} in the {{Loop}}: {{A Systematic Literature Review}}},
  shorttitle = {Hybrid {{Intelligence}} -- {{Combining}} the {{Human}} in the {{Loop}} with the {{Computer}} in the {{Loop}}},
  author = {Wiethof, Christina and Bittner, Eva},
  year = {2021},
  month = dec,
  journal = {ICIS 2021 Proceedings},
  file = {C:\Users\benja\Zotero\storage\MUWFT564\11.html}
}

@article{Wikipedia.,
  title = {Laplace-Matrix},
  author = {{Wikipedia}}
}

@article{Wikipedia.Raytracing.2019,
  title = {Raytracing},
  author = {{Phrood}},
  year = {2019},
  lastvisited = {2019-06-23}
}

@article{williamsInvertingConnectionistNetwork1986,
  title = {Inverting a Connectionist Network Mapping by Backpropagation of Error},
  author = {Williams, Ronald J.},
  year = {1986},
  journal = {Proc. of 8th Annual Conference of the Cognitive Science Society},
  pages = {859--865},
  urldate = {2024-05-23},
  file = {C:\Users\benja\Zotero\storage\S58M843L\1570009749218893952.html}
}

@techreport{winklerAcceleratingAutomotivesAI2019,
  title = {Accelerating {{Automotive}}'s {{AI Transformation}}},
  author = {Winkler, Markus and Thieullent, Anne-Laure and Khadikar, Amol and Tolido, Ron and Finck, Ingo and Buvat, Jerome and Shah, Hilal},
  year = {2019},
  institution = {Capgemini Research Institute},
  urldate = {2022-05-06},
  file = {C:\Users\benja\Zotero\storage\5E94WQ2J\30-min--Report-3.pdf}
}

@inproceedings{witzigContextAwareShared2013,
  title = {Context Aware Shared Autonomy for Robotic Manipulation Tasks},
  booktitle = {2013 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Witzig, T. and Z{\"o}llner, J. M. and Pangercic, D. and Osentoski, S. and J{\"a}kel, R. and Dillmann, R.},
  year = {2013},
  month = nov,
  pages = {5686--5693},
  issn = {2153-0866},
  doi = {10.1109/IROS.2013.6697180},
  abstract = {This paper describes a collaborative human-robot system that provides context information to enable more effective robotic manipulation. We take advantage of the semantic knowledge of a human co-worker who provides additional context information and interacts with the robot through a user interface. A Bayesian Network encodes the dependencies between this information provided by the user. The output of this model generates a ranked list of grasp poses best suitable for a given task which is then passed to the motion planner. Our system was implemented in ROS and tested on a PR2 robot. We compared the system to state-of-the-art implementations using quantitative (e.g. success rate, execution times) as well as qualitative (e.g. user convenience, cognitive load) metrics. We conducted a user study in which eight subjects were asked to perform a generic manipulation task, for instance to pour a bottle or move a cereal box, with a set of state-of-the-art shared autonomy interfaces. Our results indicate that an interface which is aware of the context provides benefits not currently provided by other state-of-the-art implementations.},
  keywords = {autonomy interfaces,Bayes methods,Bayesian network,Bayesian Network,belief networks,collaborative human-robot system,Context,context aware shared autonomy,Context Awareness,context information,control engineering computing,generic manipulation task,grasp poses,human coworker,human-robot interaction,manipulators,motion planner,path planning,PR2 robot,Probabilistic logic,Probability distribution,qualitative metrics,quantitative metrics,Robotic Manipulation,robotic manipulation tasks,Robots,ROS,semantic knowledge,Semantics,Shared Autonomy,user interface,User interfaces},
  file = {C:\Users\benja\Zotero\storage\QJIY2HC5\6697180.html}
}

@inproceedings{wolfGesturebasedRobotControl2013,
  title = {Gesture-Based Robot Control with Variable Autonomy from the {{JPL BioSleeve}}},
  booktitle = {2013 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Wolf, M. T. and Assad, C. and Vernacchia, M. T. and Fromm, J. and Jethani, H. L.},
  year = {2013},
  month = may,
  pages = {1160--1165},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2013.6630718},
  abstract = {This paper presents a new gesture-based human interface for natural robot control. Detailed activity of the user's hand and arm is acquired via a novel device, called the BioSleeve, which packages dry-contact surface electromyography (EMG) and an inertial measurement unit (IMU) into a sleeve worn on the forearm. The BioSleeve's accompanying algorithms can reliably decode as many as sixteen discrete hand gestures and estimate the continuous orientation of the forearm. These gestures and positions are mapped to robot commands that, to varying degrees, integrate with the robot's perception of its environment and its ability to complete tasks autonomously. This flexible approach enables, for example, supervisory point-to-goal commands, virtual joystick for guarded teleoperation, and high degree of freedom mimicked manipulation, all from a single device. The BioSleeve is meant for portable field use; unlike other gesture recognition systems, use of the BioSleeve for robot control is invariant to lighting conditions, occlusions, and the human-robot spatial relationship and does not encumber the user's hands. The BioSleeve control approach has been implemented on three robot types, and we present proof-of-principle demonstrations with mobile ground robots, manipulation robots, and prosthetic hands.},
  keywords = {continuous forearm orientation estimation,dexterous manipulators,discrete hand gestures,dry-contact surface electromyography,electromyography,Electromyography,EMG,gesture recognition,gesture recognition systems,gesture-based human interface,gesture-based robot control,high degree-of-freedom mimicked manipulation,human-robot interaction,human-robot spatial relationship,IMU,inertial measurement unit,JPL BioSleeve,lighting conditions,manipulation robots,mobile ground robots,mobile robots,Muscles,natural robot control,prosthetic hands,robot commands,Robot control,robot environment perception,Robot sensing systems,signal classification,supervisory point-to-goal commands,Switches,user arm activity,user hand activity,variable autonomy,virtual joystick}
}

@misc{wolfTradeoffsAlignmentHelpfulness2024,
  title = {Tradeoffs {{Between Alignment}} and {{Helpfulness}} in {{Language Models}}},
  author = {Wolf, Yotam and Wies, Noam and Shteyman, Dorin and Rothberg, Binyamin and Levine, Yoav and Shashua, Amnon},
  year = {2024},
  month = feb,
  number = {arXiv:2401.16332},
  eprint = {2401.16332},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.16332},
  urldate = {2024-03-04},
  abstract = {Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally decreases, it does so quadratically with the norm of the representation engineering vector, while the alignment increases linearly with it, indicating a regime in which it is efficient to use representation engineering. We validate our findings empirically, and chart the boundaries to the usefulness of representation engineering for alignment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\benja\Zotero\storage\HHIZE3NZ\2401.html}
}

@article{Wolpert2016a,
  title = {Computations Underlying Sensorimotor Learning},
  author = {Wolpert, Daniel M and Flanagan, J Randall},
  year = {2016},
  month = apr,
  journal = {Current Opinion in Neurobiology},
  volume = {37},
  pages = {7--11},
  publisher = {Elsevier Current Trends},
  issn = {0959-4388},
  doi = {10.1016/J.CONB.2015.12.003},
  urldate = {2018-05-10},
  abstract = {The study of sensorimotor learning has a long history. With the advent of innovative techniques for studying learning at the behavioral and computational levels new insights have been gained in recent years into how the sensorimotor system acquires, retains, represents, retrieves and forgets sensorimotor tasks. In this review we highlight recent advances in the field of sensorimotor learning from a computational perspective. We focus on studies in which computational models are used to elucidate basic mechanisms underlying adaptation and skill acquisition in human behavior.}
}

@article{Wolpert2016a,
  title = {Computations Underlying Sensorimotor Learning},
  author = {Wolpert, Daniel M and Flanagan, J Randall},
  year = {2016},
  month = apr,
  journal = {Current Opinion in Neurobiology},
  volume = {37},
  pages = {7--11},
  publisher = {Elsevier Current Trends},
  issn = {0959-4388},
  doi = {10.1016/J.CONB.2015.12.003},
  urldate = {2018-05-10},
  abstract = {The study of sensorimotor learning has a long history. With the advent of innovative techniques for studying learning at the behavioral and computational levels new insights have been gained in recent years into how the sensorimotor system acquires, retains, represents, retrieves and forgets sensorimotor tasks. In this review we highlight recent advances in the field of sensorimotor learning from a computational perspective. We focus on studies in which computational models are used to elucidate basic mechanisms underlying adaptation and skill acquisition in human behavior.}
}

@inproceedings{wongNeuralNetworkInversion2017,
  title = {Neural Network Inversion beyond Gradient Descent},
  booktitle = {{{OPTML}} 2017},
  author = {Wong, Eric and Kolter, J Zico},
  year = {2017},
  pages = {5},
  langid = {english}
}

@techreport{WorkingPanda2019,
  title = {Working {{With Panda}}},
  year = {2019},
  institution = {Franka Emika}
}

@inproceedings{wu20153d,
  title = {3d Shapenets: {{A}} Deep Representation for Volumetric Shapes},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
  year = {2015},
  pages = {1912--1920}
}

@inproceedings{wuDeepParameterOptimisation2015,
  title = {Deep {{Parameter Optimisation}}},
  booktitle = {Proceedings of the 2015 {{Annual Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Wu, Fan and Weimer, Westley and Harman, Mark and Jia, Yue and Krinke, Jens},
  year = {2015},
  month = jul,
  series = {{{GECCO}} '15},
  pages = {1375--1382},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2739480.2754648},
  urldate = {2023-06-01},
  abstract = {We introduce a mutation-based approach to automatically discover and expose `deep' (previously unavailable) parameters that affect a program's runtime costs. These discovered parameters, together with existing (`shallow') parameters, form a search space that we tune using search-based optimisation in a bi-objective formulation that optimises both time and memory consumption. We implemented our approach and evaluated it on four real-world programs. The results show that we can improve execution time by 12{\textbackslash}\% or achieve a 21{\textbackslash}\% memory consumption reduction in the best cases. In three subjects, our deep parameter tuning results in a significant improvement over the baseline of shallow parameter tuning, demonstrating the potential value of our deep parameter extraction approach.},
  isbn = {978-1-4503-3472-3},
  keywords = {memory allocation,parameter exposure,parameter tuning}
}

@article{wuDOVELearningDeformable2023,
  title = {{{DOVE}}: {{Learning Deformable 3D Objects}} by {{Watching Videos}}},
  shorttitle = {{{DOVE}}},
  author = {Wu, Shangzhe and Jakab, Tomas and Rupprecht, Christian and Vedaldi, Andrea},
  year = {2023},
  month = oct,
  journal = {International Journal of Computer Vision},
  volume = {131},
  number = {10},
  pages = {2623--2634},
  issn = {1573-1405},
  doi = {10.1007/s11263-023-01819-5},
  urldate = {2024-05-23},
  abstract = {Learning deformable 3D objects from 2D images is often an ill-posed problem. Existing methods rely on explicit supervision to establish multi-view correspondences, such as template shape models and keypoint annotations, which restricts their applicability on objects ``in the wild''. A more natural way of establishing correspondences is by watching videos of objects moving around. In this paper, we present DOVE, a method that learns textured 3D models of deformable object categories from monocular videos available online, without keypoint, viewpoint or template shape supervision. By resolving symmetry-induced pose ambiguities and leveraging temporal correspondences in videos, the model automatically learns to factor out 3D shape, articulated pose and texture from each individual RGB frame, and is ready for single-image inference at test time. In the experiments, we show that existing methods fail to learn sensible 3D shapes without additional keypoint or template supervision, whereas our method produces temporally consistent 3D models, which can be animated and rendered from arbitrary viewpoints. Project page: https://dove3d.github.io/.},
  langid = {english},
  keywords = {Deformable 3D objects,Unsupervised 3D learning}
}

@inproceedings{wuExampleDrivenModelBasedReinforcement2022,
  title = {Example-{{Driven Model-Based Reinforcement Learning}} for {{Solving Long-Horizon Visuomotor Tasks}}},
  booktitle = {Proceedings of the 5th {{Conference}} on {{Robot Learning}}},
  author = {Wu, Bohan and Nair, Suraj and {Fei-Fei}, Li and Finn, Chelsea},
  year = {2022},
  month = jan,
  pages = {1--13},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-04-28},
  abstract = {In this paper, we study the problem of learning a repertoire of low-level skills from raw images that can be sequenced to complete long-horizon visuomotor tasks. Reinforcement learning (RL) is a promising approach for acquiring short-horizon skills autonomously. However, the focus of RL algorithms has largely been on the success of those individual skills, more so than learning and grounding a large repertoire of skills that can be sequenced to complete extended multi-stage tasks. The latter demands robustness and persistence, as errors in skills can compound over time, and may require the robot to have a number of primitive skills in its repertoire, rather than just one. To this end, we introduce EMBR, a model-based RL method for learning primitive skills that are suitable for completing long-horizon visuomotor tasks. EMBR learns and plans using a learned model, critic, and success classifier, where the success classifier serves both as a reward function for RL and as a grounding mechanism to continuously detect if the robot should retry a skill when unsuccessful or under perturbations. Further, the learned model is task-agnostic and trained using data from all skills, enabling the robot to efficiently learn a number of distinct primitives. These visuomotor primitive skills and their associated pre- and post-conditions can then be directly combined with off-the-shelf symbolic planners to complete long-horizon tasks. On a Franka Emika robot arm, we find that EMBR enables the robot to complete three long-horizon visuomotor tasks at 85\% success rate, such as organizing an office desk, a file cabinet, and drawers, which require sequencing up to 12 skills, involve 14 unique learned primitives, and demand generalization to novel objects.},
  langid = {english}
}

@inproceedings{wuExploratoryStudyVModel2024,
  title = {An {{Exploratory Study}} of {{V-Model}} in {{Building ML-Enabled Software}}: {{A Systems Engineering Perspective}}},
  shorttitle = {An {{Exploratory Study}} of {{V-Model}} in {{Building ML-Enabled Software}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 3rd {{International Conference}} on {{AI Engineering}} - {{Software Engineering}} for {{AI}}},
  author = {Wu, Jie JW},
  year = {2024},
  month = jun,
  series = {{{CAIN}} '24},
  pages = {30--40},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3644815.3644951},
  urldate = {2024-09-18},
  abstract = {Machine learning (ML) components are being added to more and more critical and impactful software systems, but the software development process of real-world production systems from prototyped ML models remains challenging with additional complexity and interdisciplinary collaboration challenges. This poses difficulties in using traditional software lifecycle models such as waterfall, spiral, or agile models when building ML-enabled systems. In this research, we apply a Systems Engineering lens to investigate the use of V-Model in addressing the interdisciplinary collaboration challenges when building ML-enabled systems. By interviewing practitioners from software companies, we established a set of 8 propositions for using V-Model to manage interdisciplinary collaborations when building products with ML components. Based on the propositions, we found that despite requiring additional efforts, the characteristics of V-Model align effectively with several collaboration challenges encountered by practitioners when building ML-enabled systems. We recommend future research to investigate new process models that leverage the characteristics of V-Model such as the system decomposition, clear system boundary, and consistency of Validation \&amp; Verification (V\&amp;V) for building ML-enabled systems.},
  isbn = {9798400705915}
}

@inproceedings{wuGradientNormalizationGenerative2021,
  title = {Gradient {{Normalization}} for {{Generative Adversarial Networks}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Wu, Yi-Lun and Shuai, Hong-Han and Tam, Zhi-Rui and Chiu, Hong-Yu},
  year = {2021},
  month = oct,
  pages = {6353--6362},
  issn = {2380-7504},
  doi = {10.1109/ICCV48922.2021.00631},
  urldate = {2024-05-03},
  abstract = {In this paper, we propose a novel normalization method called gradient normalization (GN) to tackle the training instability of Generative Adversarial Networks (GANs) caused by the sharp gradient space. Unlike existing work such as gradient penalty and spectral normalization, the proposed GN only imposes a hard 1-Lipschitz constraint on the discriminator function, which increases the capacity of the discriminator. Moreover, the proposed gradient normalization can be applied to different GAN architectures with little modification. Extensive experiments on four datasets show that GANs trained with gradient normalization outperform existing methods in terms of both Frechet Inception Distance and Inception Score.},
  keywords = {Computer architecture,Computer vision,Generative adversarial networks,Neural generative models,Training},
  file = {C:\Users\benja\Zotero\storage\GJZPYQXJ\9710427.html}
}

@inproceedings{wuLearningManipulateDeformable2020,
  title = {Learning to {{Manipulate Deformable Objects}} without {{Demonstrations}}},
  booktitle = {Robotics: {{Science}} and {{Systems XVI}}},
  author = {Wu, Yilin and Yan, Wilson and Kurutach, Thanard and Pinto, Lerrel and Abbeel, Pieter},
  year = {2020},
  month = jul,
  volume = {16},
  urldate = {2022-03-30},
  isbn = {978-0-9923747-6-1},
  file = {C:\Users\benja\Zotero\storage\VN25GDP6\p065.html}
}

@article{wulfmeierRepresentationMattersImproving2021,
  title = {Representation {{Matters}}: {{Improving Perception}} and {{Exploration}} for {{Robotics}}},
  shorttitle = {Representation {{Matters}}},
  author = {Wulfmeier, Markus and Byravan, Arunkumar and Hertweck, Tim and Higgins, Irina and Gupta, Ankush and Kulkarni, Tejas and Reynolds, Malcolm and Teplyashin, Denis and Hafner, Roland and Lampe, Thomas and Riedmiller, Martin},
  year = {2021},
  month = mar,
  journal = {arXiv:2011.01758 [cs, stat]},
  eprint = {2011.01758},
  primaryclass = {cs, stat},
  urldate = {2021-06-03},
  abstract = {Projecting high-dimensional environment observations into lower-dimensional structured representations can considerably improve data-efficiency for reinforcement learning in domains with limited data such as robotics. Can a single generally useful representation be found? In order to answer this question, it is important to understand how the representation will be used by the agent and what properties such a 'good' representation should have. In this paper we systematically evaluate a number of common learnt and hand-engineered representations in the context of three robotics tasks: lifting, stacking and pushing of 3D blocks. The representations are evaluated in two use-cases: as input to the agent, or as a source of auxiliary tasks. Furthermore, the value of each representation is evaluated in terms of three properties: dimensionality, observability and disentanglement. We can significantly improve performance in both use-cases and demonstrate that some representations can perform commensurate to simulator states as agent inputs. Finally, our results challenge common intuitions by demonstrating that: 1) dimensionality strongly matters for task generation, but is negligible for inputs, 2) observability of task-relevant aspects mostly affects the input representation use-case, and 3) disentanglement leads to better auxiliary tasks, but has only limited benefits for input representations. This work serves as a step towards a more systematic understanding of what makes a 'good' representation for control in robotics, enabling practitioners to make more informed choices for developing new learned or hand-engineered representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\SSWH38WQ\2011.html}
}

@article{wuLowCostDigitalTwinDriven2022,
  title = {A {{Low-Cost Digital Twin-Driven Positioning Error Compensation Method}} for {{Industrial Robotic Arm}}},
  author = {Wu, Zhaoqian and Chen, Shaotao and Han, Jie and Zhang, Shaohui and Liang, Jinglun and Yang, Xiaoqing},
  year = {2022},
  month = dec,
  journal = {IEEE Sensors Journal},
  volume = {22},
  number = {23},
  pages = {22885--22893},
  issn = {1558-1748},
  doi = {10.1109/JSEN.2022.3213428},
  urldate = {2024-05-26},
  abstract = {In robotic arms, transmission error will cause positioning error, which increases with the wear and tear of the transmission mechanism. Positioning error will degrade the performance of the robotic arm. It is necessary to carry out intelligent maintenance for the robotic arm. In this article, a low-cost digital twin (DT)-driven positioning error compensation method is proposed for robotic arm. The DT model of the robotic arm is first established, including the physical layer, data transfer layer, model layer, functional layer, and application service layer. A low-cost attitude sensor is then applied to provide actual information of the robotic arm to the DT model. When the DT model senses the positioning error of the robotic arm, it reduces the positioning error by adjusting the motor angle. The effectiveness of the proposed method is finally verified by the experimental results under different conditions.},
  keywords = {Data transfer,Digital twin (DT),Digital twins,Error compensation,intelligent maintenance,Manipulators,positioning error,Robot sensing systems,robotic arm,Robots,Sensors},
  file = {C:\Users\benja\Zotero\storage\P4EEGYDQ\9920953.html}
}

@article{wuScalableTrustregionMethod2017,
  title = {Scalable Trust-Region Method for Deep Reinforcement Learning Using {{Kronecker-factored}} Approximation},
  author = {Wu, Yuhuai and Mansimov, Elman and Liao, Shun and Grosse, Roger and Ba, Jimmy},
  year = {2017},
  month = aug,
  journal = {arXiv:1708.05144 [cs]},
  eprint = {1708.05144},
  primaryclass = {cs},
  urldate = {2021-04-30},
  abstract = {In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\benja\\Zotero\\storage\\LY67GHI8\\Wu et al. - 2017 - Scalable trust-region method for deep reinforcemen.pdf;C\:\\Users\\benja\\Zotero\\storage\\9SHKHS8G\\1708.html}
}

@article{wuShapingRewardsReinforcement2020,
  title = {Shaping {{Rewards}} for {{Reinforcement Learning}} with {{Imperfect Demonstrations}} Using {{Generative Models}}},
  author = {Wu, Yuchen and Mozifian, Melissa and Shkurti, Florian},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.01298 [cs]},
  eprint = {2011.01298},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {The potential benefits of model-free reinforcement learning to real robotics systems are limited by its uninformed exploration that leads to slow convergence, lack of data-efficiency, and unnecessary interactions with the environment. To address these drawbacks we propose a method that combines reinforcement and imitation learning by shaping the reward function with a state-and-action-dependent potential that is trained from demonstration data, using a generative model. We show that this accelerates policy learning by specifying high-value areas of the state and action space that are worth exploring first. Unlike the majority of existing methods that assume optimal demonstrations and incorporate the demonstration data as hard constraints on policy optimization, we instead incorporate demonstration data as advice in the form of a reward shaping potential trained as a generative model of states and actions. In particular, we examine both normalizing flows and Generative Adversarial Networks to represent these potentials. We show that, unlike many existing approaches that incorporate demonstrations as hard constraints, our approach is unbiased even in the case of suboptimal and noisy demonstrations. We present an extensive range of simulations, as well as experiments on the Franka Emika 7DOF arm, to demonstrate the practicality of our method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\6JX4AZPZ\2011.html}
}

@article{wuSymbolLLMLeverageLanguage2023,
  title = {Symbol-{{LLM}}: {{Leverage Language Models}} for {{Symbolic System}} in {{Visual Human Activity Reasoning}}},
  shorttitle = {Symbol-{{LLM}}},
  author = {Wu, Xiaoqian and Li, Yong-Lu and Sun, Jianhua and Lu, Cewu},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {29680--29691},
  urldate = {2024-08-16},
  langid = {english}
}

@article{wuTidyBotPersonalizedRobot2023,
  title = {{{TidyBot}}: {{Personalized Robot Assistance}} with {{Large Language Models}}},
  shorttitle = {{{TidyBot}}},
  author = {Wu, Jimmy and Antonova, Rika and Kan, Adam and Lepert, Marion and Zeng, Andy and Song, Shuran and Bohg, Jeannette and Rusinkiewicz, Szymon and Funkhouser, Thomas},
  year = {2023},
  month = dec,
  journal = {Autonomous Robots},
  volume = {47},
  number = {8},
  pages = {1087--1102},
  issn = {1573-7527},
  doi = {10.1007/s10514-023-10139-z},
  urldate = {2024-09-25},
  abstract = {For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2\% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0\% of objects in real-world test scenarios.},
  langid = {english},
  keywords = {Artificial Intelligence,Large language models,Mobile manipulation,Service robotics},
  file = {C:\Users\benja\Zotero\storage\T33Q8AA7\Wu et al. - 2023 - TidyBot personalized robot assistance with large language models.pdf}
}

@inproceedings{xiaNeuralKalmanFiltering2024,
  title = {Neural {{Kalman Filtering}} for {{Robust Temporal Recommendation}}},
  booktitle = {Proceedings of the 17th {{ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Xia, Jiafeng and Li, Dongsheng and Gu, Hansu and Lu, Tun and Zhang, Peng and Shang, Li and Gu, Ning},
  year = {2024},
  month = mar,
  series = {{{WSDM}} '24},
  pages = {836--845},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3616855.3635837},
  urldate = {2024-03-25},
  abstract = {Temporal recommendation methods can achieve superior accuracy due to updating user/item embeddings continuously once obtaining new interactions. However, the randomness of user behaviors will introduce noises into the user interactions and cause the deviation in the modeling of user preference, resulting in sub-optimal performance. To this end, we propose NeuFilter, a robust temporal recommendation algorithm based on neural Kalman Filtering, to learn more accurate user and item embeddings with noisy interactions. Classic Kalman Filtering is time-consuming when applied to recommendation due to its covariance matrices. Thus, we propose a neural network solution to Kalman Filtering, so as to realize higher efficiency and stronger expressivity. Specifically, NeuFilter consists of three alternating units: 1) prediction unit, which predicts user and item embeddings based on their historical embeddings; 2) estimation unit, which updates user and item embeddings in a manner similar to Kalman Filtering; 3) correction unit, which corrects the updated user and item embeddings from estimation unit to ensure reliable estimation and accurate update. Experiments on two recommendation tasks show that NeuFilter can achieve higher accuracy compared with the state-of-the-art methods, while achieving high robustness. Moreover, our empirical studies on a node classification task further confirm the importance of handling noises in tasks on temporal graph, shedding a new light on temporal graph modeling.},
  isbn = {9798400703713},
  keywords = {collaborative filtering,robust recommendation,temporal graph}
}

@article{xiangPoseCNNConvolutionalNeural2017,
  title = {{{PoseCNN}}: {{A Convolutional Neural Network}} for {{6D Object Pose Estimation}} in {{Cluttered Scenes}}},
  shorttitle = {{{PoseCNN}}},
  author = {Xiang, Yu and Schmidt, Tanner and Narayanan, Venkatraman and Fox, Dieter},
  year = {2017},
  month = nov,
  urldate = {2021-10-14},
  abstract = {Estimating the 6D pose of known objects is important for robots to interact with the real world. The problem is challenging due to the variety of objects as well as the complexity of a scene caused by clutter and occlusions between objects. In this work, we introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation. PoseCNN estimates the 3D translation of an object by localizing its center in the image and predicting its distance from the camera. The 3D rotation of the object is estimated by regressing to a quaternion representation. We also introduce a novel loss function that enables PoseCNN to handle symmetric objects. In addition, we contribute a large scale video dataset for 6D object pose estimation named the YCB-Video dataset. Our dataset provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames. We conduct extensive experiments on our YCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is highly robust to occlusions, can handle symmetric objects, and provide accurate pose estimation using only color images as input. When using depth data to further refine the poses, our approach achieves state-of-the-art results on the challenging OccludedLINEMOD dataset. Our code and dataset are available at https://rse-lab.cs.washington.edu/projects/posecnn/.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\79DZ88W2\1711.html}
}

@article{xie3DObjectReconstruction2021,
  title = {Toward {{3D}} Object Reconstruction from Stereo Images},
  author = {Xie, Haozhe and Yao, Hongxun and Zhou, Shangchen and Zhang, Shengping and Tong, Xiaojun and Sun, Wenxiu},
  year = {2021},
  month = nov,
  journal = {Neurocomputing},
  volume = {463},
  pages = {444--453},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2021.07.089},
  urldate = {2024-04-12},
  abstract = {Inferring the complete 3D shape of an object from an RGB image has shown impressive results, however, existing methods rely primarily on recognizing the most similar 3D model from the training set to solve the problem. These methods suffer from poor generalization and may lead to low-quality reconstructions for unseen objects. Nowadays, stereo cameras are pervasive in emerging devices such as dual-lens smartphones and robots, which enables the use of the two-view nature of stereo images to explore the 3D structure and thus improve the reconstruction performance. In this paper, we propose a new deep learning framework for reconstructing the 3D shape of an object from a pair of stereo images, which reasons about the 3D structure of the object by taking bidirectional disparities and feature correspondences between the two views into account. Besides, we present a large-scale synthetic benchmarking dataset, namely StereoShapeNet, containing 1,052,976 pairs of stereo images rendered from ShapeNet along with the corresponding bidirectional depth and disparity maps. Experimental results on the StereoShapeNet benchmark demonstrate that the proposed framework outperforms the state-of-the-art methods. The project page is available at https://haozhexie.com/project/stereo-3d-reconstruction.},
  keywords = {3D object reconstruction,Neural network,Point cloud,Stereo vision,Voxel},
  file = {C:\Users\benja\Zotero\storage\7TMFQXFB\S0925231221011711.html}
}

@article{xieFewShotGoalInference2018,
  title = {Few-{{Shot Goal Inference}} for {{Visuomotor Learning}} and {{Planning}}},
  author = {Xie, Annie and Singh, Avi and Levine, Sergey and Finn, Chelsea},
  year = {2018},
  month = sep,
  journal = {arXiv:1810.00482 [cs, stat]},
  eprint = {1810.00482},
  primaryclass = {cs, stat},
  urldate = {2019-05-17},
  abstract = {Reinforcement learning and planning methods require an objective or reward function that encodes the desired behavior. Yet, in practice, there is a wide range of scenarios where an objective is difficult to provide programmatically, such as tasks with visual observations involving unknown object positions or deformable objects. In these cases, prior methods use engineered problem-specific solutions, e.g., by instrumenting the environment with additional sensors to measure a proxy for the objective. Such solutions require a significant engineering effort on a per-task basis, and make it impractical for robots to continuously learn complex skills outside of laboratory settings. We aim to find a more general and scalable solution for specifying goals for robot learning in unconstrained environments. To that end, we formulate the few-shot objective learning problem, where the goal is to learn a task objective from only a few example images of successful end states for that task. We propose a simple solution to this problem: meta-learn a classifier that can recognize new goals from a few examples. We show how this approach can be used with both model-free reinforcement learning and visual model-based planning and show results in three domains: rope manipulation from images in simulation, visual navigation in a simulated 3D environment, and object arrangement into user-specified configurations on a real robot.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\W7UZ3PLK\1810.html}
}

@inproceedings{xieFewShotGoalInference2018a,
  title = {Few-{{Shot Goal Inference}} for {{Visuomotor Learning}} and {{Planning}}},
  booktitle = {{{CoRL}}},
  author = {Xie, Annie and Singh, Avi and Levine, Sergey and Finn, Chelsea},
  year = {2018},
  month = oct,
  pages = {40--52},
  issn = {1938-7228},
  urldate = {2020-06-30},
  abstract = {Reinforcement learning and planning methods require an objective or reward function that encodes the desired behavior. Yet, in practice, there is a wide range of scenarios where an objective is dif...},
  chapter = {Machine Learning},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\8GV7GHGP\xie18a.html}
}

@article{xieSpeedupMethodSolving2022,
  title = {A Speedup Method for Solving the Inverse Kinematics Problem of Robotic Manipulators},
  author = {Xie, Shuxin and Sun, Lining and Wang, Zhenhua and Chen, Guodong},
  year = {2022},
  month = may,
  journal = {International Journal of Advanced Robotic Systems},
  volume = {19},
  number = {3},
  pages = {17298806221104602},
  publisher = {SAGE Publications},
  issn = {1729-8806},
  doi = {10.1177/17298806221104602},
  urldate = {2024-03-31},
  abstract = {The inverse kinematics problem involves the study that the inverse kinematics solver needs to calculate the values of the joint variables given the desired pose of the end-effector of a robot. However, to apply to seven-degree-of-freedom robots with arbitrary configuration, analytical methods need to fix one joint and set an increment when the current value fails to solve the inverse kinematics problem. Although numerical methods based on inverse differential kinematics are efficient in solving the inverse kinematics problem of seven-degree-of-freedom robots with arbitrary geometric parameters, they are deficient in numerical stability and time-consuming for convergence to one solution governed by the initial guess. In order to reduce the execution time of an inverse kinematics solver, this article introduces a speedup method for analytical and numerical methods, which can improve their performance.},
  langid = {english}
}

@article{xieUnsupervisedDataAugmentation2019,
  title = {Unsupervised {{Data Augmentation}}},
  author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.12848 [cs, stat]},
  eprint = {1904.12848},
  primaryclass = {cs, stat},
  urldate = {2019-05-21},
  abstract = {Despite its success, deep learning still needs large labeled datasets to succeed. Data augmentation has shown much promise in alleviating the need for more labeled data, but it so far has mostly been applied in supervised settings and achieved limited gains. In this work, we propose to apply data augmentation to unlabeled data in a semi-supervised learning setting. Our method, named Unsupervised Data Augmentation or UDA, encourages the model predictions to be consistent between an unlabeled example and an augmented unlabeled example. Unlike previous methods that use random noise such as Gaussian noise or dropout noise, UDA has a small twist in that it makes use of harder and more realistic noise generated by state-of-the-art data augmentation methods. This small twist leads to substantial improvements on six language tasks and three vision tasks even when the labeled set is extremely small. For example, on the IMDb text classification dataset, with only 20 labeled examples, UDA outperforms the state-of-the-art model trained on 25,000 labeled examples. On standard semi-supervised learning benchmarks, CIFAR-10 with 4,000 examples and SVHN with 1,000 examples, UDA outperforms all previous approaches and reduces more than 30\% of the error rates of state-of-the-art methods: going from 7.66\% to 5.27\% and from 3.53\% to 2.46\% respectively. UDA also works well on datasets that have a lot of labeled data. For example, on ImageNet, with 1.3M extra unlabeled data, UDA improves the top-1/top-5 accuracy from 78.28/94.36\% to 79.04/94.45\% when compared to AutoAugment.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{xiongUPSNetUnifiedPanoptic2019,
  title = {{{UPSNet}}: {{A Unified Panoptic Segmentation Network}}},
  shorttitle = {{{UPSNet}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Xiong, Yuwen and Liao, Renjie and Zhao, Hengshuang and Hu, Rui and Bai, Min and Yumer, Ersin and Urtasun, Raquel},
  year = {2019},
  pages = {8818--8826},
  urldate = {2023-02-27}
}

@inproceedings{xuAutomateSurgicalTasks2016,
  title = {Automate Surgical Tasks for a Flexible {{Serpentine Manipulator}} via Learning Actuation Space Trajectory from Demonstration},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Xu, Wenjun and Chen, Jie and Lau, Henry Y. K. and Ren, Hongliang},
  year = {2016},
  month = may,
  pages = {4406--4413},
  doi = {10.1109/ICRA.2016.7487640},
  urldate = {2024-04-12},
  abstract = {Surgical robotic systems with miniaturized flexible Tendon-driven Serpentine Manipulators (TSM) have enjoyed increasing popularities among surgeons and researchers for their advantages of working in constrained and torturous human lumen such as oral cavity and upper GI tract. However, they suffer from sufficient nonlinearities and model uncertainties due to friction, tension varying, tendon slacking, etc. Model based control is insufficient to overcome such uncertainties and automate challenging surgical related tasks. The objective of this work is to automate certain clinical tasks to alleviate surgeon fatigue and promote task efficiency in kinematics free and sensor free circumstances. We present a data-driven approach based on Learning from Demonstration (LfD), which utilizes statistical machine learning models to encode system underlying dynamics and generalize smooth motor trajectories by direct actuation space learning. Motion segmentation is enabled with soft margin Support Vector Machine (soft-SVM) in complicated tasks. We also make attempts to retrieve task-specific properties by Locally Weighted Regression (LWR). We evaluated the approach on two surgical related tasks: compliant insertion and simplified Endoscopic Submucosal Dissection (ESD). The flexible TSM successfully reproduced both tasks and demonstrated superior trajectory performance. A video is available at: https://youtu.be/rLQo6xKtyMI.},
  keywords = {Automation,Conferences},
  file = {C:\Users\benja\Zotero\storage\V4XFB3VW\7487640.html}
}

@inproceedings{xuFewShotAdaptationFoundation2023,
  title = {Towards {{Few-Shot Adaptation}} of {{Foundation Models}} via {{Multitask Finetuning}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Xu, Zhuoyan and Shi, Zhenmei and Wei, Junyi and Mu, Fangzhou and Li, Yin and Liang, Yingyu},
  year = {2023},
  month = oct,
  urldate = {2024-06-16},
  abstract = {Foundation models have emerged as a powerful tool for many AI problems. Despite the tremendous success of foundation models, effective adaptation to new tasks, particularly those with limited labels, remains an open question and lacks theoretical understanding. An emerging solution with recent success in vision and NLP involves finetuning a foundation model on a selection of relevant tasks, before its adaptation to a target task with limited labeled samples. In this paper, we study the theoretical justification of this multitask finetuning approach. Our theoretical analysis reveals that with a diverse set of related tasks, this multitask finetuning leads to reduced error in the target task, in comparison to directly adapting the same pretrained model. We quantify the relationship between finetuning tasks and target tasks by diversity and consistency metrics, and further propose a practical task selection algorithm. We substantiate our theoretical claims with extensive empirical evidence. Further, we present results affirming our task selection algorithm adeptly chooses related finetuning tasks, providing advantages to the model performance on target tasks. We believe our study shed new light on the effective adaptation of foundation models to new tasks that lack abundant labels. Our code is available at https://github.com/OliverXUZY/Foudation-Model\_Multitask.},
  langid = {english}
}

@article{xuIndustry40Industry2021,
  title = {Industry 4.0 and {{Industry}} 5.0---{{Inception}}, Conception and Perception},
  author = {Xu, Xun and Lu, Yuqian and {Vogel-Heuser}, Birgit and Wang, Lihui},
  year = {2021},
  month = oct,
  journal = {Journal of Manufacturing Systems},
  volume = {61},
  pages = {530--535},
  issn = {0278-6125},
  doi = {10.1016/j.jmsy.2021.10.006},
  urldate = {2024-01-24},
  abstract = {Industry 4.0, an initiative from Germany, has become a globally adopted term in the past decade. Many countries have introduced similar strategic initiatives, and a considerable research effort has been spent on developing and implementing some of the Industry 4.0 technologies. At the ten-year mark of the introduction of Industry 4.0, the European Commission announced Industry 5.0. Industry 4.0 is considered to be technology-driven, whereas Industry 5.0 is value-driven. The co-existence of two Industrial Revolutions invites questions and hence demands discussions and clarifications. We have elected to use five of these questions to structure our arguments and tried to be unbiased for the selection of the sources of information and for the discussions around the key issues. It is our intention that this article will spark and encourage continued debate and discussion around these topics.},
  keywords = {Industrial Revolution,Industry 4.0,Industry 5.0,Technology-driven,Value-driven},
  file = {C:\Users\benja\Zotero\storage\EBQ6XMN2\S0278612521002119.html}
}

@article{xuIndustry40Industry2021a,
  title = {Industry 4.0 and {{Industry}} 5.0---{{Inception}}, Conception and Perception},
  author = {Xu, Xun and Lu, Yuqian and {Vogel-Heuser}, Birgit and Wang, Lihui},
  year = {2021},
  month = oct,
  journal = {Journal of Manufacturing Systems},
  volume = {61},
  pages = {530--535},
  issn = {0278-6125},
  doi = {10.1016/j.jmsy.2021.10.006},
  urldate = {2024-07-01},
  abstract = {Industry 4.0, an initiative from Germany, has become a globally adopted term in the past decade. Many countries have introduced similar strategic initiatives, and a considerable research effort has been spent on developing and implementing some of the Industry 4.0 technologies. At the ten-year mark of the introduction of Industry 4.0, the European Commission announced Industry 5.0. Industry 4.0 is considered to be technology-driven, whereas Industry 5.0 is value-driven. The co-existence of two Industrial Revolutions invites questions and hence demands discussions and clarifications. We have elected to use five of these questions to structure our arguments and tried to be unbiased for the selection of the sources of information and for the discussions around the key issues. It is our intention that this article will spark and encourage continued debate and discussion around these topics.},
  keywords = {Industrial Revolution,Industry 4.0,Industry 5.0,Technology-driven,Value-driven},
  file = {C:\Users\benja\Zotero\storage\MXN7B8DM\S0278612521002119.html}
}

@inproceedings{xuNeuralTaskProgramming2018,
  title = {Neural {{Task Programming}}: {{Learning}} to {{Generalize Across Hierarchical Tasks}}},
  shorttitle = {Neural {{Task Programming}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Xu, Danfei and Nair, Suraj and Zhu, Yuke and Gao, Julian and Garg, Animesh and {Fei-Fei}, Li and Savarese, Silvio},
  year = {2018},
  month = may,
  pages = {3795--3802},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2018.8460689},
  urldate = {2024-04-29},
  abstract = {In this work, we propose a novel robot learning framework called Neural Task Programming (NTP), which bridges the idea of few-shot learning from demonstration and neural program induction. NTP takes as input a task specification (e.g., video demonstration of a task) and recursively decomposes it into finer sub-task specifications. These specifications are fed to a hierarchical neural program, where bottom-level programs are callable subroutines that interact with the environment. We validate our method in three robot manipulation tasks. NTP achieves strong generalization across sequential tasks that exhibit hierarchal and compositional structures. The experimental results show that NTP learns to generalize well towards unseen tasks with increasing lengths, variable topologies, and changing objectives.stanfordvl.github.io/ntp/.},
  keywords = {Data models,Programming,Robots,Semantics,Sorting,Task analysis,Topology},
  file = {C:\Users\benja\Zotero\storage\P9S632VK\8460689.html}
}

@inproceedings{xuRevisitingImplicitDifferentiation2023,
  title = {Revisiting {{Implicit Differentiation}} for {{Learning Problems}} in {{Optimal Control}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Xu, Ming and Molloy, Timothy L. and Gould, Stephen},
  year = {2023},
  month = nov,
  urldate = {2024-09-06},
  abstract = {This paper proposes a new method for differentiating through optimal trajectories arising from non-convex, constrained discrete-time optimal control (COC) problems using the implicit function theorem (IFT). Previous works solve a differential Karush-Kuhn-Tucker (KKT) system for the trajectory derivative, and achieve this efficiently by solving an auxiliary Linear Quadratic Regulator (LQR) problem. In contrast, we directly evaluate the matrix equations which arise from applying variable elimination on the Lagrange multiplier terms in the (differential) KKT system. By appropriately accounting for the structure of the terms within the resulting equations, we show that the trajectory derivatives scale linearly with the number of timesteps. Furthermore, our approach allows for easy parallelization, significantly improved scalability with model size, direct computation of vector-Jacobian products and improved numerical stability compared to prior works. As an additional contribution, we unify prior works, addressing claims that computing trajectory derivatives using IFT scales quadratically with the number of timesteps. We evaluate our method on a both synthetic benchmark and four challenging, learning from demonstration benchmarks including a 6-DoF maneuvering quadrotor and 6-DoF rocket powered landing.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\7PS744C4\Xu et al. - 2023 - Revisiting Implicit Differentiation for Learning Problems in Optimal Control.pdf}
}

@article{yamadaMotionPlannerAugmented2020,
  title = {Motion {{Planner Augmented Reinforcement Learning}} for {{Robot Manipulation}} in {{Obstructed Environments}}},
  author = {Yamada, Jun and Lee, Youngwoon and Salhotra, Gautam and Pertsch, Karl and Pflueger, Max and Sukhatme, Gaurav S. and Lim, Joseph J. and Englert, Peter},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.11940 [cs]},
  eprint = {2010.11940},
  primaryclass = {cs},
  urldate = {2021-05-11},
  abstract = {Deep reinforcement learning (RL) agents are able to learn contact-rich manipulation tasks by maximizing a reward signal, but require large amounts of experience, especially in environments with many obstacles that complicate exploration. In contrast, motion planners use explicit models of the agent and environment to plan collision-free paths to faraway goals, but suffer from inaccurate models in tasks that require contacts with the environment. To combine the benefits of both approaches, we propose motion planner augmented RL (MoPA-RL) which augments the action space of an RL agent with the long-horizon planning capabilities of motion planners. Based on the magnitude of the action, our approach smoothly transitions between directly executing the action and invoking a motion planner. We evaluate our approach on various simulated manipulation tasks and compare it to alternative action spaces in terms of learning efficiency and safety. The experiments demonstrate that MoPA-RL increases learning efficiency, leads to a faster exploration, and results in safer policies that avoid collisions with the environment. Videos and code are available at https://clvrai.com/mopa-rl .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\WFM8LKA2\2010.html}
}

@article{yamamotoDevelopmentHumanSupport2019,
  title = {Development of {{Human Support Robot}} as the Research Platform of a Domestic Mobile Manipulator},
  author = {Yamamoto, Takashi and Terada, Koji and Ochiai, Akiyoshi and Saito, Fuminori and Asahara, Yoshiaki and Murase, Kazuto},
  year = {2019},
  month = apr,
  journal = {ROBOMECH Journal},
  volume = {6},
  number = {1},
  pages = {4},
  issn = {2197-4225},
  doi = {10.1186/s40648-019-0132-3},
  urldate = {2020-08-26},
  abstract = {There has been an increasing interest in mobile manipulators that are capable of performing physical work in living spaces worldwide, corresponding to an aging population with declining birth rates with the expectation of improving quality of life (QoL). We assume that overall research and development will accelerate by using a common robot platform among a lot of researchers since that enables them to share their research results. Therefore we have developed a compact and safe research platform, Human Support Robot (HSR), which can be operated in an actual home environment and we have provided it to various research institutes to establish the developers community. Currently, the number of HSR users is expanding to 44 sites in 12 countries worldwide (as of November 30th, 2018). To activate the community, we assume that the robot competition will be effective. As a result of international public offering, HSR has been adopted as a standard platform for international robot competitions such as RoboCup@Home and World Robot Summit (WRS). HSR is provided to participants of those competitions. In this paper, we describe HSR's development background since 2006, and technical detail of hardware design and software architecture. Specifically, we describe its omnidirectional mobile base using the dual-wheel caster-drive mechanism, which is the basis of HSR's operational movement and a novel whole body motion control system. Finally, we describe the verification of autonomous task capability and~the results of utilization in RoboCup@Home in order to demonstrate the effect of introducing the platform.},
  langid = {english}
}

@inproceedings{yamamotoHumanSupportRobot2018,
  title = {Human Support Robot ({{HSR}})},
  booktitle = {{{ACM SIGGRAPH}} 2018 {{Emerging Technologies}}},
  author = {Yamamoto, Takashi and Nishino, Tamaki and Kajima, Hideki and Ohta, Mitsunori and Ikeda, Koichi},
  year = {2018},
  month = aug,
  series = {{{SIGGRAPH}} '18},
  pages = {1--2},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3214907.3233972},
  urldate = {2020-08-26},
  abstract = {There has been an increasing interest in mobile manipulators that is capable of performing physical work in living spaces worldwide, corresponding to population aging with declining birth rates with the expectation of improving quality of life (QOL). Research and development is a must in intelligent sensing and software which enable advanced recognition, judgment, and motion to realize household work by robots. In order to accelerate this research, we have developed a compact and safe research platform, Human Support Robot (HSR), which can be operated in an actual home environment. We assume that overall R\&D will accelerate by using a common robot platform among many researchers since that enables them to share their research results. In this paper, we introduce HSR design and its utilization.},
  isbn = {978-1-4503-5810-1},
  keywords = {mobile manipulator}
}

@inproceedings{Yamazaki.2014,
  title = {Grasping Point Selection on an Item of Crumpled Clothing Based on Relational Shape Description},
  booktitle = {2014 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems},
  author = {Yamazaki, Kimitoshi},
  year = {2014},
  pages = {3123--3128},
  publisher = {IEEE},
  doi = {10.1109/IROS.2014.6942994},
  bookpagination = {page},
  isbn = {978-1-4799-6934-0}
}

@book{yampolskiyArtificialIntelligenceSafety2018,
  title = {{Artificial Intelligence Safety and Security}},
  author = {Yampolskiy, Roman V.},
  year = {2018},
  month = aug,
  edition = {Reprint Edition},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton},
  abstract = {The history of robotics and artificial intelligence in many ways is also the history of humanity's attempts to control such technologies. From the Golem of Prague to the military robots of modernity, the debate continues as to what degree of independence such entities should have and how to make sure that they do not turn on us, its inventors. Numerous recent advancements in all aspects of research, development and deployment of intelligent systems are well publicized but safety and security issues related to AI are rarely addressed. This book is proposed to mitigate this fundamental problem. It is comprised of chapters from leading AI Safety researchers addressing different aspects of the AI control problem as it relates to the development of safe and secure artificial intelligence. The book is the first edited volume dedicated to addressing challenges of constructing safe and secure advanced machine intelligence. The chapters vary in length and technical content from broad interest opinion essays to highly formalized algorithmic approaches to specific problems. All chapters are self-contained and could be read in any order or skipped without a loss of comprehension.},
  isbn = {978-0-8153-6982-0},
  langid = {Englisch}
}

@article{yampolskiyControllabilityArtificialIntelligence2022,
  title = {On the {{Controllability}} of {{Artificial Intelligence}}: {{An Analysis}} of {{Limitations}}},
  shorttitle = {On the {{Controllability}} of {{Artificial Intelligence}}},
  author = {Yampolskiy, Roman V.},
  year = {2022},
  month = may,
  journal = {Journal of Cyber Security and Mobility},
  pages = {321--404},
  issn = {2245-4578},
  doi = {10.13052/jcsm2245-1439.1132},
  urldate = {2024-03-04},
  abstract = {The invention of artificial general intelligence is predicted to cause a shift in the trajectory of human civilization. In order to reap the benefits and avoid the pitfalls of such a powerful technology it is important to be able to control it. However, the possibility of controlling artificial general intelligence and its more advanced version, superintelligence, has not been formally established. In this paper, we present arguments as well as supporting evidence from multiple domains indicating that advanced AI cannot be fully controlled. The consequences of uncontrollability of AI are discussed with respect to the future of humanity and research on AI, and AI safety and security.},
  copyright = {Copyright (c) 2022},
  langid = {english},
  keywords = {X-risk}
}

@article{Yang.2002,
  title = {Detecting Faces in Images: A Survey},
  author = {Yang, Ming-Hsuan and Kriegman, D. J. and Ahuja, N.},
  year = {2002},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {24},
  number = {1},
  pages = {34--58},
  issn = {01628828},
  doi = {10.1109/34.982883},
  pagination = {page}
}

@inproceedings{yangGroundingInteractiveMachine2018,
  title = {Grounding {{Interactive Machine Learning Tool Design}} in {{How Non-Experts Actually Build Models}}},
  booktitle = {Proceedings of the 2018 {{Designing Interactive Systems Conference}}},
  author = {Yang, Qian and Suh, Jina and Chen, Nan-Chen and Ramos, Gonzalo},
  year = {2018},
  month = jun,
  series = {{{DIS}} '18},
  pages = {573--584},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3196709.3196729},
  urldate = {2024-09-08},
  abstract = {Machine learning (ML) promises data-driven insights and solutions for people from all walks of life, but the skill of crafting these solutions is possessed by only a few. Emerging research addresses this issue by creating ML tools that are easy and accessible to people who are not formally trained in ML (non-experts). This work investigated how non-experts build ML solutions for themselves in real life. Our interviews and surveys revealed unique potentials of non-expert ML, as well several pitfalls that non-experts are susceptible to. For example, many perceived percentage accuracy as a sole measure of performance, thus problematic models proceeded to deployment. These observations suggested that, while challenging, making ML easy and robust should both be important goals of designing novice-facing ML tools. To advance on this insight, we discuss design implications and created a sensitizing concept to demonstrate how designers might guide non-experts to easily build robust solutions.},
  isbn = {978-1-4503-5198-0}
}

@inproceedings{yangIfLLMWizard2024,
  title = {If {{LLM Is}} the {{Wizard}}, {{Then Code Is}} the {{Wand}}: {{A Survey}} on {{How Code Empowers Large Language Models}} to {{Serve}} as {{Intelligent Agents}}},
  shorttitle = {If {{LLM Is}} the {{Wizard}}, {{Then Code Is}} the {{Wand}}},
  booktitle = {{{ICLR}} 2024 {{Workshop}} on {{Large Language Model}} ({{LLM}}) {{Agents}}},
  author = {Yang, Ke and Liu, Jiateng and Wu, John and Yang, Chaoqi and Fung, Yi and Li, Sha and Huang, Zixuan and Cao, Xu and Wang, Xingyao and Ji, Heng and Zhai, ChengXiang},
  year = {2024},
  month = mar,
  urldate = {2024-09-04},
  abstract = {The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and code. As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs' training data. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs). Finally, we present several key challenges and future directions of empowering code-LLMs to serve as IAs.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\5622PJSW\Yang et al. - 2024 - If LLM Is the Wizard, Then Code Is the Wand A Survey on How Code Empowers Large Language Models to.pdf}
}

@inproceedings{yangLowComplexityAcousticEcho2023,
  title = {Low-{{Complexity Acoustic Echo Cancellation}} with {{Neural Kalman Filtering}}},
  booktitle = {{{ICASSP}} 2023 - 2023 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Yang, Dong and Jiang, Fei and Wu, Wei and Fang, Xuefei and Cao, Muyong},
  year = {2023},
  month = jun,
  pages = {1--5},
  issn = {2379-190X},
  doi = {10.1109/ICASSP49357.2023.10096597},
  urldate = {2024-03-25},
  abstract = {The Kalman filter has been adopted in acoustic echo cancellation due to its robustness to double-talk, fast convergence, and good steady-state performance. The performance of Kalman filter is closely related to the estimation accuracy of the state noise covariance and the observation noise covariance. The estimation error may lead to unacceptable results, especially when the echo path suffers abrupt changes, the tracking performance of the Kalman filter could be degraded significantly. In this paper, we propose the neural Kalman filtering (NKF), which uses neural networks to implicitly model the covariance of the state noise and observation noise and to output the Kalman gain in real-time. Experimental results on both synthetic test sets and real-recorded test sets show that, the proposed NKF has superior convergence and re-convergence performance while ensuring low near-end speech degradation compared with the state-of-the-art model-based methods. Moreover, the model size of the proposed NKF is merely 5.3 K and the RTF is as low as 0.09, which indicates that it can be deployed in low-resource platforms.},
  keywords = {Acoustic echo cancellation,Acoustics,Echo cancellers,Filtering,Kalman filter,neural networks,Neural networks,Robustness,Signal processing,Steady-state}
}

@article{yangLSTMGRUNeural2020,
  title = {{{LSTM}} and {{GRU Neural Network Performance Comparison Study}}: {{Taking Yelp Review Dataset}} as an {{Example}}},
  shorttitle = {{{LSTM}} and {{GRU Neural Network Performance Comparison Study}}},
  author = {Yang, Shudong and Yu, Xueying and Zhou, Ying},
  year = {2020},
  month = jun,
  journal = {2020 International Workshop on Electronic Communication and Artificial Intelligence (IWECAI)},
  pages = {98--101},
  publisher = {IEEE},
  address = {Shanghai, China},
  doi = {10.1109/IWECAI50956.2020.00027},
  urldate = {2024-08-05},
  abstract = {Long short-term memory networks(LSTM) and gate recurrent unit networks(GRU) are two popular variants of recurrent neural networks(RNN) with long-term memory. This study compares the performance differences of these two deep learning models, involving two dimensions: dataset size for training, long/short text, and quantitative evaluation on five indicators including running speed, accuracy, recall, F1 value, and AUC. The corpus uses the datasets officially released by Yelp Inc. In terms of model training speed, GRU is 29.29\% faster than LSTM for processing the same dataset; and in terms of performance, GRU performance will surpass LSTM in the scenario of long text and small dataset, and inferior to LSTM in other scenarios. Considering the two dimensions of both performance and computing power cost, the performance-cost ratio of GRU is higher than that of LSTM, which is 23.45\%, 27.69\%, and 26.95\% higher in accuracy ratio, recall ratio, and F1 ratio respectively.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {9781728181493}
}

@inproceedings{yangReexaminingWhetherWhy2020,
  title = {Re-Examining {{Whether}}, {{Why}}, and {{How Human-AI Interaction Is Uniquely Difficult}} to {{Design}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Yang, Qian and Steinfeld, Aaron and Ros{\'e}, Carolyn and Zimmerman, John},
  year = {2020},
  month = apr,
  series = {{{CHI}} '20},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3313831.3376301},
  urldate = {2024-09-08},
  abstract = {Artificial Intelligence (AI) plays an increasingly important role in improving HCI and user experience. Yet many challenges persist in designing and innovating valuable human-AI interactions. For example, AI systems can make unpredictable errors, and these errors damage UX and even lead to undesired societal impact. However, HCI routinely grapples with complex technologies and mitigates their unintended consequences. What makes AI different? What makes human-AI interaction appear particularly difficult to design? This paper investigates these questions. We synthesize prior research, our own design and research experience, and our observations when teaching human-AI interaction. We identify two sources of AI's distinctive design challenges: 1) uncertainty surrounding AI's capabilities, 2) AI's output complexity, spanning from simple to adaptive complex. We identify four levels of AI systems. On each level, designers encounter a different subset of the design challenges. We demonstrate how these findings reveal new insights for designers, researchers, and design tool makers in productively addressing the challenges of human-AI interaction going forward.},
  isbn = {978-1-4503-6708-0},
  file = {C:\Users\benja\Zotero\storage\FU338V45\Yang et al. - 2020 - Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design.pdf}
}

@article{yaoPositionOrientationError2015,
  title = {Position and Orientation Error Analysis and Its Compensation for a Wheeled Train Uncoupling Robot with Four Degrees-of-Freedom},
  author = {Yao, Jianjun and Gao, Shuang and Jiang, Guilin and Hill, Thomas L. and Yu, Han and Xiao, Rui and Chen, Shuo},
  year = {2015},
  journal = {IET Intelligent Transport Systems},
  volume = {9},
  number = {2},
  pages = {156--166},
  issn = {1751-9578},
  doi = {10.1049/iet-its.2014.0027},
  urldate = {2022-08-01},
  abstract = {A wheeled train uncoupling robot with four degrees-of-freedom has been developed to replace humans in the uncoupling task in a marshalling field for designating freight cars to different destinations. To successfully achieve the task in practical applications, the positioning accuracy of the robot is an important issue to be considered. Based on the kinematic model using Denavit--Hartenberg method, the matrix differential method is applied here to establish the static position and orientation error model. The impact of parameter errors upon the static pose error of the uncoupling manipulator is analysed. The flexibility of the robot's key components is taken into consideration to analyse its impact on the position and orientation error of the manipulator. The position and orientation error compensation is developed by using input motion planning method to improve the pose accuracy of the robot. Additional motions are added to each joint of the robot such that the uncoupling manipulator can generate a corresponding tiny perturbation, which is used to eliminate the positioning error, ensuring the uncoupling action is completed successfully.},
  langid = {english},
  keywords = {compensation,degrees-of-freedom,Denavit-Hartenberg method,flexible manipulators,freight cars,freight handling,kinematic model,manipulator kinematics,marshalling field,matrix algebra,matrix differential method,mobile robots,motion planning method,orientation error compensation,orientation error model,parameter errors,path planning,perturbation,perturbation techniques,pose accuracy,position control,positioning accuracy,railway rolling stock,robot flexibility,static pose error,static position,uncoupling manipulator,wheeled train uncoupling robot,wheels}
}

@article{yasunagaQAGNNReasoningLanguage2021,
  title = {{{QA-GNN}}: {{Reasoning}} with {{Language Models}} and {{Knowledge Graphs}} for {{Question Answering}}},
  shorttitle = {{{QA-GNN}}},
  author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
  year = {2021},
  month = nov,
  journal = {arXiv:2104.06378 [cs]},
  eprint = {2104.06378},
  primaryclass = {cs},
  urldate = {2021-12-19},
  abstract = {The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. In this work, we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph neural networks. We evaluate our model on QA benchmarks in the commonsense (CommonsenseQA, OpenBookQA) and biomedical (MedQA-USMLE) domains. QA-GNN outperforms existing LM and LM+KG models, and exhibits capabilities to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\V3UKUZH3\2104.html}
}

@inproceedings{yehLearningDeepLatent2017,
  title = {Learning Deep Latent Spaces for Multi-Label Classification},
  booktitle = {Proceedings of the {{Thirty-First AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Yeh, Chih-Kuan and Wu, Wei-Chieh and Ko, Wei-Jen and Wang, Yu-Chiang Frank},
  year = {2017},
  month = feb,
  series = {{{AAAI}}'17},
  pages = {2838--2844},
  publisher = {AAAI Press},
  address = {San Francisco, California, USA},
  urldate = {2024-04-27},
  abstract = {Multi-label classification is a practical yet challenging task in machine learning related fields, since it requires the prediction of more than one label category for each input instance. We propose a novel deep neural networks (DNN) based model, Canonical Correlated AutoEncoder (C2AE), for solving this task. Aiming at better relating feature and label domain data for improved classification, we uniquely perform joint feature and label embedding by deriving a deep latent space, followed by the introduction of label-correlation sensitive loss function for recovering the predicted label outputs. Our C2AE is achieved by integrating the DNN architectures of canonical correlation analysis and autoencoder, which allows end-to-end learning and prediction with the ability to exploit label dependency. Moreover, our C2AE can be easily extended to address the learning problem with missing labels. Our experiments on multiple datasets with different scales confirm the effectiveness and robustness of our proposed method, which is shown to perform favorably against state-of-the-art methods for multi-label classification.}
}

@inproceedings{yen-chenLearningSeeLearning2020,
  title = {Learning to {{See}} before {{Learning}} to {{Act}}: {{Visual Pre-training}} for {{Manipulation}}},
  shorttitle = {Learning to {{See}} before {{Learning}} to {{Act}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {{Yen-Chen}, Lin and Zeng, Andy and Song, Shuran and Isola, Phillip and Lin, Tsung-Yi},
  year = {2020},
  month = may,
  pages = {7286--7293},
  issn = {2577-087X},
  doi = {10.1109/ICRA40945.2020.9197331},
  abstract = {Does having visual priors (e.g. the ability to detect objects) facilitate learning to perform vision-based manipulation (e.g. picking up objects)? We study this problem under the framework of transfer learning, where the model is first trained on a passive vision task (i.e., the data distribution does not depend on the agent's decisions), then adapted to perform an active manipulation task (i.e., the data distribution does depend on the agent's decisions). We find that pre-training on vision tasks significantly improves generalization and sample efficiency for learning to manipulate objects. However, realizing these gains requires careful selection of which parts of the model to transfer. Our key insight is that outputs of standard vision models highly correlate with affordance maps commonly used in manipulation. Therefore, we explore directly transferring model parameters from vision networks to affordance prediction networks, and show that this can result in successful zero-shot adaptation, where a robot can pick up certain objects with zero robotic experience. With just a small amount of robotic experience, we can further fine-tune the affordance model to achieve better results. With just 10 minutes of suction experience or 1 hour of grasping experience, our method achieves 80\% success rate at picking up novel objects.},
  keywords = {Data models,Grasping,Head,Predictive models,Robots,Task analysis,Visualization},
  file = {C:\Users\benja\Zotero\storage\Y46FBUY9\9197331.html}
}

@article{yildirimTaskStructuresWorld2024,
  title = {From Task Structures to World Models: What Do {{LLMs}} Know?},
  shorttitle = {From Task Structures to World Models},
  author = {Yildirim, Ilker and Paul, L. A.},
  year = {2024},
  month = may,
  journal = {Trends in Cognitive Sciences},
  volume = {28},
  number = {5},
  pages = {404--415},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2024.02.008},
  urldate = {2024-08-16},
  langid = {english},
  pmid = {38443199},
  keywords = {instrumental knowledge,intelligence,large language models,resource rational,world models,worldly knowledge}
}

@article{yinMetaLearningMemorization2020,
  title = {Meta-{{Learning}} without {{Memorization}}},
  author = {Yin, Mingzhang and Tucker, George and Zhou, Mingyuan and Levine, Sergey and Finn, Chelsea},
  year = {2020},
  month = apr,
  journal = {arXiv:1912.03820 [cs, stat]},
  eprint = {1912.03820},
  primaryclass = {cs, stat},
  urldate = {2021-01-25},
  abstract = {The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes. This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-learning entirely inapplicable. In this paper, we address this challenge by designing a meta-regularization objective using information theory that places precedence on data-driven adaptation. This causes the meta-learner to decide what must be learned from the task training data and what should be inferred from the task testing input. By doing so, our algorithm can successfully use data from non-mutually-exclusive tasks to efficiently adapt to novel tasks. We demonstrate its applicability to both contextual and gradient-based meta-learning algorithms, and apply it in practical settings where applying standard meta-learning has been difficult. Our approach substantially outperforms standard meta-learning algorithms in these settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\benja\\Zotero\\storage\\C5U2UIZD\\Yin et al. - 2020 - Meta-Learning without Memorization.pdf;C\:\\Users\\benja\\Zotero\\storage\\DSPX2RTG\\1912.html}
}

@inproceedings{yinSyntacticNeuralModel2017,
  title = {A {{Syntactic Neural Model}} for {{General-Purpose Code Generation}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Yin, Pengcheng and Neubig, Graham},
  year = {2017},
  month = jul,
  pages = {440--450},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/P17-1041},
  urldate = {2019-07-10},
  abstract = {We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.}
}

@incollection{yipRobotAutonomySurgery2018,
  title = {Robot Autonomy for Surgery},
  booktitle = {The {{Encyclopedia}} of {{Medical Robotics}}},
  author = {Yip, Michael and Das, Nikhil},
  year = {2018},
  month = aug,
  pages = {281--313},
  publisher = {WORLD SCIENTIFIC},
  doi = {10.1142/9789813232266_0010},
  urldate = {2024-04-12},
  isbn = {978-981-323-225-9}
}

@inproceedings{yonetaniPathPlanningUsing2021,
  title = {Path {{Planning}} Using {{Neural A}}* {{Search}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Yonetani, Ryo and Taniai, Tatsunori and Barekatain, Mohammadamin and Nishimura, Mai and Kanezaki, Asako},
  year = {2021},
  month = jul,
  pages = {12029--12039},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-26},
  abstract = {We present Neural A*, a novel data-driven search method for path planning problems. Despite the recent increasing attention to data-driven path planning, machine learning approaches to search-based planning are still challenging due to the discrete nature of search algorithms. In this work, we reformulate a canonical A* search algorithm to be differentiable and couple it with a convolutional encoder to form an end-to-end trainable neural network planner. Neural A* solves a path planning problem by encoding a problem instance to a guidance map and then performing the differentiable A* search with the guidance map. By learning to match the search results with ground-truth paths provided by experts, Neural A* can produce a path consistent with the ground truth accurately and efficiently. Our extensive experiments confirmed that Neural A* outperformed state-of-the-art data-driven planners in terms of the search optimality and efficiency trade-off. Furthermore, Neural A* successfully predicted realistic human trajectories by directly performing search-based planning on natural image inputs.},
  langid = {english},
  file = {C:\Users\benja\Zotero\storage\TQYPM37I\Yonetani et al. - 2021 - Path Planning using Neural A Search.pdf}
}

@article{yooLearningLossActive2019,
  title = {Learning {{Loss}} for {{Active Learning}}},
  author = {Yoo, Donggeun and Kweon, In So},
  year = {2019},
  month = may,
  journal = {arXiv:1905.03677 [cs]},
  eprint = {1905.03677},
  primaryclass = {cs},
  urldate = {2020-12-19},
  abstract = {The performance of deep neural networks improves with more annotated data. The problem is that the budget for annotation is limited. One solution to this is active learning, where a model asks human to annotate data that it perceived as uncertain. A variety of recent methods have been proposed to apply active learning to deep networks but most of them are either designed specific for their target tasks or computationally inefficient for large networks. In this paper, we propose a novel active learning method that is simple but task-agnostic, and works efficiently with the deep networks. We attach a small parametric module, named "loss prediction module," to a target network, and learn it to predict target losses of unlabeled inputs. Then, this module can suggest data that the target model is likely to produce a wrong prediction. This method is task-agnostic as networks are learned from a single loss regardless of target tasks. We rigorously validate our method through image classification, object detection, and human pose estimation, with the recent network architectures. The results demonstrate that our method consistently outperforms the previous methods over the tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\MF9E2DDE\1905.html}
}

@inproceedings{yosinskiHowTransferableAre2014,
  title = {How Transferable Are Features in Deep Neural Networks?},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  year = {2014},
  month = dec,
  series = {{{NIPS}}'14},
  pages = {3320--3328},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  urldate = {2024-06-14},
  abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.}
}

@article{yuanADPTAutonomousDriving2023,
  title = {{{AD-PT}}: {{Autonomous Driving Pre-Training}} with {{Large-scale Point Cloud Dataset}}},
  shorttitle = {{{AD-PT}}},
  author = {Yuan, Jiakang and Zhang, Bo and Yan, Xiangchao and Shi, Botian and Chen, Tao and Li, Yikang and Qiao, Yu},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {47914--47933},
  urldate = {2024-06-14},
  langid = {english}
}

@article{yuMetaWorldBenchmarkEvaluation2019,
  title = {Meta-{{World}}: {{A Benchmark}} and {{Evaluation}} for {{Multi-Task}} and {{Meta Reinforcement Learning}}},
  shorttitle = {Meta-{{World}}},
  author = {Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.10897 [cs, stat]},
  eprint = {1910.10897},
  primaryclass = {cs, stat},
  urldate = {2019-11-03},
  abstract = {Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 6 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning}
}

@inproceedings{yuOneShotImitationObserving2018,
  title = {One-{{Shot Imitation}} from {{Observing Humans}} via {{Domain-Adaptive Meta-Learning}}},
  booktitle = {Robotics: {{Science}} and {{Systems XIV}}, {{Carnegie Mellon University}}, {{Pittsburgh}}, {{Pennsylvania}}, {{USA}}, {{June}} 26-30, 2018},
  author = {Yu, Tianhe and Finn, Chelsea and Dasari, Sudeep and Xie, Annie and Zhang, Tianhao and Abbeel, Pieter and Levine, Sergey},
  editor = {{Kress-Gazit}, Hadas and Srinivasa, Siddhartha S. and Howard, Tom and Atanasov, Nikolay},
  year = {2018},
  doi = {10.15607/RSS.2018.XIV.002},
  urldate = {2024-06-15}
}

@inproceedings{yuRetrievalaugmentedGenerationHeterogeneous2022,
  title = {Retrieval-Augmented {{Generation}} across {{Heterogeneous Knowledge}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}: {{Student Research Workshop}}},
  author = {Yu, Wenhao},
  editor = {Ippolito, Daphne and Li, Liunian Harold and Pacheco, Maria Leonor and Chen, Danqi and Xue, Nianwen},
  year = {2022},
  month = jul,
  pages = {52--58},
  publisher = {Association for Computational Linguistics},
  address = {Hybrid: Seattle, Washington + Online},
  doi = {10.18653/v1/2022.naacl-srw.7},
  urldate = {2024-09-04},
  abstract = {Retrieval-augmented generation (RAG) methods have been receiving increasing attention from the NLP community and achieved state-of-the-art performance on many NLP downstream tasks. Compared with conventional pre-trained generation models, RAG methods have remarkable advantages such as easy knowledge acquisition, strong scalability, and low training cost. Although existing RAG models have been applied to various knowledge-intensive NLP tasks, such as open-domain QA and dialogue systems, most of the work has focused on retrieving unstructured text documents from Wikipedia. In this paper, I first elaborate on the current obstacles to retrieving knowledge from a single-source homogeneous corpus. Then, I demonstrate evidence from both existing literature and my experiments, and provide multiple solutions on retrieval-augmented generation methods across heterogeneous knowledge.},
  file = {C:\Users\benja\Zotero\storage\HNKIQW8H\Yu - 2022 - Retrieval-augmented Generation across Heterogeneous Knowledge.pdf}
}

@article{yuSurveyKnowledgeenhancedText2022,
  title = {A {{Survey}} of {{Knowledge-enhanced Text Generation}}},
  author = {Yu, Wenhao and Zhu, Chenguang and Li, Zaitang and Hu, Zhiting and Wang, Qingyun and Ji, Heng and Jiang, Meng},
  year = {2022},
  month = nov,
  journal = {ACM Comput. Surv.},
  volume = {54},
  number = {11s},
  pages = {227:1--227:38},
  issn = {0360-0300},
  doi = {10.1145/3512467},
  urldate = {2024-09-04},
  abstract = {The goal of text-to-text generation is to make machines express like a human in many applications such as conversation, summarization, and translation. It is one of the most important yet challenging tasks in natural language processing (NLP). Various neural encoder-decoder models have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating (i) internal knowledge embedded in the input text and (ii) external knowledge from outside sources such as knowledge base and knowledge graph into the text generation system. This research topic is known as knowledge-enhanced text generation. In this survey, we present a comprehensive review of the research on this topic over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry.},
  file = {C:\Users\benja\Zotero\storage\37S7NAQT\Yu et al. - 2022 - A Survey of Knowledge-enhanced Text Generation.pdf}
}

@article{yuSurveyNeuralsymbolicLearning2023,
  title = {A Survey on Neural-Symbolic Learning Systems},
  author = {Yu, Dongran and Yang, Bo and Liu, Dayou and Wang, Hui and Pan, Shirui},
  year = {2023},
  month = sep,
  journal = {Neural Networks},
  volume = {166},
  pages = {105--126},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2023.06.028},
  urldate = {2024-03-06},
  abstract = {In recent years, neural systems have demonstrated highly effective learning ability and superior perception intelligence. However, they have been found to lack effective reasoning and cognitive ability. On the other hand, symbolic systems exhibit exceptional cognitive intelligence but suffer from poor learning capabilities when compared to neural systems. Recognizing the advantages and disadvantages of both methodologies, an ideal solution emerges: combining neural systems and symbolic systems to create neural-symbolic learning systems that possess powerful perception and cognition. The purpose of this paper is to survey the advancements in neural-symbolic learning systems from four distinct perspectives: challenges, methods, applications, and future directions. By doing so, this research aims to propel this emerging field forward, offering researchers a comprehensive and holistic overview. This overview will not only highlight the current state-of-the-art but also identify promising avenues for future research.},
  keywords = {Knowledge graphs,Logic,Neural networks,Neural-symbolic learning systems,Symbolic reasoning,Symbols}
}

@article{zakkaForm2FitLearningShape2020,
  title = {{{Form2Fit}}: {{Learning Shape Priors}} for {{Generalizable Assembly}} from {{Disassembly}}},
  shorttitle = {{{Form2Fit}}},
  author = {Zakka, Kevin and Zeng, Andy and Lee, Johnny and Song, Shuran},
  year = {2020},
  month = may,
  journal = {arXiv:1910.13675 [cs]},
  eprint = {1910.13675},
  primaryclass = {cs},
  urldate = {2020-06-30},
  abstract = {Is it possible to learn policies for robotic assembly that can generalize to new objects? We explore this idea in the context of the kit assembly task. Since classic methods rely heavily on object pose estimation, they often struggle to generalize to new objects without 3D CAD models or task-specific training data. In this work, we propose to formulate the kit assembly task as a shape matching problem, where the goal is to learn a shape descriptor that establishes geometric correspondences between object surfaces and their target placement locations from visual input. This formulation enables the model to acquire a broader understanding of how shapes and surfaces fit together for assembly -- allowing it to generalize to new objects and kits. To obtain training data for our model, we present a self-supervised data-collection pipeline that obtains ground truth object-to-placement correspondences by disassembling complete kits. Our resulting real-world system, Form2Fit, learns effective pick and place strategies for assembling objects into a variety of kits -- achieving \$90{\textbackslash}\%\$ average success rates under different initial conditions (e.g. varying object and kit poses), \$94{\textbackslash}\%\$ success under new configurations of multiple kits, and over \$86{\textbackslash}\%\$ success with completely new objects and kits.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\U4Q33W4H\1910.html}
}

@misc{zengLargeLanguageModels2023,
  title = {Large {{Language Models}} for {{Robotics}}: {{A Survey}}},
  shorttitle = {Large {{Language Models}} for {{Robotics}}},
  author = {Zeng, Fanlong and Gan, Wensheng and Wang, Yongheng and Liu, Ning and Yu, Philip S.},
  year = {2023},
  month = nov,
  number = {arXiv:2311.07226},
  eprint = {2311.07226},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.07226},
  urldate = {2024-04-29},
  abstract = {The human ability to learn, generalize, and control complex manipulation tasks through multi-modality feedback suggests a unique capability, which we refer to as dexterity intelligence. Understanding and assessing this intelligence is a complex task. Amidst the swift progress and extensive proliferation of large language models (LLMs), their applications in the field of robotics have garnered increasing attention. LLMs possess the ability to process and generate natural language, facilitating efficient interaction and collaboration with robots. Researchers and engineers in the field of robotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot interaction, and autonomy. Therefore, this comprehensive review aims to summarize the applications of LLMs in robotics, delving into their impact and contributions to key areas such as robot control, perception, decision-making, and path planning. We first provide an overview of the background and development of LLMs for robotics, followed by a description of the benefits of LLMs for robotics and recent advancements in robotics models based on LLMs. We then delve into the various techniques used in the model, including those employed in perception, decision-making, control, and interaction. Finally, we explore the applications of LLMs in robotics and some potential challenges they may face in the near future. Embodied intelligence is the future of intelligent science, and LLMs-based robotics is one of the promising but challenging paths to achieve this.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\FHFIA2S3\2311.html}
}

@article{zhan_color-based_2009,
  title = {Color-Based Segmentation of Point Clouds},
  author = {Zhan, Q. and Liang, Yubin and Xiao, Yinghui},
  year = {2009},
  month = jul,
  journal = {ISPRS Laser Scanning Workshop},
  volume = {38},
  abstract = {Segmentation is one of the most fundamental procedures for the automation of point cloud processing. The methods based on geometrical derivatives such as curvature and normals often lead to over-segmentation and even failure when used to segment point clouds of geometrically-complex architectures. In this paper we present a point cloud segmentation algorithm based on colorimetrical similarity and spatial proximity. The algorithm contains region growing, region merging and refinement processes. The region growing process uses kd-tree to search the k-nearest neighbors of each seed point. The resulting regions are then merged and finally refined on the basis of colorimetrical and spatial relation. In each of the process, we developed different criteria corresponding to the different tasks to carry on the segmentation. The algorithm requires a small number of manually set parameters which are used to keep balance between under-and over-segmentation. The experiments of the presented algorithm on a point cloud of Chinese ancient architecture show its effectiveness. The segmented regions can be used to reconstruct 3D models of different parts of the architectures.},
  file = {C:\Users\benja\Zotero\storage\77W8NDY2\Zhan et al_2009_Color-based segmentation of point clouds.pdf}
}

@article{zhanColorbasedSegmentationPoint2009,
  title = {Color-Based Segmentation of Point Clouds},
  author = {Zhan, Q. and Liang, Yubin and Xiao, Yinghui},
  year = {2009},
  month = jul,
  journal = {ISPRS Laser Scanning Workshop},
  volume = {38},
  abstract = {Segmentation is one of the most fundamental procedures for the automation of point cloud processing. The methods based on geometrical derivatives such as curvature and normals often lead to over-segmentation and even failure when used to segment point clouds of geometrically-complex architectures. In this paper we present a point cloud segmentation algorithm based on colorimetrical similarity and spatial proximity. The algorithm contains region growing, region merging and refinement processes. The region growing process uses kd-tree to search the k-nearest neighbors of each seed point. The resulting regions are then merged and finally refined on the basis of colorimetrical and spatial relation. In each of the process, we developed different criteria corresponding to the different tasks to carry on the segmentation. The algorithm requires a small number of manually set parameters which are used to keep balance between under-and over-segmentation. The experiments of the presented algorithm on a point cloud of Chinese ancient architecture show its effectiveness. The segmented regions can be used to reconstruct 3D models of different parts of the architectures.}
}

@article{Zhang.2000,
  title = {A Flexible New Technique for Camera Calibration},
  author = {Zhang, Z.},
  year = {2000},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {22},
  number = {11},
  pages = {1330--1334},
  issn = {01628828},
  doi = {10.1109/34.888718},
  pagination = {page}
}

@misc{zhangBERTScoreEvaluatingText2020,
  title = {{{BERTScore}}: {{Evaluating Text Generation}} with {{BERT}}},
  shorttitle = {{{BERTScore}}},
  author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
  year = {2020},
  month = feb,
  number = {arXiv:1904.09675},
  eprint = {1904.09675},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.09675},
  urldate = {2023-10-30},
  abstract = {We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\benja\Zotero\storage\PX3UJTM2\1904.html}
}

@inproceedings{zhangDeepImitationLearning2018,
  title = {Deep {{Imitation Learning}} for {{Complex Manipulation Tasks}} from {{Virtual Reality Teleoperation}}},
  booktitle = {{{ICRA}}},
  author = {Zhang, Tianhao and McCarthy, Zoe and Jow, Owen and Lee, Dennis and Chen, Xi and Goldberg, Ken and Abbeel, Pieter},
  year = {2018},
  month = may,
  pages = {5628--5635},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2018.8461249},
  abstract = {Imitation learning is a powerful paradigm for robot skill acquisition. However, obtaining demonstrations suitable for learning a policy that maps from raw pixels to actions can be challenging. In this paper we describe how consumer-grade Virtual Reality headsets and hand tracking hardware can be used to naturally teleoperate robots to perform complex tasks. We also describe how imitation learning can learn deep neural network policies (mapping from pixels to actions) that can acquire the demonstrated skills. Our experiments showcase the effectiveness of our approach for learning visuomotor skills.},
  keywords = {consumer-grade Virtual Reality headsets,control engineering computing,deep imitation learning,deep neural network policies,Grippers,hand tracking hardware,Head,human-robot interaction,learning by example,manipulation tasks,manipulators,neural nets,Neural networks,PR2 robot,raw pixels,RGB-D images,robot programming,robot skill acquisition,robot vision,Robots,Task analysis,telerobotics,Three-dimensional displays,virtual reality,virtual reality teleoperation,Visualization},
  file = {C:\Users\benja\Zotero\storage\J47H2JGA\8461249.html}
}

@inproceedings{zhangLanguageModelingTeaches2018,
  title = {Language {{Modeling Teaches You More}} than {{Translation Does}}: {{Lessons Learned Through Auxiliary Syntactic Task Analysis}}},
  shorttitle = {Language {{Modeling Teaches You More}} than {{Translation Does}}},
  booktitle = {Proceedings of the 2018 {{EMNLP Workshop BlackboxNLP}}: {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  author = {Zhang, Kelly and Bowman, Samuel},
  year = {2018},
  month = nov,
  pages = {359--361},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/W18-5448},
  urldate = {2021-06-21},
  abstract = {Recently, researchers have found that deep LSTMs trained on tasks like machine translation learn substantial syntactic and semantic information about their input sentences, including part-of-speech. These findings begin to shed light on why pretrained representations, like ELMo and CoVe, are so beneficial for neural language understanding models. We still, though, do not yet have a clear understanding of how the choice of pretraining objective affects the type of linguistic information that models learn. With this in mind, we compare four objectives---language modeling, translation, skip-thought, and autoencoding---on their ability to induce syntactic and part-of-speech information, holding constant the quantity and genre of the training data, as well as the LSTM architecture.}
}

@misc{zhangLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Zero-Shot Human Models}} for {{Human-Robot Interaction}}},
  author = {Zhang, Bowen and Soh, Harold},
  year = {2023},
  month = mar,
  number = {arXiv:2303.03548},
  eprint = {2303.03548},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.03548},
  urldate = {2023-10-11},
  abstract = {Human models play a crucial role in human-robot interaction (HRI), enabling robots to consider the impact of their actions on people and plan their behavior accordingly. However, crafting good human models is challenging; capturing context-dependent human behavior requires significant prior knowledge and/or large amounts of interaction data, both of which are difficult to obtain. In this work, we explore the potential of large-language models (LLMs) -- which have consumed vast amounts of human-generated text data -- to act as zero-shot human models for HRI. Our experiments on three social datasets yield promising results; the LLMs are able to achieve performance comparable to purpose-built models. That said, we also discuss current limitations, such as sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our findings, we demonstrate how LLM-based human models can be integrated into a social robot's planning process and applied in HRI scenarios. Specifically, we present one case study on a simulated trust-based table-clearing task and replicate past results that relied on custom models. Next, we conduct a new robot utensil-passing experiment (n = 65) where preliminary results show that planning with a LLM-based human model can achieve gains over a basic myopic plan. In summary, our results show that LLMs offer a promising (but incomplete) approach to human modeling for HRI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\MYCU26N3\2303.html}
}

@inproceedings{zhangPlanExplicabilityPredictability2017,
  title = {Plan Explicability and Predictability for Robot Task Planning},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Zhang, Yu and Sreedharan, Sarath and Kulkarni, Anagha and Chakraborti, Tathagata and Zhuo, Hankz Hankui and Kambhampati, Subbarao},
  year = {2017},
  month = may,
  pages = {1313--1320},
  doi = {10.1109/ICRA.2017.7989155},
  abstract = {Intelligent robots and machines are becoming pervasive in human populated environments. A desirable capability of these agents is to respond to goal-oriented commands by autonomously constructing task plans. However, such autonomy can add significant cognitive load and potentially introduce safety risks to humans when agents behave in unexpected ways. Hence, for such agents to be helpful, one important requirement is for them to synthesize plans that can be easily understood by humans. While there exists previous work that studied socially acceptable robots that interact with humans in ``natural ways'', and work that investigated legible motion planning, there is no general solution for high level task planning. To address this issue, we introduce the notions of plan explicability and predictability. To compute these measures, first, we postulate that humans understand agent plans by associating abstract tasks with agent actions, which can be considered as a labeling process. We learn the labeling scheme of humans for agent plans from training examples using conditional random fields (CRFs). Then, we use the learned model to label a new plan to compute its explicability and predictability. These measures can be used by agents to proactively choose or directly synthesize plans that are more explicable and predictable to humans. We provide evaluations on a synthetic domain and with a physical robot to demonstrate the effectiveness of our approach.},
  keywords = {Computational modeling,conditional random fields,CRFs,human safety risks,human-robot interaction,intelligent machines,intelligent robots,Labeling,labeling process,learning (artificial intelligence),plan explicability,plan predictability,Planning,Predictive models,random processes,robot programming,robot task planning,Robots,Training,training examples},
  file = {C:\Users\benja\Zotero\storage\5QVW4TM5\7989155.html}
}

@article{zhangSegViTSemanticSegmentation2022,
  title = {{{SegViT}}: {{Semantic Segmentation}} with {{Plain Vision Transformers}}},
  shorttitle = {{{SegViT}}},
  author = {Zhang, Bowen and Tian, Zhi and Tang, Quan and Chu, Xiangxiang and Wei, Xiaolin and Shen, Chunhua and Liu, Yifan},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {4971--4982},
  urldate = {2024-06-14},
  langid = {english}
}

@misc{zhangTimeSensitiveNetworkingTSN2024,
  title = {Time-{{Sensitive Networking}} ({{TSN}}) for {{Industrial Automation}}: {{Current Advances}} and {{Future Directions}}},
  shorttitle = {Time-{{Sensitive Networking}} ({{TSN}}) for {{Industrial Automation}}},
  author = {Zhang, Tianyu and Wang, Gang and Xue, Chuanyu and Wang, Jiachen and Nixon, Mark and Han, Song},
  year = {2024},
  month = jul,
  number = {arXiv:2306.03691},
  eprint = {2306.03691},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.03691},
  urldate = {2024-08-16},
  abstract = {With the introduction of Cyber-Physical Systems (CPS) and Internet of Things (IoT) technologies, the automation industry is undergoing significant changes, particularly in improving production efficiency and reducing maintenance costs. Industrial automation applications often need to transmit time- and safety-critical data to closely monitor and control industrial processes. Several Ethernet-based fieldbus solutions, such as PROFINET IRT, EtherNet/IP, and EtherCAT, are widely used to ensure real-time communications in industrial automation systems. These solutions, however, commonly incorporate additional mechanisms to provide latency guarantees, making their interoperability a grand challenge. The IEEE 802.1 Time Sensitive Networking (TSN) task group was formed to enhance and optimize IEEE 802.1 network standards, particularly for Ethernet-based networks. These solutions can be evolved and adapted for cross-industry scenarios, such as large-scale distributed industrial plants requiring multiple industrial entities to work collaboratively. This paper provides a comprehensive review of current advances in TSN standards for industrial automation. It presents the state-of-the-art IEEE TSN standards and discusses the opportunities and challenges of integrating TSN into the automation industry. Some promising research directions are also highlighted for applying TSN technologies to industrial automation applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Networking and Internet Architecture}
}

@article{zhengFailureAnalysisFlexspline2018,
  title = {Failure {{Analysis}} of a {{Flexspline}} of {{Harmonic Gear Drive}} in {{STC Industrial Robot}}: {{Microstructure}} and {{Stress Distribution}}},
  shorttitle = {Failure {{Analysis}} of a {{Flexspline}} of {{Harmonic Gear Drive}} in {{STC Industrial Robot}}},
  author = {Zheng, Jianlin and Yang, Wei},
  year = {2018},
  month = dec,
  journal = {IOP Conference Series: Materials Science and Engineering},
  volume = {452},
  pages = {042148},
  publisher = {IOP Publishing},
  issn = {1757-899X},
  doi = {10.1088/1757-899X/452/4/042148},
  urldate = {2022-08-01},
  abstract = {The failure process and failure mechanism of the flexspline in harmonic reducer system after 500h operation were analyzed. The microstructure, element composition and mechanical properties of flexspline were studied by optical microscope (OM), scanning electron microscope (SEM), atomic emission spectrometry (AES), infrared carbon sulfur analyzer and microhardness tester. The stress distribution of the key parts of the flexspline was calculated by LS-DYNA finite element simulation software. The results revealed that the main reason for the failure of the flexspline was the local micro crack and the variation of the dimensional accuracy, the essential reason was grain and ferrite phase inappropriately. In addition, the stress of the failed flexspline was concentrated near the crack, the maximum stress was increased by 26\%, which compared well with empirical conclusion. This work can offer a reference for the further improvement of the flexspline.},
  langid = {english}
}

@article{zhengImitationLearningProgress2022,
  title = {Imitation {{Learning}}: {{Progress}}, {{Taxonomies}} and {{Challenges}}},
  shorttitle = {Imitation {{Learning}}},
  author = {Zheng, Boyuan and Verma, Sunny and Zhou, Jianlong and Tsang, Ivor W. and Chen, Fang},
  year = {2022},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--16},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2022.3213246},
  urldate = {2024-04-28},
  abstract = {Imitation learning (IL) aims to extract knowledge from human experts' demonstrations or artificially created agents to replicate their behaviors. It promotes interdisciplinary communication and real-world automation applications. However, the process of replicating behaviors still exhibits various problems, such as the performance is highly dependent on the demonstration quality, and most trained agents are limited to perform well in task-specific environments. In this survey, we provide an insightful review on IL. We first introduce the background knowledge from development history and preliminaries, followed by presenting different taxonomies within IL and key milestones of the field. We then detail challenges in learning strategies and present research opportunities with learning policy from suboptimal demonstration, voice instructions, and other associated optimization schemes.},
  keywords = {Behavioral sciences,Entropy,Games,Imitation learning (IL),machine learning,Optimization,Task analysis,taxonomies,Taxonomy,Training},
  file = {C:\Users\benja\Zotero\storage\89E65LD3\9927439.html}
}

@inproceedings{zhengNeuralVolumetricMesh2022,
  title = {Neural {{Volumetric Mesh Generator}}},
  booktitle = {{{NeurIPS}} 2022 {{Workshop}} on {{Score-Based Methods}}},
  author = {Zheng, Yan and Wu, Lemeng and Liu, Xingchao and Chen, Zhen and Liu, Qiang and Huang, Qixing},
  year = {2022},
  month = nov,
  urldate = {2024-01-11},
  abstract = {Deep generative models have shown success in generating 3D shapes with different representations. In this work, we propose Neural Volumetric Mesh Generator (NVMG), which can generate novel and high-quality volumetric meshes. Unlike the previous 3D generative model for point cloud, voxel, and implicit surface, volumetric mesh is a ready-to-use representation in industry with details on both the surface and interior. Generating this kind of highly-structured data thus brings a great challenge. To tackle this problem, we first propose to use a diffusion-based generative model to generate voxelized shapes with realistic shape and topology information. With the voxelized shape, we can simply obtain a tetrahedral mesh as a template. Further, we use a voxel-conditional neural network to predict the surface conditioned on the voxels, and progressively project the tetrahedral mesh to the predicted surface under regularization. As shown in the experiments, without any post-processing, our pipeline can generate high-quality artifact-free volumetric and surface meshes.},
  langid = {english}
}

@misc{zhengTrafficSafetyGPTTuningPretrained2023,
  title = {{{TrafficSafetyGPT}}: {{Tuning}} a {{Pre-trained Large Language Model}} to a {{Domain-Specific Expert}} in {{Transportation Safety}}},
  shorttitle = {{{TrafficSafetyGPT}}},
  author = {Zheng, Ou and {Abdel-Aty}, Mohamed and Wang, Dongdong and Wang, Chenzhu and Ding, Shengxuan},
  year = {2023},
  month = jul,
  number = {arXiv:2307.15311},
  eprint = {2307.15311},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.15311},
  urldate = {2023-10-30},
  abstract = {Large Language Models (LLMs) have shown remarkable effectiveness in various general-domain natural language processing (NLP) tasks. However, their performance in transportation safety domain tasks has been suboptimal, primarily attributed to the requirement for specialized transportation safety expertise in generating accurate responses [1]. To address this challenge, we introduce TrafficSafetyGPT, a novel LLAMA-based model, which has undergone supervised fine-tuning using TrafficSafety-2K dataset which has human labels from government produced guiding books and ChatGPT-generated instruction-output pairs. Our proposed TrafficSafetyGPT model and TrafficSafety-2K train dataset are accessible at https://github.com/ozheng1993/TrafficSafetyGPT.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\benja\Zotero\storage\94GXAPZL\2307.html}
}

@inproceedings{Zhong.2009,
  title = {Intrinsic Shape Signatures: {{A}} Shape Descriptor for {{3D}} Object Recognition},
  booktitle = {2009 {{IEEE}} 12th International Conference on Computer Vision Workshops, {{ICCV}} Workshops},
  author = {Zhong, Yu},
  year = {2009},
  pages = {689--696},
  publisher = {IEEE},
  doi = {10.1109/ICCVW.2009.5457637},
  bookpagination = {page},
  isbn = {978-1-4244-4442-7}
}

@inproceedings{zhongAGIEvalHumanCentricBenchmark2024,
  title = {{{AGIEval}}: {{A Human-Centric Benchmark}} for {{Evaluating Foundation Models}}},
  shorttitle = {{{AGIEval}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}} 2024},
  author = {Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
  year = {2024},
  month = jun,
  pages = {2299--2314},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.findings-naacl.149},
  urldate = {2024-09-29},
  abstract = {Assessing foundation models' abilities for human-level tasks is crucial for Artificial General Intelligence (AGI) development.Traditional benchmarks, which rely on artificial datasets, may not accurately represent these capabilities. In this paper, we introduce AGIEval, a novel bilingual benchmark designed to assess foundation models in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models on our benchmark. Impressively, we show that GPT-4 exceeds the average human performance in SAT, LSAT, and math contests, with 95\% accuracy on SAT Math and 92.5\% on the Chinese college entrance English exam. This demonstrates the exceptional performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks requiring complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal their strengths and limitations, providing valuable insights into future directions for enhancing general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a meaningful and robust evaluation of foundation models' performance in real-world scenarios.},
  file = {C:\Users\benja\Zotero\storage\ISVAM8E4\Zhong et al. - 2024 - AGIEval A Human-Centric Benchmark for Evaluating Foundation Models.pdf}
}

@inproceedings{zhongCellularNeuralNetwork2006,
  title = {A {{Cellular Neural Network}} for {{Deformable Object Modelling}}},
  booktitle = {Information {{Technology For Balanced Manufacturing Systems}}},
  author = {Zhong, Y. and Shirinzadeh, B. and Yuan, X. and Alici, G. and Smith, J.},
  editor = {Shen, Weiming},
  year = {2006},
  pages = {329--336},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-0-387-36594-7_35},
  abstract = {This paper presents a new methodology for the deformation of soft objects by drawing an analogy between cellular neural network (CNN) and elastic deformation. An improved CNN model is developed to simulate the deformation of soft objects. A finite volume based method is presented to derive the discrete differential operators over irregular nets for obtaining the internal elastic forces. The proposed methodology not only models the deformation dynamics in continuum mechanics, but it also simplifies the complex deformation problem with simple setting CNN templates.},
  isbn = {978-0-387-36594-7},
  langid = {english},
  keywords = {Boundary Element Method,Cellular Neural Network,Finite Volume,Finite Volume Method,Internal Force}
}

@inproceedings{zhouLearningViaPointMovement2019,
  title = {Learning {{Via-Point Movement Primitives}} with {{Inter-}} and {{Extrapolation Capabilities}}},
  booktitle = {{{IROS}}},
  author = {Zhou, You and Gao, Jianfeng and Asfour, Tamim},
  year = {2019},
  month = nov,
  pages = {4301--4308},
  publisher = {IEEE},
  address = {Macau, China},
  doi = {10.1109/IROS40897.2019.8968586},
  urldate = {2020-07-24},
  abstract = {Movement Primitives (MPs) are a promising way for representing robot motions in a flexible and adaptable manner. Due to the simple and compact form, they have been widely used in robotics. A major goal of the research activities on MPs is to learn models, which can adapt to changing task constraints, e. g. new motion targets. However, the adaptability of current MPs is limited to a small set of constraints due to their simple structures. It is indeed not a trivial task to maintain the simplicity of MPs representation and, at the same time, enhance their adaptability. In this paper, we discuss the adaptability of popular MPs such as Dynamic Movement Primitives (DMP) and Probabilistic Movement Primitives (ProMP) and propose a new simple but efficient formulation of MPs, the Via-points Movement Primitive (VMP), that can adapt to arbitrary via-points using a simple structured model that is based on the previous approaches but outperforms those in terms of extrapolation abilities.},
  isbn = {978-1-72814-004-9},
  langid = {english}
}

@article{zhouMovementPrimitiveLearning2020,
  title = {Movement {{Primitive Learning}} and {{Generalization}}: {{Using Mixture Density Networks}}},
  shorttitle = {Movement {{Primitive Learning}} and {{Generalization}}},
  author = {Zhou, You and Gao, Jianfeng and Asfour, Tamim},
  year = {2020},
  month = jun,
  journal = {IEEE Robotics \& Automation Magazine},
  volume = {27},
  number = {2},
  pages = {22--32},
  issn = {1070-9932, 1558-223X},
  doi = {10.1109/MRA.2020.2980591},
  urldate = {2020-07-24},
  abstract = {Representing robot skills as movement primitives that can be learned from human demonstration and adapted to new tasks and situations is a promising approach towards intuitive robot programming. To allow such adaptation, a mapping between task parameters and movement primitives parameters is needed, and different approaches have been proposed in the literature to learn such a mapping. In human demonstrations, however, multiple modes and models exist, which should be taken into account when learning these mappings and generalized movement primitive representations. Here, a challenging problem is mode or model collapse. In order to solve this problem, we propose using a Mixture Density Network (MDN) that takes task parameters as input and provides a Gaussian Mixture Model (GMM) of the movement primitive parameters. To avoid mode and model collapse during MDN training, we introduce an entropy cost to achieve a more balanced association of demonstrations to GMM mixture components. Since it is often easier to collect failed examples by using an underfitted MDN model instead of additional human demonstrations, we introduce a failure cost to reduce the occurrence of failures in future executions. We evaluated our approach in simulation and real robot experiments and showed that the method outperforms previous approaches.},
  langid = {english}
}

@article{zhouSemanticUnderstandingScenes2019,
  title = {Semantic {{Understanding}} of {{Scenes Through}} the {{ADE20K Dataset}}},
  author = {Zhou, Bolei and Zhao, Hang and Puig, Xavier and Xiao, Tete and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
  year = {2019},
  month = mar,
  journal = {International Journal of Computer Vision},
  volume = {127},
  number = {3},
  pages = {302--321},
  issn = {1573-1405},
  doi = {10.1007/s11263-018-1140-0},
  urldate = {2023-03-03},
  abstract = {Semantic understanding of visual scenes is one of the holy grails of computer vision. Despite efforts of the community in data collection, there are still few image datasets covering a wide range of scenes and object categories with pixel-wise annotations for scene understanding. In this work, we present a densely annotated dataset ADE20K, which spans diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. Totally there are 25k images of the complex everyday scenes containing a variety of objects in their natural spatial context. On average there are 19.5 instances and 10.5 object classes per image. Based on ADE20K, we construct benchmarks for scene parsing and instance segmentation. We provide baseline performances on both of the benchmarks and re-implement state-of-the-art models for open source. We further evaluate the effect of synchronized batch normalization and find that a reasonably large batch size is crucial for the semantic segmentation performance. We show that the networks trained on ADE20K are able to segment a wide variety of scenes and objects.},
  langid = {english}
}

@article{zhuangComprehensiveSurveyTransfer2021,
  title = {A {{Comprehensive Survey}} on {{Transfer Learning}}},
  author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  year = {2021},
  month = jan,
  journal = {Proceedings of the IEEE},
  volume = {109},
  number = {1},
  pages = {43--76},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2020.3004555},
  urldate = {2024-05-27},
  abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target-domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning research studies, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey article reviews more than 40 representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over 20 representative transfer learning models are used for experiments. The models are performed on three different data sets, that is, Amazon Reviews, Reuters-21578, and Office-31, and the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
  keywords = {Adaptation models,Covariance matrices,Data models,Domain adaptation,interpretation,machine learning,Machine learning,Semisupervised learning,transfer learning,Transfer learning},
  file = {C:\Users\benja\Zotero\storage\6FMBKVTK\9134370.html}
}

@inproceedings{zhuDualarmRoboticManipulation2018,
  title = {Dual-Arm Robotic Manipulation of Flexible Cables},
  booktitle = {{{IROS}}},
  author = {Zhu, J. and Navarro, B. and Fraisse, P. and Crosnier, A. and Cherubini, A.},
  year = {2018},
  month = oct,
  pages = {479--484},
  doi = {10.1109/IROS.2018.8593780},
  abstract = {Deforming a cable to a desired (reachable) shape is a trivial task for a human to do without even knowing the internal dynamics of the cable. This paper proposes a framework for cable shapes manipulation with multiple robot manipulators. The shape is parameterized by a Fourier series. A local deformation model of the cable is estimated on-line with the shape parameters. Using the deformation model, a velocity control law is applied on the robot to deform the cable into the desired shape. Experiments on a dual-arm manipulator are conducted to validate the framework.},
  keywords = {arm robotic manipulation,cable shape manipulation,cables (mechanical),Deformable models,deformation,dual-arm manipulator,flexible cables,Fourier series,local deformation model,manipulator dynamics,manipulators,Manipulators,mobile robots,multi-robot systems,multiple robot manipulators,position control,Power cables,Shape,shape parameters,Strain,Task analysis,trivial task,velocity control},
  file = {C:\Users\benja\Zotero\storage\GJGFZKZE\8593780.html}
}

@article{zhuRobosuiteModularSimulation2020,
  title = {Robosuite: {{A Modular Simulation Framework}} and {{Benchmark}} for {{Robot Learning}}},
  shorttitle = {Robosuite},
  author = {Zhu, Yuke and Wong, Josiah and Mandlekar, Ajay and {Mart{\'i}n-Mart{\'i}n}, Roberto},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.12293 [cs]},
  eprint = {2009.12293},
  primaryclass = {cs},
  urldate = {2021-08-24},
  abstract = {robosuite is a simulation framework for robot learning powered by the MuJoCo physics engine. It offers a modular design for creating robotic tasks as well as a suite of benchmark environments for reproducible research. This paper discusses the key system modules and the benchmark environments of our new release robosuite v1.0.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\Users\benja\Zotero\storage\QZPZ3QKL\2009.html}
}

@article{zhuRoboticManipulationPlanning2020,
  title = {Robotic {{Manipulation Planning}} for {{Shaping Deformable Linear Objects WithEnvironmental Contacts}}},
  author = {Zhu, Jihong and Navarro, Benjamin and Passama, Robin and Fraisse, Philippe and Crosnier, Andr{\'e} and Cherubini, Andrea},
  year = {2020},
  month = jan,
  journal = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {1},
  pages = {16--23},
  issn = {2377-3766},
  doi = {10.1109/LRA.2019.2944304},
  abstract = {Humans use contacts in the environment to modify the shape of deformable objects. Yet, few papers have studied the use of contacts in robotic manipulation. In this letter, we investigate the problem of robotic manipulation of cables with environmental contacts. Instead of avoiding contacts, we propose a framework that allows the robot to use them for shaping the cable. We introduce an index to quantify the contact mobility of a cable with a circular contact. Based on this index, we present a planner to plan robot motions. The planner is aided by a vision-based contact detector. The framework is validated with robot experiments on different desired cable configurations.},
  keywords = {Contact Modeling,Dexterous Manipulation,Indexes,Manipulation Planning,Planning,Robot kinematics,Service robots,Shape,Strain},
  file = {C:\Users\benja\Zotero\storage\2WCH2RMG\8851170.html}
}

@article{zhuRobotLearningDemonstration2018,
  title = {Robot {{Learning}} from {{Demonstration}} in {{Robotic Assembly}}: {{A Survey}}},
  shorttitle = {Robot {{Learning}} from {{Demonstration}} in {{Robotic Assembly}}},
  author = {Zhu, Zuyuan and Hu, Huosheng},
  year = {2018},
  month = jun,
  journal = {Robotics},
  volume = {7},
  number = {2},
  pages = {17},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2218-6581},
  doi = {10.3390/robotics7020017},
  urldate = {2022-05-06},
  abstract = {Learning from demonstration (LfD) has been used to help robots to implement manipulation tasks autonomously, in particular, to learn manipulation behaviors from observing the motion executed by human demonstrators. This paper reviews recent research and development in the field of LfD. The main focus is placed on how to demonstrate the example behaviors to the robot in assembly operations, and how to extract the manipulation features for robot learning and generating imitative behaviors. Diverse metrics are analyzed to evaluate the performance of robot imitation learning. Specifically, the application of LfD in robotic assembly is a focal point in this paper.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {imitation learning,learning from demonstration,machine learning,robotic assembly}
}

@article{zieglerFineTuningLanguageModels2020,
  title = {Fine-{{Tuning Language Models}} from {{Human Preferences}}},
  author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  year = {2020},
  month = jan,
  journal = {arXiv:1909.08593 [cs, stat]},
  eprint = {1909.08593},
  primaryclass = {cs, stat},
  urldate = {2021-06-21},
  abstract = {Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\benja\Zotero\storage\AWS9I3B5\1909.html}
}

@patent{ziemertVorrichtungUndVerfahren2017,
  title = {Vorrichtung Und {{Verfahren}} Zur {{Oberfl{\"a}chenbearbeitung}}},
  author = {Ziemert, Michael and Hirt, Jens and Schulz, Holger and Vogel, Andreas and Haupt, Andreas and Bayer, Stefan},
  year = {2017},
  month = jun,
  number = {DE102015120870},
  urldate = {2021-03-01},
  abstract = {Die vorliegende Erfindung betrifft eine Vorrichtung (1) und ein Verfahren zur Oberfl{\"a}chenbearbeitung, insbesondere zum Schleifen von ebenen Fl{\"a}chen eines Umformwerkzeugs. Die Vorrichtung (1) umfasst eine Werkzeugeinrichtung (2) mit einem Schleifk{\"o}rper (3) mit einer ebenen Wirkfl{\"a}che (13). Dabei ist der Schleifk{\"o}rper (3) durch eine Anbindungseinrichtung (4) elastisch an einer Schnittstelleneinrichtung (5) befestigt. Die Schnittstelleneinrichtung (5) ist dazu geeignet und ausgebildet, an einen Roboter (100) gekoppelt zu werden, sodass eine Schleifbewegung des Schleifk{\"o}rpers (3) durch eine Bewegung des Roboters (100) ausf{\"u}hrbar ist.},
  assignee = {Porsche AG},
  file = {C:\Users\benja\Zotero\storage\5THA6D9J\DE102015120870A1.html}
}

@inproceedings{zitkovichRT2VisionLanguageActionModels2023,
  title = {{{RT-2}}: {{Vision-Language-Action Models Transfer Web Knowledge}} to {{Robotic Control}}},
  shorttitle = {{{RT-2}}},
  booktitle = {7th {{Annual Conference}} on {{Robot Learning}}},
  author = {Zitkovich, Brianna and Yu, Tianhe and Xu, Sichun and Xu, Peng and Xiao, Ted and Xia, Fei and Wu, Jialin and Wohlhart, Paul and Welker, Stefan and Wahid, Ayzaan and Vuong, Quan and Vanhoucke, Vincent and Tran, Huong and Soricut, Radu and Singh, Anikait and Singh, Jaspiar and Sermanet, Pierre and Sanketi, Pannag R. and Salazar, Grecia and Ryoo, Michael S. and Reymann, Krista and Rao, Kanishka and Pertsch, Karl and Mordatch, Igor and Michalewski, Henryk and Lu, Yao and Levine, Sergey and Lee, Lisa and Lee, Tsang-Wei Edward and Leal, Isabel and Kuang, Yuheng and Kalashnikov, Dmitry and Julian, Ryan and Joshi, Nikhil J. and Irpan, Alex and Ichter, Brian and Hsu, Jasmine and Herzog, Alexander and Hausman, Karol and Gopalakrishnan, Keerthana and Fu, Chuyuan and Florence, Pete and Finn, Chelsea and Dubey, Kumar Avinava and Driess, Danny and Ding, Tianli and Choromanski, Krzysztof Marcin and Chen, Xi and Chebotar, Yevgen and Carbajal, Justice and Brown, Noah and Brohan, Anthony and Arenas, Montserrat Gonzalez and Han, Kehang},
  year = {2023},
  month = aug,
  urldate = {2024-01-05},
  abstract = {We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).},
  langid = {english}
}

@misc{zivid_as_zivid_2019,
  title = {Zivid {{One}}.},
  author = {{Zivid}}
}

@misc{zividasZividOneTechnical2019,
  title = {Zivid {{One}}. {{Technical Specification}}},
  author = {Zivid AS},
  year = {2019},
  month = apr,
  file = {C:\Users\benja\Zotero\storage\YLKZX239\Zivid+One+Plus+Datasheet.pdf}
}

@article{Zollhofer.2014,
  title = {Real-Time Non-Rigid Reconstruction Using an {{RGB-D}} Camera},
  author = {Zollh{\"o}fer, Michael and Theobalt, Christian and Stamminger, Marc and Nie{\ss}ner, Matthias and Izadi, Shahram and Rehmann, Christoph and Zach, Christopher and Fisher, Matthew and Wu, Chenglei and Fitzgibbon, Andrew and Loop, Charles},
  year = {2014},
  journal = {ACM Transactions on Graphics},
  volume = {33},
  number = {4},
  pages = {1--12},
  issn = {07300301},
  doi = {10.1145/2601097.2601165},
  pagination = {page}
}

@article{Zoph2016,
  title = {Neural {{Architecture Search}} with {{Reinforcement Learning}}},
  author = {Zoph, Barret and Le, Quoc V.},
  year = {2016},
  month = nov,
  eprint = {1611.01578},
  urldate = {2019-02-17},
  abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
  archiveprefix = {arXiv}
}
