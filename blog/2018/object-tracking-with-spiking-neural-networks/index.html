<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Object Tracking With Spiking Neural Networks | Benjamin Alt </title> <meta name="author" content="Benjamin Alt"> <meta name="description" content="How Spiking Neural Networks (SNNs) and the Neurorobotics Platform can be leveraged for robot control and perception."> <meta name="keywords" content="robotics, ai, ai-safety, robot-programming, human-ai-interaction, human-robot-interaction, task-planning, motion-planning, robot-manipulation, llm, vla"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://benjaminalt.github.io/blog/2018/object-tracking-with-spiking-neural-networks/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Benjamin</span> Alt </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">CV </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/assets/pdf/resume.pdf">Resume</a> <a class="dropdown-item " href="/assets/pdf/cv.pdf">Academic CV</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Object Tracking With Spiking Neural Networks</h1> <p class="post-meta"> Created on April 25, 2018 </p> <p class="post-tags"> <a href="/blog/2018"> <i class="fa-solid fa-calendar fa-sm"></i> 2018 </a>   ·   <a href="/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> neural-networks</a>   <a href="/blog/tag/human-brain-project"> <i class="fa-solid fa-hashtag fa-sm"></i> human-brain-project</a>   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/robotics"> <i class="fa-solid fa-hashtag fa-sm"></i> robotics</a>   ·   <a href="/blog/category/blog"> <i class="fa-solid fa-tag fa-sm"></i> blog</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Last semester, I had the fortune of doing some work with the <a href="https://www.humanbrainproject.eu/en/" rel="external nofollow noopener" target="_blank">Human Brain Project</a>, an EU-funded research project with the aim of understanding, mapping and partly reconstructing the human brain through computer simulations and actual hardware. Using the <a href="https://neurorobotics.net/" rel="external nofollow noopener" target="_blank">Neurorobotics Platform (NRP)</a>, an environment for developing and running experiments with spiking neural networks and simulated robots, I implemented a closed loop control-based solution for tracking an object in space using spiking neural networks. In this post, I am outlining spiking neural networks, the Neurorobotics platform and how one can implement simple control loops for solving tracking tasks using spiking neural networks and the NRP toolset. The code for the corresponding NRP experiment can be found <a href="https://github.com/Scaatis/hbpprak_perception" rel="external nofollow noopener" target="_blank">on GitHub</a>.</p> <h2 id="spiking-neural-networks">Spiking Neural Networks</h2> <p>Spiking neural networks attempt to model the neurons of the human brain much more closely than the “classical” neural networks used for deep learning. While in deep neural networks, units instantly react to their inputs and produce outputs immediately, spiking neural networks are dynamical systems: The output at any given time is dependent on previous inputs as well as on the state of the neuron at that time. The physiology of biological neurons and the relevance of dynamical systems in the modelling of neurons is explained in some detail in <a href="http://jackterwilliger.com/biological-neural-networks-part-i-spiking-neurons/" rel="external nofollow noopener" target="_blank">Jack Terwilliger’s blog post</a>.</p> <h3 id="neuron-models">Neuron Models</h3> <p>Many different neuron models exist and spiking neural networks often combine multiple models. In all models, neurons fire impulses (or spikes) dependent on the current and past input voltage. The input voltage is computed based on the spike activity of other, connected neurons. The most simple <a href="http://neuronaldynamics.epfl.ch/online/Ch1.S3.html" rel="external nofollow noopener" target="_blank">“integrate-and-fire” model</a> integrates over the input voltage, emits a spike when the integrated input reaches a threshold and resets the integrator to zero. Most commonly, the integrator is <em>leaky</em>: Its value decreases over time. Mathematically, a leaky integrator can be modelled using a simple linear differential equation and is functionally equivalent to an RC circuit. Much more complex models have been devised in literature to add a refractory period, during which the neuron cannot fire, or the capacity to oscillate or resonate. While these models model the behavior of human neurons much more closely, simple models such as integrate-and-fire incur less computational overhead and often suffice to solve real-world problems with spiking neural networks.</p> <h3 id="computing-with-spiking-neural-networks">Computing with Spiking Neural Networks</h3> <p>It has been shown that <a href="https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf" rel="external nofollow noopener" target="_blank">every possible function can be realized with a traditional deep neural network</a>. Simples spiking neural networks work similarly to traditional deep neural networks: The network is specified by connecting <em>input populations</em> of neurons to <em>output populations</em> using <em>synapses</em>. Arbitrarily many layers can be formed. In its simplest form, a synapse connects <em>M</em> input neurons to <em>N</em> output neurons. With the simplest possible synapse, the strength of each connection is specified by a multiplier, the <em>synapse weight</em>, resulting in an <em>NxM</em> weight matrix for a fully connected synapse which encodes for each output neuron which input neuron contributes how much to its input voltage. In other words, each output neuron’s input voltage is the weighted sum of its inputs - just like in “regular” deep neural networks. More complex synapse models differ from the connections in traditional neural networks in that they model synapses in the human brain more closely, can be stateful and “learn” by changing their characteristics depending on the input spike frequency and timing, a property called <a href="http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity" rel="external nofollow noopener" target="_blank">spike-timing dependent plasticity</a>. <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0161335" rel="external nofollow noopener" target="_blank">Learning with spiking neural networks</a> still presents a number of open problems and constitutes a highly productive field of research.</p> <p>Spiking neural networks mostly differ from deep neural networks because they are dynamical systems - their outputs depend on the inputs <em>over time</em>. Unlike deep neural networks, where information is encoded in tensors, spiking neural networks encode information in the <em>spike train</em>, the function of spiking activity over time, through the frequency and timing of spikes. Depending on the neuron model, spike emission may be stochastic; in this case, the information encoding is called a <a href="https://link.springer.com/referenceworkentry/10.1007%2F978-3-540-92910-9_10" rel="external nofollow noopener" target="_blank">spike density code</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbp/screenshot_spike_train.png" sizes="95vw"></source> <img src="/assets/img/hbp/screenshot_spike_train.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> A spike train of a population of neurons, visualized in the Neurorobotics platform. </div> <h2 id="the-neurorobotics-platform">The Neurorobotics Platform</h2> <p>The Neurorobotics Platform (NRP) is an application developed by the Neurorobotics group of the Human Brain Project for developing and running experiments about controlling robots using spiking neural networks. Its backend is a combination of <a href="http://www.ros.org/" rel="external nofollow noopener" target="_blank">ROS</a> for infrastructure and interaction with (simulated) robots, the <a href="http://gazebosim.org/" rel="external nofollow noopener" target="_blank">Gazebo robot simulator</a> and the <a href="http://www.nest-simulator.org/" rel="external nofollow noopener" target="_blank">NEST neural network simulator</a>; it has a Javascript-based web interface and allows manipulation of the 3D environment, neural network and experiment setup in a browser. An NRP experiment is defined in an XML file and consists of a state machine, a brain file, an (SDF) environment model and an optional ROS launch file.</p> <h3 id="state-machine">State Machine</h3> <p>The state machine defines any action on the part of the environment. If something in the environment needs to be moved, such as the target object for object tracking, or if objects need to be spawned at runtime, it can be done in a Python <a href="http://library.isr.ist.utl.pt/docs/roswiki/smach.html" rel="external nofollow noopener" target="_blank">smach</a> state machine. It has access to the environment, additionally loaded 3D models as well as to all available ROS topics and services.</p> <h3 id="brain-definition-and-transfer-functions">Brain Definition and Transfer Functions</h3> <p>In the NRP, the brain consists of two components: A <a href="http://neuralensemble.org/PyNN/" rel="external nofollow noopener" target="_blank">PyNN</a> brain description file and one or more <em>transfer functions</em>. PyNN provides a simulator-independent interface to neural network simulators such as NEST. In the brain file, populations of spiking neurons are defined and connected through synapses. An example brain file for object tracking is provided below. Transfer functions map neuron activity to robot motion or vice versa - they are Python functions which are called in every cycle of the NRP’s <a href="https://developer.humanbrainproject.eu/docs/projects/HBP%20Neurorobotics%20Platform/0.9/nrp/developer_manual/CLE/cle_architecture.html" rel="external nofollow noopener" target="_blank"><em>Closed Loop Engine</em></a>, and have access to both the brain as defined in the brain file and the ROS topics for robot control. In their purest form, transfer functions implement <em>visuomotor coupling</em> - a closed loop between neural activity and the resulting motion. This closed loop is crucial for implementing object tracking in the NRP.</p> <h2 id="closed-loop-control-for-object-tracking">Closed-Loop Control for Object Tracking</h2> <p>There are <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.8588&amp;rep=rep1&amp;type=pdf" rel="external nofollow noopener" target="_blank">many approaches for tracking objects</a>, many of which rely on direct or indirect solutions to the correspondence problem or particle filters. When humans track objects, however, they do not analytically compute two-dimensional convolutions with a reference pattern, like many traditional filter-based approaches do. Instead, humans move their eyes to keep the object in the center of their field of vision. When trying to evict a fly from your living room, for example, you constantly keep your eyes centered on the fly and follow its motion. This can be modelled in a straightforward way with a closed-loop control circuit: The sensors on the retina cause excitations of the neurons of the visual cortex; these excitation patterns instruct the eye and neck muscles to move in order to keep the tracked object in the center. Small deviations in the position of the tracked object cause compensatory movements of the eyes, which in turn cause the image on the retina to change, etc. In control theory terms, the retina is the sensor, the eye muscles are the controller and the position and orientation of the eyes are the controlled system.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbp/control_loop.png" sizes="95vw"></source> <img src="/assets/img/hbp/control_loop.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Object tracking realized with closed-loop control. </div> <p>Because the NRP comes with a Closed Loop Engine (CLE), implementing closed loop control with spiking neural networks is relatively straightforward in the NRP. The spiking activity of an input population (“virtual retina”) encodes where in the field of vision a tracked object is located at a given time. Via a set of synapses and transfer functions (in the human brain, this would happen in the visual cortex and <a href="https://www.researchgate.net/figure/Summary-of-sensorimotor-circuitry-for-the-generation-of-visuomotor-and-vestibulomotor-eye_fig4_7428849" rel="external nofollow noopener" target="_blank">sensorimotor circuits</a>), the outputs of this population are connected to the inputs of a motor population which, in turn, controls the joint actuators in the eye and neck of the robot.</p> <h3 id="thimblerigger-experiment">Thimblerigger Experiment</h3> <p>To try this out, my colleagues and I created an experiment (the code is on <a href="https://github.com/Scaatis/hbpprak_perception" rel="external nofollow noopener" target="_blank">GitHub</a>) in which a simulated iCub robot is to play the thimblerigger game: A ball is hidden under one of three cups; after the cups are shuffled, the robot has to guess which cup the ball is hidden under. Humans instinctively approach this task by trying to follow the cup hiding the ball with their eyes. While it would be easier and probably more robust to solve this problem using traditional image processing and frame-by-frame analysis, we chose to implement a closed-loop solution for tracking which is as close to the human approach as possible to demonstrate how closed-loop control for visuomotor coupling can be implemented with spiking neural networks in just a few lines of code.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbp/icub_perception.png" sizes="95vw"></source> <img src="/assets/img/hbp/icub_perception.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The thimblerigger experiment in the Neurorobotics platform. </div> <p>The experiment itself is defined using an SMACH state machine (the code for it is <a href="https://github.com/Scaatis/hbpprak_perception/blob/master/state_machine.exd" rel="external nofollow noopener" target="_blank">on GitHub</a>). When the user starts the experiment, three red cups are spawned. One cup is selected at random to conceal a green ball. This cup is lifted to briefly reveal the ball, then lowered again. The cups are shuffled by moving them to a series of random permutations. In the end, the ball is revealed again - if the tracking system worked, the robot is now looking directly at the ball.</p> <h3 id="virtual-retina--brain-file">Virtual Retina &amp; Brain File</h3> <p>The central component of our implementation is the brain file. The brain itself consists of two populations: An input population, representing the virtual retina, and an output population whose outputs are used to control the robot’s joints.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">hbp_nrp_cle.brainsim</span> <span class="kn">import</span> <span class="n">simulator</span> <span class="k">as</span> <span class="n">sim</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">resolution</span> <span class="o">=</span> <span class="mi">17</span>
<span class="n">n_motors</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># down, up, left, right
</span>
<span class="n">sensors</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">Population</span><span class="p">(</span><span class="n">resolution</span> <span class="o">*</span> <span class="n">resolution</span><span class="p">,</span> <span class="n">cellclass</span><span class="o">=</span><span class="n">sim</span><span class="p">.</span><span class="nc">IF_curr_exp</span><span class="p">())</span>
<span class="n">down</span><span class="p">,</span> <span class="n">up</span><span class="p">,</span> <span class="n">left</span><span class="p">,</span> <span class="n">right</span> <span class="o">=</span> <span class="p">[</span><span class="n">sim</span><span class="p">.</span><span class="nc">Population</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">cellclass</span><span class="o">=</span><span class="n">sim</span><span class="p">.</span><span class="nc">IF_curr_exp</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_motors</span><span class="p">)]</span>

<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">resolution</span> <span class="o">*</span> <span class="n">resolution</span><span class="p">).</span><span class="nf">reshape</span><span class="p">((</span><span class="n">resolution</span><span class="p">,</span> <span class="n">resolution</span><span class="p">))</span>
<span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">((</span><span class="n">resolution</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">resolution</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="n">upper_half</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">PopulationView</span><span class="p">(</span><span class="n">sensors</span><span class="p">,</span> <span class="n">indices</span><span class="p">[:</span><span class="n">resolution</span> <span class="o">//</span> <span class="mi">2</span><span class="p">].</span><span class="nf">flatten</span><span class="p">())</span>
<span class="n">lower_half</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">PopulationView</span><span class="p">(</span><span class="n">sensors</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="n">resolution</span> <span class="o">-</span> <span class="n">resolution</span><span class="o">//</span><span class="mi">2</span><span class="p">:].</span><span class="nf">flatten</span><span class="p">())</span>
<span class="n">left_half</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">PopulationView</span><span class="p">(</span><span class="n">sensors</span><span class="p">,</span> <span class="n">indices</span><span class="p">[:,</span> <span class="p">:</span><span class="n">resolution</span> <span class="o">//</span> <span class="mi">2</span><span class="p">].</span><span class="nf">flatten</span><span class="p">())</span>
<span class="n">right_half</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">PopulationView</span><span class="p">(</span><span class="n">sensors</span><span class="p">,</span> <span class="n">indices</span><span class="p">[:,</span> <span class="n">resolution</span> <span class="o">-</span> <span class="n">resolution</span><span class="o">//</span><span class="mi">2</span><span class="p">:].</span><span class="nf">flatten</span><span class="p">())</span>

<span class="n">pro_down</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">Projection</span><span class="p">(</span><span class="n">lower_half</span><span class="p">,</span> <span class="n">down</span><span class="p">,</span> <span class="n">sim</span><span class="p">.</span><span class="nc">AllToAllConnector</span><span class="p">(),</span> <span class="n">sim</span><span class="p">.</span><span class="nc">StaticSynapse</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">ones</span><span class="p">))</span>
<span class="n">pro_up</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">Projection</span><span class="p">(</span><span class="n">upper_half</span><span class="p">,</span> <span class="n">up</span><span class="p">,</span> <span class="n">sim</span><span class="p">.</span><span class="nc">AllToAllConnector</span><span class="p">(),</span> <span class="n">sim</span><span class="p">.</span><span class="nc">StaticSynapse</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">ones</span><span class="p">))</span>
<span class="n">pro_left</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">Projection</span><span class="p">(</span><span class="n">left_half</span><span class="p">,</span> <span class="n">left</span><span class="p">,</span> <span class="n">sim</span><span class="p">.</span><span class="nc">AllToAllConnector</span><span class="p">(),</span> <span class="n">sim</span><span class="p">.</span><span class="nc">StaticSynapse</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">ones</span><span class="p">))</span>
<span class="n">pro_right</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nc">Projection</span><span class="p">(</span><span class="n">right_half</span><span class="p">,</span> <span class="n">right</span><span class="p">,</span> <span class="n">sim</span><span class="p">.</span><span class="nc">AllToAllConnector</span><span class="p">(),</span> <span class="n">sim</span><span class="p">.</span><span class="nc">StaticSynapse</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">ones</span><span class="p">))</span>

<span class="n">circuit</span> <span class="o">=</span> <span class="n">sensors</span> <span class="o">+</span> <span class="n">down</span> <span class="o">+</span> <span class="n">up</span> <span class="o">+</span> <span class="n">left</span> <span class="o">+</span> <span class="n">right</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">sensors</code> population contains the sensor neurons whose spike train correlates with the position of the tracked object. Each neuron is responsible for a small rectangular part of the field of view and spikes when a part of the tracked object is inside this rectangle. For our retina, we chose a resolution of 17x17 neurons, so <code class="language-plaintext highlighter-rouge">sensors</code> is a population of 289 neurons.</p> <p>The <code class="language-plaintext highlighter-rouge">down</code>, <code class="language-plaintext highlighter-rouge">up</code>, <code class="language-plaintext highlighter-rouge">left</code> and <code class="language-plaintext highlighter-rouge">right</code> populations consist of one neuron each. They are our <em>output</em> or <em>motor populations</em> and encode whether and how quickly the eye is to move in either direction. For simplicity and mechanical stability,<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> we chose to actuate only iCub’s left eye as opposed to both eyes and used only the image from the left eye’s camera as opposed the combined (stereo) image.</p> <p><code class="language-plaintext highlighter-rouge">upper_half</code>, <code class="language-plaintext highlighter-rouge">lower_half</code>, <code class="language-plaintext highlighter-rouge">left_half</code> and <code class="language-plaintext highlighter-rouge">right_half</code> are <em>PopulationViews</em> which divide the <code class="language-plaintext highlighter-rouge">sensors</code> population into four different groups corresponding to larger regions in the image. Our <code class="language-plaintext highlighter-rouge">down</code>, <code class="language-plaintext highlighter-rouge">up</code>, <code class="language-plaintext highlighter-rouge">left</code> and <code class="language-plaintext highlighter-rouge">right</code> populations will end up controlling the eye, but we still need to somehow connect “what the retina sees” to these output populations. To that end, we divide the retina into these upper, lower, left and right areas. If the tracked object is in the upper half, the neurons in the <code class="language-plaintext highlighter-rouge">upper_half</code> PopulationView fire more frequently than those in the other areas, so we know that we have to move our eye upwards to keep the tracked object in the center of the field of view.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbp/retina_regions.png" sizes="95vw"></source> <img src="/assets/img/hbp/retina_regions.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The PopulationViews dividing the retina into four regions. </div> <p>The coupling between the output of the retinal sensors and the <code class="language-plaintext highlighter-rouge">down</code>, <code class="language-plaintext highlighter-rouge">up</code>, <code class="language-plaintext highlighter-rouge">left</code> and <code class="language-plaintext highlighter-rouge">right</code> motor populations is done via the <em>Projections</em> <code class="language-plaintext highlighter-rouge">pro_down</code>, <code class="language-plaintext highlighter-rouge">pro_up</code>, <code class="language-plaintext highlighter-rouge">pro_left</code> and <code class="language-plaintext highlighter-rouge">pro_right</code>. Projections are connections between populations (or, in our case, PopulationViews) which define which neurons from the input population are connected to which neurons from the output population (fully connected (all to all) in our case) as well as the synapse type used for the connections. We use <code class="language-plaintext highlighter-rouge">StaticSynapse</code>s whose properties do not change over time. The <code class="language-plaintext highlighter-rouge">weight</code> parameter is a matrix defining the connection weights. Because we used an <code class="language-plaintext highlighter-rouge">AllToAllConnector</code>, <code class="language-plaintext highlighter-rouge">weight</code> is a 136 * 1 matrix assigning a weight for each of the incoming connections from the (17//2)*17 input neurons per half of the retina. Our <code class="language-plaintext highlighter-rouge">weight</code> matrix contains only ones - each input neuron is weighted equally for the computation of the output. How the input spike trains are aggregated in each motor neuron and what their output looks like is defined by the <code class="language-plaintext highlighter-rouge">cellclass</code> used when defining the motor neuron populations. We use PyNN’s <a href="http://pynn.readthedocs.io/en/latest/reference/neuronmodels.html#pyNN.standardmodels.cells.IF_curr_exp" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">IF_curr_exp</code></a>, a <a href="http://pynn.readthedocs.io/en/latest/reference/neuronmodels.html" rel="external nofollow noopener" target="_blank">basic integrate-and-fire model</a>.</p> <h3 id="transfer-functions">Transfer Functions</h3> <p>In the NRP, <em>transfer functions</em> connect the brain defined in the brain file to the robot and the environment. Activating sensor populations based on input from (simulated) sensors such as the cameras in iCub’s eyes or from joint encoders as well as the coupling between the outputs of the motor neurons and the joint actuators is done in transfer functions. To implement our object-tracking solution, we need two transfer functions: One to connect the sensory input (the camera image) to the sensor population, and another to connect the motor neurons to the eye’s actuators.</p> <h4 id="object-detection">Object detection</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># detect_object.py
</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">sensor_msgs.msg</span>
<span class="kn">import</span> <span class="n">std_msgs.msg</span>
<span class="kn">from</span> <span class="n">cv_bridge</span> <span class="kn">import</span> <span class="n">CvBridge</span>
<span class="nd">@nrp.MapRobotSubscriber</span><span class="p">(</span><span class="sh">"</span><span class="s">camera</span><span class="sh">"</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">"</span><span class="s">/icub_model/left_eye_camera/image_raw</span><span class="sh">"</span><span class="p">,</span> <span class="n">sensor_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">Image</span><span class="p">))</span>
<span class="nd">@nrp.MapRobotSubscriber</span><span class="p">(</span><span class="sh">"</span><span class="s">shuffle_status_sub</span><span class="sh">"</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">"</span><span class="s">/group_3/shuffling</span><span class="sh">"</span><span class="p">,</span> <span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">Bool</span><span class="p">))</span>
<span class="nd">@nrp.MapSpikeSource</span><span class="p">(</span><span class="sh">"</span><span class="s">sensors</span><span class="sh">"</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="nf">map_neurons</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">brain_root</span><span class="p">.</span><span class="n">resolution</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">nrp</span><span class="p">.</span><span class="n">brain</span><span class="p">.</span><span class="n">sensors</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">nrp</span><span class="p">.</span><span class="n">dc_source</span><span class="p">)</span>
<span class="nd">@nrp.Robot2Neuron</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">grab_image</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">camera</span><span class="p">,</span> <span class="n">shuffle_status_sub</span><span class="p">,</span> <span class="n">sensors</span><span class="p">):</span>
    <span class="n">resolution</span> <span class="o">=</span> <span class="n">nrp</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">brain_root</span><span class="p">.</span><span class="n">resolution</span>

    <span class="c1"># Take the image from the robot's left eye
</span>    <span class="n">image_msg</span> <span class="o">=</span> <span class="n">camera</span><span class="p">.</span><span class="n">value</span>
    <span class="k">if</span> <span class="n">image_msg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">cvBridge</span> <span class="o">=</span> <span class="nc">CvBridge</span><span class="p">()</span>

        <span class="n">img</span> <span class="o">=</span> <span class="n">cvBridge</span><span class="p">.</span><span class="nf">imgmsg_to_cv2</span><span class="p">(</span><span class="n">image_msg</span><span class="p">,</span> <span class="sh">"</span><span class="s">rgb8</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">img_height</span><span class="p">,</span> <span class="n">img_width</span><span class="p">,</span> <span class="n">color_dim</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span>

        <span class="n">detect_red</span> <span class="o">=</span> <span class="n">shuffle_status_sub</span><span class="p">.</span><span class="n">value</span><span class="p">.</span><span class="n">data</span> <span class="k">if</span> <span class="n">shuffle_status_sub</span><span class="p">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">False</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">detect_red</span><span class="p">:</span>
            <span class="c1"># Detect green in the whole image
</span>            <span class="n">col_width</span> <span class="o">=</span> <span class="n">img_width</span> <span class="o">//</span> <span class="n">resolution</span>
            <span class="n">row_height</span> <span class="o">=</span> <span class="n">img_height</span> <span class="o">//</span> <span class="n">resolution</span>
            <span class="n">green_threshold</span> <span class="o">=</span> <span class="mf">0.5</span>
            <span class="n">amp_scaling_factor</span> <span class="o">=</span> <span class="mf">32.</span>
            
            <span class="c1"># Split the image into regions of same size
</span>            <span class="c1"># Sensor neurons are addressed in row_major order, top left to bottom right
</span>            <span class="c1"># Loop over the neurons in the retina...
</span>            <span class="k">for</span> <span class="n">row_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">resolution</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">col_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">resolution</span><span class="p">):</span>
                    <span class="n">x_start</span> <span class="o">=</span> <span class="n">col_idx</span> <span class="o">*</span> <span class="n">col_width</span>
                    <span class="n">x_end</span> <span class="o">=</span> <span class="n">x_start</span> <span class="o">+</span> <span class="n">col_width</span>
                    <span class="n">y_start</span> <span class="o">=</span> <span class="n">row_idx</span> <span class="o">*</span> <span class="n">row_height</span>
                    <span class="n">y_end</span> <span class="o">=</span> <span class="n">y_start</span> <span class="o">+</span> <span class="n">row_height</span>
                    <span class="n">mean_red</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="n">y_start</span><span class="p">:</span><span class="n">y_end</span><span class="p">,</span><span class="n">x_start</span><span class="p">:</span><span class="n">x_end</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
                    <span class="n">mean_green</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="n">y_start</span><span class="p">:</span><span class="n">y_end</span><span class="p">,</span><span class="n">x_start</span><span class="p">:</span><span class="n">x_end</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
                    <span class="n">mean_blue</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="n">y_start</span><span class="p">:</span><span class="n">y_end</span><span class="p">,</span><span class="n">x_start</span><span class="p">:</span><span class="n">x_end</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>

                    <span class="n">green_proportion</span> <span class="o">=</span> <span class="n">mean_green</span> <span class="o">/</span> <span class="nf">float</span><span class="p">(</span><span class="n">mean_red</span> <span class="o">+</span> <span class="n">mean_green</span> <span class="o">+</span> <span class="n">mean_blue</span><span class="p">)</span>

                    <span class="n">idx</span> <span class="o">=</span> <span class="n">row_idx</span> <span class="o">*</span> <span class="n">resolution</span> <span class="o">+</span> <span class="n">col_idx</span>
                    <span class="n">amp</span> <span class="o">=</span> <span class="n">amp_scaling_factor</span> <span class="o">*</span> <span class="n">green_proportion</span> <span class="k">if</span> <span class="n">green_proportion</span> <span class="o">&gt;</span> <span class="n">green_threshold</span> <span class="k">else</span> <span class="mi">0</span>

                    <span class="n">sensors</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="n">amplitude</span> <span class="o">=</span> <span class="n">amp</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># Detect red in the center of the image
</span>          <span class="c1"># ...
</span></code></pre></div></div> <p><a href="https://github.com/Scaatis/hbpprak_perception/blob/master/detect_object.py" rel="external nofollow noopener" target="_blank">detect_object.py</a> is a <code class="language-plaintext highlighter-rouge">Robot2Neuron</code> transfer function, meaning that information is sent from the robot (or the environment) to the brain. The inputs of the transfer function are defined using the NRP’s <a href="https://developer.humanbrainproject.eu/docs/projects/HBP%20Neurorobotics%20Platform/1.2/nrp/user_manual/simulation_setup/transfer_functions.html" rel="external nofollow noopener" target="_blank">decorator syntax</a> and consist of a subscriber to the camera’s ROS topic, the sensor population and a subscriber to the topic published by the experiment state machine which announces the current state of the experiment (<code class="language-plaintext highlighter-rouge">true</code> if shuffling, <code class="language-plaintext highlighter-rouge">false</code> otherwise). This is required because during shuffling, the object to be tracked is the red cup concealing the ball, while during the reveal in the beginning, the green ball must be tracked.</p> <p>In the central loop, we iterate over all neurons in the retina, compute this neuron’s rectangular area of the field of view and compute the mean for each RGB channel over this region. The neuron’s amplitude (proportional to its firing rate) is set in proportion to the “amount” of green in the region relative to the other primary colors. This has the effect that neurons which “see” green fire more frequently than others. Because the sensor neurons are iterated using a nested loop, like for a 2D array, the sensor neuron activity spatially encodes the positions of green things in the image - and because of the wiring defined in the brain, the <code class="language-plaintext highlighter-rouge">down</code>, <code class="language-plaintext highlighter-rouge">up</code>, <code class="language-plaintext highlighter-rouge">left</code> and <code class="language-plaintext highlighter-rouge">right</code> motor populations are excited according to these positions: <code class="language-plaintext highlighter-rouge">left</code> fires frequently if there is a lot of green in the left side of the image, etc.</p> <p>The code for tracking red objects has been omitted for brevity but works similarly with the only exception that not the entire image is considered, but a rectangular area around the center of the image, causing the margins to be ignored. This is necessary because three cups may be in the image at any given time, but we only want to track the one we centered on when the green ball was revealed.</p> <h4 id="object-following">Object following</h4> <p>The last remaining component of our solution is the transfer function for following the moving object (depending on the stage of the experiment either the ball or the cup) with iCub’s left eye. To that end, the outputs of the <code class="language-plaintext highlighter-rouge">down</code>,<code class="language-plaintext highlighter-rouge">up</code>, <code class="language-plaintext highlighter-rouge">left</code> and <code class="language-plaintext highlighter-rouge">right</code> motor populations must be connected to the two actuators for moving the eye.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># follow_object.py
</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="nd">@nrp.MapSpikeSink</span><span class="p">(</span><span class="sh">"</span><span class="s">motors_down</span><span class="sh">"</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">brain</span><span class="p">.</span><span class="n">down</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">leaky_integrator_alpha</span><span class="p">)</span>
<span class="nd">@nrp.MapSpikeSink</span><span class="p">(</span><span class="sh">"</span><span class="s">motors_left</span><span class="sh">"</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">brain</span><span class="p">.</span><span class="n">left</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">leaky_integrator_alpha</span><span class="p">)</span>
<span class="nd">@nrp.MapSpikeSink</span><span class="p">(</span><span class="sh">"</span><span class="s">motors_up</span><span class="sh">"</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">brain</span><span class="p">.</span><span class="n">up</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">leaky_integrator_alpha</span><span class="p">)</span>
<span class="nd">@nrp.MapSpikeSink</span><span class="p">(</span><span class="sh">"</span><span class="s">motors_right</span><span class="sh">"</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">brain</span><span class="p">.</span><span class="n">right</span><span class="p">,</span> <span class="n">nrp</span><span class="p">.</span><span class="n">leaky_integrator_alpha</span><span class="p">)</span>
<span class="nd">@nrp.MapRobotPublisher</span><span class="p">(</span><span class="sh">'</span><span class="s">eye_tilt_pos</span><span class="sh">'</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">'</span><span class="s">/robot/eye_tilt/pos</span><span class="sh">'</span><span class="p">,</span> <span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">Float64</span><span class="p">))</span>
<span class="nd">@nrp.MapRobotPublisher</span><span class="p">(</span><span class="sh">'</span><span class="s">eye_pan_pos</span><span class="sh">'</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">'</span><span class="s">/robot/left_eye_pan/pos</span><span class="sh">'</span><span class="p">,</span> <span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">Float64</span><span class="p">))</span>
<span class="nd">@nrp.MapRobotPublisher</span><span class="p">(</span><span class="sh">'</span><span class="s">eye_tilt_vel</span><span class="sh">'</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">'</span><span class="s">/robot/eye_tilt/vel</span><span class="sh">'</span><span class="p">,</span> <span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">Float64</span><span class="p">))</span>
<span class="nd">@nrp.MapRobotPublisher</span><span class="p">(</span><span class="sh">'</span><span class="s">eye_pan_vel</span><span class="sh">'</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">'</span><span class="s">/robot/left_eye_pan/vel</span><span class="sh">'</span><span class="p">,</span> <span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">Float64</span><span class="p">))</span>
<span class="nd">@nrp.MapRobotSubscriber</span><span class="p">(</span><span class="sh">"</span><span class="s">joint_state_sub</span><span class="sh">"</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">"</span><span class="s">/robot/joints</span><span class="sh">"</span><span class="p">,</span> <span class="n">sensor_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">JointState</span><span class="p">))</span>
<span class="nd">@nrp.MapRobotSubscriber</span><span class="p">(</span><span class="sh">"</span><span class="s">shuffle_status_sub</span><span class="sh">"</span><span class="p">,</span> <span class="nc">Topic</span><span class="p">(</span><span class="sh">"</span><span class="s">/group_3/shuffling</span><span class="sh">"</span><span class="p">,</span> <span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="n">Bool</span><span class="p">))</span>
<span class="nd">@nrp.Neuron2Robot</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">center_on_green</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">motors_down</span><span class="p">,</span> <span class="n">motors_left</span><span class="p">,</span> <span class="n">motors_up</span><span class="p">,</span> <span class="n">motors_right</span><span class="p">,</span> <span class="n">eye_tilt_pos</span><span class="p">,</span> <span class="n">eye_pan_pos</span><span class="p">,</span> <span class="n">eye_tilt_vel</span><span class="p">,</span> <span class="n">eye_pan_vel</span><span class="p">,</span> <span class="n">joint_state_sub</span><span class="p">,</span> <span class="n">shuffle_status_sub</span><span class="p">):</span>

    <span class="n">stage_two</span> <span class="o">=</span> <span class="n">shuffle_status_sub</span><span class="p">.</span><span class="n">value</span><span class="p">.</span><span class="n">data</span> <span class="k">if</span> <span class="n">shuffle_status_sub</span><span class="p">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">False</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">stage_two</span><span class="p">:</span>
        <span class="c1"># Stage one: Velocity-controlled motion to green ball
</span>        <span class="n">scaling_factor</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="n">tilt</span> <span class="o">=</span> <span class="n">scaling_factor</span> <span class="o">*</span> <span class="p">(</span><span class="n">motors_up</span><span class="p">.</span><span class="n">voltage</span> <span class="o">-</span> <span class="n">motors_down</span><span class="p">.</span><span class="n">voltage</span><span class="p">)</span>
        <span class="n">pan</span> <span class="o">=</span> <span class="n">scaling_factor</span> <span class="o">*</span> <span class="p">(</span> <span class="n">motors_left</span><span class="p">.</span><span class="n">voltage</span> <span class="o">-</span> <span class="n">motors_right</span><span class="p">.</span><span class="n">voltage</span><span class="p">)</span>
        <span class="n">eye_tilt_vel</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="nc">Float64</span><span class="p">(</span><span class="n">tilt</span><span class="p">))</span>
        <span class="n">eye_pan_vel</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="nc">Float64</span><span class="p">(</span><span class="n">pan</span><span class="p">))</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Stage two: Position-controlled motion to red cup
</span>        <span class="n">scaling_factor</span> <span class="o">=</span> <span class="mf">0.03</span>
        <span class="n">joint_names</span> <span class="o">=</span> <span class="n">joint_state_sub</span><span class="p">.</span><span class="n">value</span><span class="p">.</span><span class="n">name</span>
        <span class="n">joint_positions</span> <span class="o">=</span> <span class="n">joint_state_sub</span><span class="p">.</span><span class="n">value</span><span class="p">.</span><span class="n">position</span>
        <span class="n">current_tilt</span> <span class="o">=</span> <span class="n">joint_positions</span><span class="p">[</span><span class="n">joint_names</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="sh">"</span><span class="s">eye_tilt</span><span class="sh">"</span><span class="p">)]</span>
        <span class="n">current_pan</span> <span class="o">=</span> <span class="n">joint_positions</span><span class="p">[</span><span class="n">joint_names</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="sh">"</span><span class="s">left_eye_pan</span><span class="sh">"</span><span class="p">)]</span>

        <span class="n">tilt</span> <span class="o">=</span> <span class="n">current_tilt</span> <span class="o">+</span> <span class="n">scaling_factor</span> <span class="o">*</span> <span class="p">(</span><span class="n">motors_up_stage_two</span><span class="p">.</span><span class="n">voltage</span> <span class="o">-</span> <span class="n">motors_down_stage_two</span><span class="p">.</span><span class="n">voltage</span><span class="p">)</span>
        <span class="n">pan</span> <span class="o">=</span> <span class="n">current_pan</span> <span class="o">+</span> <span class="n">scaling_factor</span> <span class="o">*</span> <span class="p">(</span> <span class="n">motors_left_stage_two</span><span class="p">.</span><span class="n">voltage</span> <span class="o">-</span> <span class="n">motors_right_stage_two</span><span class="p">.</span><span class="n">voltage</span><span class="p">)</span>

        <span class="n">eye_tilt_pos</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="nc">Float64</span><span class="p">(</span><span class="n">tilt</span><span class="p">))</span>
        <span class="n">eye_pan_pos</span><span class="p">.</span><span class="nf">send_message</span><span class="p">(</span><span class="n">std_msgs</span><span class="p">.</span><span class="n">msg</span><span class="p">.</span><span class="nc">Float64</span><span class="p">(</span><span class="n">pan</span><span class="p">))</span>
</code></pre></div></div> <p><a href="https://github.com/Scaatis/hbpprak_perception/blob/master/follow_object.py" rel="external nofollow noopener" target="_blank">follow_object.py</a> is a <code class="language-plaintext highlighter-rouge">Neuron2Robot</code> transfer function for mapping brain activity to robot motion. <code class="language-plaintext highlighter-rouge">eye_tilt_pos</code>, <code class="language-plaintext highlighter-rouge">eye_pan_pos</code>, <code class="language-plaintext highlighter-rouge">eye_tilt_vel</code> and <code class="language-plaintext highlighter-rouge">eye_pan_vel</code> are handles to the ROS topics for writing to the simulated robot’s velocity and position controllers for the left eye’s tilt and pan joints. In the first stage of the experiment, when centering on the green ball, we use the difference between <code class="language-plaintext highlighter-rouge">up</code> and <code class="language-plaintext highlighter-rouge">down</code>or <code class="language-plaintext highlighter-rouge">left</code> and <code class="language-plaintext highlighter-rouge">right</code> as inputs for the <em>velocity controllers</em> of the corresponding joints. We chose to use the velocity controllers for smoother control, as writing directly to the position controllers can cause very quick and jerky motion.</p> <p>In the second stage of the experiment, when tracking a fast-moving red cup, we write directly to the position controllers, mainly for speed and for avoiding the oscillations which can occur when the input to a velocity controllers changes quickly.</p> <h3 id="results">Results</h3> <p>The resulting control scheme turned out to be surprisingly stable with hardly any oscillations. Because the voltages used for writing to the robot controllers are computed by integrating over the spike train, noise is reliably smoothed out and erratic motions are avoided. Only when the cups are shuffled extremely quickly is the controller too slow to follow. Our approach shows that spiking neural networks are well suited for the implementation of closed control loops, particularly when coupling sensor input with motion, and illustrates how architectural features of the NRP such as the closed loop engine can be leveraged to concisely implement object tracking using closed-loop control.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" style="display: flex; justify-content: center;"> <figure> <iframe src="https://www.youtube.com/embed/7fiBUZ9i9GQ?rel=0" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"></iframe> </figure> </div> </div> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>At least with the <a href="http://bulletphysics.org/wordpress/" rel="external nofollow noopener" target="_blank">Bullet</a> physics engine used in the NRP, the robot started oscillating and sometimes toppled over if the neck was moved too quickly. For quick-moving objects like the cups in our experiment, these oscillations in turn caused the input to our closed-loop controller (the image) to oscillate and made stable control very difficult. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/reproducible-robot-experiments-in-the-cloud/">Reproducible Robot Experiments in the Cloud</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/ai-based-framework-for-wire-harness-assembly/">AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/semi-autonomous-robotic-assistance-for-gallbladder-retraction/">Semi-Autonomous Robotic Assistance for Gallbladder Retraction in Surgery</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/aicor-virtual-research-building/">The AICOR Virtual Research Building Collaborative, Open Science and Education in the Cloud</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/introduction-to-ai-safety/">A Human-Friendly Introduction to AI Safety</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Benjamin Alt. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: October 31, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"Repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"dropdown-resume",title:"Resume",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-academic-cv",title:"Academic CV",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"post-reproducible-robot-experiments-in-the-cloud",title:"Reproducible Robot Experiments in the Cloud",description:"At the 1st Workshop on Embodied AI and Robotics for Future Scientific Discovery (AIR4S) at IROS 2025, my colleagues and I are presenting a robot execution tracing system and virtual lab environment to facilitate reproducible research with robots.",section:"Posts",handler:()=>{window.location.href="/blog/2025/reproducible-robot-experiments-in-the-cloud/"}},{id:"post-ai-based-framework-for-robust-model-based-connector-mating-in-robotic-wire-harness-installation",title:"AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation",description:"At CASE 2025, my colleagues and I are presenting a novel AI-based framework for robust, data-driven connector mating in robotic wire harness installation, combining visuotactile learning with model-based optimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/ai-based-framework-for-wire-harness-assembly/"}},{id:"post-semi-autonomous-robotic-assistance-for-gallbladder-retraction-in-surgery",title:"Semi-Autonomous Robotic Assistance for Gallbladder Retraction in Surgery",description:"A summary of our recent paper on a semi-autonomous robotic assistance system for minimally invasive surgery that allows surgeons to demonstrate retraction tasks while ensuring the interpretability of resulting behavior.",section:"Posts",handler:()=>{window.location.href="/blog/2025/semi-autonomous-robotic-assistance-for-gallbladder-retraction/"}},{id:"post-the-aicor-virtual-research-building-collaborative-open-science-and-education-in-the-cloud",title:"The AICOR Virtual Research Building Collaborative, Open Science and Education in the Cloud...",description:"The AICOR Virtual Research Building (VRB) is our contribution to open science, replicable experiments and open education in robotics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/aicor-virtual-research-building/"}},{id:"post-a-human-friendly-introduction-to-ai-safety",title:"A Human-Friendly Introduction to AI Safety",description:"A talk I gave at an online LessWrong Meetup in Karlsruhe, Germany, covering the basics of technical AI safety.",section:"Posts",handler:()=>{window.location.href="/blog/2023/introduction-to-ai-safety/"}},{id:"post-a-g-i-2022-a-year-in-review",title:"A(G)I 2022: A Year in Review",description:"A talk I gave at the 2023 Effective Altruism Retreat in Sigmaringen, Germany, outlining how pivotal AI technologies shifted narratives around AI and its impacts on society.",section:"Posts",handler:()=>{window.location.href="/blog/2023/agi-2022-a-year-in-review/"}},{id:"post-every-researcher-needs-a-pet-fish",title:"Every Researcher Needs a Pet Fish",description:"Why researchers should care about science communication, and how to use symbols and slogans to make a talk memorable.",section:"Posts",handler:()=>{window.location.href="/blog/2022/every-researcher-needs-a-pet-fish/"}},{id:"post-knowledge-representation-amp-reasoning-in-industrial-robotics",title:"Knowledge Representation &amp; Reasoning in Industrial Robotics",description:"A lecture I gave at the 2022 EASE Fall School in Bremen, Germany, on the combination of explicit knowledge representation, reasoning and deep learning for industrial robotics.",section:"Posts",handler:()=>{window.location.href="/blog/2022/krr-in-industrial-robotics/"}},{id:"post-icra-2021-reading-list",title:"ICRA 2021 Reading List",description:"My personal post-ICRA 2021 reading list with papers I came across while attending the conference and which I particularly want to read. It is also intended as a resource for my colleagues who did not attend ICRA this year.",section:"Posts",handler:()=>{window.location.href="/blog/2021/icra-reading-list/"}},{id:"post-robot-program-parameter-inference-via-differentiable-shadow-program-inversion",title:"Robot Program Parameter Inference via Differentiable Shadow Program Inversion",description:"A companion blog post to our 2021 ICRA paper on data-driven robot program parameter optimization.",section:"Posts",handler:()=>{window.location.href="/blog/2021/shadow-program-inversion/"}},{id:"post-object-tracking-with-spiking-neural-networks",title:"Object Tracking With Spiking Neural Networks",description:"How Spiking Neural Networks (SNNs) and the Neurorobotics Platform can be leveraged for robot control and perception.",section:"Posts",handler:()=>{window.location.href="/blog/2018/object-tracking-with-spiking-neural-networks/"}},{id:"news-2-papers-accepted-at-icra-2025",title:"2 papers accepted at ICRA 2025",description:"",section:"News",handler:()=>{window.location.href="/news/icra-2025/"}},{id:"news-semi-autonomous-robotic-assistance-for-gallbladder-retraction-in-surgery-accepted-to-ra-l",title:"Semi-Autonomous Robotic Assistance for Gallbladder Retraction in Surgery accepted to RA-L",description:"",section:"News",handler:()=>{window.location.href="/news/gallbladder-ral/"}},{id:"news-ai-based-framework-for-robust-model-based-connector-mating-in-robotic-wire-harness-installation-accepted-at-case-2025",title:"AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation accepted...",description:"",section:"News",handler:()=>{window.location.href="/news/wire-harness-case-2025/"}},{id:"news-best-poster-award-at-iros-2025-workshop-on-embodied-ai-and-robotics-for-future-scientific-discovery-air4s",title:"Best Poster Award at IROS 2025 Workshop on Embodied AI and Robotics for...",description:"",section:"News",handler:()=>{window.location.href="/news/robots-scientific-discovery/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%62%65%6E%6A%61%6D%69%6E.%61%6C%74@%75%6E%69-%62%72%65%6D%65%6E.%64%65","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0009-0002-8790-1671","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=GJy9_HAAAAAJ","_blank")}},{id:"socials-researchgate",title:"ResearchGate",section:"Socials",handler:()=>{window.open("https://www.researchgate.net/profile/https://www.researchgate.net/profile/Benjamin-Alt/","_blank")}},{id:"socials-ieee-xplore",title:"IEEE Xplore",section:"Socials",handler:()=>{window.open("https://ieeexplore.ieee.org/author/37088995866/","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/benjaminalt","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/benjamin-alt","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/289/0707","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>